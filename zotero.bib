
@inproceedings{Sarkar2004nanopass,
  location = {{New York, NY, USA}},
  title = {A Nanopass Infrastructure for Compiler Education},
  isbn = {1-58113-905-5},
  url = {http://doi.acm.org/10.1145/1016850.1016878},
  doi = {10.1145/1016850.1016878},
  abstract = {Compilers structured as a small number of monolithic passes are difficult to understand and difficult to maintain. Adding new optimizations often requires major restructuring of existing passes that cannot be understood in isolation. The steep learning curve is daunting, and even experienced developers find it hard to modify existing passes without introducing subtle and tenacious bugs. These problems are especially frustrating when the developer is a student in a compiler class.An attractive alternative is to structure a compiler as a collection of many small passes, each of which performs a single task. This "micropass" structure aligns the actual implementation of a compiler with its logical organization, simplifying development, testing, and debugging. Unfortunately, writing many small passes duplicates code for traversing and rewriting abstract syntax trees and can obscure the meaningful transformations performed by individual passes.To address these problems, we have developed a methodology and associated tools that simplify the task of building compilers composed of many fine-grained passes. We describe these compilers as "nanopass" compilers to indicate both the intended granularity of the passes and the amount of source code required to implement each pass. This paper describes the methodology and tools comprising the nanopass framework.},
  booktitle = {Proceedings of the Ninth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '04},
  publisher = {{ACM}},
  urldate = {2015-09-11},
  date = {2004},
  pages = {201--212},
  keywords = {compiler writing tools,domain-specific languages,nanopass compilers,syntactic abstraction},
  author = {Sarkar, Dipanwita and Waddell, Oscar and Dybvig, R. Kent},
  file = {/Users/pgiarrusso/Zotero/storage/W5WG9JC7/Sarkar et al - 2004 - A Nanopass Infrastructure for Compiler Education.pdf}
}

@inproceedings{Augustsson1984compiler,
  location = {{New York, NY, USA}},
  title = {A Compiler for Lazy ML},
  isbn = {0-89791-142-3},
  url = {http://doi.acm.org/10.1145/800055.802038},
  doi = {10.1145/800055.802038},
  abstract = {LML is a strongly typed, statically scoped functional Language with Lazy evaluation. It is compiled trough a number of program transformations which makes the code generation easier. Code is generated in two steps, first code for an abstract graph manipulation machine, the G-machine. From this code machine code is generated. Some benchmark tests are also presented.},
  booktitle = {Proceedings of the 1984 ACM Symposium on LISP and Functional Programming},
  series = {LFP '84},
  publisher = {{ACM}},
  urldate = {2015-09-16},
  date = {1984},
  pages = {218--227},
  author = {Augustsson, Lennart},
  file = {/Users/pgiarrusso/Zotero/storage/FZK2ISDE/Augustsson - 1984 - A Compiler for Lazy ML.pdf}
}

@incollection{Wadler1995monads,
  langid = {english},
  title = {Monads for functional programming},
  isbn = {978-3-540-59451-2 978-3-540-49270-2},
  url = {http://link.springer.com/chapter/10.1007/3-540-59451-5_2},
  number = {925},
  booktitle = {Advanced Functional Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-09-29},
  date = {1995-05-24},
  pages = {24-52},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  author = {Wadler, Philip},
  editor = {Jeuring, Johan and Meijer, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/3QZFFS9M/Wadler - 1995 - Monads for functional programming.pdf;/Users/pgiarrusso/Zotero/storage/AGGW46EN/3-540-59451-5_2.html}
}
% == BibLateX quality report for Wadler1995monads:
% 'isbn': not a valid ISBN

@inproceedings{Burstall1980hope,
  location = {{New York, NY, USA}},
  title = {HOPE: An Experimental Applicative Language},
  url = {http://doi.acm.org/10.1145/800087.802799},
  doi = {10.1145/800087.802799},
  shorttitle = {HOPE},
  abstract = {An applicative language called HOPE is described and discussed. The underlying goal of the design and implementation effort was to produce a very simple programming language which encourages the construction of clear and manipulable programs. HOPE does not include an assignment statement; this is felt to be an important simplification. The user may freely define his own data types, without the need to devise a complicated encoding in terms of low-level types. The language is very strongly typed, and as implemented it incorporates a typechecker which handles polymorphic types and overloaded operators. Functions are defined by a set of recursion equations; the left-hand side of each equation includes a pattern used to determine which equation to use for a given argument. The availability of arbitrary higher-order types allows functions to be defined which 'package' recursion. Lazily-evaluated lists are provided, allowing the use of infinite lists which could be used to provide interactive input/output and concurrency. HOPE also includes a simple modularisation facility which may be used to protect the implementation of an abstract data type.},
  booktitle = {Proceedings of the 1980 ACM Conference on LISP and Functional Programming},
  series = {LFP '80},
  publisher = {{ACM}},
  urldate = {2015-09-21},
  date = {1980},
  pages = {136--143},
  author = {Burstall, R. M. and MacQueen, D. B. and Sannella, D. T.},
  file = {/Users/pgiarrusso/Zotero/storage/AV5PQRQK/Burstall et al - 1980 - HOPE - An Experimental Applicative Language.pdf}
}

@inproceedings{Fourtounis2013supporting,
  location = {{Dagstuhl, Germany}},
  title = {Supporting Separate Compilation in a Defunctionalizing Compiler},
  volume = {29},
  isbn = {978-3-939897-52-1},
  url = {http://drops.dagstuhl.de/opus/volltexte/2013/4029},
  doi = {http://dx.doi.org/10.4230/OASIcs.SLATE.2013.39},
  booktitle = {2nd Symposium on Languages, Applications and Technologies},
  series = {OpenAccess Series in Informatics (OASIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2013},
  pages = {39-49},
  author = {Fourtounis, Georgios and Papaspyrou, Nikolaos S.},
  editor = {Leal, José Paulo and Rocha, Ricardo and Simões, Alberto},
  file = {/Users/pgiarrusso/Zotero/storage/QD62GGEW/Fourtounis_Papaspyrou - 2013 - Supporting Separate Compilation in a Defunctionalizing Compiler.pdf},
  urn = {urn:nbn:de:0030-drops-40294}
}
% == BibLateX quality report for Fourtounis2013supporting:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@incollection{Danvy2002lambdalifting,
  langid = {english},
  title = {Lambda-Lifting in Quadratic Time},
  isbn = {978-3-540-44233-2 978-3-540-45788-6},
  url = {http://link.springer.com/chapter/10.1007/3-540-45788-7_8},
  number = {2441},
  booktitle = {Functional and Logic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-09-15},
  date = {2002-09-15},
  pages = {134-151},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs},
  author = {Danvy, Olivier and Schultz, Ulrik P.},
  editor = {Hu, Zhenjiang and Rodríguez-Artalejo, Mario},
  file = {/Users/pgiarrusso/Zotero/storage/SJQRXUMB/Danvy_Schultz - 2002 - Lambda-Lifting in Quadratic Time.pdf;/Users/pgiarrusso/Zotero/storage/5J6ADAFF/10.html}
}
% == BibLateX quality report for Danvy2002lambdalifting:
% 'isbn': not a valid ISBN

@article{Jones1991modular,
  langid = {english},
  title = {A modular fully-lazy lambda lifter in Haskell},
  volume = {21},
  issn = {1097-024X},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380210505/abstract},
  doi = {10.1002/spe.4380210505},
  abstract = {An important step in many compilers for functional languages is lambda lifting. In his thesis, Hughes showed that by doing lambda lifting in a particular way, a useful property called full laziness can be preserved. Full laziness has been seen as intertwined with lambda lifting ever since. We show that, on the contrary, full laziness can be regarded as a completely separate process to lambda lifting, thus making it easy to use different lambda lifters following a full-laziness transformation, or to use the full-laziness transformation in compilers which do not require lambda lifting. On the way, we present the complete code for our modular fully-lazy lambda lifter, written in the HASKELL functional programming language.},
  number = {5},
  journaltitle = {Software: Practice and Experience},
  shortjournal = {Softw: Pract. Exper.},
  urldate = {2015-09-15},
  date = {1991-05-01},
  pages = {479-506},
  keywords = {Full laziness,functional programming,Lambda lifting,program transformation},
  author = {Jones, Simon L. Peyton and Lester, David},
  file = {/Users/pgiarrusso/Zotero/storage/XW3MD9DC/Jones_Lester - 1991 - A modular fully-lazy lambda lifter in Haskell.pdf;/Users/pgiarrusso/Zotero/storage/ACTM8FMZ/abstract.html}
}

@incollection{Johnsson1985lambda,
  langid = {english},
  title = {Lambda lifting: Transforming programs to recursive equations},
  isbn = {978-3-540-15975-9 978-3-540-39677-2},
  url = {http://link.springer.com/chapter/10.1007/3-540-15975-4_37},
  shorttitle = {Lambda lifting},
  number = {201},
  booktitle = {Functional Programming Languages and Computer Architecture},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-09-15},
  date = {1985-09-16},
  pages = {190-203},
  keywords = {Processor Architectures},
  author = {Johnsson, Thomas},
  editor = {Jouannaud, Jean-Pierre},
  file = {/Users/pgiarrusso/Zotero/storage/I9Z29UDX/Johnsson - 1985 - Lambda lifting - Transforming programs to recursive equations.pdf;/Users/pgiarrusso/Zotero/storage/JEX7KZA7/3-540-15975-4_37.html}
}
% == BibLateX quality report for Johnsson1985lambda:
% 'isbn': not a valid ISBN

@inproceedings{Bell1997typedriven,
  location = {{New York, NY, USA}},
  title = {Type-driven Defunctionalization},
  isbn = {0-89791-918-1},
  url = {http://doi.acm.org/10.1145/258948.258953},
  doi = {10.1145/258948.258953},
  abstract = {In 1972, Reynolds outlined a general method for eliminating functional arguments known as defunctionalization. The idea underlying defunctionalization is encoding a functional value as first-order data, and then realizing the applications of the encoded function via an apply function. Although this process is simple enough, problems arise when defunctionalization is used in a polymorphic language. In such a language, a functional argument of a higher-order function can take different type instances in different applications. As a consequence, its associated apply function can be untypable in the source language. In the paper we present a defunctionalization transformation which preserves typability. Moreover, the transformation imposes no restriction on functional arguments of recursive functions, and it handles functions as results as well as functions encapsulated in constructors. The key to this success is the use of type information in the defunctionalization transformation. Run-time characteristics are preserved by defunctionalization; hence, there is no performance improvement coming from the transformation itself. However closures need not be implemented to compile the transformed program.},
  booktitle = {Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '97},
  publisher = {{ACM}},
  urldate = {2015-09-15},
  date = {1997},
  pages = {25--37},
  author = {Bell, Jeffrey M. and Bellegarde, Françoise and Hook, James},
  file = {/Users/pgiarrusso/Zotero/storage/4U2HSJTI/Bell et al - 1997 - Type-driven Defunctionalization.pdf}
}

@article{Turner1979new,
  langid = {english},
  title = {A new implementation technique for applicative languages},
  volume = {9},
  issn = {1097-024X},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380090105/abstract},
  doi = {10.1002/spe.4380090105},
  abstract = {It is shown how by using results from combinatory logic an applicative language, such as LISP, can be translated into a form from which all bound variables have been removed. A machine is described which can efficiently execute the resulting code. This implementation is compared with a conventional interpreter and found to have a number of advantages. Of these the most important is that programs which exploit higher order functions to achieve great compactness of expression are executed much more efficiently.},
  number = {1},
  journaltitle = {Software: Practice and Experience},
  shortjournal = {Softw: Pract. Exper.},
  urldate = {2015-09-11},
  date = {1979-01-01},
  pages = {31-49},
  keywords = {Applicative languages,Bracket abstraction,Combinators,Lazy Evaluation,Normal graph reduction,Substitution machine},
  author = {Turner, D. A.},
  file = {/Users/pgiarrusso/Zotero/storage/9J2Q5KBT/abstract.html}
}

@inproceedings{Karachalias2015gadts,
  location = {{New York, NY, USA}},
  title = {GADTs Meet Their Match: Pattern-matching Warnings That Account for GADTs, Guards, and Laziness},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784748},
  doi = {10.1145/2784731.2784748},
  shorttitle = {GADTs Meet Their Match},
  abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {424--436},
  keywords = {Generalized Algebraic Data Types,Haskell,OutsideIn(X),pattern matching,_tablet},
  author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/5PCSQXEB/Karachalias-Schrijvers-Vytiniotis-Jones - 2015 - GADTs Meet Their Match - Pattern-matching Warnings That Account for GADTs, Guards, and Laziness.pdf}
}

@inproceedings{Pavlinovic2015practical,
  location = {{New York, NY, USA}},
  title = {Practical SMT-based Type Error Localization},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784765},
  doi = {10.1145/2784731.2784765},
  abstract = {Compilers for statically typed functional programming languages are notorious for generating confusing type error messages. When the compiler detects a type error, it typically reports the program location where the type checking failed as the source of the error. Since other error sources are not even considered, the actual root cause is often missed. A more adequate approach is to consider all possible error sources and report the most useful one subject to some usefulness criterion. In our previous work, we showed that this approach can be formulated as an optimization problem related to satisfiability modulo theories (SMT). This formulation cleanly separates the heuristic nature of usefulness criteria from the underlying search problem. Unfortunately, algorithms that search for an optimal error source cannot directly use principal types which are crucial for dealing with the exponential-time complexity of the decision problem of polymorphic type checking. In this paper, we present a new algorithm that efficiently finds an optimal error source in a given ill-typed program. Our algorithm uses an improved SMT encoding to cope with the high complexity of polymorphic typing by iteratively expanding the typing constraints from which principal types are derived. The algorithm preserves the clean separation between the heuristics and the actual search. We have implemented our algorithm for OCaml. In our experimental evaluation, we found that the algorithm reduces the running times for optimal type error localization from minutes to seconds and scales better than previous localization algorithms.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {412--423},
  keywords = {polymorphic types,Satisfiability Modulo Theories,Type Error Localization},
  author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas},
  file = {/Users/pgiarrusso/Zotero/storage/BRD5FJDR/Pavlinovic et al - 2015 - Practical SMT-based Type Error Localization.pdf}
}

@inproceedings{Zhu2015learning,
  location = {{New York, NY, USA}},
  title = {Learning Refinement Types},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784766},
  doi = {10.1145/2784731.2784766},
  abstract = {We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {400--411},
  keywords = {Higher-Order Verification,Learning,refinement types,Testing},
  author = {Zhu, He and Nori, Aditya V. and Jagannathan, Suresh},
  file = {/Users/pgiarrusso/Zotero/storage/6RUWM83U/Zhu et al - 2015 - Learning Refinement Types.pdf}
}

@inproceedings{Stucki2015rrb,
  location = {{New York, NY, USA}},
  title = {RRB Vector: A Practical General Purpose Immutable Sequence},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784739},
  doi = {10.1145/2784731.2784739},
  shorttitle = {RRB Vector},
  abstract = {State-of-the-art immutable collections have wildly differing performance characteristics across their operations, often forcing programmers to choose different collection implementations for each task. Thus, changes to the program can invalidate the choice of collections, making code evolution costly. It would be desirable to have a collection that performs well for a broad range of operations. To this end, we present the RRB-Vector, an immutable sequence collection that offers good performance across a large number of sequential and parallel operations. The underlying innovations are: (1) the Relaxed-Radix-Balanced (RRB) tree structure, which allows efficient structural reorganization, and (2) an optimization that exploits spatio-temporal locality on the RRB data structure in order to offset the cost of traversing the tree. In our benchmarks, the RRB-Vector speedup for parallel operations is lower bounded by 7x when executing on 4 CPUs of 8 cores each. The performance for discrete operations, such as appending on either end, or updating and removing elements, is consistently good and compares favorably to the most important immutable sequence collections in the literature and in use today. The memory footprint of RRB-Vector is on par with arrays and an order of magnitude less than competing collections.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {342--354},
  keywords = {arrays,Data Structures,Immutable,Radix-Balanced,Relaxed-Radix-Balanced,Sequences,Trees,Vectors},
  author = {Stucki, Nicolas and Rompf, Tiark and Ureche, Vlad and Bagwell, Phil},
  file = {/Users/pgiarrusso/Zotero/storage/3TUH86K4/Stucki et al - 2015 - RRB Vector - A Practical General Purpose Immutable Sequence.pdf}
}

@inproceedings{Jaskelioff2015functional,
  location = {{New York, NY, USA}},
  title = {Functional Pearl: A Smart View on Datatypes},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784743},
  doi = {10.1145/2784731.2784743},
  shorttitle = {Functional Pearl},
  abstract = {Left-nested list concatenations, left-nested binds on the free monad, and left-nested choices in many non-determinism monads have an algorithmically bad performance. Can we solve this problem without losing the ability to pattern-match on the computation? Surprisingly, there is a deceptively simple solution: use a smart view to pattern-match on the datatype. We introduce the notion of smart view and show how it solves the problem of slow left-nested operations. In particular, we use the technique to obtain fast and simple implementations of lists, of free monads, and of two non-determinism monads.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {355--361},
  keywords = {data structure,List,Monad,MonadPlus},
  author = {Jaskelioff, Mauro and Rivas, Exequiel},
  file = {/Users/pgiarrusso/Zotero/storage/IEJBC4GP/Jaskelioff_Rivas - 2015 - Functional Pearl - A Smart View on Datatypes.pdf}
}

@inproceedings{Smolka2015fast,
  location = {{New York, NY, USA}},
  title = {A Fast Compiler for NetKAT},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784761},
  doi = {10.1145/2784731.2784761},
  abstract = {High-level programming languages play a key role in a growing number of networking platforms, streamlining application development and enabling precise formal reasoning about network behavior. Unfortunately, current compilers only handle "local" programs that specify behavior in terms of hop-by-hop forwarding behavior, or modest extensions such as simple paths. To encode richer "global" behaviors, programmers must add extra state -- something that is tricky to get right and makes programs harder to write and maintain. Making matters worse, existing compilers can take tens of minutes to generate the forwarding state for the network, even on relatively small inputs. This forces programmers to waste time working around performance issues or even revert to using hardware-level APIs. This paper presents a new compiler for the NetKAT language that handles rich features including regular paths and virtual networks, and yet is several orders of magnitude faster than previous compilers. The compiler uses symbolic automata to calculate the extra state needed to implement "global" programs, and an intermediate representation based on binary decision diagrams to dramatically improve performance. We describe the design and implementation of three essential compiler stages: from virtual programs (which specify behavior in terms of virtual topologies) to global programs (which specify network-wide behavior in terms of physical topologies), from global programs to local programs (which specify behavior in terms of single-switch behavior), and from local programs to hardware-level forwarding tables. We present results from experiments on real-world benchmarks that quantify performance in terms of compilation time and forwarding table size.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {328--341},
  keywords = {domain-specific languages,binary decision diagrams,Frenetic,Kleene Algebra with tests,NetKAT,Software-defined networking,virtualization},
  author = {Smolka, Steffen and Eliopoulos, Spiridon and Foster, Nate and Guha, Arjun},
  file = {/Users/pgiarrusso/Zotero/storage/GPE5TIUZ/Smolka et al - 2015 - A Fast Compiler for NetKAT.pdf}
}

@inproceedings{Ploeg2015practical,
  location = {{New York, NY, USA}},
  title = {Practical Principled FRP: Forget the Past, Change the Future, FRPNow!},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784752},
  doi = {10.1145/2784731.2784752},
  shorttitle = {Practical Principled FRP},
  abstract = {We present a new interface for practical Functional Reactive Programming (FRP) that (1) is close in spirit to the original FRP ideas, (2) does not have the original space-leak problems, without using arrows or advanced types, and (3) provides a simple and expressive way for performing IO actions from FRP code. We also provide a denotational semantics for this new interface, and a technique (using Kripke logical relations) for reasoning about which FRP functions may "forget their past", i.e. which functions do not have an inherent space-leak. Finally, we show how we have implemented this interface as a Haskell library called FRPNow.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {302--314},
  keywords = {functional reactive programming,Kripke Logical Relations,Purely Functional IO,Space-leak},
  author = {van der Ploeg, Atze and Claessen, Koen},
  file = {/Users/pgiarrusso/Zotero/storage/FQIAG6IC/Ploeg_Claessen - 2015 - Practical Principled FRP - Forget the Past, Change the Future, FRPNow!.pdf}
}

@inproceedings{Buiras2015hlio,
  location = {{New York, NY, USA}},
  title = {HLIO: Mixing Static and Dynamic Typing for Information-flow Control in Haskell},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784758},
  doi = {10.1145/2784731.2784758},
  shorttitle = {HLIO},
  abstract = {Information-Flow Control (IFC) is a well-established approach for allowing untrusted code to manipulate sensitive data without disclosing it. IFC is typically enforced via type systems and static analyses or via dynamic execution monitors. The LIO Haskell library, originating in operating systems research, implements a purely dynamic monitor of the sensitivity level of a computation, particularly suitable when data sensitivity levels are only known at runtime. In this paper, we show how to give programmers the flexibility of deferring IFC checks to runtime (as in LIO), while also providing static guarantees---and the absence of runtime checks---for parts of their programs that can be statically verified (unlike LIO). We present the design and implementation of our approach, HLIO (Hybrid LIO), as an embedding in Haskell that uses a novel technique for deferring IFC checks based on singleton types and constraint polymorphism. We formalize HLIO, prove non-interference, and show how interesting IFC examples can be programmed. Although our motivation is IFC, our technique for deferring constraints goes well beyond and offers a methodology for programmer-controlled hybrid type checking in Haskell.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {289--301},
  keywords = {constraint kinds,data kinds,dynamic typing,gradual typing,hybrid typing,Information-flow control,singleton types},
  author = {Buiras, Pablo and Vytiniotis, Dimitrios and Russo, Alejandro},
  file = {/Users/pgiarrusso/Zotero/storage/ZUHDGTNT/Buiras et al - 2015 - HLIO - Mixing Static and Dynamic Typing for Information-flow Control in Haskell.pdf}
}

@inproceedings{Dunfield2015elaborating,
  location = {{New York, NY, USA}},
  title = {Elaborating Evaluation-order Polymorphism},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784744},
  doi = {10.1145/2784731.2784744},
  abstract = {We classify programming languages according to evaluation order: each language fixes one evaluation order as the default, making it transparent to program in that evaluation order, and troublesome to program in the other. This paper develops a type system that is impartial with respect to evaluation order. Evaluation order is implicit in terms, and explicit in types, with by-value and by-name versions of type connectives. A form of intersection type quantifies over evaluation orders, describing code that is agnostic over (that is, polymorphic in) evaluation order. By allowing such generic code, programs can express the by-value and by-name versions of a computation without code duplication. We also formulate a type system that only has by-value connectives, plus a type that generalizes the difference between by-value and by-name connectives: it is either a suspension (by name) or a "no-op" (by value). We show a straightforward encoding of the impartial type system into the more economical one. Then we define an elaboration from the economical language to a call-by-value semantics, and prove that elaborating a well-typed source program, where evaluation order is implicit, produces a well-typed target program where evaluation order is explicit. We also prove a simulation between evaluation of the target program and reductions (either by-value or by-name) in the source program. Finally, we prove that typing, elaboration, and evaluation are faithful to the type annotations given in the source program: if the programmer only writes by-value types, no by-name reductions can occur at run time.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {256--268},
  keywords = {evaluation order,intersection types,polymorphism},
  author = {Dunfield, Joshua},
  file = {/Users/pgiarrusso/Zotero/storage/CCDSRXBF/Dunfield - 2015 - Elaborating Evaluation-order Polymorphism.pdf}
}

@inproceedings{Scherer2015which,
  location = {{New York, NY, USA}},
  title = {Which Simple Types Have a Unique Inhabitant?},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784757},
  doi = {10.1145/2784731.2784757},
  abstract = {We study the question of whether a given type has a unique inhabitant modulo program equivalence. In the setting of simply-typed lambda-calculus with sums, equipped with the strong --equivalence, we show that uniqueness is decidable. We present a saturating focused logic that introduces irreducible cuts on positive types ``as soon as possible''. Backward search in this logic gives an effective algorithm that returns either zero, one or two distinct inhabitants for any given type. Preliminary application studies show that such a feature can be useful in strongly-typed programs, inferring the code of highly-polymorphic library functions, or ``glue code'' inside more complex terms.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {243--255},
  keywords = {canonicity,code inference,focusing,proof search,saturation,simply-typed lambda-calculus,sums,Unique inhabitants},
  author = {Scherer, Gabriel and Rémy, Didier},
  file = {/Users/pgiarrusso/Zotero/storage/MR6K7W5U/Scherer_Rémy - 2015 - Which Simple Types Have a Unique Inhabitant.pdf}
}

@inproceedings{Steuwer2015generating,
  location = {{New York, NY, USA}},
  title = {Generating Performance Portable Code Using Rewrite Rules: From High-level Functional Expressions to High-performance OpenCL Code},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784754},
  doi = {10.1145/2784731.2784754},
  shorttitle = {Generating Performance Portable Code Using Rewrite Rules},
  abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {205--217},
  keywords = {Algorithmic patterns,code generation,GPU,OpenCL,performance portability,rewrite rules},
  author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe},
  file = {/Users/pgiarrusso/Zotero/storage/F8NUXE2K/Steuwer et al - 2015 - Generating Performance Portable Code Using Rewrite Rules - From High-level Functional Expressions to High-performance OpenCL Code.pdf}
}

@inproceedings{Ziliani2015unification,
  location = {{New York, NY, USA}},
  title = {A Unification Algorithm for Coq Featuring Universe Polymorphism and Overloading},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784751},
  doi = {10.1145/2784731.2784751},
  abstract = {Unification is a core component of every proof assistant or programming language featuring dependent types. In many cases, it must deal with higher-order problems up to conversion. Since unification in such conditions is undecidable, unification algorithms may include several heuristics to solve common problems. However, when the stack of heuristics grows large, the result and complexity of the algorithm can become unpredictable. Our contributions are twofold: (1) We present a full description of a new unification algorithm for the Calculus of Inductive Constructions (the base logic of Coq), including universe polymorphism, canonical structures (the overloading mechanism baked into Coq's unification), and a small set of useful heuristics. (2) We implemented our algorithm, and tested it on several libraries, providing evidence that the selected set of heuristics suffices for large developments.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {179--191},
  keywords = {coq,interactive theorem proving,overloading,unification,universe polymorphism},
  author = {Ziliani, Beta and Sozeau, Matthieu},
  file = {/Users/pgiarrusso/Zotero/storage/VPPADBJV/Ziliani_Sozeau - 2015 - A Unification Algorithm for Coq Featuring Universe Polymorphism and Overloading.pdf}
}

@inproceedings{Neis2015pilsner,
  location = {{New York, NY, USA}},
  title = {Pilsner: A Compositionally Verified Compiler for a Higher-order Imperative Language},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784764},
  doi = {10.1145/2784731.2784764},
  shorttitle = {Pilsner},
  abstract = {Compiler verification is essential for the construction of fully verified software, but most prior work (such as CompCert) has focused on verifying whole-program compilers. To support separate compilation and to enable linking of results from different verified compilers, it is important to develop a compositional notion of compiler correctness that is modular (preserved under linking), transitive (supports multi-pass compilation), and flexible (applicable to compilers that use different intermediate languages or employ non-standard program transformations). In this paper, building on prior work of Hur et al., we develop a novel approach to compositional compiler verification based on parametric inter-language simulations (PILS). PILS are modular: they enable compiler verification in a manner that supports separate compilation. PILS are transitive: we use them to verify Pilsner, a simple (but non-trivial) multi-pass optimizing compiler (programmed in Coq) from an ML-like source language S to an assembly-like target language T, going through a CPS-based intermediate language. Pilsner is the first multi-pass compiler for a higher-order imperative language to be compositionally verified. Lastly, PILS are flexible: we use them to additionally verify (1) Zwickel, a direct non-optimizing compiler for S, and (2) a hand-coded self-modifying T module, proven correct w.r.t. an S-level specification. The output of Zwickel and the self-modifying T module can then be safely linked together with the output of Pilsner. All together, this has been a significant undertaking, involving several person-years of work and over 55,000 lines of Coq.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {166--178},
  keywords = {abstract types,Compositional compiler verification,higher-order state,parametric simulations,recursive types,transitivity},
  author = {Neis, Georg and Hur, Chung-Kil and Kaiser, Jan-Oliver and McLaughlin, Craig and Dreyer, Derek and Vafeiadis, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/CXJHSBAC/Neis et al - 2015 - Pilsner - A Compositionally Verified Compiler for a Higher-order Imperative Language.pdf}
}

@inproceedings{Sheeran2015functional,
  location = {{New York, NY, USA}},
  title = {Functional Programming and Hardware Design: Still Interesting After All These Years},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2789053},
  doi = {10.1145/2784731.2789053},
  shorttitle = {Functional Programming and Hardware Design},
  abstract = {Higher order functions provide an elegant way to express algorithms designed for implementation in hardware. By showing examples of both classic and new algorithms, I will explain why higher order functions deserve to be studied. Next, I will consider the extent to which ideas from functional programming, and associated formal verification methods, have influenced hardware design in practice. What can we learn from looking back? You might ask "Why are methods of hardware design still important to our community?". Maybe we should just give up? One reason for not giving up is that hardware design is really a form of parallel programming. And here there is still a lot to do! Inspired by Blelloch's wonderful invited talk at ICFP 2010, I still believe that functional programming has much to offer in the central question of how to program the parallel machines of today, and, more particularly, of the future. I will briefly present some of the areas where I think that we are poised to make great contributions. But maybe we need to work harder on getting our act together?},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {165--165},
  keywords = {functional programming,Hardware design,higher order functions,parallel algorithms,parallel programming},
  author = {Sheeran, Mary},
  file = {/Users/pgiarrusso/Zotero/storage/ICBJ3DBD/Sheeran - 2015 - Functional Programming and Hardware Design - Still Interesting After All These Years.pdf}
}

@inproceedings{Avanzini2015analysing,
  location = {{New York, NY, USA}},
  title = {Analysing the Complexity of Functional Programs: Higher-order Meets First-order},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784753},
  doi = {10.1145/2784731.2784753},
  shorttitle = {Analysing the Complexity of Functional Programs},
  abstract = {We show how the complexity of higher-order functional programs can be analysed automatically by applying program transformations to a defunctionalised versions of them, and feeding the result to existing tools for the complexity analysis of first-order term rewrite systems. This is done while carefully analysing complexity preservation and reflection of the employed transformations such that the complexity of the obtained term rewrite system reflects on the complexity of the initial program. Further, we describe suitable strategies for the application of the studied transformations and provide ample experimental data for assessing the viability of our method.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {152--164},
  keywords = {defunctionalisation,term rewriting,termination and resource analysis},
  author = {Avanzini, Martin and Dal Lago, Ugo and Moser, Georg},
  file = {/Users/pgiarrusso/Zotero/storage/SQCX2FBZ/Avanzini et al - 2015 - Analysing the Complexity of Functional Programs - Higher-order Meets First-order.pdf}
}

@inproceedings{Danner2015denotational,
  location = {{New York, NY, USA}},
  title = {Denotational Cost Semantics for Functional Languages with Inductive Types},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784749},
  doi = {10.1145/2784731.2784749},
  abstract = {A central method for analyzing the asymptotic complexity of a functional program is to extract and then solve a recurrence that expresses evaluation cost in terms of input size. The relevant notion of input size is often specific to a datatype, with measures including the length of a list, the maximum element in a list, and the height of a tree. In this work, we give a formal account of the extraction of cost and size recurrences from higher-order functional programs over inductive datatypes. Our approach allows a wide range of programmer-specified notions of size, and ensures that the extracted recurrences correctly predict evaluation cost. To extract a recurrence from a program, we first make costs explicit by applying a monadic translation from the source language to a complexity language, and then abstract datatype values as sizes. Size abstraction can be done semantically, working in models of the complexity language, or syntactically, by adding rules to a preorder judgement. We give several different models of the complexity language, which support different notions of size. Additionally, we prove by a logical relations argument that recurrences extracted by this process are upper bounds for evaluation cost; the proof is entirely syntactic and therefore applies to all of the models we consider.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {140--151},
  keywords = {analysis,complexity,Semi-automatic},
  author = {Danner, Norman and Licata, Daniel R. and Ramyaa, Ramyaa},
  file = {/Users/pgiarrusso/Zotero/storage/HN7V8JSB/Danner et al - 2015 - Denotational Cost Semantics for Functional Languages with Inductive Types.pdf}
}

@inproceedings{Downen2015structures,
  location = {{New York, NY, USA}},
  title = {Structures for Structural Recursion},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784762},
  doi = {10.1145/2784731.2784762},
  abstract = {Our goal is to develop co-induction from our understanding of induction, putting them on level ground as equal partners for reasoning about programs. We investigate several structures which represent well-founded forms of recursion in programs. These simple structures encapsulate reasoning by primitive and noetherian induction principles, and can be composed together to form complex recursion schemes for programs operating over a wide class of data and co-data types. At its heart, this study is guided by duality: each structure for recursion has a dual form, giving perfectly symmetric pairs of equal and opposite data and co-data types for representing recursion in programs. Duality is brought out through a framework presented in sequent style, which inherently includes control effects that are interpreted logically as classical reasoning principles. To accommodate the presence of effects, we give a calculus parameterized by a notion of strategy, which is strongly normalizing for a wide range of strategies. We also present a more traditional calculus for representing effect-free functional programs, but at the cost of losing some of the founding dualities.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {127--139},
  keywords = {Classical Logic,coinduction,Duality,Induction,recursion,sequent calculus,Strong Normalization,Structures},
  author = {Downen, Paul and Johnson-Freyd, Philip and Ariola, Zena M.},
  file = {/Users/pgiarrusso/Zotero/storage/IIT7PBDB/Downen et al - 2015 - Structures for Structural Recursion.pdf}
}

@inproceedings{Vazou2015bounded,
  location = {{New York, NY, USA}},
  title = {Bounded Refinement Types},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784745},
  doi = {10.1145/2784731.2784745},
  abstract = {We present a notion of bounded quantification for refinement types and show how it expands the expressiveness of refinement typing by using it to develop typed combinators for: (1) relational algebra and safe database access, (2) Floyd-Hoare logic within a state transformer monad equipped with combinators for branching and looping, and (3) using the above to implement a refined IO monad that tracks capabilities and resource usage. This leap in expressiveness comes via a translation to ``ghost" functions, which lets us retain the automated and decidable SMT based checking and inference that makes refinement typing effective in practice.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {48--61},
  keywords = {Haskell,refinement types,abstract interpretation},
  author = {Vazou, Niki and Bakst, Alexander and Jhala, Ranjit},
  file = {/Users/pgiarrusso/Zotero/storage/B6SRKFW7/Vazou et al - 2015 - Bounded Refinement Types.pdf}
}

@inproceedings{Rossberg20151ml,
  location = {{New York, NY, USA}},
  title = {1ML – Core and Modules United (F-ing First-class Modules)},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784738},
  doi = {10.1145/2784731.2784738},
  abstract = {ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it reduces expressiveness. For example, selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent, are syntactically cumbersome, and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values, and core and module layer are unified into one language. In this "1ML", functions, functors, and even type constructors are one and the same construct; likewise, no distinction is made between structures, records, or tuples. Or viewed the other way round, everything is just ("a mode of use of") modules. Yet, 1ML does not require dependent types, and its type structure is expressible in terms of plain System Fω, in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML, and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System Fω that allows combining term and type abstraction in a more compositional manner than the bare calculus.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {35--47},
  keywords = {abstract data types,elaboration,existential types,first-class modules,ML modules,system f,type systems},
  author = {Rossberg, Andreas},
  file = {/Users/pgiarrusso/Zotero/storage/7SHT345D/Rossberg - 2015 - 1ML – Core and Modules United (F-ing First-class Modules).pdf}
}

@inproceedings{Chlipala2015optimizing,
  location = {{New York, NY, USA}},
  title = {An Optimizing Compiler for a Purely Functional Web-application Language},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784741},
  doi = {10.1145/2784731.2784741},
  abstract = {High-level scripting languages have become tremendously popular for development of dynamic Web applications. Many programmers appreciate the productivity benefits of automatic storage management, freedom from verbose type annotations, and so on. While it is often possible to improve performance substantially by rewriting an application in C or a similar language, very few programmers bother to do so, because of the consequences for human development effort. This paper describes a compiler that makes it possible to have most of the best of both worlds, coding Web applications in a high-level language but compiling to native code with performance comparable to handwritten C code. The source language is Ur/Web, a domain-specific, purely functional, statically typed language for the Web. Through a coordinated suite of relatively straightforward program analyses and algebraic optimizations, we transform Ur/Web programs into almost-idiomatic C code, with no garbage collection, little unnecessary memory allocation for intermediate values, etc. Our compiler is in production use for commercial Web sites supporting thousands of users, and microbenchmarks demonstrate very competitive performance versus mainstream tools.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {10--21},
  keywords = {pure functional programming,Web programming languages,whole-program optimization},
  author = {Chlipala, Adam},
  file = {/Users/pgiarrusso/Zotero/storage/SM33TG8D/Chlipala - 2015 - An Optimizing Compiler for a Purely Functional Web-application Language.pdf}
}

@inproceedings{Rompf2015functional,
  location = {{New York, NY, USA}},
  title = {Functional Pearl: A SQL to C Compiler in 500 Lines of Code},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784760},
  doi = {10.1145/2784731.2784760},
  shorttitle = {Functional Pearl},
  abstract = {We present the design and implementation of a SQL query processor that outperforms existing database systems and is written in just about 500 lines of Scala code -- a convincing case study that high-level functional programming can handily beat C for systems-level programming where the last drop of performance matters. The key enabler is a shift in perspective towards generative programming. The core of the query engine is an interpreter for relational algebra operations, written in Scala. Using the open-source LMS Framework (Lightweight Modular Staging), we turn this interpreter into a query compiler with very low effort. To do so, we capitalize on an old and widely known result from partial evaluation known as Futamura projections, which state that a program that can specialize an interpreter to any given input program is equivalent to a compiler. In this pearl, we discuss LMS programming patterns such as mixed-stage data structures (e.g. data records with static schema and dynamic field components) and techniques to generate low-level C code, including specialized data structures and data loading primitives.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {2--9},
  keywords = {Futamura projections,generative programming,Query Compilation,sql,staging},
  author = {Rompf, Tiark and Amin, Nada},
  file = {/Users/pgiarrusso/Zotero/storage/3CNA5SET/Rompf_Amin - 2015 - Functional Pearl - A SQL to C Compiler in 500 Lines of Code.pdf}
}

@inproceedings{Scibior2015practical,
  location = {{New York, NY, USA}},
  title = {Practical Probabilistic Programming with Monads},
  isbn = {978-1-4503-3808-0},
  url = {http://doi.acm.org/10.1145/2804302.2804317},
  doi = {10.1145/2804302.2804317},
  abstract = {The machine learning community has recently shown a lot of interest in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probability distributions, but their use is often limited to very simple inference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a probability distribution and apply Sequential Monte Carlo-based methods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementation that achieves performance comparable with Anglican, a state-of-the-art probabilistic programming system.},
  booktitle = {Proceedings of the 8th ACM SIGPLAN Symposium on Haskell},
  series = {Haskell 2015},
  publisher = {{ACM}},
  urldate = {2015-09-08},
  date = {2015},
  pages = {165--176},
  keywords = {Haskell,Bayesian statistics,Monads,Monte Carlo,probabilistic programming},
  author = {Ścibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.},
  file = {/Users/pgiarrusso/Zotero/storage/4F2Q3H7V/Ścibior et al - 2015 - Practical Probabilistic Programming with Monads.pdf}
}

@inproceedings{Minamide1996typed,
  location = {{New York, NY, USA}},
  title = {Typed Closure Conversion},
  isbn = {0-89791-769-3},
  url = {http://doi.acm.org/10.1145/237721.237791},
  doi = {10.1145/237721.237791},
  abstract = {Closure conversion is a program transformation used by compilers to separate code from data. Previous accounts of closure conversion use only untyped target languages. Recent studies show that translating to typed target languages is a useful methodology for building compilers, because a compiler can use the types to implement efficient data representations, calling conventions, and tag-free garbage collection. Furthermore, type-based translations facilitate security and debugging through automatic type checking, as well as correctness arguments through the method of logical relations.We present closure conversion as a type-directed, and type-preserving translation for both the simply-typed and the polymorphic ¿-calculus. Our translations are based on a simple "closures as objects" principle: higher-order functions are viewed as objects consisting of a single method (the code) and a single instance variable (the environment). In the simply-typed case, the Pierce-Turner model of object typing where objects are packages of existential type suffices. In the polymorphic case, more careful tracking of type sharing is required. We exploit a variant of the Harper-Lillibridge "translucent type" formalism to characterize the types of polymorphic closures.},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '96},
  publisher = {{ACM}},
  urldate = {2015-09-04},
  date = {1996},
  pages = {271--283},
  author = {Minamide, Yasuhiko and Morrisett, Greg and Harper, Robert}
}

@inproceedings{Frisby2012pattern,
  location = {{New York, NY, USA}},
  title = {A Pattern for Almost Homomorphic Functions},
  isbn = {978-1-4503-1576-0},
  url = {http://doi.acm.org/10.1145/2364394.2364396},
  doi = {10.1145/2364394.2364396},
  abstract = {Modern type systems present the programmer with a trade-off between correctness and code complexity--more precise, or exact, types that allow only legal values prevent runtime errors while less precise types enable more reuse. Unfortunately, the software engineering benefits of reuse and avoiding duplicate code currently outweigh assurance gains of exact types. We factor out a pattern common in conversions that result from using exact types as a reusable function, extending existing generic programming techniques to avoid code duplication and enable reuse.},
  booktitle = {Proceedings of the 8th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '12},
  publisher = {{ACM}},
  urldate = {2014-06-14},
  date = {2012},
  pages = {1--12},
  keywords = {data types,generic programming,type invariants,type-level programming},
  author = {Frisby, Nicolas and Gill, Andy and Alexander, Perry},
  file = {/Users/pgiarrusso/Zotero/storage/IH2V7PA9/Frisby et al - 2012 - A Pattern for Almost Homomorphic Functions.pdf}
}

@inproceedings{Schrijvers2008type,
  location = {{New York, NY, USA}},
  title = {Type Checking with Open Type Functions},
  isbn = {978-1-59593-919-7},
  url = {http://doi.acm.org/10.1145/1411204.1411215},
  doi = {10.1145/1411204.1411215},
  abstract = {We report on an extension of Haskell with open type-level functions and equality constraints that unifies earlier work on GADTs, functional dependencies, and associated types. The contribution of the paper is that we identify and characterise the key technical challenge of entailment checking; and we give a novel, decidable, sound, and complete algorithm to solve it, together with some practically-important variants. Our system is implemented in GHC, and is already in active use.},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '08},
  publisher = {{ACM}},
  urldate = {2015-09-06},
  date = {2008},
  pages = {51--62},
  keywords = {Haskell,type checking,type families,type functions},
  author = {Schrijvers, Tom and Peyton Jones, Simon and Chakravarty, Manuel and Sulzmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/7V99FSGD/Schrijvers et al - 2008 - Type Checking with Open Type Functions.pdf}
}

@inproceedings{Appel1989continuationpassing,
  location = {{New York, NY, USA}},
  title = {Continuation-passing, Closure-passing Style},
  isbn = {0-89791-294-2},
  url = {http://doi.acm.org/10.1145/75277.75303},
  doi = {10.1145/75277.75303},
  abstract = {We implemented a continuation-passing style (CPS) code generator for ML. Our CPS language is represented as an ML datatype in which all functions are named and most kinds of ill-formed expressions are impossible. We separate the code generation into phases that rewrite this representation into ever-simpler forms. Closures are represented explicitly as records, so that closure strategies can be communicated from one phase to another. No stack is used. Our benchmark data shows that the new method is an improvement over our previous, abstract-machine based code generator.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '89},
  publisher = {{ACM}},
  urldate = {2015-09-04},
  date = {1989},
  pages = {293--302},
  author = {Appel, A. W. and Jim, T.},
  file = {/Users/pgiarrusso/Zotero/storage/22SJ3Q8K/Appel_Jim - 1989 - Continuation-passing, Closure-passing Style.pdf}
}

@article{Morrisett1998typed,
  title = {Typed Closure Conversion for Recursively-Defined Functions: Extended Abstract},
  volume = {10},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105807029},
  doi = {10.1016/S1571-0661(05)80702-9},
  shorttitle = {Typed Closure Conversion for Recursively-Defined Functions},
  abstract = {Much recent work on the compilation of statically typed languages such as ML relies on the propagation of type information from source to object code in order to increase the reliability and maintainabilty of the compiler itself and to improve the efficiency and verifiability of generated code. To achieve this the program transformations performed by a compiler must be cast as type-preserving translations between typed intermediate languages. In earlier work with Minamide we studied one important compiler transformation, closure conversion, for the case of pure simply-typed and polymorphic λ-calculus. Here we extend the treatment of simply-typed closure conversion to account for recursively-defined functions such as are found in ML. We consider three main approaches, one based on a recursive code construct, one based on a self-referential data structure, and one based on recursive types. We discuss their relative advantages and disadvantages, and sketch correctness proofs for these transformations based on the method of logical relations.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {HOOTS II, Second Workshop on Higher-Order Operational Techniques in Semantics},
  urldate = {2015-09-04},
  date = {1998},
  pages = {230-241},
  author = {Morrisett, Greg and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/RZHJF4CT/S1571066105807029.html}
}

@inproceedings{Kuhn2015choosy,
  location = {{New York, NY, USA}},
  title = {Choosy and Picky: Configuration of Language Product Lines},
  isbn = {978-1-4503-3613-0},
  url = {http://doi.acm.org/10.1145/2791060.2791092},
  doi = {10.1145/2791060.2791092},
  shorttitle = {Choosy and Picky},
  abstract = {Although most programming languages naturally share several language features, they are typically implemented as a monolithic product. Language features cannot be plugged and unplugged from a language and reused in another language. Some modular approaches to language construction do exist but composing language features requires a deep understanding of its implementation hampering their use. The choose and pick approach from software product lines provides an easy way to compose a language out of a set of language features. However, current approaches to language product lines are not sufficient enough to cope with the complexity and evolution of real world programming languages. In this work, we propose a general light-weight bottom-up approach to automatically extract a feature model from a set of tagged language components. We applied this approach to the Neverlang language development framework and developed the AiDE tool to guide language developers towards a valid language composition. The approach has been evaluated on a decomposed version of Javascript to highlight the benefits of such a language product line.},
  booktitle = {Proceedings of the 19th International Conference on Software Product Line},
  series = {SPLC '15},
  publisher = {{ACM}},
  urldate = {2015-08-31},
  date = {2015},
  pages = {71--80},
  keywords = {language composition,language product lines},
  author = {Kühn, Thomas and Cazzola, Walter and Olivares, Diego Mathias}
}

@inproceedings{Gill2015remote,
  location = {{New York, NY, USA}},
  title = {The Remote Monad Design Pattern},
  isbn = {978-1-4503-3808-0},
  url = {http://doi.acm.org/10.1145/2804302.2804311},
  doi = {10.1145/2804302.2804311},
  abstract = {Remote Procedure Calls are expensive. This paper demonstrates how to reduce the cost of calling remote procedures from Haskell by using the remote monad design pattern, which amortizes the cost of remote calls. This gives the Haskell community access to remote capabilities that are not directly supported, at a surprisingly inexpensive cost. We explore the remote monad design pattern through six models of remote execution patterns, using a simulated Internet of Things toaster as a running example. We consider the expressiveness and optimizations enabled by each remote execution model, and assess the feasibility of our approach. We then present a full-scale case study: a Haskell library that provides a Foreign Function Interface to the JavaScript Canvas API. Finally, we discuss existing instances of the remote monad design pattern found in Haskell libraries.},
  booktitle = {Proceedings of the 8th ACM SIGPLAN Symposium on Haskell},
  series = {Haskell 2015},
  publisher = {{ACM}},
  urldate = {2015-08-26},
  date = {2015},
  pages = {59--70},
  keywords = {Monads,Design Pattern,FFI,Remote Procedure Call},
  author = {Gill, Andy and Sculthorpe, Neil and Dawson, Justin and Eskilson, Aleksander and Farmer, Andrew and Grebe, Mark and Rosenbluth, Jeffrey and Scott, Ryan and Stanton, James},
  file = {/Users/pgiarrusso/Zotero/storage/Z473DBUG/Gill et al - 2015 - The Remote Monad Design Pattern.pdf}
}

@inproceedings{McDirmid2011coding,
  location = {{New York, NY, USA}},
  title = {Coding at the Speed of Touch},
  isbn = {978-1-4503-0941-7},
  url = {http://doi.acm.org/10.1145/2048237.2048246},
  doi = {10.1145/2048237.2048246},
  abstract = {Although programming is one of the most creative things that one can do with a computer, there is currently no way to make programs on an increasingly popular class of tablet computers. Tablets appear unable to support capable (proficient) programming experiences because of their small form factor and touch-centric input method. This paper demonstrates how co-design of a programming language, YinYang, and its environment can overcome these challenges to enable do-it-yourself game creation on tablets. YinYang's programming model is based on tile and behavior constructs that simplify program structure for effective display and input on tablets, and also supports the definition and safe reuse of new abstractions to be competitive with capable programming languages. This paper details YinYang's design and evaluates our initial experience through a prototype that runs on current tablet hardware.},
  booktitle = {Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  series = {Onward! 2011},
  publisher = {{ACM}},
  urldate = {2015-08-09},
  date = {2011},
  pages = {61--76},
  keywords = {behavior,tiles,touch},
  author = {McDirmid, Sean}
}

@article{Halpern2015why,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05282},
  primaryClass = {cs},
  title = {Why Bother With Syntax?},
  url = {http://arxiv.org/abs/1506.05282},
  abstract = {This short note discusses the role of syntax vs. semantics and the interplay between logic, philosophy, and language in computer science and game theory.},
  urldate = {2015-08-05},
  date = {2015-06-17},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  author = {Halpern, Joseph Y.},
  file = {/Users/pgiarrusso/Zotero/storage/2W4KAVHA/Halpern - 2015 - Why Bother With Syntax.pdf;/Users/pgiarrusso/Zotero/storage/UT6UA8NH/1506.html}
}
% == BibLateX quality report for Halpern2015why:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Acar2011selective,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.0447},
  primaryClass = {cs},
  title = {Selective Memoization},
  url = {http://arxiv.org/abs/1106.0447},
  abstract = {This paper presents language techniques for applying memoization selectively. The techniques provide programmer control over equality, space usage, and identification of precise dependences so that memoization can be applied according to the needs of an application. Two key properties of the approach are that it accepts and efficient implementation and yields programs whose performance can be analyzed using standard analysis techniques. We describe our approach in the context of a functional language called MFL and an implementation as a Standard ML library. The MFL language employs a modal type system to enable the programmer to express programs that reveal their true data dependences when executed. We prove that the MFL language is sound by showing that that MFL programs yield the same result as they would with respect to a standard, non-memoizing semantics. The SML implementation cannot support the modal type system of MFL statically but instead employs run-time checks to ensure correct usage of primitives.},
  urldate = {2015-08-05},
  date = {2011-06-02},
  keywords = {Computer Science - Programming Languages},
  author = {Acar, Umut A. and Blelloch, Guy E. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/I7EWEU89/1106.html}
}
% == BibLateX quality report for Acar2011selective:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Nguyen2015higherorder,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.04817},
  primaryClass = {cs},
  title = {Higher-order symbolic execution for contract verification and refutation},
  url = {http://arxiv.org/abs/1507.04817},
  abstract = {We present a new approach to automated reasoning about higher-order programs by endowing symbolic execution with a notion of higher-order, symbolic values. Our approach is sound and relatively complete with respect to a first-order solver for base type values. Therefore, it can form the basis of automated verification and bug-finding tools for higher-order programs. To validate our approach, we use it to develop and evaluate a system for verifying and refuting behavioral software contracts of components in a functional language, which we call soft contract verification. In doing so, we discover a mutually beneficial relation between behavioral contracts and higher-order symbolic execution. Our system uses higher-order symbolic execution, leveraging contracts as a source of symbolic values including unknown behavioral values, and employs an updatable heap of contract invariants to reason about flow-sensitive facts. Whenever a contract is refuted, it reports a concrete counterexample reproducing the error, which may involve solving for an unknown function. The approach is able to analyze first-class contracts, recursive data structures, unknown functions, and control-flow-sensitive refinements of values, which are all idiomatic in dynamic languages. It makes effective use of an off-the-shelf solver to decide problems without heavy encodings. The approach is competitive with a wide range of existing tools---including type systems, flow analyzers, and model checkers---on their own benchmarks. We have built a tool which analyzes programs written in Racket, and report on its effectiveness in verifying and refuting contracts.},
  urldate = {2015-08-05},
  date = {2015-07-16},
  keywords = {Computer Science - Programming Languages},
  author = {Nguyen, Phuc C. and Tobin-Hochstadt, Sam and Van Horn, David},
  file = {/Users/pgiarrusso/Zotero/storage/7ZURI35Q/1507.html}
}
% == BibLateX quality report for Nguyen2015higherorder:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Johnson1995memoization,
  title = {Memoization in Top-down Parsing},
  volume = {21},
  issn = {0891-2017},
  url = {http://dl.acm.org/citation.cfm?id=216261.216269},
  number = {3},
  journaltitle = {Comput. Linguist.},
  urldate = {2015-07-24},
  date = {1995-09},
  pages = {405--417},
  author = {Johnson, Mark}
}
% == BibLateX quality report for Johnson1995memoization:
% ? Possibly abbreviated journal title Comput. Linguist.

@inproceedings{Delaware2013metatheory,
  location = {{New York, NY, USA}},
  title = {Meta-theory à La Carte},
  isbn = {978-1-4503-1832-7},
  url = {http://doi.acm.org/10.1145/2429069.2429094},
  doi = {10.1145/2429069.2429094},
  abstract = {Formalizing meta-theory, or proofs about programming languages, in a proof assistant has many well-known benefits. Unfortunately, the considerable effort involved in mechanizing proofs has prevented it from becoming standard practice. This cost can be amortized by reusing as much of existing mechanized formalizations as possible when building a new language or extending an existing one. One important challenge in achieving reuse is that the inductive definitions and proofs used in these formalizations are closed to extension. This forces language designers to cut and paste existing definitions and proofs in an ad-hoc manner and to expend considerable effort to patch up the results. The key contribution of this paper is the development of an induction technique for extensible Church encodings using a novel reinterpretation of the universal property of folds. These encodings provide the foundation for a framework, formalized in Coq, which uses type classes to automate the composition of proofs from modular components. This framework enables a more structured approach to the reuse of meta-theory formalizations through the composition of modular inductive definitions and proofs. Several interesting language features, including binders and general recursion, illustrate the capabilities of our framework. We reuse these features to build fully mechanized definitions and proofs for a number of languages, including a version of mini-ML. Bounded induction enables proofs of properties for non-inductive semantic functions, and mediating type classes enable proof adaptation for more feature-rich languages.},
  booktitle = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '13},
  publisher = {{ACM}},
  urldate = {2015-03-03},
  date = {2013},
  pages = {207--218},
  keywords = {coq,extensible church encodings,modular mechanized meta-theory},
  author = {Delaware, Benjamin and d. S. Oliveira, Bruno C. and Schrijvers, Tom},
  options = {useprefix=true}
}

@article{Hu2015how,
  langid = {english},
  title = {How Functional Programming Mattered},
  issn = {2095-5138, 2053-714X},
  url = {http://nsr.oxfordjournals.org/content/early/2015/07/13/nsr.nwv042},
  doi = {10.1093/nsr/nwv042},
  abstract = {In 1989 when functional programming was still considered a niche topic, Hughes wrote a visionary paper arguing convincingly “why functional programming matters”. More than two decades have passed. Has functional programming really mattered? Our answer is a resounding “Yes!”. Functional programming is now at the forefront of a new generation of programming technologies, and enjoying increasing popularity and influence. In this paper, we review the impact of functional programming, focusing on how it has changed the way we may construct programs, the way we may verify programs, and fundamentally the way we may think about programs.},
  journaltitle = {National Science Review},
  shortjournal = {Natl Sci Rev},
  urldate = {2015-07-18},
  date = {2015-07-13},
  pages = {nwv042},
  keywords = {functional programming,declarative programming,equational reasoning,functional languages},
  author = {Hu, Zhenjiang and Hughes, John and Wang, Meng},
  file = {/Users/pgiarrusso/Zotero/storage/ZAZRTSKX/nsr.nwv042.html}
}
% == BibLateX quality report for Hu2015how:
% 'issn': not a valid ISSN

@article{Pfenning2001judgmental,
  title = {A judgmental reconstruction of modal logic},
  volume = {11},
  issn = {1469-8072},
  url = {http://journals.cambridge.org/article_S0960129501003322},
  doi = {10.1017/S0960129501003322},
  abstract = {We reconsider the foundations of modal logic, following Martin-Löf's methodology of distinguishing judgments from propositions. We give constructive meaning explanations for necessity and possibility, which yields a simple and uniform system of natural deduction for intuitionistic modal logic that does not exhibit anomalies found in other proposals. We also give a new presentation of lax logic and find that the lax modality is already expressible using possibility and necessity. Through a computational interpretation of proofs in modal logic we further obtain a new formulation of Moggi's monadic metalanguage.},
  number = {04},
  journaltitle = {Mathematical Structures in Computer Science},
  urldate = {2015-07-15},
  date = {2001-08},
  pages = {511--540},
  author = {Pfenning, Frank and Davies, Rowan},
  file = {/Users/pgiarrusso/Zotero/storage/2NWNTR6M/Pfenning_Davies - 2001 - A judgmental reconstruction of modal logic.pdf;/Users/pgiarrusso/Zotero/storage/RMQ4XW8F/Pfenning-Davies - 2001 - A judgmental reconstruction of modal logic.pdf;/Users/pgiarrusso/Zotero/storage/6VF6A82V/975027BB7F07B59619913EAD4CEE52F4.html}
}

@inproceedings{Warth2008packrat,
  location = {{New York, NY, USA}},
  title = {Packrat Parsers Can Support Left Recursion},
  isbn = {978-1-59593-977-7},
  url = {http://doi.acm.org/10.1145/1328408.1328424},
  doi = {10.1145/1328408.1328424},
  abstract = {Packrat parsing offers several advantages over other parsing techniques, such as the guarantee of linear parse times while supporting backtracking and unlimited look-ahead. Unfortunately, the limited support for left recursion in packrat parser implementations makes them difficult to use for a large class of grammars (Java's, for example). This paper presents a modification to the memoization mechanism used by packrat parser implementations that makes it possible for them to support (even indirectly or mutually) left-recursive rules. While it is possible for a packrat parser with our modification to yield super-linear parse times for some left-recursive grammars, our experimentsshow that this is not the case for typical uses of left recursion.},
  booktitle = {Proceedings of the 2008 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation},
  series = {PEPM '08},
  publisher = {{ACM}},
  urldate = {2015-07-14},
  date = {2008},
  pages = {103--110},
  keywords = {left recursion,packrat parsing},
  author = {Warth, Alessandro and Douglass, James R. and Millstein, Todd}
}

@inproceedings{Foster2005combinators,
  location = {{New York, NY, USA}},
  title = {Combinators for Bi-directional Tree Transformations: A Linguistic Approach to the View Update Problem},
  isbn = {1-58113-830-X},
  url = {http://doi.acm.org/10.1145/1040305.1040325},
  doi = {10.1145/1040305.1040325},
  shorttitle = {Combinators for Bi-directional Tree Transformations},
  abstract = {We propose a novel approach to the well-known view update problem for the case of tree-structured data: a domain-specific programming language in which all expressions denote bi-directional transformations on trees. In one direction, these transformations---dubbed lenses---map a "concrete" tree into a simplified "abstract view"; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses.We identify a natural space of well-behaved bi-directional transformations over arbitrary structures, study definedness and continuity in this setting, and state a precise connection with the classical theory of "update translation under a constant complement" from databases. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, copying, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bi-directional list-processing transformations as derived forms.},
  booktitle = {Proceedings of the 32Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '05},
  publisher = {{ACM}},
  urldate = {2015-07-13},
  date = {2005},
  pages = {233--246},
  keywords = {bi-directional programming,harmony,lenses,view update problem,XML},
  author = {Foster, J. Nathan and Greenwald, Michael B. and Moore, Jonathan T. and Pierce, Benjamin C. and Schmitt, Alan}
}

@article{Barendregt2009applications,
  title = {Applications of infinitary lambda calculus},
  volume = {207},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S089054010900025X},
  doi = {10.1016/j.ic.2008.09.003},
  abstract = {We present an introduction to infinitary lambda calculus, highlighting its main properties. Subsequently we give three applications of infinitary lambda calculus. The first addresses the non-definability of Surjective Pairing, which was shown by the first author not to be definable in lambda calculus. We show how this result follows easily as an application of Berry’s Sequentiality Theorem, which itself can be proved in the setting of infinitary lambda calculus. The second pertains to the notion of relative recursiveness of number-theoretic functions. The third application concerns an explanation of counterexamples to confluence of lambda calculus extended with non-left-linear reduction rules: Adding non-left-linear reduction rules such as δ xx → x or the reduction rules for Surjective Pairing to the lambda calculus yields non-confluence, as proved by the second author. We discuss how an extension to the infinitary lambda calculus, where Böhm trees can be directly manipulated as infinite terms, yields a more simple and intuitive explanation of the correctness of these Church-Rosser counterexamples.},
  number = {5},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  series = {From Type Theory to Morphological Complexity: Special Issue dedicated to the 60th Birthday Anniversary of Giuseppe Longo},
  urldate = {2015-07-09},
  date = {2009-05},
  pages = {559-582},
  author = {Barendregt, Henk and Klop, Jan Willem}
}

@article{Kennaway1997infinitary,
  title = {Infinitary lambda calculus},
  volume = {175},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397596001715},
  doi = {10.1016/S0304-3975(96)00171-5},
  abstract = {In a previous paper we have established the theory of transfinite reduction for orthogonal term rewriting systems. In this paper we perform the same task for the lambda calculus.

From the viewpoint of infinitary rewriting, the Böhm model of the lambda calculus can be seen as an infinitary term model. In contrast to term rewriting, there are several different possible notions of infinite term, which give rise to different Böhm-like models, which embody different notions of lazy or eager computation.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2015-07-09},
  date = {1997-03-30},
  pages = {93-125},
  author = {Kennaway, J. R. and Klop, J. W. and Sleep, M. R. and de Vries, F. J.},
  options = {useprefix=true}
}

@book{Ursini1996logic,
  title = {Logic and Algebra},
  isbn = {978-0-8247-9606-8},
  url = {https://books.google.de/books?id=T2ekTUiuqxMC},
  series = {Lecture Notes in Pure and Applied Mathematics},
  publisher = {{Taylor \& Francis}},
  date = {1996},
  author = {Ursini, A. and Agliano, P.}
}

@incollection{Urzyczyn1995positive,
  langid = {english},
  title = {Positive recursive type assignment},
  isbn = {978-3-540-60246-0 978-3-540-44768-9},
  url = {http://link.springer.com/chapter/10.1007/3-540-60246-1_144},
  abstract = {We consider several different definitions of type assignment with positive recursive types, from the point of view of their typing ability. We discuss the relationships between these systems. In particular, we show that the class of typable pure lambda terms remains the same for different type disciplines involving positive type fixpoints, and that type reconstruction is decidable.},
  number = {969},
  booktitle = {Mathematical Foundations of Computer Science 1995},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-07-09},
  date = {1995},
  pages = {382-391},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Urzyczyn, Pawel},
  editor = {Wiedermann, Jiří and Hájek, Petr}
}
% == BibLateX quality report for Urzyczyn1995positive:
% 'isbn': not a valid ISBN

@inproceedings{Abadi1996syntactic,
  location = {{Washington, DC, USA}},
  title = {Syntactic Considerations on Recursive Types},
  isbn = {0-8186-7463-6},
  url = {http://dl.acm.org/citation.cfm?id=788018.788829},
  abstract = {We study recursive types from a syntactic perspective. In particular, we compare the formulations of recursive types that are used in programming languages and formal systems. Our main tool is a new syntactic explanation of type expressions as functors. We also introduce a simple logic for programs with recursive types in which we carry out our proofs.},
  booktitle = {Proceedings of the 11th Annual IEEE Symposium on Logic in Computer Science},
  series = {LICS '96},
  publisher = {{IEEE Computer Society}},
  urldate = {2015-07-09},
  date = {1996},
  pages = {242--252},
  author = {Abadi, Martin and Fiore, Marcelo P.}
}

@article{MacQueen1986ideal,
  title = {An ideal model for recursive polymorphic types},
  volume = {71},
  issn = {0019-9958},
  url = {http://www.sciencedirect.com/science/article/pii/S0019995886800195},
  doi = {10.1016/S0019-9958(86)80019-5},
  number = {1–2},
  journaltitle = {Information and Control},
  shortjournal = {Information and Control},
  urldate = {2015-07-09},
  date = {1986-10},
  pages = {95-130},
  author = {MacQueen, David and Plotkin, Gordon and Sethi, Ravi}
}

@inproceedings{MacQueen1984ideal,
  location = {{New York, NY, USA}},
  title = {An Ideal Model for Recursive Polymorphic Types},
  isbn = {0-89791-125-3},
  url = {http://doi.acm.org/10.1145/800017.800528},
  doi = {10.1145/800017.800528},
  booktitle = {Proceedings of the 11th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '84},
  publisher = {{ACM}},
  urldate = {2015-07-09},
  date = {1984},
  pages = {165--174},
  author = {MacQueen, David and Plotkin, Gordon and Sethi, Ravi}
}

@article{Hinze2002polytypic,
  title = {Polytypic values possess polykinded types},
  volume = {43},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642302000254},
  doi = {10.1016/S0167-6423(02)00025-4},
  abstract = {A polytypic value is one that is defined by induction on the structure of types. In Haskell types are assigned so-called kinds that distinguish between manifest types like the type of integers and functions on types like the list type constructor. Previous approaches to polytypic programming were restricted in that they only allowed to parameterize values by types of one fixed kind. In this paper, we show how to define values that are indexed by types of arbitrary kinds. It turns out that these polytypic values possess types that are indexed by kinds. We present several examples that demonstrate that the additional flexibility is useful in practice. One paradigmatic example is the mapping function, which describes the functorial action on arrows. A single polytypic definition yields mapping functions for data types of arbitrary kinds including first- and higher-order functors.

Haskell's type system essentially corresponds to the simply typed lambda calculus with kinds playing the role of types. We show that the specialization of a polytypic value to concrete instances of data types can be phrased as an interpretation of the simply typed lambda calculus. This allows us to employ logical relations to prove properties of polytypic values. Among other things, we show that the polytypic mapping function satisfies suitable generalizations of the functorial laws.},
  number = {2–3},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  series = {Mathematics of Program Construction (MPC 2000)},
  urldate = {2015-07-09},
  date = {2002-05},
  pages = {129-159},
  author = {Hinze, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/C29TTIGD/S0167642302000254.html}
}

@inproceedings{Crary1999what,
  location = {{New York, NY, USA}},
  title = {What is a Recursive Module?},
  isbn = {1-58113-094-5},
  url = {http://doi.acm.org/10.1145/301618.301641},
  doi = {10.1145/301618.301641},
  abstract = {A hierarchical module system is an effective tool for structuring large programs. Strictly hierarchical module systems impose an acyclic ordering on import dependencies among program units. This can impede modular programming by forcing mutually-dependent components to be consolidated into a single module. Recently there have been several proposals for module systems that admit cyclic dependencies, but it is not clear how these proposals relate to one another, nor how one might integrate them into an expressive module system such as that of ML.To address this question we provide a type-theoretic analysis of the notion of a recursive module in the context of a "phase-distinction" formalism for higher-order module systems. We extend this calculus with a recursive module mechanism and a new form of signature, called a recursively dependent signature, to support the definition of recursive modules. These extensions are justified by an interpretation in terms of more primitive language constructs. This interpretation may also serve as a guide for implementation.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  series = {PLDI '99},
  publisher = {{ACM}},
  urldate = {2015-07-09},
  date = {1999},
  pages = {50--63},
  author = {Crary, Karl and Harper, Robert and Puri, Sidd},
  file = {/Users/pgiarrusso/Zotero/storage/TGQDRA74/Crary-Harper-Puri - 1999 - What is a Recursive Module.pdf}
}

@incollection{Bird1998nested,
  langid = {english},
  title = {Nested datatypes},
  isbn = {978-3-540-64591-7 978-3-540-69345-1},
  url = {http://link.springer.com/chapter/10.1007/BFb0054285},
  abstract = {A nested datatype, also known as a non-regular datatype, is a parametrised datatype whose declaration involves different instances of the accompanying type parameters. Nested datatypes have been mostly ignored in functional programming until recently, but they are turning out to be both theoretically important and useful in practice. The aim of this paper is to suggest a functorial semantics for such datatypes, with an associated calculational theory that mirrors and extends the standard theory for regular datatypes. Though elegant and generic, the proposed approach appears more limited than one would like, and some of the limitations are discussed.},
  number = {1422},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-07-09},
  date = {1998},
  pages = {52-67},
  keywords = {Programming Techniques,Software Engineering,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Mathematics of Computing},
  author = {Bird, Richard and Meertens, Lambert},
  editor = {Jeuring, Johan},
  file = {/Users/pgiarrusso/Zotero/storage/UZASKZWV/BFb0054285.html}
}
% == BibLateX quality report for Bird1998nested:
% 'isbn': not a valid ISBN

@inproceedings{deVries2014true,
  location = {{New York, NY, USA}},
  title = {True Sums of Products},
  isbn = {978-1-4503-3042-8},
  url = {http://doi.acm.org/10.1145/2633628.2633634},
  doi = {10.1145/2633628.2633634},
  abstract = {We introduce the sum-of-products (SOP) view for datatype-generic programming (in Haskell). While many of the libraries that are commonly in use today represent datatypes as arbitrary combinations of binary sums and products, SOP reflects the structure of datatypes more faithfully: each datatype is a single n-ary sum, where each component of the sum is a single n-ary product. This representation turns out to be expressible accurately in GHC with today's extensions. The resulting list-like structure of datatypes allows for the definition of powerful high-level traversal combinators, which in turn encourage the definition of generic functions in a compositional and concise style. A major plus of the SOP view is that it allows to separate function-specific metadata from the main structural representation and recombining this information later.},
  booktitle = {Proceedings of the 10th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '14},
  publisher = {{ACM}},
  urldate = {2015-07-08},
  date = {2014},
  pages = {83--94},
  keywords = {lenses,Datatype-generic programming,generic views,json,metadata,sums of products,universes},
  author = {de Vries, Edsko and Löh, Andres},
  options = {useprefix=true}
}

@incollection{Holdermans2006generic,
  langid = {english},
  title = {Generic Views on Data Types},
  isbn = {978-3-540-35631-8 978-3-540-35632-5},
  url = {http://link.springer.com/chapter/10.1007/11783596_14},
  abstract = {A generic function is defined by induction on the structure of types. The structure of a data type can be defined in several ways. For example, in PolyP a pattern functor gives the structure of a data type viewed as a fixed point, and in Generic Haskell a structural representation type gives an isomorphic type view of a data type in terms of sums of products. Depending on this generic view on the structure of data types, some generic functions are easier, more difficult, or even impossible to define. Furthermore, the efficiency of some generic functions can be improved by choosing a different view. This paper introduces generic views on data types and shows why they are useful. Furthermore, it shows how generic views have been added to Generic Haskell, an extension of the functional programming language Haskell that supports the construction of generic functions. The separation between inductive definitions on type structure and generic views allows us to combine many approaches to generic programming in a single framework.},
  number = {4014},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-07-08},
  date = {2006},
  pages = {209-234},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Holdermans, Stefan and Jeuring, Johan and Löh, Andres and Rodriguez, Alexey},
  editor = {Uustalu, Tarmo},
  file = {/Users/pgiarrusso/Zotero/storage/WQA3HVPZ/11783596_14.html}
}
% == BibLateX quality report for Holdermans2006generic:
% 'isbn': not a valid ISBN

@inproceedings{Endrullis2013infinitary,
  location = {{Dagstuhl, Germany}},
  title = {Infinitary Rewriting Coinductively},
  volume = {19},
  isbn = {978-3-939897-49-1},
  url = {http://drops.dagstuhl.de/opus/volltexte/2013/3897},
  doi = {http://dx.doi.org/10.4230/LIPIcs.TYPES.2011.16},
  booktitle = {18th International Workshop on Types for Proofs and Programs (TYPES 2011)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2013},
  pages = {16-27},
  author = {Endrullis, Jörg and Polonsky, Andrew},
  editor = {Danielsson, Nils Anders and Nordström, Bengt},
  urn = {urn:nbn:de:0030-drops-38971}
}
% == BibLateX quality report for Endrullis2013infinitary:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Flatt2007adding,
  location = {{New York, NY, USA}},
  title = {Adding Delimited and Composable Control to a Production Programming Environment},
  isbn = {978-1-59593-815-2},
  url = {http://doi.acm.org/10.1145/1291151.1291178},
  doi = {10.1145/1291151.1291178},
  abstract = {Operators for delimiting control and for capturing composable continuations litter the landscape of theoretical programming language research. Numerous papers explain their advantages, how the operators explain each other (or don't), and other aspects of the operators' existence. Production programming languages, however, do not support these operators, partly because their relationship to existing and demonstrably useful constructs - such as exceptions and dynamic binding - remains relatively unexplored. In this paper, we report on our effort of translating the theory of delimited and composable control into a viable implementation for a production system. The report shows how this effort involved a substantial design element, including work with a formal model, as well as significant practical exploration and engineering. The resulting version of PLT Scheme incorporates the expressive combination of delimited and composable control alongside dynamic-wind, dynamic binding, and exception handling. None of the additional operators subvert the intended benefits of existing control operators, so that programmers can freely mix and match control operators.},
  booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '07},
  publisher = {{ACM}},
  urldate = {2015-07-06},
  date = {2007},
  pages = {165--176},
  author = {Flatt, Matthew and Yu, Gang and Findler, Robert Bruce and Felleisen, Matthias}
}

@inproceedings{Santo2015curryhoward,
  location = {{Dagstuhl, Germany}},
  title = {Curry-Howard for Sequent Calculus at Last!},
  volume = {38},
  isbn = {978-3-939897-87-3},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5162},
  doi = {http://dx.doi.org/10.4230/LIPIcs.TLCA.2015.165},
  booktitle = {13th International Conference on Typed Lambda Calculi and Applications (TLCA 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {165-179},
  author = {Santo, José Espirito},
  editor = {Altenkirch, Thorsten},
  urn = {urn:nbn:de:0030-drops-51626}
}
% == BibLateX quality report for Santo2015curryhoward:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Keller2010hereditary,
  location = {{New York, NY, USA}},
  title = {Hereditary Substitutions for Simple Types, Formalized},
  isbn = {978-1-4503-0255-5},
  url = {http://doi.acm.org/10.1145/1863597.1863601},
  doi = {10.1145/1863597.1863601},
  abstract = {We analyze a normalization function for the simply typed λ-calculus based on hereditary substitutions, a technique developed by Pfenning et al. The normalizer is implemented in Agda, a total language where all programs terminate. It requires no termination proof since it is structurally recursive which is recognized by Agda's termination checker. Using Agda as an interactive theorem prover we establish that our normalization function precisely identifies Βη-equivalent terms and hence can be used to decide Βη-equality. An interesting feature of this approach is that it is clear from the construction that Βη-equality is primitive recursive.},
  booktitle = {Proceedings of the Third ACM SIGPLAN Workshop on Mathematically Structured Functional Programming},
  series = {MSFP '10},
  publisher = {{ACM}},
  urldate = {2015-07-06},
  date = {2010},
  pages = {3--10},
  keywords = {decidability of $\\beta$η-equality,hereditary substitutions,normalizer,type theory},
  author = {Keller, Chantal and Altenkirch, Thorsten},
  file = {/Users/pgiarrusso/Zotero/storage/588I6JQ7/Keller_Altenkirch - 2010 - Hereditary Substitutions for Simple Types, Formalized.pdf}
}

@inproceedings{Wurthinger2013one,
  location = {{New York, NY, USA}},
  title = {One VM to Rule Them All},
  isbn = {978-1-4503-2472-4},
  url = {http://doi.acm.org/10.1145/2509578.2509581},
  doi = {10.1145/2509578.2509581},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
  booktitle = {Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
  series = {Onward! '13},
  publisher = {{ACM}},
  urldate = {2014-04-20},
  date = {2013},
  pages = {187--204},
  keywords = {dynamic languages,java,javascript,language implementation,optimization,virtual machine},
  author = {Würthinger, Thomas and Wimmer, Christian and Wöß, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  file = {/Users/pgiarrusso/Zotero/storage/AJMNHHD2/Würthinger et al - 2013 - One VM to Rule Them All.pdf}
}

@article{Bird1989algebraic,
  langid = {english},
  title = {Algebraic Identities for Program Calculation},
  volume = {32},
  issn = {0010-4620, 1460-2067},
  url = {http://comjnl.oxfordjournals.org/content/32/2/122},
  doi = {10.1093/comjnl/32.2.122},
  abstract = {To calculate a program means to derive it from a suitable specification by a process of equational reasoning. We describe a number of basic algebraic identities that turn out to be extremely useful in this task. These identities express relationship between the higher-order functions commonly encountered in functional programming. The idea of program calculation is illustrated with two non-trivial examples.},
  number = {2},
  journaltitle = {The Computer Journal},
  shortjournal = {The Computer Journal},
  urldate = {2014-04-23},
  date = {1989-01-01},
  pages = {122-126},
  author = {Bird, R. S.},
  file = {/Users/pgiarrusso/Zotero/storage/MHWTISIN/122.html}
}
% == BibLateX quality report for Bird1989algebraic:
% 'issn': not a valid ISSN

@incollection{Bird1989lectures,
  langid = {english},
  title = {Lectures on Constructive Functional Programming},
  isbn = {978-3-642-74886-8 978-3-642-74884-4},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-74884-4_5},
  abstract = {The subject of these lectures is a calculus of functions for deriving programs from their specifications. This calculus consists of a range of concepts and notations for defining functions over various data types — including lists, trees, and arrays — together with their algebraic and other properties. Each lecture begins with a specific problem, and the theory necessary to solve it is then developed. In this way we hope to show that a functional approach to the problem of systematically calculating programs from their specifications can take its place alongside other methodologies.},
  number = {55},
  booktitle = {Constructive Methods in Computing Science},
  series = {NATO ASI Series},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-04-23},
  date = {1989-01-01},
  pages = {151-217},
  keywords = {Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Bird, Richard S.},
  editor = {Broy, Manfred},
  file = {/Users/pgiarrusso/Zotero/storage/4RSPZTPI/978-3-642-74884-4_5.html}
}
% == BibLateX quality report for Bird1989lectures:
% 'isbn': not a valid ISBN

@incollection{Hu2002accumulative,
  langid = {english},
  title = {An Accumulative Parallel Skeleton for All},
  isbn = {978-3-540-43363-7 978-3-540-45927-9},
  url = {http://link.springer.com/chapter/10.1007/3-540-45927-8_7},
  abstract = {Parallel skeletons intend to encourage programmers to build a parallel program from ready-made components for which efficient implementations are known to exist, making the parallelization process simpler. However, it is neither easy to develop efficient parallel programs using skeletons nor to use skeletons to manipulate irregular data, and moreover there lacks a systematic way to optimize skeletal parallel programs. To remedy this situation, we propose a novel parallel skeleton, called accumulate, which not only efficiently describes data dependency in computation but also exhibits nice algebraic properties for manipulation. We show that this skeleton significantly eases skeletal parallel programming in practice, efficiently manipulating both regular and irregular data, and systematically optimizing skeletal parallel programs.},
  number = {2305},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-04-23},
  date = {2002-01-01},
  pages = {83-97},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Data Structures,Mathematical Logic and Formal Languages},
  author = {Hu, Zhenjiang and Iwasaki, Hideya and Takeichi, Masato},
  editor = {Métayer, Daniel Le},
  file = {/Users/pgiarrusso/Zotero/storage/BAHH2DC6/3-540-45927-8_7.html}
}
% == BibLateX quality report for Hu2002accumulative:
% 'isbn': not a valid ISBN

@incollection{Bird1990calculus,
  location = {{Boston, MA, USA}},
  title = {A calculus of functions for program derivation},
  isbn = {0-201-17236-4},
  url = {http://dl.acm.org/citation.cfm?id=119830.119841},
  booktitle = {Research Topics in Functional Programming},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  urldate = {2014-04-23},
  date = {1990},
  pages = {287--307},
  author = {Bird, R. S.},
  editor = {Turner, David A.},
  file = {/Users/pgiarrusso/Zotero/storage/XPAP7Q3V/Bird - 1990 - A calculus of functions for program derivation.pdf}
}

@inproceedings{Mainland2013exploiting,
  location = {{New York, NY, USA}},
  title = {Exploiting Vector Instructions with Generalized Stream Fusion},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500601},
  doi = {10.1145/2500365.2500601},
  abstract = {Stream fusion is a powerful technique for automatically transforming high-level sequence-processing functions into efficient implementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all.  In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together multiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are implemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2014-04-23},
  date = {2013},
  pages = {37--48},
  keywords = {Haskell,simd,stream fusion,vectorization},
  author = {Mainland, Geoffrey and Leshchinskiy, Roman and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/MK77RWJ4/Mainland et al - 2013 - Exploiting Vector Instructions with Generalized Stream Fusion.pdf}
}

@inproceedings{Farmer2014hermit,
  location = {{New York, NY, USA}},
  title = {The HERMIT in the Stream: Fusing Stream Fusion's concatMap},
  isbn = {978-1-4503-2619-3},
  url = {http://doi.acm.org/10.1145/2543728.2543736},
  doi = {10.1145/2543728.2543736},
  shorttitle = {The HERMIT in the Stream},
  abstract = {Stream Fusion, a popular deforestation technique in the Haskell community, cannot fuse the concatMap combinator. This is a serious limitation, as concatMap represents computations on nested streams. The original implementation of Stream Fusion used the Glasgow Haskell Compiler's user-directed rewriting system. A transformation which allows the compiler to fuse many uses of concatMap has previously been proposed, but never implemented, because the host rewrite system was not expressive enough to implement the proposed transformation. In this paper, we develop a custom optimization plugin which implements the proposed concatMap transformation, and study the effectiveness of the transformation in practice. We also provide a new translation scheme for list comprehensions which enables them to be optimized. Within this framework, we extend the transformation to monadic streams. Code featuring uses of concatMap experiences significant speedup when compiled with this optimization. This allows Stream Fusion to outperform its rival, foldr/build, on many list computations, and enables performance-sensitive code to be expressed at a higher level of abstraction.},
  booktitle = {Proceedings of the ACM SIGPLAN 2014 Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM '14},
  publisher = {{ACM}},
  urldate = {2014-04-23},
  date = {2014},
  pages = {97--108},
  keywords = {functional programming,program transformation,Haskell,optimization,stream fusion,deforestation,ghc,program fusion},
  author = {Farmer, Andrew and Hoener zu Siederdissen, Christian and Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/5V9CNA4T/Farmer et al. - 2014 - The HERMIT in the Stream Fusing Stream Fusion's c.pdf}
}

@article{Back1988calculus,
  langid = {english},
  title = {A calculus of refinements for program derivations},
  volume = {25},
  issn = {0001-5903, 1432-0525},
  url = {http://link.springer.com/article/10.1007/BF00291051},
  doi = {10.1007/BF00291051},
  abstract = {A calculus of program refinements is described, to be used as a tool for the step-by-step derivation of correct programs. A derivation step is considered correct if the new program preserves the total correctness of the old program. This requirement is expressed as a relation of (correct) refinement between nondeterministic program statements. The properties of this relation are studied in detail. The usual sequential statement constructors are shown to be monotone with respect to this relation and it is shown how refinement between statements can be reduced to a proof of total correctness of the refining statement. A special emphasis is put on the correctness of replacement steps, where some component of a program is replaced by another component. A method by which assertions can be added to statements to justify replacements in specific contexts is developed. The paper extends the weakest precondition technique of Dijkstra to proving correctness of larger program derivation steps, thus providing a unified framework for the axiomatic, the stepwise refinement and the transformational approach to program construction and verification.},
  number = {6},
  journaltitle = {Acta Informatica},
  shortjournal = {Acta Informatica},
  urldate = {2014-04-23},
  date = {1988-08-01},
  pages = {593-624},
  keywords = {Computational Mathematics and Numerical Analysis,Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Information Systems and Communication Service,Software Engineering/Programming and Operating Systems,Theory of Computation},
  author = {Back, R. J. R.},
  file = {/Users/pgiarrusso/Zotero/storage/88XZXMND/BF00291051.html}
}
% == BibLateX quality report for Back1988calculus:
% 'issn': not a valid ISSN

@article{Burstall1977transformation,
  title = {A Transformation System for Developing Recursive Programs},
  volume = {24},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/321992.321996},
  doi = {10.1145/321992.321996},
  abstract = {A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.},
  number = {1},
  journaltitle = {J. ACM},
  urldate = {2014-04-23},
  date = {1977-01},
  pages = {44--67},
  author = {Burstall, R. M. and Darlington, John},
  file = {/Users/pgiarrusso/Zotero/storage/6TRIHBF6/Burstall_Darlington - 1977 - A Transformation System for Developing Recursive Programs.pdf}
}
% == BibLateX quality report for Burstall1977transformation:
% ? Possibly abbreviated journal title J. ACM

@inproceedings{Lippmeier2012guiding,
  location = {{New York, NY, USA}},
  title = {Guiding Parallel Array Fusion with Indexed Types},
  isbn = {978-1-4503-1574-6},
  url = {http://doi.acm.org/10.1145/2364506.2364511},
  doi = {10.1145/2364506.2364511},
  abstract = {We present a refined approach to parallel array fusion that uses indexed types to specify the internal representation of each array. Our approach aids the client programmer in reasoning about the performance of their program in terms of the source code. It also makes the intermediate code easier to transform at compile-time, resulting in faster compilation and more reliable runtimes. We demonstrate how our new approach improves both the clarity and performance of several end-user written programs, including a fluid flow solver and an interpolator for volumetric data.},
  booktitle = {Proceedings of the 2012 Haskell Symposium},
  series = {Haskell '12},
  publisher = {{ACM}},
  urldate = {2014-04-24},
  date = {2012},
  pages = {25--36},
  keywords = {Haskell,arrays,data parallelism},
  author = {Lippmeier, Ben and Chakravarty, Manuel and Keller, Gabriele and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/ND268EAE/Lippmeier et al - 2012 - Guiding Parallel Array Fusion with Indexed Types.pdf}
}

@inproceedings{Chakravarty2007data,
  location = {{New York, NY, USA}},
  title = {Data Parallel Haskell: A Status Report},
  isbn = {978-1-59593-690-5},
  url = {http://doi.acm.org/10.1145/1248648.1248652},
  doi = {10.1145/1248648.1248652},
  shorttitle = {Data Parallel Haskell},
  abstract = {We describe the design and current status of our effort to implement the programming model of nested data parallelism into the Glasgow Haskell Compiler. We extended the original programming model and its implementation, both of which were first popularised by the NESL language, in terms of expressiveness as well as efficiency. Our current aim is to provide a convenient programming environment for SMP parallelism, and especially multicore architectures. Preliminary benchmarks show that we are, at least for some programs, able to achieve good absolute performance and excellent speedups.},
  booktitle = {Proceedings of the 2007 Workshop on Declarative Aspects of Multicore Programming},
  series = {DAMP '07},
  publisher = {{ACM}},
  urldate = {2014-04-24},
  date = {2007},
  pages = {10--18},
  author = {Chakravarty, Manuel M. T. and Leshchinskiy, Roman and Peyton Jones, Simon and Keller, Gabriele and Marlow, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/MZZVQZ6F/Chakravarty et al - 2007 - Data Parallel Haskell - A Status Report.pdf}
}

@report{Gorlatch1997optimizing,
  title = {Optimizing Compositions of Scans and Reductions in Parallel Program Derivation},
  abstract = {Introduction We study two popular programming schemas: scan (also known as prefix sums, parallel prefix, etc.) and reduction (also known as fold). Originally from the functional world [3], they are becoming increasingly popular as primitives of parallel programming. The reasons are that, first, such higher-order combinators are adequate and useful for a broad class of applications [4], second, they encourage well-structured, coarse-grained parallel programming and, third, their implementation in the MPI standard [14] makes the target programs portable across different parallel architectures with predictable performance. Our contributions are as follows: -- We formally prove two optimization rules: the first rule transforms a sequential composition of scan and reduction into a single reduction, the second rule transforms a composition of two scans into a single scan. -- We apply the first rule in the formal derivation of a parallel algorithm for the},
  date = {1997},
  author = {Gorlatch, Sergei},
  file = {/Users/pgiarrusso/Zotero/storage/K45Z777C/Gorlatch - 1997 - Optimizing Compositions of Scans and Reductions in Parallel Program Derivation.pdf;/Users/pgiarrusso/Zotero/storage/HPHE96CS/summary.html}
}
% == BibLateX quality report for Gorlatch1997optimizing:
% Missing required field 'type'
% Missing required field 'institution'

@article{Hu1997formal,
  title = {Formal Derivation of Efficient Parallel Programs by Construction of List Homomorphisms},
  volume = {19},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/256167.256201},
  doi = {10.1145/256167.256201},
  abstract = {It has been attracting much attention to make use of list homomorphisms in parallel programming because they ideally suit the divide-and-conquer parallel paradigm. However, they have been usually treated rather informally and ad hoc in the development of efficient parallel programs. What is worse is that some interesting functions, e.g., the maximum segment sum problem, are basically not list homomorphisms. In this article, we propose a systematic and formal way for the construction of a list homomorphism for a given problem so that an efficient parallel program is derived. We show, with several well-known but nontrivial problems, how a straightforward, and “obviously” correct, but quite inefficient solution to the problem can be successfully turned into a semantically  equivalent “almost list homomorphism.” The derivation is based on two transformations, namely tupling and fusion, which are defined according to the specific recursive structures of list homomorphisms.},
  number = {3},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2014-04-24},
  date = {1997-05},
  pages = {444--461},
  keywords = {list homomorphism,parallel functional programming,program transformation and derivation},
  author = {Hu, Zhenjiang and Iwasaki, Hideya and Takechi, Masato},
  file = {/Users/pgiarrusso/Zotero/storage/ICI3UAK9/Hu et al - 1997 - Formal Derivation of Efficient Parallel Programs by Construction of List.pdf}
}
% == BibLateX quality report for Hu1997formal:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@article{Goguen1977initial,
  title = {Initial Algebra Semantics and Continuous Algebras},
  volume = {24},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/321992.321997},
  doi = {10.1145/321992.321997},
  abstract = {Many apparently divergent approaches to specifying formal semantics of programming languages are applications of initial algebra semantics. In this paper an overview of initial algebra semantics is provided. The major technical feature is an initial continuous algebra which permits unified algebraic treatment of iterative and recursive semantic features in the same framework as more basic operations.},
  number = {1},
  journaltitle = {J. ACM},
  urldate = {2014-04-24},
  date = {1977-01},
  pages = {68--95},
  author = {Goguen, J. A. and Thatcher, J. W. and Wagner, E. G. and Wright, J. B.}
}
% == BibLateX quality report for Goguen1977initial:
% ? Possibly abbreviated journal title J. ACM

@inproceedings{Bernardy2012computational,
  location = {{Washington, DC, USA}},
  title = {A Computational Interpretation of Parametricity},
  isbn = {978-0-7695-4769-5},
  url = {http://dx.doi.org/10.1109/LICS.2012.25},
  doi = {10.1109/LICS.2012.25},
  abstract = {Reynolds' abstraction theorem has recently been extended to lambda-calculi with dependent types. In this paper, we show how this theorem can be internalized. More precisely, we describe an extension of the Pure Type Systems with a special parametricity rule (with computational content), and prove fundamental properties such as Church-Rosser's and strong normalization. All instances of the abstraction theorem can be both expressed and proved in the calculus itself. Moreover, one can apply parametricity to the parametricity rule: parametricity is itself parametric.},
  booktitle = {Proceedings of the 2012 27th Annual IEEE/ACM Symposium on Logic in Computer Science},
  series = {LICS '12},
  publisher = {{IEEE Computer Society}},
  urldate = {2014-04-25},
  date = {2012},
  pages = {135--144},
  keywords = {lambda calculus,Type structure},
  author = {Bernardy, Jean-Philippe and Moulin, Guilhem},
  file = {/Users/pgiarrusso/Zotero/storage/U4PTI7I4/Bernardy_Moulin - 2012 - A Computational Interpretation of Parametricity.pdf}
}

@inproceedings{Bird1996algebra,
  location = {{Secaucus, NJ, USA}},
  title = {The Algebra of Programming},
  isbn = {3-540-60947-4},
  url = {http://dl.acm.org/citation.cfm?id=256095.256116},
  booktitle = {Proceedings of the NATO Advanced Study Institute on Deductive Program Design},
  publisher = {{Springer-Verlag New York, Inc.}},
  urldate = {2014-04-25},
  date = {1996},
  pages = {167--203},
  author = {Bird, Richard and de Moor, Oege},
  options = {useprefix=true}
}
% == BibLateX quality report for Bird1996algebra:
% 'isbn': not a valid ISBN

@incollection{Visser2001stratego,
  langid = {english},
  title = {Stratego: A Language for Program Transformation Based on Rewriting Strategies System Description of Stratego 0.5},
  isbn = {978-3-540-42117-7 978-3-540-45127-3},
  url = {http://link.springer.com/chapter/10.1007/3-540-45127-7_27},
  shorttitle = {Stratego},
  abstract = {Program transformation is used in many areas of software engineering. Examples include compilation, optimization, synthesis, refactoring, migration, normalization and improvement [15]. Rewrite rules are a natural formalism for expressing single program transformations. However, using a standard strategy for normalizing a program with a set of rewrite rules is not adequate for implementing program transformation systems. It may be necessary to apply a rule only in some phase of a transformation, to apply rules in some order, or to apply a rule only to part of a program. These restrictions may be necessary to avoid non-termination or to choose a specific path in a non-con uent rewrite system. Stratego is a language for the specification of program transformation systems based on the paradigm of rewriting strategies. It supports the separation of strategies from transformation rules, thus allowing careful control over the application of these rules. As a result of this separation, transformation rules are reusable in multiple difierent transformations and generic strategies capturing patterns of control can be described independently of the transformation rules they apply. Such strategies can even be formulated independently of the object language by means of the generic term traversal capabilities of Stratego. In this short paper I give a description of version 0.5 of the Stratego system, discussing the features of the language (Section 2), the library (Section 3), the compiler (Section 4) and some of the applications that have been built (Section 5). Stratego is available as free software under the GNU General Public License from http://www.stratego-language.org.},
  number = {2051},
  booktitle = {Rewriting Techniques and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-04-27},
  date = {2001-01-01},
  pages = {357-361},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Symbolic and Algebraic Manipulation},
  author = {Visser, Eelco},
  editor = {Middeldorp, Aart},
  file = {/Users/pgiarrusso/Zotero/storage/AMHGCPMS/Visser and Middeldorp - 2001 - Stratego A Language for Program Transformation Ba.html}
}
% == BibLateX quality report for Visser2001stratego:
% 'isbn': not a valid ISBN

@inproceedings{Visser1998building,
  location = {{New York, NY, USA}},
  title = {Building Program Optimizers with Rewriting Strategies},
  isbn = {1-58113-024-4},
  url = {http://doi.acm.org/10.1145/289423.289425},
  doi = {10.1145/289423.289425},
  abstract = {We describe a language for defining term rewriting strategies, and its application to the production of program optimizers. Valid transformations on program terms can be described by a set of rewrite rules; rewriting strategies are used to describe when and how the various rules should be applied in order to obtain the desired optimization effects. Separating rules from strategies in this fashion makes it easier to reason about the behavior of the optimizer as a whole, compared to traditional monolithic optimizer implementations. We illustrate the expressiveness of our language by using it to describe a simple optimizer for an ML-like intermediate representation.The basic strategy language uses operators such as sequential composition, choice, and recursion to build transformers from a set of labeled unconditional rewrite rules. We also define an extended language in which the side-conditions and contextual rules that arise in realistic optimizer specifications can themselves be expressed as strategy-driven rewrites. We show that the features of the basic and extended languages can be expressed by breaking down the rewrite rules into their primitive building blocks, namely matching and building terms in variable binding environments. This gives us a low-level core language which has a clear semantics, can be implemented straightforwardly and can itself be optimized. The current implementation generates C code from a strategy specification.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2014-04-27},
  date = {1998},
  pages = {13--26},
  author = {Visser, Eelco and Benaissa, Zine-el-Abidine and Tolmach, Andrew}
}

@inproceedings{Clinger1988implementation,
  location = {{New York, NY, USA}},
  title = {Implementation Strategies for Continuations},
  isbn = {0-89791-273-X},
  url = {http://doi.acm.org/10.1145/62678.62692},
  doi = {10.1145/62678.62692},
  abstract = {Scheme and Smalltalk continuations may have unlimited extent. This means that a purely stack-based implementation of continuations, as suffices for most languages, is inadequate. Several implementation strategies have been described in the literature. Determining which is best requires knowledge of the kinds of programs that will commonly be run.
Danvy, for example, has conjectured that continuation captures occur in clusters. That is, the same continuation, once captured, is likely to be captured again. As evidence, Danvy cited the use of continuations in a research setting. We report that Danvy's conjecture is somewhat true in the commercial setting of MacScheme+Toolsmith™, which provides tools for developing Macintosh user interfaces in Scheme. These include an interrupt-driven event system and multitasking, both implemented by liberal use of continuations.
We describe several implementation strategies for continuations and compare four of them using benchmarks. We conclude that the most popular strategy may have a slight edge when continuations are not used at all, but that other strategies perform better when continuations are used and Danvy's conjecture holds.},
  booktitle = {Proceedings of the 1988 ACM Conference on LISP and Functional Programming},
  series = {LFP '88},
  publisher = {{ACM}},
  urldate = {2014-04-27},
  date = {1988},
  pages = {124--131},
  author = {Clinger, Will and Hartheimer, Anne and Ost, Eric},
  file = {/Users/pgiarrusso/Zotero/storage/5XDXA7VV/Clinger et al - 1988 - Implementation Strategies for Continuations.pdf}
}

@article{Clinger1999implementation,
  langid = {english},
  title = {Implementation Strategies for First-Class Continuations},
  volume = {12},
  issn = {1388-3690, 1573-0557},
  url = {http://link.springer.com/article/10.1023/A%3A1010016816429},
  doi = {10.1023/A:1010016816429},
  abstract = {Scheme and Smalltalk continuations may have unlimited extent. This means that a purely stack-based implementation of continuations, as suffices for most languages, is inadequate. We review several implementation strategies for continuations and compare their performance using instruction counts for the normal case and continuation-intensive synthetic benchmarks for other scenarios, including coroutines and multitasking. All of the strategies constrain a compiler in some way, resulting in indirect costs that are hard to measure directly. We use related measurements on a set of benchmarks to calculate upper bounds for these indirect costs.},
  number = {1},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order and Symbolic Computation},
  urldate = {2014-04-27},
  date = {1999-04-01},
  pages = {7-45},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Software Engineering/Programming and Operating Systems,continuations,coroutines,heap allocation,multitasking,Numeric Computing,Scheme,Smalltalk,stacks},
  author = {Clinger, William D. and Hartheimer, Anne H. and Ost, Eric M.},
  file = {/Users/pgiarrusso/Zotero/storage/DFNAFEHR/Clinger et al - 1999 - Implementation Strategies for First-Class Continuations.pdf;/Users/pgiarrusso/Zotero/storage/BI568ECK/A1010016816429.html}
}
% == BibLateX quality report for Clinger1999implementation:
% 'issn': not a valid ISSN

@book{Schmidt1987denotational,
  title = {Denotational Semantics: A Methodology for Language Development},
  isbn = {0-205-08974-7},
  url = {http://people.cis.ksu.edu/~schmidt/text/densem.html},
  shorttitle = {Denotational Semantics},
  publisher = {{McGraw-Hill Professional}},
  date = {1987},
  author = {Schmidt, David A.}
}

@inproceedings{Rompf2010lightweight,
  location = {{New York, NY, USA}},
  title = {Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs},
  isbn = {978-1-4503-0154-1},
  url = {http://doi.acm.org/10.1145/1868294.1868314},
  doi = {10.1145/1868294.1868314},
  shorttitle = {Lightweight Modular Staging},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  booktitle = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering},
  series = {GPCE '10},
  publisher = {{ACM}},
  urldate = {2014-04-28},
  date = {2010},
  pages = {127--136},
  keywords = {domain-specific languages,code generation,language virtualization,multi-stage programming},
  author = {Rompf, Tiark and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/S2IIJEFZ/Rompf_Odersky - 2010 - Lightweight Modular Staging - A Pragmatic Approach to Runtime Code Generation and Compiled DSLs.pdf}
}

@inproceedings{Kiczales2005aspectoriented,
  location = {{New York, NY, USA}},
  title = {Aspect-oriented Programming and Modular Reasoning},
  isbn = {1-58113-963-2},
  url = {http://doi.acm.org/10.1145/1062455.1062482},
  doi = {10.1145/1062455.1062482},
  abstract = {Aspects cut new interfaces through the primary decomposition of a system. This implies that in the presence of aspects, the complete interface of a module can only be determined once the complete configuration of modules in the system is known. While this may seem anti-modular, it is an inherent property of crosscutting concerns, and using aspect-oriented programming enables modular reasoning in the presence of such concerns.},
  booktitle = {Proceedings of the 27th International Conference on Software Engineering},
  series = {ICSE '05},
  publisher = {{ACM}},
  urldate = {2014-04-28},
  date = {2005},
  pages = {49--58},
  keywords = {aspect-oriented programming,modular reasoning,modularity},
  author = {Kiczales, Gregor and Mezini, Mira},
  file = {/Users/pgiarrusso/Zotero/storage/VXPUR28N/Kiczales_Mezini - 2005 - Aspect-oriented Programming and Modular Reasoning.pdf}
}

@inproceedings{Solodkyy2012open,
  location = {{New York, NY, USA}},
  title = {Open and Efficient Type Switch for C++},
  isbn = {978-1-4503-1561-6},
  url = {http://doi.acm.org/10.1145/2384616.2384686},
  doi = {10.1145/2384616.2384686},
  abstract = {Selecting operations based on the run-time type of an object is key to many object-oriented and functional programming techniques. We present a technique for implementing open and efficient type switching on hierarchical extensible data types. The technique is general and copes well with C++ multiple inheritance. To simplify experimentation and gain realistic performance using production-quality compilers and tool chains, we implement a type switch construct as an ISO C++11 library, called Mach7. This library-only implementation provides concise notation and outperforms the visitor design pattern, commonly used for case analysis on types in object-oriented programming. For closed sets of types, its performance roughly equals equivalent code in functional languages, such as OCaml and Haskell. The type-switching code is easier to use and is more expressive than hand-coded visitors are. The library is non-intrusive and circumvents most of the extensibility restrictions typical of the visitor design pattern. It was motivated by applications involving large, typed, abstract syntax trees.},
  booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '12},
  publisher = {{ACM}},
  urldate = {2014-04-28},
  date = {2012},
  pages = {963--982},
  keywords = {C++,memoization,type switch,typecase,visitor design pattern},
  author = {Solodkyy, Yuriy and Dos Reis, Gabriel and Stroustrup, Bjarne}
}

@inproceedings{Kastner2011road,
  location = {{New York, NY, USA}},
  title = {The Road to Feature Modularity?},
  isbn = {978-1-4503-0789-5},
  url = {http://doi.acm.org/10.1145/2019136.2019142},
  doi = {10.1145/2019136.2019142},
  abstract = {Modularity of feature representations has been a long standing goal of feature-oriented software development. While some researchers regard feature modules and corresponding composition mechanisms as a modular solution, other researchers have challenged the notion of feature modularity and pointed out that most feature-oriented implementation mechanisms lack proper interfaces and support neither modular type checking nor separate compilation. We step back and reflect on the feature-modularity discussion. We distinguish two notions of modularity, cohesion without interfaces and information hiding with interfaces, and point out the different expectations that, we believe, are the root of many heated discussions. We discuss whether feature interfaces should be desired and weigh their potential benefits and costs, specifically regarding crosscutting, granularity, feature interactions, and the distinction between closed-world and open-world reasoning. Because existing evidence for and against feature modularity and feature interfaces is shaky and inconclusive, more research is needed, for which we outline possible directions.},
  booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
  series = {SPLC '11},
  publisher = {{ACM}},
  urldate = {2014-04-28},
  date = {2011},
  pages = {5:1--5:8},
  keywords = {modularity,crosscutting,feature interactions,feature models,feature modules,granularity,interfaces,module systems,variability},
  author = {Kästner, Christian and Apel, Sven and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/EK45FC8X/Kästner et al - 2011 - The Road to Feature Modularity.pdf;/Users/pgiarrusso/Zotero/storage/XEE7D53K/Kästner et al - 2011 - The Road to Feature Modularity.pdf}
}

@inproceedings{Apel2013exploring,
  location = {{New York, NY, USA}},
  title = {Exploring Feature Interactions in the Wild: The New Feature-interaction Challenge},
  isbn = {978-1-4503-2168-6},
  url = {http://doi.acm.org/10.1145/2528265.2528267},
  doi = {10.1145/2528265.2528267},
  shorttitle = {Exploring Feature Interactions in the Wild},
  abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
  booktitle = {Proceedings of the 5th International Workshop on Feature-Oriented Software Development},
  series = {FOSD '13},
  publisher = {{ACM}},
  urldate = {2014-04-28},
  date = {2013},
  pages = {1--8},
  keywords = {feature interactions,feature modularity,feature-interaction problem,feature-oriented software development},
  author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and Kästner, Christian and Garvin, Brady},
  file = {/Users/pgiarrusso/Zotero/storage/4VNFEA3V/Apel et al - 2013 - Exploring Feature Interactions in the Wild - The New Feature-interaction.pdf}
}

@article{Madni2012elegant,
  langid = {english},
  title = {Elegant systems design: Creative fusion of simplicity and power},
  volume = {15},
  issn = {1520-6858},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/sys.21209/abstract},
  doi = {10.1002/sys.21209},
  shorttitle = {Elegant systems design},
  abstract = {Elegance is a term frequently associated with aesthetics in design. It typically connotes simplicity, beauty, and grace. When it comes to complex systems, it also connotes predictable behavior, power, and creative functionality. Elegance is what separates the merely functional from the engaging. An elegant design usually has a thematic vision that drives its creation. It engages both designers and users and supports creative exploration on their part. The process of elegant design is an iterative, creative process that exploits systems thinking, probing questioning, and appropriate analogies and metaphors to gain insights that can be transformed novel solutions. This paper provides key insights into elegant systems design and characteristics of elegant systems designers. It offers a heuristics-driven elegant systems design process along with metrics for assessing elegance. © 2012 Wiley Periodicals, Inc. Syst Eng 15},
  number = {3},
  journaltitle = {Systems Engineering},
  shortjournal = {Syst. Engin.},
  urldate = {2014-04-28},
  date = {2012-09-01},
  pages = {347-354},
  keywords = {complexity,creativity,design thinking,elegant design,stakeholders,systems thinking},
  author = {Madni, Azad M.},
  file = {/Users/pgiarrusso/Zotero/storage/DWIRQBI2/full.html}
}

@inproceedings{Eisenberg2014closed,
  location = {{New York, NY, USA}},
  title = {Closed Type Families with Overlapping Equations},
  isbn = {978-1-4503-2544-8},
  url = {http://doi.acm.org/10.1145/2535838.2535856},
  doi = {10.1145/2535838.2535856},
  abstract = {Open, type-level functions are a recent innovation in Haskell that move Haskell towards the expressiveness of dependent types, while retaining the look and feel of a practical programming language. This paper shows how to increase expressiveness still further, by adding closed type functions whose equations may overlap, and may have non-linear patterns over an open type universe. Although practically useful and simple to implement, these features go beyond conventional dependent type theory in some respects, and have a subtle metatheory.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '14},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2014},
  pages = {671--683},
  keywords = {Haskell,type families,system fc,type-level computation},
  author = {Eisenberg, Richard A. and Vytiniotis, Dimitrios and Peyton Jones, Simon and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/BTTQSTWU/Eisenberg et al - 2014 - Closed Type Families with Overlapping Equations.pdf}
}

@inproceedings{Dreyer2003type,
  location = {{New York, NY, USA}},
  title = {A Type System for Higher-order Modules},
  isbn = {1-58113-628-5},
  url = {http://doi.acm.org/10.1145/604131.604151},
  doi = {10.1145/604131.604151},
  abstract = {We present a type theory for higher-order modules that accounts for many central issues in module system design, including translucency, applicativity, generativity, and modules as first-class values. Our type system harmonizes design elements from previous work, resulting in a simple, economical account of modular programming. The main unifying principle is the treatment of abstraction mechanisms as computational effects. Our language is the first to provide a complete and practical formalization of all of these critical issues in module system design.},
  booktitle = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '03},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2003},
  pages = {236--249},
  keywords = {singleton types,abstract data types,type theory,modularity,computational effects,functors,generativity},
  author = {Dreyer, Derek and Crary, Karl and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/WU3EV92S/Dreyer-Crary-Harper - 2003 - A Type System for Higher-order Modules.pdf}
}

@inproceedings{Lee2007mechanized,
  location = {{New York, NY, USA}},
  title = {Towards a Mechanized Metatheory of Standard ML},
  isbn = {1-59593-575-4},
  url = {http://doi.acm.org/10.1145/1190216.1190245},
  doi = {10.1145/1190216.1190245},
  abstract = {We present an internal language with equivalent expressive power to Standard ML, and discuss its formalization in LF and the machine-checked verification of its type safety in Twelf. The internal language is intended to serve as the target of elaboration in an elaborative semantics for Standard ML in the style of Harper and Stone. Therefore, it includes all the programming mechanisms necessary to implement Standard ML, including translucent modules, abstraction, polymorphism, higher kinds, references, exceptions, recursive types, and recursive functions. Our successful formalization of the proof involved a careful interplay between the precise formulations of the various mechanisms, and required the invention of new representation and proof techniques of general interest.},
  booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '07},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2007},
  pages = {173--184},
  keywords = {language definitions,logical frameworks,mechanized metatheory,standard ML,twelf,type safety},
  author = {Lee, Daniel K. and Crary, Karl and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/WRC6W3J4/Lee et al - 2007 - Towards a Mechanized Metatheory of Standard ML.pdf}
}

@inproceedings{Fisher1999design,
  location = {{New York, NY, USA}},
  title = {The Design of a Class Mechanism for Moby},
  isbn = {1-58113-094-5},
  url = {http://doi.acm.org/10.1145/301618.301638},
  doi = {10.1145/301618.301638},
  abstract = {Typical class-based languages, such as C++ and JAVA, provide complex class mechanisms but only weak module systems. In fact, classes in these languages incorporate many of the features found in richer module mechanisms. In this paper, we describe an alternative approach to designing a language that has both classes and modules. In our design, we rely on a rich ML-style module system to provide features such as visibility control and parameterization, while providing a minimal class mechanism that includes only those features needed to support inheritance. Programmers can then use the combination of modules and classes to implement the full range of class-based features and idioms. Our approach has the advantage that it provides a full-featured module system (useful in its own right), while keeping the class mechanism quite simple.We have incorporated this design in MOBY, which is an ML-style language that supports class-based object-oriented programming. In this paper, we describe our design via a series of simple examples, show how various class-based features and idioms are realized in MOBY, compare our design with others, and sketch its formal semantics.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  series = {PLDI '99},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {1999},
  pages = {37--49},
  author = {Fisher, Kathleen and Reppy, John}
}

@incollection{Fisher2000extending,
  langid = {english},
  title = {Extending Moby with Inheritance-Based Subtyping},
  isbn = {978-3-540-67660-7 978-3-540-45102-0},
  url = {http://link.springer.com/chapter/10.1007/3-540-45102-1_5},
  abstract = {Classes play a dual role in mainstream statically-typed object-oriented languages, serving as both object generators and object types. In such languages, inheritance implies subtyping. In contrast, the theoretical language community has viewed this linkage as a mistake and has focused on subtyping relationships determined by the structure of object types, without regard to their underlying implementations. In this paper,we explore why inheritance-based subtyping relations are useful and we present an extension to the MOBY programming language that supports both inheritance-based and structural subtyping relations. In addition, we present a formal accounting of this extension.},
  number = {1850},
  booktitle = {ECOOP 2000 — Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-04-29},
  date = {2000-01-01},
  pages = {83-107},
  keywords = {Programming Techniques,Software Engineering,Logics and Meanings of Programs,Business Information Systems,Computer Communication Networks},
  author = {Fisher, Kathleen and Reppy, John},
  editor = {Bertino, Elisa},
  file = {/Users/pgiarrusso/Zotero/storage/TK9PQAAG/3-540-45102-1_5.html}
}
% == BibLateX quality report for Fisher2000extending:
% 'isbn': not a valid ISBN

@inproceedings{Allais2013new,
  location = {{New York, NY, USA}},
  title = {New Equations for Neutral Terms: A Sound and Complete Decision Procedure, Formalized},
  isbn = {978-1-4503-2384-0},
  url = {http://doi.acm.org/10.1145/2502409.2502411},
  doi = {10.1145/2502409.2502411},
  shorttitle = {New Equations for Neutral Terms},
  abstract = {The definitional equality of an intensional type theory is its test of type compatibility. Today's systems rely on ordinary evaluation semantics to compare expressions in types, frustrating users with type errors arising when evaluation fails to identify two `obviously' equal terms. If only the machine could decide a richer theory! We propose a way to decide theories which supplement evaluation with `ν-rules', rearranging the neutral parts of normal forms, and report a successful initial experiment. We study a simple λ-calculus with primitive fold, map and append operations on lists and develop in Agda a sound and complete decision procedure for an equational theory enriched with monoid, functor and fusion laws.},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN Workshop on Dependently-typed Programming},
  series = {DTP '13},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2013},
  pages = {13--24},
  keywords = {logical relations,map fusion,normalization by evaluation,simply typed lambda calculus},
  author = {Allais, Guillaume and McBride, Conor and Boutillier, Pierre},
  file = {/Users/pgiarrusso/Zotero/storage/DBIMJM7S/Allais et al - 2013 - New Equations for Neutral Terms - A Sound and Complete Decision Procedure,.pdf}
}

@inproceedings{Lindley2013hasochism,
  location = {{New York, NY, USA}},
  title = {Hasochism: The Pleasure and Pain of Dependently Typed Haskell Programming},
  isbn = {978-1-4503-2383-3},
  url = {http://doi.acm.org/10.1145/2503778.2503786},
  doi = {10.1145/2503778.2503786},
  shorttitle = {Hasochism},
  abstract = {Haskell's type system has outgrown its Hindley-Milner roots to the extent that it now stretches to the basics of dependently typed programming. In this paper, we collate and classify techniques for programming with dependent types in Haskell, and contribute some new ones. In particular, through extended examples---merge-sort and rectangular tilings---we show how to exploit Haskell's constraint solver as a theorem prover, delivering code which, as Agda programmers, we envy. We explore the compromises involved in simulating variations on the theme of the dependent function space in an attempt to help programmers put dependent types to work, and to inform the evolving language design both of Haskell and of dependently typed languages more broadly.},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '13},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2013},
  pages = {81--92},
  keywords = {proof search,data type promotion,dependent types,invariants,singletons},
  author = {Lindley, Sam and McBride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/FW4P46G9/Lindley_McBride - 2013 - Hasochism - The Pleasure and Pain of Dependently Typed Haskell Programming.pdf}
}

@inproceedings{Erdweg2014modular,
  location = {{New York, NY, USA}},
  title = {Modular Specification and Dynamic Enforcement of Syntactic Language Constraints when Generating Code},
  isbn = {978-1-4503-2772-5},
  url = {http://doi.acm.org/10.1145/2577080.2577089},
  doi = {10.1145/2577080.2577089},
  abstract = {A key problem in metaprogramming and specifically in generative programming is to guarantee that generated code is well-formed with respect to the context-free and context-sensitive constraints of the target language. We propose typesmart constructors as a dynamic approach to enforcing the well-formedness of generated code. A typesmart constructor is a function that is used in place of a regular constructor to create values, but it may reject the creation of values if the given data violates some language-specific constraint. While typesmart constructors can be implemented individually, we demonstrate how to derive them automatically from a grammar, so that the grammar remains the sole specification of a language's syntax and is not duplicated. We have integrated support for typesmart constructors into the run-time system of Stratego to enforce usage of typesmart constructors implicitly whenever a regular constructor is called. We evaluate the applicability, performance, and usefulness of typesmart constructors for syntactic constraints in a compiler for MiniJava developed with Spoofax and in various language extensions of Java and Haskell implemented with SugarJ and SugarHaskell.},
  booktitle = {Proceedings of the 13th International Conference on Modularity},
  series = {MODULARITY '14},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2014},
  pages = {241--252},
  keywords = {program transformation,generative programming,abstract syntax tree,dynamic analysis,spoofax,sugarj,typesmart constructors,well-formedness checks},
  author = {Erdweg, Sebastian and Vergu, Vlad and Mezini, Mira and Visser, Eelco},
  file = {/Users/pgiarrusso/Zotero/storage/H7687VZ6/Erdweg et al - 2014 - Modular Specification and Dynamic Enforcement of Syntactic Language Constraints.pdf}
}

@letter{Weeks2003mltondevel,
  title = {[MLton-devel] CPS vs SSA},
  url = {http://mlton.org/pipermail/mlton/2003-January/023054.html},
  type = {E-mail},
  urldate = {2015-01-06},
  year = {Fri, 10 Jan 2003 17:35:38 -0800},
  author = {Weeks, Stephen},
  file = {/Users/pgiarrusso/Zotero/storage/KTAQT83C/023054.html}
}
% == BibLateX quality report for Weeks2003mltondevel:
% Unexpected field 'title'
% Unexpected field 'type'
% Unexpected field 'author'

@incollection{Simpson1995categorical,
  langid = {english},
  title = {Categorical completeness results for the simply-typed lambda-calculus},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  url = {http://link.springer.com/chapter/10.1007/BFb0014068},
  abstract = {We investigate, in a categorical setting, some completeness properties of beta-eta conversion between closed terms of the simplytyped lambda calculus. A cartesian-closed category is said to be complete if, for any two unconvertible terms, there is some interpretation of the calculus in the category that distinguishes them. It is said to have a complete interpretation if there is some interpretation that equates only interconvertible terms. We give simple necessary and sufficient conditions on the category for each of the two forms of completeness to hold. The classic completeness results of, e.g., Friedman and Plotkin are immediate consequences. As another application, we derive a syntactic theorem of Statman characterizing beta-eta conversion as a maximum consistent congruence relation satisfying a property known as typical ambiguity.},
  number = {902},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-08},
  date = {1995-01-01},
  pages = {414-427},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Simpson, Alex K.},
  editor = {Dezani-Ciancaglini, Mariangiola and Plotkin, Gordon},
  file = {/Users/pgiarrusso/Zotero/storage/C4RFFPTJ/Simpson - 1995 - Categorical completeness results for the simply-typed lambda-calculus.pdf;/Users/pgiarrusso/Zotero/storage/A49Q2P22/10.html}
}
% == BibLateX quality report for Simpson1995categorical:
% 'isbn': not a valid ISBN

@article{Leroy2000modular,
  title = {A modular module system},
  volume = {10},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796800003683},
  doi = {null},
  abstract = {A simple implementation of an SML-like module system is presented as a module parameterized by a base language and its type-checker. This implementation is useful both as a detailed tutorial on the Harper–Lillibridge–Leroy module system and its implementation, and as a constructive demonstration of the applicability of that module system to a wide range of programming languages.},
  number = {03},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-01-08},
  date = {2000-05},
  pages = {269--303},
  author = {Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/HW5K5TD5/Leroy - 2000 - A modular module system.pdf;/Users/pgiarrusso/Zotero/storage/M9VWEYVR/A8D022C76CBFB0DD9EEA05458D5C662D.html;/Users/pgiarrusso/Zotero/storage/RQSPVNWR/displayAbstract.html}
}

@article{Seely1987categorical,
  title = {Categorical semantics for higher order polymorphic lambda calculus},
  volume = {52},
  issn = {1943-5886},
  url = {http://journals.cambridge.org/article_S0022481200029364},
  doi = {10.2307/2273831},
  abstract = {A categorical structure suitable for interpreting polymorphic lambda calculus (PLC) is defined, providing an algebraic semantics for PLC which is sound and complete. In fact, there is an equivalence between the theories and the categories. Also presented is a definitional extension of PLC including “subtypes”, for example, equality subtypes, together with a construction providing models of the extended language, and a context for Girard's extension of the Dialectica interpretation.},
  number = {04},
  journaltitle = {Journal of Symbolic Logic},
  urldate = {2015-01-08},
  date = {1987-12},
  pages = {969--989},
  author = {Seely, R. a. G.},
  file = {/Users/pgiarrusso/Zotero/storage/WKW7ZEXK/displayAbstract.html}
}

@incollection{Caccamo2001higherorder,
  langid = {english},
  title = {A Higher-Order Calculus for Categories},
  isbn = {978-3-540-42525-0 978-3-540-44755-9},
  url = {http://link.springer.com/chapter/10.1007/3-540-44755-5_11},
  abstract = {A calculus for a fragment of category theory is presented. The types in the language denote categories and the expressions functors. The judgements of the calculus systematise categorical arguments such as: an expression is functorial in its free variables; two expressions are naturally isomorphic in their free variables. There are special binders for limits and more general ends. The rules for limits and ends support an algebraic manipulation of universal constructions as opposed to a more traditional diagrammatic approach. Duality within the calculus and applications in proving continuity are discussed with examples. The calculus gives a basis for mechanising a theory of categories in a generic theorem prover like Isabelle.},
  number = {2152},
  booktitle = {Theorem Proving in Higher Order Logics},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-08},
  date = {2001-01-01},
  pages = {136-153},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Logic Design},
  author = {Cáccamo, Mario and Winskel, Glynn},
  editor = {Boulton, Richard J. and Jackson, Paul B.},
  file = {/Users/pgiarrusso/Zotero/storage/97Q3MCR4/3-540-44755-5_11.html}
}
% == BibLateX quality report for Caccamo2001higherorder:
% 'isbn': not a valid ISBN

@article{Ramos2014sequences,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.2105},
  primaryClass = {cs, math},
  title = {Sequences of Rewrites: A Categorical Interpretation},
  url = {http://arxiv.org/abs/1412.2105},
  shorttitle = {Sequences of Rewrites},
  abstract = {In Martin-L$\backslash$"of's Intensional Type Theory, identity type is a heavily used and studied concept. The reason for that is the fact that it's responsible for the recently discovered connection between Type Theory and Homotopy Theory. The main problem with identity types, as originally formulated, is that they are complex to understand and use. Using that fact as motivation, a much simpler formulation for the identity type was proposed by Anjolina de Oliveira and Ruy de Queiroz. In this formulation, an element of an identity type is seen as a sequence of rewrites (or computational paths). Together with the logical rules of this new entity, there exists a system of reduction rules between sequence of rewrites called LND\_EQS-RWS. This system is constructed using the labelled natural deduction (i.e. Prawitz' Natural Deduction plus derivations-as-terms) and is responsible for establishing how a sequence of rewrites can be rewritten, resulting in a new sequence of rewrites. In this context, we propose a categorical interpretation for this new entity, using the types as objects and the rules of rewrites as morphisms. Moreover, we show that our interpretation is in accordance with some known results, like that types have a groupoidal structure. We also interpret more complicated structures, like the one formed by a rewrite of a sequence of rewrites.},
  urldate = {2015-01-12},
  date = {2014-12-05},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  author = {Ramos, Arthur and de Queiroz, Ruy J. G. B. and de Oliveira, Anjolina G.},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/XPZ52S79/Ramos et al - 2014 - Sequences of Rewrites - A Categorical Interpretation.pdf;/Users/pgiarrusso/Zotero/storage/MB96JEG2/1412.html}
}
% == BibLateX quality report for Ramos2014sequences:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Yang2004encoding,
  title = {Encoding types in ML-like languages},
  volume = {315},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397503006212},
  doi = {10.1016/j.tcs.2003.11.017},
  abstract = {This article presents several general approaches to programming with type-indexed families of values within a Hindley–Milner type system. A type-indexed family of values is a function that maps a family of types to a family of values. The function performs a case analysis on the input types and returns values of possibly different types. Such a case analysis on types seems to be prohibited by the Hindley–Milner type system. Our approaches solve the problem by using type encodings. The compile-time types of the type encodings reflect the types themselves, thereby making the approaches type-safe, in the sense that the underlying type system statically prevents any mismatch between the input type and the function arguments that depend on this type.

A type encoding could be either value-dependent, meaning that the type encoding is tied to a specific type-indexed family, or value-independent, meaning that the type encoding can be shared by various type-indexed families. Our first approach is value-dependent: we simply interpret a type as its corresponding value. Our second approach provides value-independent type encodings through embedding and projection functions; they are universal type interpretations, in that they can be used to compute other type interpretations. We also present an alternative approach to value-independent type encodings, using higher-order functors.

We demonstrate our techniques through applications such as C printf-like formatting, type-directed partial evaluation, and subtype coercions.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Mathematical Foundations of Programming Semantics},
  urldate = {2015-01-14},
  date = {2004-05-05},
  pages = {151-190},
  author = {Yang, Zhe},
  file = {/Users/pgiarrusso/Zotero/storage/SRHHSVIP/Yang - 2004 - Encoding types in ML-like languages.pdf;/Users/pgiarrusso/Zotero/storage/KJPMQGIT/S0304397503006212.html}
}

@inproceedings{Yang1998encoding,
  location = {{New York, NY, USA}},
  title = {Encoding Types in ML-like Languages},
  isbn = {1-58113-024-4},
  url = {http://doi.acm.org/10.1145/289423.289458},
  doi = {10.1145/289423.289458},
  abstract = {A Hindley-Milner type system such as ML's seems to prohibit type-indexed values, i.e., functions that map a family of types to a family of values. Such functions generally perform case analysis on the input types and return values of possibly different types. The goal of our work is to demonstrate how to program with type-indexed values within a Hindley-Milner type system.Our first approach is to interpret an input type as its corresponding value, recursively. This solution is type-safe, in the sense that the ML type system statically prevents any mismatch between the input type and function arguments that depend on this type.Such specific type interpretations, however, prevent us from combining different type-indexed values that share the same type. To meet this objection, we focus on finding a value-independent type encoding that can be shared by different functions. We propose and compare two solutions. One requires first-class and higher-order polymorphism, and, thus, is not implementable in the core language of ML, but it can be programmed using higher-order functors in Standard ML of New Jersey. Its usage, however, is clumsy. The other approach uses embedding/projection functions. It appears to be more practical.We demonstrate the usefulness of type-indexed values through examples including type-directed partial evaluation, C printf-like formatting, and subtype coercions. Finally, we discuss the tradeoffs between our approach and some other solutions based on more expressive typing disciplines.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2015-01-14},
  date = {1998},
  pages = {289--300},
  author = {Yang, Zhe}
}

@article{Wadler2003marriage,
  title = {The Marriage of Effects and Monads},
  volume = {4},
  issn = {1529-3785},
  url = {http://doi.acm.org/10.1145/601775.601776},
  doi = {10.1145/601775.601776},
  abstract = {Gifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect system into a corresponding monad system.},
  number = {1},
  journaltitle = {ACM Trans. Comput. Logic},
  urldate = {2015-01-14},
  date = {2003-01},
  pages = {1--32},
  keywords = {Monad,effect,region,type,type reconstruction},
  author = {Wadler, Philip and Thiemann, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/U5PT7K7H/Wadler_Thiemann - 2003 - The Marriage of Effects and Monads.pdf}
}
% == BibLateX quality report for Wadler2003marriage:
% ? Possibly abbreviated journal title ACM Trans. Comput. Logic

@inproceedings{Bransen2015incremental,
  location = {{New York, NY, USA}},
  title = {Incremental Evaluation of Higher Order Attributes},
  isbn = {978-1-4503-3297-2},
  url = {http://doi.acm.org/10.1145/2678015.2682541},
  doi = {10.1145/2678015.2682541},
  abstract = {Compilers, amongst other programs, often work with data that (slowly) changes over time. When the changes between subsequent runs of the compiler are small, one would hope the compiler to incrementally update its results, resulting in much lower running times. However, the manual construction of an incremental compiler is very hard and error prone and therefore usually not an option. Attribute grammars provide an attractive way of constructing compilers, as they are compositional in nature and allow for aspect oriented programming. In this work we extend previous work on the automatic generation of incremental attribute grammar evaluators, with the purpose of (semi-)automatically generating an incremental compiler from the regular attribute grammar definition, by adding support for incremental evaluation of higher order attributes, a well known extension to the classical attribute grammars that is used in many ways in compiler construction, for example to model different compiler phases.},
  booktitle = {Proceedings of the 2015 Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM '15},
  publisher = {{ACM}},
  urldate = {2015-01-15},
  date = {2015},
  pages = {39--48},
  keywords = {program transformation,attribute grammars,change propagation,incremental evaluation,type inference},
  author = {Bransen, Jeroen and Dijkstra, Atze and Swierstra, S. Doaitse},
  file = {/Users/pgiarrusso/Zotero/storage/W6PCEH89/Bransen et al - 2015 - Incremental Evaluation of Higher Order Attributes.pdf}
}

@article{Vytiniotis2010parametricity,
  title = {Parametricity, type equality, and higher-order polymorphism},
  volume = {20},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796810000079},
  doi = {10.1017/S0956796810000079},
  abstract = {Propositions that express type equality are a frequent ingredient of modern functional programming – they can encode generic functions, dynamic types, and GADTs. Via the Curry–Howard correspondence, these propositions are ordinary types inhabited by proof terms, computed using runtime type representations. In this paper we show that two examples of type equality propositions actually do reflect type equality; they are only inhabited when their arguments are equal and their proofs are unique (up to equivalence.) We show this result in the context of a strongly normalizing language with higher-order polymorphism and primitive recursion over runtime-type representations by proving Reynolds's abstraction theorem. We then use this theorem to derive “free” theorems about equality types.},
  number = {02},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-01-15},
  date = {2010-03},
  pages = {175--210},
  author = {Vytiniotis, Dimitrios and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/8J4HURNH/Vytiniotis_Weirich - 2010 - Parametricity, type equality, and higher-order polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/W5KRGI5N/Vytiniotis and Weirich - 2010 - Parametricity, type equality, and higher-order pol.html}
}

@inproceedings{Liang1995monad,
  location = {{New York, NY, USA}},
  title = {Monad Transformers and Modular Interpreters},
  isbn = {0-89791-692-1},
  url = {http://doi.acm.org/10.1145/199448.199528},
  doi = {10.1145/199448.199528},
  abstract = {We show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (call-by-name, call-by-value and lazy evaluation), references and assignment, nondeterminism, first-class continuations, and program tracing.The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions.Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that incudes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.},
  booktitle = {Proceedings of the 22Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '95},
  publisher = {{ACM}},
  urldate = {2015-01-17},
  date = {1995},
  pages = {333--343},
  author = {Liang, Sheng and Hudak, Paul and Jones, Mark},
  file = {/Users/pgiarrusso/Zotero/storage/EWZZQ66N/Liang et al - 1995 - Monad Transformers and Modular Interpreters.pdf}
}

@thesis{Liang1998modular,
  location = {{New Haven, CT, USA}},
  title = {Modular Monadic Semantics and Compilation},
  abstract = {Modular monadic semantics is a high-level and modular form of denotational semantics. It is capable of capturing individual programming language features and their interactions. This thesis explores the theory and applications of modular monadic semantics, including: building blocks for individual programming features, equational reasoning with laws and axioms, modular proofs, program transformation, modular interpreters, and compiler construction. We will demonstrate that the modular monadic semantics framework makes programming languages easy to
specify, reason about, and implement.},
  institution = {{Yale University}},
  date = {1998},
  author = {Liang, Sheng},
  note = {AAI9835276}
}
% == BibLateX quality report for Liang1998modular:
% Missing required field 'type'

@online{Beringerabstract,
  title = {An Abstract View of Programming Languages},
  url = {http://www.lfcs.inf.ed.ac.uk/reports/90/ECS-LFCS-90-113/},
  urldate = {2015-01-17},
  author = {Beringer, Lennart},
  file = {/Users/pgiarrusso/Zotero/storage/M5P85MA4/ECS-LFCS-90-113.html}
}
% == BibLateX quality report for Beringerabstract:
% Exactly one of 'date' / 'year' must be present

@inproceedings{Krishnaswami2009focusing,
  location = {{New York, NY, USA}},
  title = {Focusing on Pattern Matching},
  isbn = {978-1-60558-379-2},
  url = {http://doi.acm.org/10.1145/1480881.1480927},
  doi = {10.1145/1480881.1480927},
  abstract = {In this paper, we show how pattern matching can be seen to arise from a proof term assignment for the focused sequent calculus. This use of the Curry-Howard correspondence allows us to give a novel coverage checking algorithm, and makes it possible to give a rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns.},
  booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '09},
  publisher = {{ACM}},
  urldate = {2015-01-19},
  date = {2009},
  pages = {366--378},
  keywords = {pattern matching,focusing,type theory,_tablet,Curry-Howard},
  author = {Krishnaswami, Neelakantan R.},
  file = {/Users/pgiarrusso/Zotero/storage/F4MDIRHB/Krishnaswami - 2009 - Focusing on Pattern Matching.pdf}
}

@incollection{Filinski1989declarative,
  langid = {english},
  title = {Declarative continuations: An investigation of duality in programming language semantics},
  isbn = {978-3-540-51662-0 978-3-540-46740-3},
  url = {http://link.springer.com/chapter/10.1007/BFb0018355},
  shorttitle = {Declarative continuations},
  abstract = {This paper presents a formalism for including first-class continuations in a programming language as a declarative concept, rather than an imperative one. A symmetric extension of the typed λ-calculus is introduced, where values and continuations play dual roles, permitting mirror-image syntax for dual categorical concepts like products and coproducts. An implementable semantic description and a static type system for this calculus are presented. We also give a categorical description of the language, by presenting a correspondence with a system of combinatory logic, similar to a cartesian closed category, but with a completely symmetrical set of axioms.},
  number = {389},
  booktitle = {Category Theory and Computer Science},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-19},
  date = {1989-01-01},
  pages = {224-249},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Filinski, Andrzej},
  editor = {Pitt, David H. and Rydeheard, David E. and Dybjer, Peter and Pitts, Andrew M. and Poigné, Axel},
  file = {/Users/pgiarrusso/Zotero/storage/DHUVQJ9Z/BFb0018355.html}
}
% == BibLateX quality report for Filinski1989declarative:
% 'isbn': not a valid ISBN

@incollection{Liang1996modular,
  langid = {english},
  title = {Modular denotational semantics for compiler construction},
  isbn = {978-3-540-61055-7 978-3-540-49942-8},
  url = {http://link.springer.com/chapter/10.1007/3-540-61055-3_39},
  abstract = {We show the benefits of applying modular monadic semantics to compiler construction. Modular monadic semantics allows us to define a language with a rich set of features from reusable building blocks, and use program transformation and equational reasoning to improve code. Compared to denotational semantics, reasoning in monadic style offers the added benefits of highly modularized proofs and more widely applicable results. To demonstrate, we present an axiomatization of environments, and use it to prove the correctness of a well-known compilation technique. The monadic approach also facilitates generating code in various target languages with different sets of built-in features.},
  number = {1058},
  booktitle = {Programming Languages and Systems — ESOP '96},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-19},
  date = {1996-01-01},
  pages = {219-234},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Liang, Sheng and Hudak, Paul},
  editor = {Nielson, Hanne Riis},
  file = {/Users/pgiarrusso/Zotero/storage/3KCEZ7RT/Liang_Hudak - 1996 - Modular denotational semantics for compiler construction.pdf;/Users/pgiarrusso/Zotero/storage/ZCENX96V/3-540-61055-3_39.html}
}
% == BibLateX quality report for Liang1996modular:
% 'isbn': not a valid ISBN

@article{Ilik2013interpretation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.5089},
  primaryClass = {cs, math},
  title = {An interpretation of the Sigma-2 fragment of classical Analysis in System T},
  url = {http://arxiv.org/abs/1301.5089},
  abstract = {We show that it is possible to define a realizability interpretation for the \$$\backslash$Sigma\_2\$-fragment of classical Analysis using G$\backslash$"odel's System T only. This supplements a previous result of Schwichtenberg regarding bar recursion at types 0 and 1 by showing how to avoid using bar recursion altogether. Our result is proved via a conservative extension of System T with an operator for composable continuations from the theory of programming languages due to Danvy and Filinski. The fragment of Analysis is therefore essentially constructive, even in presence of the full Axiom of Choice schema: Weak Church's Rule holds of it in spite of the fact that it is strong enough to refute the formal arithmetical version of Church's Thesis.},
  urldate = {2015-01-21},
  date = {2013-01-22},
  keywords = {Computer Science - Logic in Computer Science,03F25; 03F10; 03F60; 03B30; 03B20; 03E25; 03D65; 68N15,Mathematics - Logic},
  author = {Ilik, Danko},
  file = {/Users/pgiarrusso/Zotero/storage/V3T7QFA9/Ilik - 2013 - An interpretation of the Sigma-2 fragment of classical Analysis in System T.pdf;/Users/pgiarrusso/Zotero/storage/R6JMG3MN/1301.html}
}
% == BibLateX quality report for Ilik2013interpretation:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Hinze2000memo,
  title = {Memo Functions, Polytypically!},
  abstract = {. This paper presents a polytypic implementation of memo  functions that are based on digital search trees. A memo function can  be seen as the composition of a tabulation function that creates a memo  table and a look-up function that queries the table. We show that tabulation  can be derived from look-up by inverse function construction. The  type of memo tables is dened by induction on the structure of argument  types and is parametric with respect to the result type of memo  functions. A memo table for a xed argument type is then a functor and  look-up and tabulation are natural isomorphisms. We provide simple  polytypic proofs of these properties.  1 Introduction  A memo function [11] is like an ordinary function except that it caches previously computed values. If it is applied a second time to a particular argument, it immediately returns the cached result, rather than recomputing it. For storing arguments and results a memo function internally employs an index structure, the ...},
  booktitle = {Proceedings of the 2nd Workshop on Generic Programming, Ponte de},
  date = {2000},
  pages = {17--32},
  author = {Hinze, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/UDW965VN/Hinze - 2000 - Memo Functions, Polytypically!.pdf;/Users/pgiarrusso/Zotero/storage/HM36S2V9/summary.html}
}

@article{Knuth1968semantics,
  langid = {english},
  title = {Semantics of context-free languages},
  volume = {2},
  issn = {0025-5661, 1433-0490},
  url = {http://link.springer.com/article/10.1007/BF01692511},
  doi = {10.1007/BF01692511},
  abstract = {“Meaning” may be assigned to a string in a context-free language by defining “attributes” of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are “synthesized”, i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are “inherited”, i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  number = {2},
  journaltitle = {Mathematical systems theory},
  shortjournal = {Math. Systems Theory},
  urldate = {2015-01-29},
  date = {1968-06-01},
  pages = {127-145},
  keywords = {Computational Mathematics and Numerical Analysis,Theory of Computation},
  author = {Knuth, Donald E.},
  file = {/Users/pgiarrusso/Zotero/storage/SCXS42JI/Knuth - 1968 - Semantics of context-free languages.pdf;/Users/pgiarrusso/Zotero/storage/3JE6DGRP/BF01692511.html}
}
% == BibLateX quality report for Knuth1968semantics:
% 'issn': not a valid ISSN

@article{Hoare1969axiomatic,
  title = {An Axiomatic Basis for Computer Programming},
  volume = {12},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/363235.363259},
  doi = {10.1145/363235.363259},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
  number = {10},
  journaltitle = {Commun. ACM},
  urldate = {2015-01-29},
  date = {1969-10},
  pages = {576--580},
  keywords = {axiomatic method,formal language definition,machine-independent programming,program documentation,programming language design,theory of programming' proofs of programs},
  author = {Hoare, C. A. R.},
  file = {/Users/pgiarrusso/Zotero/storage/PDTZ386S/Hoare - 1969 - An Axiomatic Basis for Computer Programming.pdf}
}
% == BibLateX quality report for Hoare1969axiomatic:
% ? Possibly abbreviated journal title Commun. ACM

@inproceedings{Hess2014automatic,
  location = {{New York, NY, USA}},
  title = {Automatic Locality-friendly Interface Extension of Numerical Functions},
  isbn = {978-1-4503-3161-6},
  url = {http://doi.acm.org/10.1145/2658761.2658772},
  doi = {10.1145/2658761.2658772},
  abstract = {Raising the level of abstraction is a key concern of software engineering, and libraries (either used directly or as a target of a program generation system) are a successful technique to raise programmer productivity and to improve software quality. Unfortunately successful libraries may contain functions that may not be general enough. For example, many numeric performance libraries contain functions that work on one- or higher-dimensional arrays. A problem arises if a program wants to invoke such a function on a non-contiguous subarray (e.g., in C the column of a matrix or a subarray of an image). If the library developer did not foresee this scenario, the client program must include explicit copy steps before and after the library function call, incurring a possibly high performance penalty. A better solution would be an enhanced library function that allows for the desired access pattern. Exposing the access pattern allows the compiler to optimize for the intended usage scenario(s). As we do not want the library developer to generate all interesting versions manually, we present a tool that takes a library function written in C and generates such a customized function for typical accesses. We describe the approach, discuss limitations, and report on the performance. As example access patterns we consider those most common in numerical applications: striding and block striding, general permutations, as well as scaling. We evaluate the tool on various library functions including filters, scans, reductions, sorting, FFTs, and linear algebra operations. The automatically generated custom version is in most cases significantly faster than using individual steps, offering speed-ups that are typically in the range of 1.2-1.8x.},
  booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
  series = {GPCE 2014},
  publisher = {{ACM}},
  urldate = {2015-01-30},
  date = {2014},
  pages = {83--92},
  keywords = {components,interface extension,Libraries,performance,preprocessors,programming language features interaction,software product lines},
  author = {Hess, Benjamin and Gross, Thomas R. and Püschel, Markus},
  file = {/Users/pgiarrusso/Zotero/storage/P4SC999Q/Hess et al - 2014 - Automatic Locality-friendly Interface Extension of Numerical Functions.pdf}
}

@incollection{Bird2008zippy,
  langid = {english},
  title = {Zippy Tabulations of Recursive Functions},
  isbn = {978-3-540-70593-2 978-3-540-70594-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-70594-9_7},
  abstract = {This paper is devoted to the statement and proof of a theorem showing how recursive definitions whose associated call graphs satisfy certain shape conditions can be converted systematically into efficient bottom-up tabulation schemes. The increase in efficiency can be dramatic, typically transforming an exponential time algorithm into one that takes only quadratic time. The proof of the theorem relies heavily on the theory of zips developed by Roland Backhouse and Paul Hoogendijk.},
  number = {5133},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-02-01},
  date = {2008},
  pages = {92-109},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Bird, Richard S.},
  editor = {Audebaud, Philippe and Paulin-Mohring, Christine},
  file = {/Users/pgiarrusso/Zotero/storage/XRSDZTBH/978-3-540-70594-9_7.html}
}
% == BibLateX quality report for Bird2008zippy:
% 'isbn': not a valid ISBN

@article{Chin1995transformation,
  langid = {english},
  title = {A transformation method for dynamic-sized tabulation},
  volume = {32},
  issn = {0001-5903, 1432-0525},
  url = {http://link.springer.com/article/10.1007/BF01177742},
  doi = {10.1007/BF01177742},
  abstract = {Tupling is a transformation tactic to obtain new functions, without redundant calls and/or multiple traversals of common inputs. It achieves this feat by allowing each set (tuple) of function calls to be computed recursively from its previous set. In previous works by Chin and Khoo [8,9], a safe (terminating) fold/unfold transformation algorithm was developed for some classes of functions which are guaranteed to be successfully tupled. However, these classes of functions currently usestatic-sized tables for eliminating the redundant calls. As shown by Richard Bird in [3], there are also other classes of programs whose redundant calls could only be eliminated by usingdynamic-sized tabulation. This paper proposes a new solution to dynamic-sized tabulation by an extension to the tupling tactic. Our extension useslambda abstractions which can be viewed as either dynamic-sized tables or applications of the higher-order generalisation technique to facilitate tupling. Significant speedups could be obtained after the transformed programs were vectorised, as confirmed by experiment.},
  number = {2},
  journaltitle = {Acta Informatica},
  shortjournal = {Acta Informatica},
  urldate = {2015-02-01},
  date = {1995-02-01},
  pages = {93-115},
  keywords = {Computational Mathematics and Numerical Analysis,Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Information Systems and Communication Service,Software Engineering/Programming and Operating Systems,Theory of Computation},
  author = {Chin, Wei-Ngan and Hagiya, Masami},
  file = {/Users/pgiarrusso/Zotero/storage/AKR6JPI8/Chin_Hagiya - 1995 - A transformation method for dynamic-sized tabulation.pdf;/Users/pgiarrusso/Zotero/storage/BF786SDI/10.html}
}
% == BibLateX quality report for Chin1995transformation:
% 'issn': not a valid ISSN

@article{Bird1980tabulation,
  title = {Tabulation Techniques for Recursive Programs},
  volume = {12},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/356827.356831},
  doi = {10.1145/356827.356831},
  number = {4},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2015-02-01},
  date = {1980-12},
  pages = {403--417},
  author = {Bird, R. S.}
}
% == BibLateX quality report for Bird1980tabulation:
% ? Possibly abbreviated journal title ACM Comput. Surv.

@article{Goltz2014design,
  langid = {english},
  title = {Design for future: managed software evolution},
  issn = {1865-2034, 1865-2042},
  url = {http://link.springer.com/article/10.1007/s00450-014-0273-9},
  doi = {10.1007/s00450-014-0273-9},
  shorttitle = {Design for future},
  abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future—Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
  journaltitle = {Computer Science - Research and Development},
  shortjournal = {Comput Sci Res Dev},
  urldate = {2015-02-10},
  date = {2014-10-08},
  pages = {1-11},
  keywords = {Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Software Engineering/Programming and Operating Systems,Theory of Computation,Co-evolution,Computer Hardware,Computer Science; general,Design; maintenance and operation,Knowledge carrying software,Legacy systems,Software life cycle},
  author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and Märtin, Lukas and Vogel-Heuser, Birgit},
  file = {/Users/pgiarrusso/Zotero/storage/ZGZZK78F/s00450-014-0273-9.html}
}
% == BibLateX quality report for Goltz2014design:
% 'issn': not a valid ISSN

@article{Hoare2009viewpointretrospective,
  langid = {english},
  title = {ViewpointRetrospective: an axiomatic basis for computer programming},
  volume = {52},
  issn = {00010782},
  url = {http://cacm.acm.org/magazines/2009/10/42360-retrospective-an-axiomatic-basis-for-computer-programming/fulltext},
  doi = {10.1145/1562764.1562779},
  shorttitle = {ViewpointRetrospective},
  number = {10},
  journaltitle = {Communications of the ACM},
  urldate = {2015-02-11},
  date = {2009-10-01},
  pages = {30},
  author = {Hoare, C.A.R.},
  file = {/Users/pgiarrusso/Zotero/storage/ZEW92I7C/fulltext.html}
}

@inproceedings{Sheard2001generic,
  location = {{New York, NY, USA}},
  title = {Generic Unification via Two-level Types and Parameterized Modules},
  isbn = {1-58113-415-0},
  url = {http://doi.acm.org/10.1145/507635.507648},
  doi = {10.1145/507635.507648},
  abstract = {As a functional pearl, we describe an efficient, modularized
implementation of unification using the state of mutable reference
cells to encode substitutions. We abstract our algorithms along two
dimensions, first abstracting away from the structure of the terms
to be unified, and second over the monad in which the mutable state
is encapsulated. We choose this example to illustrate two important
techniques that we believe many functional programmers would find
useful. The first of these is the definition of recursive data
types using two levels: a structure defining level, and a recursive
knot-tying level. The second is the use of rank-2 polymorphism
inside Haskell's record types to implement a form of type
parameterized modules.},
  booktitle = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '01},
  publisher = {{ACM}},
  urldate = {2015-02-16},
  date = {2001},
  pages = {86--97},
  keywords = {unification,generic programs,parameterized modules},
  author = {Sheard, Tim},
  file = {/Users/pgiarrusso/Zotero/storage/JICHHCQW/Sheard - 2001 - Generic Unification via Two-level Types and Parame.pdf}
}

@article{Jansson1998polytypic,
  title = {Polytypic unification},
  volume = {8},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S095679689800313X},
  doi = {null},
  abstract = {Unification, or two-way pattern matching, is the process of solving an equation involving two first-order terms with variables. Unification is used in type inference in many programming languages and in the execution of logic programs. This means that unification algorithms have to be written over and over again for different term types. Many other functions also make sense for a large class of datatypes; examples are pretty printers, equality checks, maps etc. They can be defined by induction on the structure of user-defined datatypes. Implementations of these functions for different datatypes are closely related to the structure of the datatypes. We call such functions polytypic. This paper describes a unification algorithm parametrised on the type of the terms, and shows how to use polytypism to obtain a unification algorithm that works for all regular term types.},
  number = {05},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-02-16},
  date = {1998-09},
  pages = {527--536},
  author = {Jansson, Patrik and Jeuring, Johan},
  file = {/Users/pgiarrusso/Zotero/storage/487B9FKV/Jansson_Jeuring - 1998 - Polytypic unification.pdf;/Users/pgiarrusso/Zotero/storage/9QEK28N3/Jansson and Jeuring - 1998 - Polytypic unification.html}
}

@article{Letia2009crdts,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0907.0929},
  primaryClass = {cs},
  title = {CRDTs: Consistency without concurrency control},
  url = {http://arxiv.org/abs/0907.0929},
  shorttitle = {CRDTs},
  abstract = {A CRDT is a data type whose operations commute when they are concurrent. Replicas of a CRDT eventually converge without any complex concurrency control. As an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer called Treedoc. We outline the design, implementation and performance of Treedoc. We discuss how the CRDT concept can be generalised, and its limitations.},
  urldate = {2015-02-16},
  date = {2009-07-06},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  author = {Letia, Mihai and Preguiça, Nuno and Shapiro, Marc},
  file = {/Users/pgiarrusso/Zotero/storage/GRRZM4EJ/Letia et al - 2009 - CRDTs - Consistency without concurrency control.pdf;/Users/pgiarrusso/Zotero/storage/IV4NDBRH/0907.html}
}
% == BibLateX quality report for Letia2009crdts:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Schroeder-Heister2014prooftheoretic,
  title = {Proof-Theoretic Semantics},
  edition = {Summer 2014},
  url = {http://plato.stanford.edu/archives/sum2014/entries/proof-theoretic-semantics/},
  abstract = {Proof-theoretic semantics is an alternative to truth-conditionsemantics. It is based on the fundamental assumption that the centralnotion in terms of which meanings are assigned to certain expressionsof our language, in particular to logical constants, is that ofproof rather than truth. In this senseproof-theoretic semantics is semantics in terms of proof .Proof-theoretic semantics also means the semantics of proofs,i.e., the semantics of entities which describe how we arrive at certainassertions given certain assumptions. Both aspects of proof-theoreticsemantics can be intertwined, i.e. the semantics of proofs is itselfoften given in terms of proofs., Proof-theoretic semantics has several roots, the most specific onebeing Gentzen's remarks that the introduction rules in hiscalculus of natural deduction define the meanings of logical constants,while the elimination rules can be obtained as a consequence of thisdefinition (see section 2.2.1). Morebroadly, it belongs to what Prawitz called general prooftheory (see section 1.1). Even morebroadly, it is part of the tradition according to which the meaning ofa term should be explained by reference to the way it is usedin our language., Within philosophy, proof-theoretic semantics has mostly figuredunder the heading “theory of meaning”. This terminologyfollows Dummett, who claimed that the theory of meaning is the basis oftheoretical philosophy, a view which he attributed to Frege. The term“proof-theoretic semantics” was proposed bySchroeder-Heister (1991; used already in 1987 lectures in Stockholm) in order not to leave the term“semantics” to denotationalism alone—after all,“semantics” is the standard term for investigations dealingwith the meaning of linguistic expressions. Furthermore, unlike“theory of meaning”, the term “proof-theoreticsemantics” covers philosophical and technical aspects likewise.In 1999, the first conference with this title took place inTübingen.},
  booktitle = {The Stanford Encyclopedia of Philosophy},
  urldate = {2015-02-16},
  date = {2014},
  keywords = {type theory,category theory,connectives: sentence connectives in formal logic,Curry's paradox,Hilbert; David: program in the foundations of mathematics,logic; history of: intuitionistic logic,logic: classical,logic: intuitionistic,logic: linear,logic: substructural,logical constants,mathematics; philosophy of: intuitionism,paradoxes: and contemporary logic,proof theory: development of,realism: challenges to metaphysical,Russell's paradox,self-reference,truth: revision theory of},
  author = {Schroeder-Heister, Peter},
  editor = {Zalta, Edward N.},
  file = {/Users/pgiarrusso/Zotero/storage/C96NZ6FN/proof-theoretic-semantics.html}
}

@inproceedings{Oliveira2008scala,
  location = {{New York, NY, USA}},
  title = {Scala for Generic Programmers},
  isbn = {978-1-60558-060-9},
  url = {http://doi.acm.org/10.1145/1411318.1411323},
  doi = {10.1145/1411318.1411323},
  abstract = {Datatype-generic programming involves parametrization by the shape of data, in the form of type constructors such as "list of". Most approaches to datatype-generic programming are developed in the lazy functional programming language Haskell. We argue that the functional object-oriented language Scala is in many ways a better setting. Not only does Scala provide equivalents of all the necessary functional programming features (such parametric polymorphism, higher-order functions, higher-kinded type operations, and type- and constructor-classes), but it also provides the most useful features of object-oriented languages (such as subtyping, overriding, traditional single inheritance, and multiple inheritance in the form of traits). We show how this combination of features benefits datatype-generic programming, using three different approaches as illustrations.},
  booktitle = {Proceedings of the ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '08},
  publisher = {{ACM}},
  urldate = {2015-02-19},
  date = {2008},
  pages = {25--36},
  keywords = {Datatype-generic programming,polytypic programming,Scala},
  author = {Oliveira, Bruno C.d.S. and Gibbons, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/DHCQVPVZ/Oliveira_Gibbons - 2008 - Scala for Generic Programmers.pdf}
}

@incollection{Gibbons2007datatypegeneric,
  langid = {english},
  title = {Datatype-Generic Programming},
  isbn = {978-3-540-76785-5 978-3-540-76786-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-76786-2_1},
  abstract = {Generic programming aims to increase the flexibility of programming languages, by expanding the possibilities for parametrization — ideally, without also expanding the possibilities for uncaught errors. The term means different things to different people: parametric polymorphism, data abstraction, meta-programming, and so on. We use it to mean polytypism, that is, parametrization by the shape of data structures rather than their contents. To avoid confusion with other uses, we have coined the qualified term datatype-generic programming for this purpose. In these lecture notes, we expand on the definition of datatype-generic programming, and present some examples of datatype-generic programs. We also explore the connection with design patterns in object-oriented programming; in particular, we argue that certain design patterns are just higher-order datatype-generic programs.},
  number = {4719},
  booktitle = {Datatype-Generic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-02-19},
  date = {2007},
  pages = {1-71},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Data Structures},
  author = {Gibbons, Jeremy},
  editor = {Backhouse, Roland and Gibbons, Jeremy and Hinze, Ralf and Jeuring, Johan},
  file = {/Users/pgiarrusso/Zotero/storage/52R99RZS/Gibbons - 2007 - Datatype-Generic Programming.pdf;/Users/pgiarrusso/Zotero/storage/Z72KWSD8/978-3-540-76786-2_1.html}
}
% == BibLateX quality report for Gibbons2007datatypegeneric:
% 'isbn': not a valid ISBN

@inproceedings{Kagawa2006polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Variants in Haskell},
  isbn = {1-59593-489-8},
  url = {http://doi.acm.org/10.1145/1159842.1159848},
  doi = {10.1145/1159842.1159848},
  abstract = {In languages that support polymorphic variants, a single variant value can be passed to many contexts that accept different sets of constructors. Polymorphic variants can be used in order to introduce extensible algebraic datatypes into functional programming languages and are potentially useful for application domains such as interpreters, graphical user interface (GUI) libraries and database interfaces, where the number of necessary constructors cannot be determined in advance. Very few functional languages, however, have a mechanism to extend existing datatypes by adding new constructors. In general, for polymorphic variants to be useful, we would need some mechanisms to reuse existing functions and extend them for new constructors.Actually, the type system of Haskell, when extended with parametric type classes (or multi-parameter type classes with functional dependencies), has enough power not only to mimic polymorphic variants but also to extend existing functions for new constructors.This paper, first, explains how to do this in Haskell's type system (Haskell 98 with popular extensions). However, this encoding of polymorphic variants is difficult to use in practice. This is because it is quite tedious for programmers to write mimic codes by hand and because the problem of ambiguous overloading resolution would embarrass programmers. Therefore, the paper proposes an extension of Haskell's type classes that supports polymorphic variants directly. It has a novel form of instance declarations where records and variants are handled symmetrically.This type system can produce vanilla Haskell codes as a result of type inference. Therefore it behaves as a preprocessor which translates the extended language into plain Haskell. Programmers would be able to use polymorphic variants without worrying nasty problems such as ambiguities.},
  booktitle = {Proceedings of the 2006 ACM SIGPLAN Workshop on Haskell},
  series = {Haskell '06},
  publisher = {{ACM}},
  urldate = {2015-02-19},
  date = {2006},
  pages = {37--47},
  keywords = {Haskell,extensibility,polymorphic variants,type classes},
  author = {Kagawa, Koji},
  file = {/Users/pgiarrusso/Zotero/storage/ZUCRQV63/Kagawa - 2006 - Polymorphic Variants in Haskell.pdf}
}

@article{Garrigue2000code,
  title = {Code reuse through polymorphic variants},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.8079},
  abstract = {Their support for code reuse has made object-oriented languages popular. However,

they do not succeed equally in all areas, particularly when data has a complex

structure, making hard to keep the parallel between data and code.

On the other hand, functional programming languages, which separate data from

code, are better at handling complex structures, but they do not provide direct ways

to reuse code for a different datatype.

We show here a way to achieve code reuse, even when data and...},
  urldate = {2015-02-20},
  date = {2000},
  keywords = {polymorphism,extensibility,expression-problem,polymorphic-variants},
  author = {Garrigue, J},
  file = {/Users/pgiarrusso/Zotero/storage/NRHN748W/Garrigue - 2000 - Code reuse through polymorphic variants.pdf}
}
% == BibLateX quality report for Garrigue2000code:
% Missing required field 'journaltitle'

@article{Garrigue1998programming,
  title = {Programming with polymorphic variants},
  url = {http://citeseer.ist.psu.edu/garrigue98programming.html},
  abstract = {Type inference for structural polymorphism ---i.e. record and variant polymorphism--- has been an active area of research since more than 10 years ago, and many results have been obtained. However these results are yet to be applied to real programming languages. Based on our experience with the Objective Label system, we describe how variant polymorphism can be integrated in a programming language, and what are the benefits. We give a detailed account of our type inference and compilation...},
  urldate = {2015-02-20},
  date = {1998},
  keywords = {polymorphism,polymorphic-variants,pattern-matching,structural-types,subtyping,type-inference},
  author = {Garrigue, J},
  file = {/Users/pgiarrusso/Zotero/storage/AJFC9WAT/Garrigue - 1998 - Programming with polymorphic variants.pdf}
}
% == BibLateX quality report for Garrigue1998programming:
% Missing required field 'journaltitle'

@article{Krivine2011realizability,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1005.2395},
  title = {Realizability algebras: a program to well order R},
  volume = {7},
  issn = {18605974},
  url = {http://arxiv.org/abs/1005.2395},
  doi = {10.2168/LMCS-7(3:2)2011},
  shorttitle = {Realizability algebras},
  abstract = {The theory of classical realizability is a framework in which we can develop the proof-program correspondence. Using this framework, we show how to transform into programs the proofs in classical analysis with dependent choice and the existence of a well ordering of the real line. The principal tools are: The notion of realizability algebra, which is a three-sorted variant of the well known combinatory algebra of Curry. An adaptation of the method of forcing used in set theory to prove consistency results. Here, it is used in another way, to obtain programs associated with a well ordering of R and the existence of a non trivial ultrafilter on N.},
  number = {3},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2015-02-22},
  date = {2011-08-09},
  keywords = {Computer Science - Logic in Computer Science,F.4.1},
  author = {Krivine, Jean-Louis},
  file = {/Users/pgiarrusso/Zotero/storage/EBUXIE2A/Krivine - 2011 - Realizability algebras - a program to well order R.pdf;/Users/pgiarrusso/Zotero/storage/WDDG65MU/1005.html}
}
% == BibLateX quality report for Krivine2011realizability:
% Unexpected field 'archivePrefix'

@article{Ramsey2006ml,
  title = {ML Module Mania: A Type-Safe, Separately Compiled, Extensible Interpreter},
  volume = {148},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106001319},
  doi = {10.1016/j.entcs.2005.11.045},
  shorttitle = {ML Module Mania},
  abstract = {To illustrate the utility of a powerful modules language, this paper presents the embedded interpreter Lua-ML. The interpreter combines extensibility and separate compilation without compromising type safety. Its types are extended by applying a sum constructor to built-in types and to extensions, then tying a recursive knot using a two-level type; the sum constructor is written using an ML functor. The initial basis is extended by composing initialization functions from individual extensions, also using ML functors.},
  number = {2},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the ACM-SIGPLAN Workshop on ML (ML 2005) ACM-SIGPLAN Workshop on ML 2005},
  urldate = {2015-02-22},
  date = {2006-03-24},
  pages = {181-209},
  keywords = {ML modules,embedded interpreters,extensible interpreters,higher-order functors,scripting languages},
  author = {Ramsey, Norman},
  file = {/Users/pgiarrusso/Zotero/storage/5VDIJV9T/Ramsey - 2006 - ML Module Mania - A Type-Safe, Separately Compiled, Extensible Interpreter.pdf;/Users/pgiarrusso/Zotero/storage/WJ5V4I3S/S1571066106001319.html}
}

@incollection{Meertens1996calculate,
  langid = {english},
  title = {Calculate polytypically!},
  isbn = {978-3-540-61756-3 978-3-540-70654-0},
  url = {http://link.springer.com/chapter/10.1007/3-540-61756-6_73},
  abstract = {A polytypic function definition is a function definition that is parametrised with a datatype. It embraces a class of algorithms. As an example we define a simple polytypic “crush” combinator that can be used to calculate polytypically. The ability to define functions polytypically adds another level of flexibility in the reusability of programming idioms and in the design of libraries of interoperable components.},
  number = {1140},
  booktitle = {Programming Languages: Implementations, Logics, and Programs},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-02-22},
  date = {1996},
  pages = {1-16},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Meertens, Lambert},
  editor = {Kuchen, Herbert and Swierstra, S. Doaitse},
  file = {/Users/pgiarrusso/Zotero/storage/IG5WGPDK/Meertens - 1996 - Calculate polytypically!.pdf;/Users/pgiarrusso/Zotero/storage/6S938XKD/3-540-61756-6_73.html}
}
% == BibLateX quality report for Meertens1996calculate:
% 'isbn': not a valid ISBN

@incollection{Lammel2008expression,
  langid = {english},
  title = {The Expression Lemma},
  isbn = {978-3-540-70593-2 978-3-540-70594-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-70594-9_12},
  abstract = {Algebraic data types and catamorphisms (folds) play a central role in functional programming as they allow programmers to define recursive data structures and operations on them uniformly by structural recursion. Likewise, in object-oriented (OO) programming, recursive hierarchies of object types with virtual methods play a central role for the same reason. There is a semantical correspondence between these two situations which we reveal and formalize categorically. To this end, we assume a coalgebraic model of OO programming with functional objects. The development may be helpful in deriving refactorings that turn sufficiently disciplined functional programs into OO programs of a designated shape and vice versa.},
  number = {5133},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-02-22},
  date = {2008},
  pages = {193-219},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages,catamorphism,cofree comonad,distributive law,expression lemma,expression problem,fold,free monad,functional object,program calculation,the composite design pattern},
  author = {Lämmel, Ralf and Rypacek, Ondrej},
  editor = {Audebaud, Philippe and Paulin-Mohring, Christine},
  file = {/Users/pgiarrusso/Zotero/storage/I4ESCXQT/978-3-540-70594-9_12.html}
}
% == BibLateX quality report for Lammel2008expression:
% 'isbn': not a valid ISBN

@article{Buchlovsky2006typetheoretic,
  title = {A Type-theoretic Reconstruction of the Visitor Pattern},
  volume = {155},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066106001988},
  doi = {10.1016/j.entcs.2005.11.061},
  abstract = {In object-oriented languages, the Visitor pattern can be used to traverse tree-like data structures: a visitor object contains some operations, and the data structure objects allow themselves to be traversed by accepting visitors. In the polymorphic lambda calculus (System F), tree-like data structures can be encoded as polymorphic higher-order functions. In this paper, we reconstruct the Visitor pattern from the polymorphic encoding by way of generics in Java. We sketch how the quantified types in the polymorphic encoding can guide reasoning about visitors in general.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the 21st Annual Conference on Mathematical Foundations of Programming Semantics (MFPS XXI) Mathematical Foundations of Programming Semantics XXI},
  urldate = {2015-02-23},
  date = {2006-05-12},
  pages = {309-329},
  keywords = {polymorphic types,Generic Java,object-oriented programming,visitor pattern},
  author = {Buchlovsky, Peter and Thielecke, Hayo},
  file = {/Users/pgiarrusso/Zotero/storage/A29E5GSZ/Buchlovsky_Thielecke - 2006 - A Type-theoretic Reconstruction of the Visitor Pattern.pdf;/Users/pgiarrusso/Zotero/storage/9ECRT28M/S1571066106001988.html}
}

@article{Havelund2000model,
  langid = {english},
  title = {Model checking JAVA programs using JAVA PathFinder},
  volume = {2},
  issn = {1433-2779},
  url = {http://link.springer.com/article/10.1007/s100090050043},
  doi = {10.1007/s100090050043},
  abstract = {This paper describes a translator called Java PathFinder (Jpf), which translates from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated into similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA’s areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi-threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications.},
  number = {4},
  journaltitle = {International Journal on Software Tools for Technology Transfer},
  shortjournal = {STTT},
  urldate = {2015-02-23},
  date = {2000-03-01},
  pages = {366-381},
  keywords = {Key words: Program verification – Java – Model checking – Spin – Concurrent programming – Assertions – Deadlocks},
  author = {Havelund, Klaus and Pressburger, Thomas},
  file = {/Users/pgiarrusso/Zotero/storage/B9X3Z3XS/Havelund_Pressburger - 2000 - Model checking JAVA programs using JAVA PathFinder.pdf;/Users/pgiarrusso/Zotero/storage/8BKMPSEE/s100090050043.html}
}

@inproceedings{Hinze2015conjugate,
  location = {{New York, NY, USA}},
  title = {Conjugate Hylomorphisms – Or: The Mother of All Structured Recursion Schemes},
  isbn = {978-1-4503-3300-9},
  url = {http://doi.acm.org/10.1145/2676726.2676989},
  doi = {10.1145/2676726.2676989},
  shorttitle = {Conjugate Hylomorphisms – Or},
  abstract = {The past decades have witnessed an extensive study of structured recursion schemes. A general scheme is the hylomorphism, which captures the essence of divide-and-conquer: a problem is broken into sub-problems by a coalgebra; sub-problems are solved recursively; the sub-solutions are combined by an algebra to form a solution. In this paper we develop a simple toolbox for assembling recursive coalgebras, which by definition ensure that their hylo equations have unique solutions, whatever the algebra. Our main tool is the conjugate rule, a generic rule parametrized by an adjunction and a conjugate pair of natural transformations. We show that many basic adjunctions induce useful recursion schemes. In fact, almost every structured recursion scheme seems to arise as an instance of the conjugate rule. Further, we adapt our toolbox to the more expressive setting of parametrically recursive coalgebras, where the original input is also passed to the algebra. The formal development is complemented by a series of worked-out examples in Haskell.},
  booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '15},
  publisher = {{ACM}},
  urldate = {2015-02-24},
  date = {2015},
  pages = {527--538},
  keywords = {adjunctions,hylomorphisms,recursion schemes},
  author = {Hinze, Ralf and Wu, Nicolas and Gibbons, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/7S3STZX6/Hinze et al - 2015 - Conjugate Hylomorphisms – Or - The Mother of All Structured Recursion Schemes.pdf}
}

@article{Swierstra2008data,
  title = {Data types à la carte},
  volume = {18},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796808006758},
  doi = {10.1017/S0956796808006758},
  abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell's monolithic IO monad.},
  number = {04},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-02-26},
  date = {2008-07},
  pages = {423--436},
  author = {Swierstra, Wouter},
  file = {/Users/pgiarrusso/Zotero/storage/QTHKZ6NC/displayAbstract.html}
}

@article{OConnor2011functor,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1103.2841},
  primaryClass = {cs},
  title = {Functor is to Lens as Applicative is to Biplate: Introducing Multiplate},
  url = {http://arxiv.org/abs/1103.2841},
  shorttitle = {Functor is to Lens as Applicative is to Biplate},
  abstract = {This paper gives two new categorical characterisations of lenses: one as a coalgebra of the store comonad, and the other as a monoidal natural transformation on a category of a certain class of coalgebras. The store comonad of the first characterisation can be generalized to a Cartesian store comonad, and the coalgebras of this Cartesian store comonad turn out to be exactly the Biplates of the Uniplate generic programming library. On the other hand, the monoidal natural transformations on functors can be generalized to work on a category of more specific coalgebras. This generalization turns out to be the type of compos from the Compos generic programming library. A theorem, originally conjectured by van Laarhoven, proves that these two generalizations are isomorphic, thus the core data types of the Uniplate and Compos libraries supporting generic program on single recursive types are the same. Both the Uniplate and Compos libraries generalize this core functionality to support mutually recursive types in different ways. This paper proposes a third extension to support mutually recursive data types that is as powerful as Compos and as easy to use as Uniplate. This proposal, called Multiplate, only requires rank 3 polymorphism in addition to the normal type class mechanism of Haskell.},
  urldate = {2015-02-26},
  date = {2011-03-15},
  keywords = {Computer Science - Programming Languages},
  author = {O'Connor, Russell},
  file = {/Users/pgiarrusso/Zotero/storage/E9GAUAJI/O'Connor - 2011 - Functor is to Lens as Applicative is to Biplate - Introducing Multiplate.pdf;/Users/pgiarrusso/Zotero/storage/ZUDXQ4J2/1103.html}
}
% == BibLateX quality report for OConnor2011functor:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@online{Beringerintroduction,
  title = {An introduction to fibrations, topos theory, the effective topos and modest sets},
  url = {http://www.lfcs.inf.ed.ac.uk/reports/92/ECS-LFCS-92-208/},
  urldate = {2015-02-27},
  author = {Beringer, Lennart},
  file = {/Users/pgiarrusso/Zotero/storage/ZXX36J37/ECS-LFCS-92-208.html}
}
% == BibLateX quality report for Beringerintroduction:
% Exactly one of 'date' / 'year' must be present

@article{Krishnamurthi2015real,
  title = {The Real Software Crisis: Repeatability As a Core Value},
  volume = {58},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/2658987},
  doi = {10.1145/2658987},
  shorttitle = {The Real Software Crisis},
  abstract = {Sharing experiences running artifact evaluation committees for five major conferences.},
  number = {3},
  journaltitle = {Commun. ACM},
  urldate = {2015-03-02},
  date = {2015-02},
  pages = {34--36},
  author = {Krishnamurthi, Shriram and Vitek, Jan}
}
% == BibLateX quality report for Krishnamurthi2015real:
% ? Possibly abbreviated journal title Commun. ACM

@inproceedings{Hinze2011categorical,
  location = {{New York, NY, USA}},
  title = {Towards a Categorical Foundation for Generic Programming},
  isbn = {978-1-4503-0861-8},
  url = {http://doi.acm.org/10.1145/2036918.2036926},
  doi = {10.1145/2036918.2036926},
  abstract = {Generic Haskell is an extension of Haskell that supports datatype generic programming. The central idea of Generic Haskell is to interpret a type by a function, the so-called instance of a generic function at that type. Since types in Haskell include parametric types such as 'list of', Generic Haskell represents types by terms of the simply-typed lambda calculus. This paper puts the idea of interpreting types as functions on a firm theoretical footing, exploiting the fact that the simply-typed lambda calculus can be interpreted in a cartesian closed category. We identify a suitable target category, a subcategory of Cat, and argue that slice, coslice and comma categories are a good fit for interpreting generic functions at base types.},
  booktitle = {Proceedings of the Seventh ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '11},
  publisher = {{ACM}},
  urldate = {2015-03-03},
  date = {2011},
  pages = {47--58},
  keywords = {generic programming,category theory,comma category,slice category},
  author = {Hinze, Ralf and Wu, Nicolas},
  file = {/Users/pgiarrusso/Zotero/storage/A9IAUGP3/Hinze_Wu - 2011 - Towards a Categorical Foundation for Generic Programming.pdf}
}

@inproceedings{Bhatotia2015ithreads,
  location = {{New York, NY, USA}},
  title = {iThreads: A Threading Library for Parallel Incremental Computation},
  isbn = {978-1-4503-2835-7},
  url = {http://doi.acm.org/10.1145/2694344.2694371},
  doi = {10.1145/2694344.2694371},
  shorttitle = {iThreads},
  abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
  booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
  series = {ASPLOS '15},
  publisher = {{ACM}},
  urldate = {2015-03-03},
  date = {2015},
  pages = {645--659},
  keywords = {memoization,concurrent dynamic dependence graph (CDDG),incremental computation,release consistency (RC) memory model,self-adjusting computation,shared-memory multithreading},
  author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Björn B. and Rodrigues, Rodrigo}
}

@incollection{Oliveira2012extensibility,
  langid = {english},
  title = {Extensibility for the Masses},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-31057-7_2},
  abstract = {This paper presents a new solution to the expression problem (EP) that works in OO languages with simple generics (including Java or C\#). A key novelty of this solution is that advanced typing features, including F-bounded quantification, wildcards and variance annotations, are not needed. The solution is based on object algebras, which are an abstraction closely related to algebraic datatypes and Church encodings. Object algebras also have much in common with the traditional forms of the Visitor pattern, but without many of its drawbacks: they are extensible, remove the need for accept methods, and do not compromise encapsulation. We show applications of object algebras that go beyond toy examples usually presented in solutions for the expression problem. In the paper we develop an increasingly more complex set of features for a mini-imperative language, and we discuss a real-world application of object algebras in an implementation of remote batches. We believe that object algebras bring extensibility to the masses: object algebras work in mainstream OO languages, and they significantly reduce the conceptual overhead by using only features that are used by everyday programmers.},
  number = {7313},
  booktitle = {ECOOP 2012 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-03},
  date = {2012},
  pages = {2-27},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Computer Communication Networks},
  author = {Oliveira, Bruno C. d S. and Cook, William R.},
  editor = {Noble, James},
  file = {/Users/pgiarrusso/Zotero/storage/ZAGJ2725/978-3-642-31057-7_2.html}
}
% == BibLateX quality report for Oliveira2012extensibility:
% 'isbn': not a valid ISBN

@inproceedings{Delaware2013modular,
  location = {{New York, NY, USA}},
  title = {Modular Monadic Meta-theory},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500587},
  doi = {10.1145/2500365.2500587},
  abstract = {This paper presents 3MT, a framework for modular mechanized meta-theory of languages with effects. Using 3MT, individual language features and their corresponding definitions -- semantic functions, theorem statements and proofs-- can be built separately and then reused to create different languages with fully mechanized meta-theory. 3MT combines modular datatypes and monads to define denotational semantics with effects on a per-feature basis, without fixing the particular set of effects or language constructs. One well-established problem with type soundness proofs for denotational semantics is that they are notoriously brittle with respect to the addition of new effects. The statement of type soundness for a language depends intimately on the effects it uses, making it particularly challenging to achieve modularity. 3MT solves this long-standing problem by splitting these theorems into two separate and reusable parts: a feature theorem that captures the well-typing of denotations produced by the semantic function of an individual feature with respect to only the effects used, and an effect theorem that adapts well-typings of denotations to a fixed superset of effects. The proof of type soundness for a particular language simply combines these theorems for its features and the combination of their effects. To establish both theorems, 3MT uses two key reasoning techniques: modular induction and algebraic laws about effects. Several effectful language features, including references and errors, illustrate the capabilities of 3MT. A case study reuses these features to build fully mechanized definitions and proofs for 28 languages, including several versions of mini-ML with effects.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2015-03-03},
  date = {2013},
  pages = {319--330},
  keywords = {Monads,modularity,mechanized meta-theory,side-effects},
  author = {Delaware, Benjamin and Keuchel, Steven and Schrijvers, Tom and Oliveira, Bruno C.d.S.},
  file = {/Users/pgiarrusso/Zotero/storage/THJCME5Z/Delaware et al - 2013 - Modular Monadic Meta-theory.pdf}
}

@article{Reynolds1993functors,
  title = {On Functors Expressible in the Polymorphic Typed Lambda Calculus},
  volume = {105},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540183710370},
  doi = {10.1006/inco.1993.1037},
  abstract = {Given a model of the polymorphic typed lambda calculus based upon a Cartesian closed category K, there will be functors from K to K whose action on objects can be expressed by type expressions and whose action on morphisms can be expressed by ordinary expressions. We show that if T is such a functor then there is a weak initial T-algebra and if, in addition, K possesses equalizers of all subsets of its morphism sets, then there is an initial T-algebra. These results are used to establish the impossibility of certain models, including those in which types denote sets and S → S′ denotes the set of all functions from S to S′.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2015-03-03},
  date = {1993-07},
  pages = {1-29},
  author = {Reynolds, J. C. and Plotkin, G. D.},
  file = {/Users/pgiarrusso/Zotero/storage/X8R4FCBI/Reynolds_Plotkin - 1993 - On Functors Expressible in the Polymorphic Typed Lambda Calculus.pdf;/Users/pgiarrusso/Zotero/storage/JZIZ8GUJ/S0890540183710370.html}
}

@article{Parnas1972criteria,
  title = {On the Criteria to Be Used in Decomposing Systems into Modules},
  volume = {15},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/361598.361623},
  doi = {10.1145/361598.361623},
  abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
  number = {12},
  journaltitle = {Commun. ACM},
  urldate = {2015-03-04},
  date = {1972-12},
  pages = {1053--1058},
  keywords = {Software Engineering,modularity,KWIC index,modules,software,software design},
  author = {Parnas, D. L.},
  file = {/Users/pgiarrusso/Zotero/storage/7FSG4X7J/Parnas - 1972 - On the Criteria to Be Used in Decomposing Systems into Modules.pdf}
}
% == BibLateX quality report for Parnas1972criteria:
% ? Possibly abbreviated journal title Commun. ACM

@incollection{Bruce1997comparing,
  langid = {english},
  title = {Comparing object encodings},
  isbn = {978-3-540-63388-4 978-3-540-69530-1},
  url = {http://link.springer.com/chapter/10.1007/BFb0014561},
  abstract = {Recent years have seen the development of several foundational models for statically typed object-oriented programming. But despite their intuitive similarity, differences in the technical machinery used to formulate the various proposals have made them difficult to compare. Using the typed lambda-calculus F $<$ ω : as a common basis, we now offer a detailed comparison of four models: (1) a recursive-record encoding similar to the ones used by Cardelli [Car84], Reddy [Red88, KR94], Cook [Coo89, CHC90], and others; (2) Hofmann, Pierce, and Turner's existential encoding [PT94, HP95]; (3) Bruce's model based on existential and recursive types [Bru94]; and (4) Abadi, Cardelli, and Viswanathan's type-theoretic encoding [ACV96] of a calculus of primitive objects.},
  number = {1281},
  booktitle = {Theoretical Aspects of Computer Software},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-05},
  date = {1997},
  pages = {415-438},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering/Programming and Operating Systems},
  author = {Bruce, Kim B. and Cardelli, Luca and Pierce, Benjamin C.},
  editor = {Abadi, Martín and Ito, Takayasu},
  file = {/Users/pgiarrusso/Zotero/storage/7WPSSSS4/Bruce et al - 1997 - Comparing object encodings.pdf;/Users/pgiarrusso/Zotero/storage/H2FB85RZ/BFb0014561.html}
}
% == BibLateX quality report for Bruce1997comparing:
% 'isbn': not a valid ISBN

@inproceedings{Pitts1997operationallybased,
  title = {Operationally-based theories of program equivalence},
  abstract = {null},
  booktitle = {Semantics and Logics of Computation},
  publisher = {{Cambridge University Press}},
  date = {1997},
  pages = {241--298},
  author = {Pitts, Andrew},
  file = {/Users/pgiarrusso/Zotero/storage/U26WJA2N/Pitts - 1997 - Operationally-based theories of program equivalence.pdf;/Users/pgiarrusso/Zotero/storage/EQPE97EN/summary.html}
}
% == BibLateX quality report for Pitts1997operationallybased:
% ? Unsure about the formatting of the booktitle

@article{Gallier1995proving,
  title = {Proving properties of typed λ-terms using realizability, covers, and sheaves},
  volume = {142},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397594002800},
  doi = {10.1016/0304-3975(94)00280-0},
  abstract = {The main purpose of this paper is to take apart the reducibility method in order to understand how its pieces fit together, and in particular, to recast the conditions on candidates of reducibility as sheaf conditions. There has been a feeling among experts on this subject that it should be possible to present the reducibility method using more semantic means, and that a deeper understanding would then be gained. This paper gives mathematical substance to this feeling, by presenting a generalization of the reducibility method based on a semantic notion of realizability which uses the notion of a cover algebra (as in abstract sheaf theory). A key technical ingredient is the introduction of a new class of semantic structures equipped with preorders, called pre-applicative structures. These structures need not be extensional. In this framework, a general realizability theorem can be shown. Kleene's recursive realizability and a variant of Kreisel's modified realizability both fit into this framework. We are then able to prove a metatheorem which shows that if a property of realizers satisfies some simple conditions, then it holds for the semantic interpretations of all terms. Applying this theorem to the special case of the term model, yields a general theorem for proving properties of typed λ-terms, in particular, strong normalization and confluence. This approach clarifies the reducibility method by showing that the closure conditions on candidates of reducibility can be viewed as sheaf conditions. The above approach is applied to the simply-typed λ-calculus (with types →, ×, +, and ⊥), and to the second-order (polymorphic) λ-calculus (with types → and ∀2), for which it yields a new theorem.},
  number = {2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2015-03-08},
  date = {1995-05-15},
  pages = {299-368},
  author = {Gallier, Jean},
  file = {/Users/pgiarrusso/Zotero/storage/QQ58XDHM/0304397594002800.html}
}

@inproceedings{Paulin-Mohring1989extracting,
  location = {{New York, NY, USA}},
  title = {Extracting \&Ohgr;'s Programs from Proofs in the Calculus of Constructions},
  isbn = {0-89791-294-2},
  url = {http://doi.acm.org/10.1145/75277.75285},
  doi = {10.1145/75277.75285},
  abstract = {We define in this paper a notion of realizability for the Calculus of Constructions. The extracted programs are terms of the Calculus that do not contain dependent types. We introduce a distinction between informative and non-informative propositions. This distinction allows the removal of the “logical” part in the development of a program. We show also how to use our notion of realizability in order to interpret various axioms like the axiom of choice or the induction on integers. A practical example of development of program is given in the appendix.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '89},
  publisher = {{ACM}},
  urldate = {2015-03-08},
  date = {1989},
  pages = {89--104},
  author = {Paulin-Mohring, C.}
}

@article{Wadler2007girardreynolds,
  title = {The Girard–Reynolds isomorphism (second edition)},
  volume = {375},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397506009236},
  doi = {10.1016/j.tcs.2006.12.042},
  abstract = {Jean-Yves Girard and John Reynolds independently discovered the second-order polymorphic lambda calculus, F2. Girard additionally proved a Representation Theorem: every function on natural numbers that can be proved total in second-order intuitionistic predicate logic, P2, can be represented in F2. Reynolds additionally proved an Abstraction Theorem: every term in F2 satisfies a suitable notion of logical relation; and formulated a notion of parametricity satisfied by well-behaved models.

We observe that the essence of Girard’s result is a projection from P2 into F2, and that the essence of Reynolds’s result is an embedding of F2 into P2, and that the Reynolds embedding followed by the Girard projection is the identity. We show that the inductive naturals are exactly those values of type natural that satisfy Reynolds’s notion of parametricity, and as a consequence characterize situations in which the Girard projection followed by the Reynolds embedding is also the identity.

An earlier version of this paper used a logic over untyped terms. This version uses a logic over typed term, similar to ones considered by Abadi and Plotkin and by Takeuti, which better clarifies the relationship between F2 and P2.

This paper uses colour to enhance its presentation. If the link below is not blue, follow it for the colour version.

http://homepages.inf.ed.ac.uk/wadler},
  number = {1–3},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Festschrift for John C. Reynolds’s 70th birthday},
  urldate = {2015-03-08},
  date = {2007-05-01},
  pages = {201-226},
  keywords = {system f,Abstraction Theorem,Curry–Howard,Girard–Reynolds type system,Polymorphic lambda calculus,Representation Theorem},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/IGZTECHJ/Wadler - 2007 - The Girard–Reynolds isomorphism (second edition).pdf;/Users/pgiarrusso/Zotero/storage/D5SF9E2N/S0304397506009236.html}
}

@incollection{Mishra-Linger2008erasure,
  langid = {english},
  title = {Erasure and Polymorphism in Pure Type Systems},
  isbn = {978-3-540-78497-5 978-3-540-78499-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-78499-9_25},
  abstract = {We introduce Erasure Pure Type Systems, an extension to Pure Type Systems with an erasure semantics centered around a type constructor ∀ indicating parametric polymorphism. The erasure phase is guided by lightweight program annotations. The typing rules guarantee that well-typed programs obey a phase distinction between erasable (compile-time) and non-erasable (run-time) terms. The erasability of an expression depends only on how its value is used in the rest of the program. Despite this simple observation, most languages treat erasability as an intrinsic property of expressions, leading to code duplication problems. Our approach overcomes this deficiency by treating erasability extrinsically. Because the execution model of EPTS generalizes the familiar notions of type erasure and parametric polymorphism, we believe functional programmers will find it quite natural to program in such a setting.},
  number = {4962},
  booktitle = {Foundations of Software Science and Computational Structures},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-09},
  date = {2008},
  pages = {350-364},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Mishra-Linger, Nathan and Sheard, Tim},
  editor = {Amadio, Roberto},
  file = {/Users/pgiarrusso/Zotero/storage/DM3SETP4/Mishra-Linger_Sheard - 2008 - Erasure and Polymorphism in Pure Type Systems.pdf;/Users/pgiarrusso/Zotero/storage/V2U3CEDZ/978-3-540-78499-9_25.html}
}
% == BibLateX quality report for Mishra-Linger2008erasure:
% 'isbn': not a valid ISBN

@inproceedings{Acar2003selective,
  location = {{New York, NY, USA}},
  title = {Selective Memoization},
  isbn = {1-58113-628-5},
  url = {http://doi.acm.org/10.1145/604131.604133},
  doi = {10.1145/604131.604133},
  abstract = {We present a framework for applying memoization selectively. The framework provides programmer control over equality, space usage, and identification of precise dependences so that memoization can be applied according to the needs of an application. Two key properties of the framework are that it is efficient and yields programs whose performance can be analyzed using standard techniques.We describe the framework in the context of a functional language and an implementation as an SML library. The language is based on a modal type system and allows the programmer to express programs that reveal their true data dependences when executed. The SML implementation cannot support this modal type system statically, but instead employs run-time checks to ensure correct usage of primitives.},
  booktitle = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '03},
  publisher = {{ACM}},
  urldate = {2015-03-09},
  date = {2003},
  pages = {14--25},
  keywords = {memoization,performance,programmer controlled,selective},
  author = {Acar, Umut A. and Blelloch, Guy E. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/FUGSA7ZX/Acar-Blelloch-Harper - 2003 - Selective Memoization.pdf}
}

@inproceedings{Geuvers1994churchrosser,
  title = {On the Church-Rosser property for expressive type systems and its consequences for their metatheoretic study},
  doi = {10.1109/LICS.1994.316057},
  abstract = {We consider two alternative definitions for the conversion rule in pure type systems. We study the consequences of this choice for the metatheory and point out the related implementation issues. We relate two open problems by showing that if a PTS allows the construction of a fixed point combinator, then Church-Rosser for βη-reduction fails. We present a new formalization of Russell's paradox in a slight extension of Martin-Lof's inconsistent theory with Type:Type and show that the resulting term leads to a fix-point construction. The main consequence is that the corresponding system is non-confluent. This example shows that in some typed λ-calculi, the Church-Rosser proof for the βη-reduction is not purely combinatorial anymore, as in pure λ-calculus, but relies on the normalization and thus the logical consistency of the system},
  eventtitle = {, Symposium on Logic in Computer Science, 1994. LICS '94. Proceedings},
  booktitle = {, Symposium on Logic in Computer Science, 1994. LICS '94. Proceedings},
  date = {1994-07},
  pages = {320-329},
  keywords = {type theory,lambda calculus,Buildings,Calculus,Church-Rosser property,Computer science,conversion rule,expressive type systems,fixed point combinator,formal logic,implementation issues,logical consistency,Mathematical model,Mathematics,metatheoretic study,metatheory,pure type systems,theorem proving},
  author = {Geuvers, H. and Werner, B.},
  file = {/Users/pgiarrusso/Zotero/storage/RQ24HUC8/Geuvers_Werner - 1994 - On the Church-Rosser property for expressive type systems and its consequences for their metatheoretic study.pdf;/Users/pgiarrusso/Zotero/storage/NFZGGQKK/login.html}
}
% == BibLateX quality report for Geuvers1994churchrosser:
% ? Unsure about the formatting of the booktitle

@incollection{Parnas2002secret,
  langid = {english},
  title = {The Secret History of Information Hiding},
  isbn = {978-3-642-63970-8 978-3-642-59412-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-59412-0_25},
  abstract = {The concept of “information-hiding” as a software design principle is widely accepted in academic circles. Many successful designs can be seen as successful applications of abstraction or information hiding. On the other hand, most industrial software developers do not apply the idea and many consider it unrealistic. This paper describes how the idea developed, discusses difficulties in its application, and speculates on why it has not been completely successful.},
  booktitle = {Software Pioneers},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-10},
  date = {2002},
  pages = {398-409},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Software Engineering/Programming and Operating Systems,History of Computing},
  author = {Parnas, David L.},
  editor = {Broy, Prof Dr Manfred and Denert, Prof Dr Ernst}
}
% == BibLateX quality report for Parnas2002secret:
% 'isbn': not a valid ISBN

@article{Bernardy2012proofs,
  title = {Proofs for free},
  volume = {22},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796812000056},
  doi = {10.1017/S0956796812000056},
  abstract = {Reynolds' abstraction theorem (Reynolds, J. C. (1983) Types, abstraction and parametric polymorphism, Inf. Process. 83(1), 513–523) shows how a typing judgement in System F can be translated into a relational statement (in second-order predicate logic) about inhabitants of the type. We obtain a similar result for pure type systems (PTSs): for any PTS used as a programming language, there is a PTS that can be used as a logic for parametricity. Types in the source PTS are translated to relations (expressed as types) in the target. Similarly, values of a given type are translated to proofs that the values satisfy the relational interpretation. We extend the result to inductive families. We also show that the assumption that every term satisfies the parametricity condition generated by its type is consistent with the generated logic.},
  number = {02},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-03-10},
  date = {2012-03},
  pages = {107--152},
  author = {Bernardy, Jean-Philippe and Jansson, Patrik and Paterson, Ross},
  file = {/Users/pgiarrusso/Zotero/storage/DF9ZJXWC/Bernardy et al - 2012 - Proofs for free.pdf;/Users/pgiarrusso/Zotero/storage/SZ6H4KKX/displayAbstract.html}
}

@inproceedings{Jia2010dependent,
  location = {{New York, NY, USA}},
  title = {Dependent Types and Program Equivalence},
  isbn = {978-1-60558-479-9},
  url = {http://doi.acm.org/10.1145/1706299.1706333},
  doi = {10.1145/1706299.1706333},
  abstract = {The definition of type equivalence is one of the most important design issues for any typed language. In dependently typed languages, because terms appear in types, this definition must rely on a definition of term equivalence. In that case, decidability of type checking requires decidability for the term equivalence relation. Almost all dependently-typed languages require this relation to be decidable. Some, such as Coq, Epigram or Agda, do so by employing analyses to force all programs to terminate. Conversely, others, such as DML, ATS, Ωmega, or Haskell, allow nonterminating computation, but do not allow those terms to appear in types. Instead, they identify a terminating index language and use singleton types to connect indices to computation. In both cases, decidable type checking comes at a cost, in terms of complexity and expressiveness. Conversely, the benefits to be gained by decidable type checking are modest. Termination analyses allow dependently typed programs to verify total correctness properties. However, decidable type checking is not a prerequisite for type safety. Furthermore, decidability does not imply tractability. A decidable approximation of program equivalence may not be useful in practice. Therefore, we take a different approach: instead of a fixed notion for term equivalence, we parameterize our type system with an abstract relation that is not necessarily decidable. We then design a novel set of typing rules that require only weak properties of this abstract relation in the proof of the preservation and progress lemmas. This design provides flexibility: we compare valid instantiations of term equivalence which range from beta-equivalence, to contextual equivalence, to some exotic equivalences.},
  booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '10},
  publisher = {{ACM}},
  urldate = {2015-03-12},
  date = {2010},
  pages = {275--286},
  keywords = {dependent types,program equivalence},
  author = {Jia, Limin and Zhao, Jianzhou and Sjöberg, Vilhelm and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/RVC44HDS/Jia et al - 2010 - Dependent Types and Program Equivalence.pdf}
}

@inproceedings{Sjoberg2015programming,
  location = {{New York, NY, USA}},
  title = {Programming Up to Congruence},
  isbn = {978-1-4503-3300-9},
  url = {http://doi.acm.org/10.1145/2676726.2676974},
  doi = {10.1145/2676726.2676974},
  abstract = {This paper presents the design of Zombie, a dependently-typed programming language that uses an adaptation of a congruence closure algorithm for proof and type inference. This algorithm allows the type checker to automatically use equality assumptions from the context when reasoning about equality. Most dependently-typed languages automatically use equalities that follow from beta-reduction during type checking; however, such reasoning is incompatible with congruence closure. In contrast, Zombie does not use automatic beta-reduction because types may contain potentially diverging terms. Therefore Zombie provides a unique opportunity to explore an alternative definition of equivalence in dependently-typed language design. Our work includes the specification of the language via a bidirectional type system, which works "up-to-congruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system, and always produces well typed terms in the core language. This algorithm has been implemented in the Zombie language, which includes general recursion, irrelevant arguments, heterogeneous equality and datatypes.},
  booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '15},
  publisher = {{ACM}},
  urldate = {2015-03-12},
  date = {2015},
  pages = {369--382},
  keywords = {dependent types,congruence closure},
  author = {Sjöberg, Vilhelm and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/2AZMUCNJ/Sjöberg_Weirich - 2015 - Programming Up to Congruence (Extended version).pdf;/Users/pgiarrusso/Zotero/storage/UJH8V5EW/Sjöberg_Weirich - 2015 - Programming Up to Congruence.pdf}
}

@article{OHearn2000algol,
  title = {From Algol to Polymorphic Linear Lambda-calculus},
  volume = {47},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/331605.331611},
  doi = {10.1145/331605.331611},
  abstract = {In a linearly-typed functional language, one can define functions that consume their arguments in the process of computing their results. This is reminiscent of state transformations in imperative languages, where execition of an assignment statement alters the contents of the store. We explore this connection by translating two variations on Algol 60 into a purely functional language with polymorphic linear types. On the one hand, the translations lead to a semantic analysis of Algol-like programs, in terms of a model of the linear language. On the other hand, they demonstrate that a linearly-typed functional language can be at least as expressive as Algol.},
  number = {1},
  journaltitle = {J. ACM},
  urldate = {2015-03-12},
  date = {2000-01},
  pages = {167--223},
  keywords = {logical relations,linear logic,local state,parametric polymorphism},
  author = {O'Hearn, Peter W. and Reynolds, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/4PPTZDKH/O'Hearn_Reynolds - 2000 - From Algol to Polymorphic Linear Lambda-calculus.pdf}
}
% == BibLateX quality report for OHearn2000algol:
% ? Possibly abbreviated journal title J. ACM

@article{Hagino1989codatatypes,
  title = {Codatatypes in ML},
  volume = {8},
  issn = {0747-7171},
  url = {http://www.sciencedirect.com/science/article/pii/S0747717189800653},
  doi = {10.1016/S0747-7171(89)80065-3},
  abstract = {A new data type declaration mechanism of defining codatatypes is introduced to a functional programming language ML. Codatatypes are dual to datatypes for which ML already has a mechanism of defining. Sums and finite lists are defined as datatypes, but their duals, products and infinite lists, could not be defined in ML. This new facility gives ML the missing half of data types and makes ML symmetric. Categorical and domain-theoretic characterization of codatatypes are also given.},
  number = {6},
  journaltitle = {Journal of Symbolic Computation},
  shortjournal = {Journal of Symbolic Computation},
  urldate = {2015-03-12},
  date = {1989-12},
  pages = {629-650},
  author = {Hagino, Tatsuya},
  file = {/Users/pgiarrusso/Zotero/storage/KRWTPTRK/S0747717189800653.html}
}

@report{Hagino1987categorical,
  title = {A Categorical Programming Language},
  abstract = {A theory of data types and a programming language based on category theory are presented. Data types play a crucial role in programming. They enable us to write programs easily and elegantly. Various programming languages have been developed, each of which may use different kinds of data types. Therefore, it becomes important to organize data types systematically so that we can understand the relationship between one data type and another and investigate future directions which lead us to discover exciting new data types. There have been several approaches to systematically organize data types: algebraic specification methods using algebras, domain theory using complete partially ordered sets and type theory using the connection between logics and data types. Here, we use category theory. Category theory has proved to be remarkably good at revealing the nature of mathematical objects, and we use it to understand the true nature of data types in programming.},
  date = {1987},
  author = {Hagino, Tatsuya},
  file = {/Users/pgiarrusso/Zotero/storage/QXMPUHHX/Hagino - 1987 - A Categorical Programming Language.pdf;/Users/pgiarrusso/Zotero/storage/6APD8CJJ/summary.html}
}
% == BibLateX quality report for Hagino1987categorical:
% Missing required field 'type'
% Missing required field 'institution'

@incollection{Hagino1987typed,
  langid = {english},
  title = {A typed lambda calculus with categorical type constructors},
  isbn = {978-3-540-18508-6 978-3-540-48006-8},
  url = {http://link.springer.com/chapter/10.1007/3-540-18508-9_24},
  abstract = {A typed lambda calculus with categorical type constructors is introduced. It has a uniform category theoretic mechanism to declare new types. Its type structure includes categorical objects like products and coproducts as well as recursive types like natural numbers and lists. It also allows duals of recursive types, i.e. lazy types, like infinite lists. It has generalized iterators for recursive types and duals of iterators for lazy types. We will give reduction rules for this simply typed lambda calculus and show that they are strongly normalizing even though it has infinite things like infinite lists.},
  number = {283},
  booktitle = {Category Theory and Computer Science},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-12},
  date = {1987},
  pages = {140-157},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Hagino, Tatsuya},
  editor = {Pitt, David H. and Poigné, Axel and Rydeheard, David E.},
  file = {/Users/pgiarrusso/Zotero/storage/EFSAHKHS/Hagino - 1987 - A typed lambda calculus with categorical type constructors.pdf;/Users/pgiarrusso/Zotero/storage/P5M8TUZG/3-540-18508-9_24.html}
}
% == BibLateX quality report for Hagino1987typed:
% 'isbn': not a valid ISBN

@inproceedings{Zeilberger2008focusing,
  location = {{New York, NY, USA}},
  title = {Focusing and Higher-order Abstract Syntax},
  isbn = {978-1-59593-689-9},
  url = {http://doi.acm.org/10.1145/1328438.1328482},
  doi = {10.1145/1328438.1328482},
  abstract = {Focusing is a proof-search strategy, originating in linear logic, that elegantly eliminates inessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs with explicit evaluation order. Higher-order abstract syntax (HOAS) is a technique for representing higher-order programming language constructs (e.g., λ's) by higher-order terms at the"meta-level", thereby avoiding some of the bureaucratic headaches of first-order representations (e.g., capture-avoiding substitution). This paper begins with a fresh, judgmental analysis of focusing for intuitionistic logic (with a full suite of propositional connectives), recasting the "derived rules" of focusing as iterated inductive definitions. This leads to a uniform presentation, allowing concise, modular proofs of the identity and cut principles. Then we show how this formulation of focusing induces, through the Curry-Howard isomorphism, a new kind of higher-order encoding of abstract syntax: functions are encoded by maps from patterns to expressions. Dually, values are encoded as patterns together with explicit substitutions. This gives us pattern-matching "for free", and lets us reason about a rich type system with minimal syntactic overhead. We describe how to translate the language and proof of type safety almost directly into Coq using HOAS, and finally, show how the system's modular design pays off in enabling a very simple extension with recursion and recursive types.},
  booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '08},
  publisher = {{ACM}},
  urldate = {2015-03-12},
  date = {2008},
  pages = {359--369},
  keywords = {focusing,pattern-matching,higher-order abstract syntax},
  author = {Zeilberger, Noam},
  file = {/Users/pgiarrusso/Zotero/storage/QBSI3XGA/Zeilberger - 2008 - Focusing and Higher-order Abstract Syntax.pdf}
}

@incollection{Schroeder-Heister1994definitional,
  langid = {english},
  title = {Definitional reflection and the completion},
  isbn = {978-3-540-58025-6 978-3-540-48417-2},
  url = {http://link.springer.com/chapter/10.1007/3-540-58025-5_65},
  abstract = {The logic of definitional reflection is extended with a theory of free equality. Based on this equality theory a sequent-style notion of the completion of a definition is motivated. Definitional reflection with free equality turns out to be equivalent to the completion in this sense.},
  number = {798},
  booktitle = {Extensions of Logic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-12},
  date = {1994},
  pages = {333-347},
  keywords = {Programming Techniques,Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages},
  author = {Schroeder-Heister, Peter},
  editor = {Dyckhoff, Roy},
  file = {/Users/pgiarrusso/Zotero/storage/JCR2RTR3/Schroeder-Heister - 1994 - Definitional reflection and the completion.pdf;/Users/pgiarrusso/Zotero/storage/T9RZS3W7/3-540-58025-5_65.html}
}
% == BibLateX quality report for Schroeder-Heister1994definitional:
% 'isbn': not a valid ISBN

@inproceedings{Le2014mucheck,
  location = {{New York, NY, USA}},
  title = {MuCheck: An Extensible Tool for Mutation Testing of Haskell Programs},
  isbn = {978-1-4503-2645-2},
  url = {http://doi.acm.org/10.1145/2610384.2628052},
  doi = {10.1145/2610384.2628052},
  shorttitle = {MuCheck},
  abstract = {This paper presents MuCheck, a mutation testing tool for Haskell programs. MuCheck is a counterpart to the widely used QuickCheck random testing tool for functional programs, and can be used to evaluate the efficacy of QuickCheck property definitions. The tool implements mutation operators that are specifically designed for functional programs, and makes use of the type system of Haskell to achieve a more relevant set of mutants than otherwise possible. Mutation coverage is particularly valuable for functional programs due to highly compact code, referential transparency, and clean semantics; these make augmenting a test suite or specification based on surviving mutants a practical method for improved testing.},
  booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
  series = {ISSTA 2014},
  publisher = {{ACM}},
  urldate = {2015-03-17},
  date = {2014},
  pages = {429--432},
  keywords = {Haskell,functional programming languages,Mutatation Testing,Mutation Operators},
  author = {Le, Duc and Alipour, Mohammad Amin and Gopinath, Rahul and Groce, Alex},
  file = {/Users/pgiarrusso/Zotero/storage/MFN5T5DF/Le et al - 2014 - MuCheck - An Extensible Tool for Mutation Testing of Haskell Programs.pdf}
}

@incollection{Reynolds1985three,
  langid = {english},
  title = {Three approaches to type structure},
  isbn = {978-3-540-15198-2 978-3-540-39302-3},
  url = {http://link.springer.com/chapter/10.1007/3-540-15198-2_7},
  abstract = {We examine three disparate views of the type structure of programming languages: Milner's type deduction system and polymorphic let construct, the theory of subtypes and generic operators, and the polymorphic or second-order typed lambda calculus. These approaches are illustrated with a functional language including product, sum and list constructors. The syntactic behavior of types is formalized with type inference rules, but their semantics is treated intuitively.},
  number = {185},
  booktitle = {Mathematical Foundations of Software Development},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-17},
  date = {1985},
  pages = {97-138},
  keywords = {Software Engineering,Logics and Meanings of Programs,Combinatorics},
  author = {Reynolds, John C.},
  editor = {Ehrig, Hartmut and Floyd, Christiane and Nivat, Maurice and Thatcher, James},
  file = {/Users/pgiarrusso/Zotero/storage/A3CSCHZC/10.html}
}
% == BibLateX quality report for Reynolds1985three:
% 'isbn': not a valid ISBN

@article{Bloo1996barendregt,
  title = {The Barendregt Cube with Definitions and Generalised Reduction},
  volume = {126},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540196900413},
  doi = {10.1006/inco.1996.0041},
  abstract = {In this paper, we propose to extend the Barendregt Cube by generalisingβ-reduction and by adding definition mechanisms. Generalised reduction allows contracting more visible redexes than usual, and definitions are an important tool to allow for a more flexible typing system. We show that this extension satisfies most of the original properties of the Cube including Church-Rosser, Subject Reduction and Strong Normalisation.},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2015-03-17},
  date = {1996-05-01},
  pages = {123-143},
  author = {Bloo, Roel and Kamareddine, Fairouz and Nederpelt, Rob},
  file = {/Users/pgiarrusso/Zotero/storage/GDF7QUC4/Bloo et al - 1996 - The Barendregt Cube with Definitions and Generalised Reduction.pdf;/Users/pgiarrusso/Zotero/storage/BU3S5I9H/S0890540196900413.html}
}

@inproceedings{Steensgaard1996pointsto,
  location = {{New York, NY, USA}},
  title = {Points-to Analysis in Almost Linear Time},
  isbn = {0-89791-769-3},
  url = {http://doi.acm.org/10.1145/237721.237727},
  doi = {10.1145/237721.237727},
  abstract = {We present an interprocedural flow-insensitive points-to analysis based on type inference methods with an almost linear time cost complexity To our knowledge, this is the asymptotically fastest non-trivial interprocedural points-to analysis algorithm yet described The algorithm is based on a non-standard type system. The type inferred for any variable represents a set of locations and includes a type which in turn represents a set of locations possibly pointed to by the variable. The type inferred for a function variable represents a set of functions It may point to and includes a type signature for these functions The results are equivalent to those of a flow-insensitive alias analysis (and control flow analysis) that assumes alias relations are reflexive and transitive.This work makes three contributions. The first is a type system for describing a universally valid storage shape graph for a program in linear space. The second is a constraint system which often leads to better results than the "obvious" constraint system for the given type system The third is an almost linear time algorithm for points-to analysis by solving a constraint system.},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '96},
  publisher = {{ACM}},
  urldate = {2015-03-19},
  date = {1996},
  pages = {32--41},
  author = {Steensgaard, Bjarne},
  file = {/Users/pgiarrusso/Zotero/storage/TK8HRF2J/Steensgaard - 1996 - Points-to Analysis in Almost Linear Time.pdf}
}

@inproceedings{Ma2007investigating,
  location = {{New York, NY, USA}},
  title = {Investigating the Viability of Mental Models Held by Novice Programmers},
  isbn = {1-59593-361-1},
  url = {http://doi.acm.org/10.1145/1227310.1227481},
  doi = {10.1145/1227310.1227481},
  abstract = {This paper describes an investigation into the viability of mental models used by novice programmers at the end of a first year Java programming course. The qualitative findings identify the range of mental models of value and reference assignment held by the participants. The quantitative analysis reveals that approximately one third of students held non-viable mental models of value assignment and only 17\% of students held a viable mental model of reference assignment. Further, in terms of a comparison between the participants' mental models and their performance in in-course assessments and final examination, it was found that students with viable mental models performed significantly better than those with non-viable models. These findings are used to propose a more "constructivist" approach to teaching programming based on the integration of "cognitive conflict" and program visualisation.},
  booktitle = {Proceedings of the 38th SIGCSE Technical Symposium on Computer Science Education},
  series = {SIGCSE '07},
  publisher = {{ACM}},
  urldate = {2015-03-20},
  date = {2007},
  pages = {499--503},
  keywords = {CS1,mental models,novice,programming},
  author = {Ma, Linxiao and Ferguson, John and Roper, Marc and Wood, Murray},
  file = {/Users/pgiarrusso/Zotero/storage/TFE744JR/Ma et al - 2007 - Investigating the Viability of Mental Models Held by Novice Programmers.pdf}
}

@inproceedings{Harrison1993subjectoriented,
  location = {{New York, NY, USA}},
  title = {Subject-oriented Programming: A Critique of Pure Objects},
  isbn = {0-89791-587-9},
  url = {http://doi.acm.org/10.1145/165854.165932},
  doi = {10.1145/165854.165932},
  shorttitle = {Subject-oriented Programming},
  booktitle = {Proceedings of the Eighth Annual Conference on Object-oriented Programming Systems, Languages, and Applications},
  series = {OOPSLA '93},
  publisher = {{ACM}},
  urldate = {2015-03-25},
  date = {1993},
  pages = {411--428},
  author = {Harrison, William and Ossher, Harold}
}

@inproceedings{Dunfield2013complete,
  location = {{New York, NY, USA}},
  title = {Complete and Easy Bidirectional Typechecking for Higher-rank Polymorphism},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500582},
  doi = {10.1145/2500365.2500582},
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike Damas-Milner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higher-rank polymorphism, grounded in proof theory; this calculus enjoys many properties such as eta-reduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and well-behaved, despite being both sound and complete.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2015-03-26},
  date = {2013},
  pages = {429--442},
  keywords = {bidirectional typechecking,higher-rank polymorphism},
  author = {Dunfield, Joshua and Krishnaswami, Neelakantan R.},
  file = {/Users/pgiarrusso/Zotero/storage/Q7FFPHX5/Dunfield_Krishnaswami - 2013 - Complete and Easy Bidirectional Typechecking for Higher-rank Polymorphism.pdf}
}

@incollection{Devai2015edsls,
  langid = {english},
  title = {The EDSL’s Struggle for Their Sources},
  isbn = {978-3-319-15939-3 978-3-319-15940-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-15940-9_7},
  abstract = {Embedded Domain Specific Languages make language design and implementation easier, because lexical and syntactical analysis and part of the semantic checks can be completed by the compiler of the host language. On the other hand, by the nature of embedding, EDSL compilers have to work with a syntax tree that stores no information about the source file processed and the location of the program entities within the source file. This makes it hard to produce user-friendly error messages and connect the generated target code with the source code for debugging and profiling purposes. This lecture note presents this problem in detail and shows possible solutions. The first, lightweight solution uses macro preprocessing. The second one is based on syntax tree transformations to add missing source-related information. This is more powerful, but also more heavyweight. The last technique avoids the problem by turning the embedded language implementation to a standalone one (with own parser) after the experimental phase of the language development process: It turns out that most of the embedded implementation can be reused in the standalone one.},
  number = {8606},
  booktitle = {Central European Functional Programming School},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2015-03-26},
  date = {2015},
  pages = {300-335},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs},
  author = {Dévai, Gergely and Leskó, Dániel and Tejfel, Máté},
  editor = {Zsók, Viktória and Horváth, Zoltán and Csató, Lehel},
  file = {/Users/pgiarrusso/Zotero/storage/8WHHU3FV/978-3-319-15940-9_7.html}
}
% == BibLateX quality report for Devai2015edsls:
% 'isbn': not a valid ISBN

@article{Amadio2013certifying,
  langid = {english},
  title = {Certifying and reasoning about cost annotations of functional programs},
  url = {https://hal.inria.fr/inria-00629473/document},
  abstract = {We present a so-called labelling method to insert cost annotations in a higher-order functional program, to certify their correctness with respect to a standard compilation chain to assembly code including safe memory management, and to reason on them in a higher-order Hoare logic.},
  journaltitle = {Higher-Order and Symbolic Computation},
  urldate = {2015-03-26},
  date = {2013-01-16},
  author = {Amadio, Roberto M. and Régis-Gianas, Yann},
  file = {/Users/pgiarrusso/Zotero/storage/QIMQX25D/Amadio_Régis-Gianas - 2013 - Certifying and reasoning about cost annotations of functional programs.pdf;/Users/pgiarrusso/Zotero/storage/GEU6IAJB/inria-00629473v2.html}
}

@incollection{Partush2013abstract,
  langid = {english},
  title = {Abstract Semantic Differencing for Numerical Programs},
  isbn = {978-3-642-38855-2 978-3-642-38856-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-38856-9_14},
  abstract = {We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists. We focus on computing semantic differences in numerical programs where the values of variables have no a-priori bounds, and use abstract interpretation to compute an over-approximation of program differences. Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, we first construct a correlating program in which these relationships can be tracked, and then use a correlating abstract domain to compute a sound approximation of these relationships. To better establish equivalence between correlated variables and precisely capture differences, our domain has to represent non-convex information using a partially-disjunctive abstract domain. To balance precision and cost of this representation, our domain over-approximates numerical information while preserving equivalence between correlated variables by dynamically partitioning the disjunctive state according to equivalence criteria. We have implemented our approach in a tool called DIZY, and applied it to a number of real-world examples, including programs from the GNU core utilities, Mozilla Firefox and the Linux Kernel. Our evaluation shows that DIZY often manages to establish equivalence, describes precise approximation of semantic differences when difference exists, and reports only a few false differences.},
  number = {7935},
  booktitle = {Static Analysis},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-03-26},
  date = {2013},
  pages = {238-258},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Partush, Nimrod and Yahav, Eran},
  editor = {Logozzo, Francesco and Fähndrich, Manuel},
  file = {/Users/pgiarrusso/Zotero/storage/8PUXP4AC/Partush_Yahav - 2013 - Abstract Semantic Differencing for Numerical Programs.pdf;/Users/pgiarrusso/Zotero/storage/J48CXFEM/978-3-642-38856-9_14.html}
}
% == BibLateX quality report for Partush2013abstract:
% 'isbn': not a valid ISBN

@article{Mitchell1991kripkestyle,
  title = {Kripke-style models for typed lambda calculus},
  volume = {51},
  issn = {0168-0072},
  url = {http://www.sciencedirect.com/science/article/pii/016800729190067V},
  doi = {10.1016/0168-0072(91)90067-V},
  abstract = {Mitchell, J.C. and E. Moggi, Kripke-style models for typed lambda calculus, Annals of Pure and Applied Logic 51 (1991) 99–124.

The semantics of typed lambda calculus is usually described using Henkin models, consisting of functions over some collection of sets, or concrete cartesian closed categories, which are essentially equivalent. We describe a more general class of Kripke-style models. In categorical terms, our Kripke lambda models are cartesian closed subcategories of the presheaves over a poset. To those familiar with Kripke models of modal or intuitionistic logics, Kripke lambda models are likely to seem adequately ‘semantic’. However, when viewed as cartesian closed categories, they do not have the property variously referred to as concreteness, well- pointedness or having enough points. While the traditional lambda calculus proof system is not complete for Henkin models that may have empty types, we prove strong completeness for Kripke models. In fact, every set of equations that is closed under implication is the theory of a single Kripke model. We also develop some properties of logical relations over Kripke structures, showing that every theory is the theory of a model determined by a Kripke equivalence relation over a Henkin model. We discuss cartesian closed categories but present the main definitions and results without the use of category theory.},
  number = {1–2},
  journaltitle = {Annals of Pure and Applied Logic},
  shortjournal = {Annals of Pure and Applied Logic},
  urldate = {2015-03-29},
  date = {1991-03-14},
  pages = {99-124},
  author = {Mitchell, John C. and Moggi, Eugenio},
  file = {/Users/pgiarrusso/Zotero/storage/2WDJE4JM/016800729190067V.html}
}

@article{Hammer2015incremental,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.07792},
  primaryClass = {cs},
  title = {Incremental Computation with Names},
  url = {http://arxiv.org/abs/1503.07792},
  abstract = {Over the past thirty years, there has been significant progress in developing general-purpose, language-based approaches to incremental computation, which aims to efficiently update the result of a computation when an input is changed. A key design challenge in such approaches is how to provide efficient incremental support for a broad range of programs. In this paper, we argue that first-class names are a critical linguistic feature for efficient incremental computation. Names identify computations to be reused across differing runs of a program, and making them first class gives programmers a high level of control over reuse. We demonstrate the benefits of names by presenting Nominal Adapton, an ML-like language for incremental computation with names. We describe how to use Nominal Adapton to efficiently incrementalize several standard programming patterns---including maps, folds, and unfolds---and show how to build efficient, incremental probabilistic trees and tries. Since Nominal Adapton's implementation is subtle, we formalize it as a core calculus and prove it is from-scratch consistent, meaning it always produces the same answer as simply re-running the computation. Finally, we demonstrate that Nominal Adapton can provide large speedups over both from-scratch computation and Adapton, a previous state-of-the-art incremental system.},
  urldate = {2015-03-30},
  date = {2015-03-26},
  keywords = {Computer Science - Programming Languages,_tablet},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Labich, Nicholas and Foster, Jeffrey S. and Hicks, Michael and Van Horn, David},
  file = {/Users/pgiarrusso/Zotero/storage/URT899BS/Hammer-Dunfield-Headley-Labich-Foster-Hicks-Van Horn - 2015 - Incremental Computation with Names.pdf;/Users/pgiarrusso/Zotero/storage/NEDFVM3M/1503.html}
}
% == BibLateX quality report for Hammer2015incremental:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Curien2007definability,
  title = {Definability and Full Abstraction},
  volume = {172},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066107000837},
  doi = {10.1016/j.entcs.2007.02.011},
  abstract = {Game semantics has renewed denotational semantics. It offers among other things an attractive classification of programming features, and has brought a bunch of new definability results. In parallel, in the denotational semantics of proof theory, several full completeness results have been shown since the early nineties. In this note, we review the relation between definability and full abstraction, and we put a few old and recent results of this kind in perspective.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
  urldate = {2015-03-30},
  date = {2007-04-01},
  pages = {301-310},
  keywords = {Denotational semantics,operational semantics,sequentiality},
  author = {Curien, Pierre-Louis},
  file = {/Users/pgiarrusso/Zotero/storage/DVI9K9DM/Curien - 2007 - Definability and Full Abstraction.pdf;/Users/pgiarrusso/Zotero/storage/V8SHNZA9/S1571066107000837.html}
}

@inproceedings{Dunfield2012elaborating,
  location = {{New York, NY, USA}},
  title = {Elaborating Intersection and Union Types},
  isbn = {978-1-4503-1054-3},
  url = {http://doi.acm.org/10.1145/2364527.2364534},
  doi = {10.1145/2364527.2364534},
  abstract = {Designing and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features. In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation. We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary λ-calculus terms. The key feature is a Forsythe-like merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary call-by-value evaluation of the elaborated program corresponds to a type-preserving evaluation of the source program. We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '12},
  publisher = {{ACM}},
  urldate = {2015-03-30},
  date = {2012},
  pages = {17--28},
  keywords = {intersection,types},
  author = {Dunfield, Joshua},
  file = {/Users/pgiarrusso/Zotero/storage/BEQ662VC/Dunfield - 2012 - Elaborating Intersection and Union Types.pdf}
}

@article{Dunfield2014elaborating,
  title = {Elaborating intersection and union types},
  volume = {24},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796813000270},
  doi = {10.1017/S0956796813000270},
  abstract = {Designing and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features. In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation. We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary λ-calculus terms. The key feature is a Forsythe-like merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary call-by-value evaluation of the elaborated program corresponds to a type-preserving evaluation of the source program. We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.},
  issue = {Special Issue 2-3},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-03-30},
  date = {2014-05},
  pages = {133--165},
  author = {Dunfield, Joshua},
  file = {/Users/pgiarrusso/Zotero/storage/UCMZ5B7N/Dunfield - 2014 - Elaborating intersection and union types.pdf;/Users/pgiarrusso/Zotero/storage/5K4NCNGM/displayAbstract.html}
}

@incollection{Erwig2012semantics,
  langid = {english},
  title = {Semantics First!},
  isbn = {978-3-642-28829-6 978-3-642-28830-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-28830-2_14},
  abstract = {The design of languages is still more of an art than an engineering discipline. Although recently tools have been put forward to support the language design process, such as language workbenches, these have mostly focused on a syntactic view of languages. While these tools are quite helpful for the development of parsers and editors, they provide little support for the underlying design of the languages. In this paper we illustrate how to support the design of languages by focusing on their semantics first. Specifically, we will show that powerful and general language operators can be employed to adapt and grow sophisticated languages out of simple semantics concepts. We use Haskell as a metalanguage and will associate generic language concepts, such as semantics domains, with Haskell-specific ones, such as data types. We do this in a way that clearly distinguishes our approach to language design from the traditional syntax-oriented one. This will reveal some unexpected correlations, such as viewing type classes as language multipliers. We illustrate the viability of our approach with several real-world examples.},
  number = {6940},
  booktitle = {Software Language Engineering},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-04-01},
  date = {2012},
  pages = {243-262},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,User Interfaces and Human Computer Interaction},
  author = {Erwig, Martin and Walkingshaw, Eric},
  editor = {Sloane, Anthony and Aßmann, Uwe},
  file = {/Users/pgiarrusso/Zotero/storage/JRESBNCE/978-3-642-28830-2_14.html}
}
% == BibLateX quality report for Erwig2012semantics:
% 'isbn': not a valid ISBN

@inproceedings{Hanenberg2010faith,
  location = {{New York, NY, USA}},
  title = {Faith, Hope, and Love: An Essay on Software Science's Neglect of Human Factors},
  isbn = {978-1-4503-0203-6},
  url = {http://doi.acm.org/10.1145/1869459.1869536},
  doi = {10.1145/1869459.1869536},
  shorttitle = {Faith, Hope, and Love},
  abstract = {Research in the area of programming languages has different facets -- from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers -- and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.},
  booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '10},
  publisher = {{ACM}},
  urldate = {2015-04-01},
  date = {2010},
  pages = {933--946},
  keywords = {Software Engineering,empirical research,programming language research,research methods},
  author = {Hanenberg, Stefan},
  file = {/Users/pgiarrusso/Zotero/storage/K7S4G67T/Hanenberg - 2010 - Faith, Hope, and Love - An Essay on Software Science's Neglect of Human Factors.pdf}
}

@inproceedings{Hudak2007history,
  location = {{New York, NY, USA}},
  title = {A History of Haskell: Being Lazy with Class},
  isbn = {978-1-59593-766-7},
  url = {http://doi.acm.org/10.1145/1238844.1238856},
  doi = {10.1145/1238844.1238856},
  shorttitle = {A History of Haskell},
  abstract = {This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.},
  booktitle = {Proceedings of the Third ACM SIGPLAN Conference on History of Programming Languages},
  series = {HOPL III},
  publisher = {{ACM}},
  urldate = {2015-04-01},
  date = {2007},
  pages = {12-1--12-55},
  author = {Hudak, Paul and Hughes, John and Peyton Jones, Simon and Wadler, Philip}
}

@article{Schmidhuber2008driven,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0812.4360},
  primaryClass = {cs},
  title = {Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes},
  url = {http://arxiv.org/abs/0812.4360},
  shorttitle = {Driven by Compression Progress},
  abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.},
  urldate = {2015-04-03},
  date = {2008-12-23},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  author = {Schmidhuber, Juergen},
  file = {/Users/pgiarrusso/Zotero/storage/W69Z4CMS/Schmidhuber - 2008 - Driven by Compression Progress - A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity,.pdf;/Users/pgiarrusso/Zotero/storage/D8PQU5HQ/0812.html}
}
% == BibLateX quality report for Schmidhuber2008driven:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Smyth1982categorytheoretic,
  title = {The Category-Theoretic Solution of Recursive Domain Equations},
  volume = {11},
  issn = {0097-5397},
  url = {http://epubs.siam.org/doi/abs/10.1137/0211062},
  doi = {10.1137/0211062},
  abstract = {Recursive specifications of domains plays a crucial role in denotational semantics as developed by Scott and Strachey and their followers. The purpose of the present paper is to set up a categorical framework in which the known techniques for solving these equations find a natural place. The idea is to follow the well-known analogy between partial orders and categories, generalizing from least fixed-points of continuous functions over cpos to initial ones of continuous functors over \$$\backslash$omega \$-categories. To apply these general ideas we introduce Wand’s \$\{$\backslash$bf O\}\$-categories where the morphism-sets have a partial order structure and which include almost all the categories occurring in semantics. The idea is to find solutions in a derived category of embeddings and we give order-theoretic conditions which are easy to verify and which imply the needed categorical ones. The main tool is a very general form of the limit-colimit coincidence remarked by Scott. In the concluding section we outline how compatibility considerations are to be included in the framework. A future paper will show how Scott’s universal domain method can be included too.,  Recursive specifications of domains plays a crucial role in denotational semantics as developed by Scott and Strachey and their followers. The purpose of the present paper is to set up a categorical framework in which the known techniques for solving these equations find a natural place. The idea is to follow the well-known analogy between partial orders and categories, generalizing from least fixed-points of continuous functions over cpos to initial ones of continuous functors over \$$\backslash$omega \$-categories. To apply these general ideas we introduce Wand’s \$\{$\backslash$bf O\}\$-categories where the morphism-sets have a partial order structure and which include almost all the categories occurring in semantics. The idea is to find solutions in a derived category of embeddings and we give order-theoretic conditions which are easy to verify and which imply the needed categorical ones. The main tool is a very general form of the limit-colimit coincidence remarked by Scott. In the concluding section we outline how compatibility considerations are to be included in the framework. A future paper will show how Scott’s universal domain method can be included too.},
  number = {4},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  urldate = {2015-04-03},
  date = {1982-11-01},
  pages = {761-783},
  author = {Smyth, M. and Plotkin, G.},
  file = {/Users/pgiarrusso/Zotero/storage/QKPJ8FD9/0211062.html}
}

@inproceedings{Rompf2014surgical,
  location = {{New York, NY, USA}},
  title = {Surgical Precision JIT Compilers},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594316},
  doi = {10.1145/2594291.2594316},
  abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable. In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks. We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction. In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '14},
  publisher = {{ACM}},
  urldate = {2015-04-05},
  date = {2014},
  pages = {41--52},
  keywords = {staging,JIT compilation,program generation},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
  file = {/Users/pgiarrusso/Zotero/storage/S93ZT32I/Rompf et al - 2014 - Surgical Precision JIT Compilers.pdf}
}

@incollection{Yallop2014lightweight,
  langid = {english},
  title = {Lightweight Higher-Kinded Polymorphism},
  isbn = {978-3-319-07150-3 978-3-319-07151-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-07151-0_8},
  abstract = {Higher-kinded polymorphism —i.e. abstraction over type constructors— is an essential component of many functional programming techniques such as monads, folds, and embedded DSLs. ML-family languages typically support a form of abstraction over type constructors using functors, but the separation between the core language and the module language leads to awkwardness as functors proliferate. We show how to express higher-kinded polymorphism in OCaml without functors, using an abstract type app to represent type application, and opaque brands to denote abstractable type constructors. We demonstrate the flexibility of our approach by using it to translate a variety of standard higher-kinded programs into functor-free OCaml code.},
  number = {8475},
  booktitle = {Functional and Logic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2015-04-11},
  date = {2014-06-04},
  pages = {119-135},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Yallop, Jeremy and White, Leo},
  editor = {Codish, Michael and Sumii, Eijiro},
  file = {/Users/pgiarrusso/Zotero/storage/NJHME9PI/Yallop_White - 2014 - Lightweight Higher-Kinded Polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/IFQBESMA/978-3-319-07151-0_8.html}
}
% == BibLateX quality report for Yallop2014lightweight:
% 'isbn': not a valid ISBN

@article{Dwork1984sequential,
  title = {On the sequential nature of unification},
  volume = {1},
  issn = {0743-1066},
  url = {http://www.sciencedirect.com/science/article/pii/0743106684900220},
  doi = {10.1016/0743-1066(84)90022-0},
  abstract = {The problem of unification of terms is log-space complete for P. In deriving this lower bound no use is made of the potentially concise representation of terms by directed acyclic graphs. In addition, the problem remains complete even if infinite substitutions are allowed. A consequence of this result is that parallelism cannot significantly improve on the best sequential solutions for unification. However, we show that for the problem of term matching, an important subcase of unification, there is a good parallel algorithm using O(log2n) time and nO(1) processors on a PRAM. For the O(log2n) parallel time upper bound we assume that the terms are represented by directed acyclic graphs; if the longer string representation is used we obtain an O(log n) parallel time bound.},
  number = {1},
  journaltitle = {The Journal of Logic Programming},
  shortjournal = {The Journal of Logic Programming},
  urldate = {2015-04-14},
  date = {1984-06},
  pages = {35-50},
  author = {Dwork, Cynthia and Kanellakis, Paris C. and Mitchell, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/7WQ67ZTB/0743106684900220.html}
}

@inproceedings{Robinson1994reflexive,
  title = {Reflexive graphs and parametric polymorphism},
  doi = {10.1109/LICS.1994.316053},
  abstract = {The pioneering work on relational parametricity for the second order lambda calculus was done by Reynolds (1983) under the assumption of the existence of set-based models, and subsequently reformulated by him, in conjunction with his student Ma, using the technology of PL-categories. The aim of this paper is to use the different technology of internal category theory to re-examine Ma and Reynolds' definitions. Apart from clarifying some of their constructions, this view enables us to prove that if we start with a non-parametric model which is left exact and which satisfies a completeness condition corresponding to Ma and Reynolds “suitability for polymorphism”, then we can recover a parametric model with the same category of closed types. This implies, for example, that any suitably complete model (such as the PER model) has a parametric counterpart},
  eventtitle = {, Symposium on Logic in Computer Science, 1994. LICS '94. Proceedings},
  booktitle = {, Symposium on Logic in Computer Science, 1994. LICS '94. Proceedings},
  date = {1994-07},
  pages = {364-371},
  keywords = {polymorphism,type theory,lambda calculus,category theory,Calculus,formal logic,parametric polymorphism,closed types,completeness condition,Indexing,internal category theory,nonparametric model,parametric model,PER model,reflexive graphs,relational parametricity,second order lambda calculus,set theory,set-based models,Space technology},
  author = {Robinson, E.P. and Rosolini, G.},
  file = {/Users/pgiarrusso/Zotero/storage/8G8SD6CX/Robinson_Rosolini - 1994 - Reflexive graphs and parametric polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/7QPU6JHD/Robinson and Rosolini - 1994 - Reflexive graphs and parametric polymorphism.html}
}
% == BibLateX quality report for Robinson1994reflexive:
% ? Unsure about the formatting of the booktitle

@article{Fragkoulis2015interactive,
  langid = {english},
  title = {An interactive SQL relational interface for querying main-memory data structures},
  issn = {0010-485X, 1436-5057},
  url = {http://link.springer.com/article/10.1007/s00607-015-0452-y},
  doi = {10.1007/s00607-015-0452-y},
  abstract = {Query formalisms and facilities have received significant attention in the past decades resulting in the development of query languages with varying characteristics; many of them resemble sql. Query facilities typically ship as part of database management systems or, sometimes, bundled with programming languages. For applications written in imperative programming languages, database management systems impose an expensive model transformation. In-memory data structures can represent sophisticated relationships in a manner that is efficient in terms of storage and processing overhead, but most general purpose programming languages lack an interpreter and/or an expressive query language for manipulating interactive queries. Issuing interactive ad-hoc queries on program data structures is tough. This work presents a method and an implementation for representing an application’s arbitrary imperative programming data model as a queryable relational one. The Pico COllections Query Library (pico ql) uses a domain specific language to define a relational representation of application data structures and an sql interface implementation. Queries are issued interactively and are type safe. We demonstrate our relational representation for objects and the library’s usefulness on three large c++ projects. pico ql enhances query expressiveness and boosts productivity compared to querying via traditional programming constructs.},
  journaltitle = {Computing},
  shortjournal = {Computing},
  urldate = {2015-04-25},
  date = {2015-04-23},
  pages = {1-24},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),sql,Computer Communication Networks,Computer Science; general,68N15,Computer Appl. in Administrative Data Processing,Information Systems Applications (incl. Internet),Interactive,Main-memory,Object,Query},
  author = {Fragkoulis, Marios and Spinellis, Diomidis and Louridas, Panos},
  file = {/Users/pgiarrusso/Zotero/storage/2QEZ3XR2/Fragkoulis et al. - 2015 - An interactive SQL relational interface for queryi.html}
}
% == BibLateX quality report for Fragkoulis2015interactive:
% 'issn': not a valid ISSN

@inproceedings{Atkey2014relationally,
  location = {{New York, NY, USA}},
  title = {A Relationally Parametric Model of Dependent Type Theory},
  isbn = {978-1-4503-2544-8},
  url = {http://doi.acm.org/10.1145/2535838.2535852},
  doi = {10.1145/2535838.2535852},
  abstract = {Reynolds' theory of relational parametricity captures the invariance of polymorphically typed programs under change of data representation. Reynolds' original work exploited the typing discipline of the polymorphically typed lambda-calculus System F, but there is now considerable interest in extending relational parametricity to type systems that are richer and more expressive than that of System F. This paper constructs parametric models of predicative and impredicative dependent type theory. The significance of our models is twofold. Firstly, in the impredicative variant we are able to deduce the existence of initial algebras for all indexed=functors. To our knowledge, ours is the first account of parametricity for dependent types that is able to lift the useful deduction of the existence of initial algebras in parametric models of System F to the dependently typed setting. Secondly, our models offer conceptual clarity by uniformly expressing relational parametricity for dependent types in terms of reflexive graphs, which allows us to unify the interpretations of types and kinds, instead of taking the relational interpretation of types as a primitive notion. Expressing our model in terms of reflexive graphs ensures that it has canonical choices for the interpretations of the standard type constructors of dependent type theory, except for the interpretation of the universe of small types, where we formulate a refined interpretation tailored for relational parametricity. Moreover, our reflexive graph model opens the door to generalisations of relational parametricity, for example to higher-dimensional relational parametricity.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '14},
  publisher = {{ACM}},
  urldate = {2015-04-26},
  date = {2014},
  pages = {503--515},
  keywords = {relational parametricity,Dependent type theory},
  author = {Atkey, Robert and Ghani, Neil and Johann, Patricia},
  file = {/Users/pgiarrusso/Zotero/storage/6RI9SUJI/Atkey et al - 2014 - A Relationally Parametric Model of Dependent Type Theory.pdf}
}

@inproceedings{Cockx2014pattern,
  location = {{New York, NY, USA}},
  title = {Pattern Matching Without K},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628139},
  doi = {10.1145/2628136.2628139},
  abstract = {Dependent pattern matching is an intuitive way to write programs and proofs in dependently typed languages. It is reminiscent of both pattern matching in functional languages and case analysis in on-paper mathematics. However, in general it is incompatible with new type theories such as homotopy type theory (HoTT). As a consequence, proofs in such theories are typically harder to write and to understand. The source of this incompatibility is the reliance of dependent pattern matching on the so-called K axiom - also known as the uniqueness of identity proofs - which is inadmissible in HoTT. The Agda language supports an experimental criterion to detect definitions by pattern matching that make use of the K axiom, but so far it lacked a formal correctness proof. In this paper, we propose a new criterion for dependent pattern matching without K, and prove it correct by a translation to eliminators in the style of Goguen et al. (2006). Our criterion both allows more good definitions than existing proposals, and solves a previously undetected problem in the criterion offered by Agda. It has been implemented in Agda and is the first to be supported by a formal proof. Thus it brings the benefits of dependent pattern matching to contexts where we cannot assume K, such as HoTT. It also points the way to new forms of dependent pattern matching, for example on higher inductive types.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2015-04-30},
  date = {2014},
  pages = {257--268},
  keywords = {Agda,dependent pattern matching,homotopy type theory,k axiom},
  author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank},
  file = {/Users/pgiarrusso/Zotero/storage/SI6XSE6P/Cockx et al - 2014 - Pattern Matching Without K.pdf}
}

@inproceedings{Cook1989denotational,
  location = {{New York, NY, USA}},
  title = {A Denotational Semantics of Inheritance and Its Correctness},
  isbn = {0-89791-333-7},
  url = {http://doi.acm.org/10.1145/74877.74922},
  doi = {10.1145/74877.74922},
  abstract = {This paper presents a denotational model of inheritance. The model is based on an intuitive motivation of the purpose of inheritance. The correctness of the model is demonstrated by proving it equivalent to an operational semantics of inheritance based upon the method-lookup algorithm of object-oriented languages. Although it was originally developed to explain inheritance in object-oriented languages, the model shows that inheritance is a general mechanism that may be applied to any form of recursive definition.},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages and Applications},
  series = {OOPSLA '89},
  publisher = {{ACM}},
  urldate = {2015-05-04},
  date = {1989},
  pages = {433--443},
  author = {Cook, W. and Palsberg, J.},
  file = {/Users/pgiarrusso/Zotero/storage/5HETFID6/Cook_Palsberg - 1989 - A Denotational Semantics of Inheritance and Its Correctness.pdf}
}

@inproceedings{Atkey2012relational,
  title = {Relational Parametricity for Higher Kinds},
  volume = {16},
  doi = {10.4230/LIPIcs.CSL.2012.46},
  booktitle = {Computer Science Logic (CSL'12) - 26th International Workshop/21st Annual Conference of the EACSL},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  date = {2012},
  pages = {46-61},
  author = {Atkey, Robert},
  editor = {Cégielski, Patrick and Durand, Arnaud},
  file = {/Users/pgiarrusso/Zotero/storage/N897FBJT/Atkey - 2012 - Relational Parametricity for Higher Kinds.pdf}
}
% == BibLateX quality report for Atkey2012relational:
% ? Unsure about the formatting of the booktitle

@inproceedings{Harper1995compiling,
  location = {{New York, NY, USA}},
  title = {Compiling Polymorphism Using Intensional Type Analysis},
  isbn = {0-89791-692-1},
  url = {http://doi.acm.org/10.1145/199448.199475},
  doi = {10.1145/199448.199475},
  abstract = {Traditional techniques for implementing polymorphism use a universal representation for objects of unknown type. Often, this forces a compiler to use universal representations even if the types of objects are known. We examine an alternative approach for compiling polymorphism where types are passed as arguments to polymorphic routines in order to determine the representation of an object. This approach allows monomorphic code to use natural, efficient representations, supports separate compilation of polymorphic definitions and, unlike coercion-based implementations of polymorphism, natural representations can be used for mutable objects such as refs and arrays.We are particularly interested in the typing properties of an intermediate language that allows run-time type  analysis to be coded within the language. This allows us to compile many representation transformations and many language features without adding new primitive operations to the language. In this paper, we provide a core target language where type-analysis operators can be coded within the language and the types of such operators can be accurately tracked. The target language is powerful enough to code a variety of useful features, yet type checking remains decidable. We show how to translate an ML-like language into the target language so that primitive operators can analyze types to produce efficient representations. We demonstrate the power of the “user-level” operators by coding flattened tuples, marshalling, type classes, and a form of type dynamic within the  language.},
  booktitle = {Proceedings of the 22Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '95},
  publisher = {{ACM}},
  urldate = {2014-11-10},
  date = {1995},
  pages = {130--141},
  author = {Harper, Robert and Morrisett, Greg}
}

@article{Wadler1992comprehending,
  title = {Comprehending monads},
  volume = {2},
  issn = {1469-8072},
  url = {http://journals.cambridge.org/article_S0960129500001560},
  doi = {10.1017/S0960129500001560},
  abstract = {Category theorists invented monads in the 1960's to express concisely certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to express concisely certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can express concisely in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.},
  number = {04},
  journaltitle = {Mathematical Structures in Computer Science},
  urldate = {2014-11-10},
  date = {1992-12},
  pages = {461--493},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/JQXMSADK/displayAbstract.html}
}

@article{Harper2000typetheoretic,
  title = {A Type-Theoretic Interpretation of Standard ML},
  url = {http://repository.cmu.edu/compsci/898},
  journaltitle = {Proof, Language, and Interaction Essays in Honour of Robin Milner Edited by Gordon Plotkin, Colin Stirling and Mads Tofte},
  date = {2000-05-01},
  author = {Harper, Robert and Stone, Christopher},
  file = {/Users/pgiarrusso/Zotero/storage/KZASC44C/Harper_Stone - 2000 - A Type-Theoretic Interpretation of Standard ML.pdf;/Users/pgiarrusso/Zotero/storage/KMRQVSNK/898.html}
}

@article{Bauer2009rz,
  langid = {english},
  title = {RZ: a Tool for Bringing Constructive and Computable Mathematics Closer to Programming Practice},
  volume = {19},
  issn = {0955-792X, 1465-363X},
  url = {http://logcom.oxfordjournals.org/content/19/1/17},
  doi = {10.1093/logcom/exn026},
  shorttitle = {RZ},
  abstract = {Realizability theory is not just a fundamental tool in logic and computability. It also has direct application to the design and implementation of programs, since it can produce code interfaces for the data structure corresponding to a mathematical theory. Our tool, called RZ, serves as a bridge between the worlds of constructive mathematics and programming. By using the realizability interpretation of constructive mathematics, RZ translates specifications in constructive logic into annotated interface code in Objective Caml. The system supports a rich input language allowing descriptions of complex mathematical structures. RZ does not extract code from proofs, but allows any implementation method, from handwritten code to code extracted from proofs by other tools.},
  number = {1},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {J Logic Computation},
  urldate = {2014-11-10},
  date = {2009-01-02},
  pages = {17-43},
  author = {Bauer, Andrej and Stone, Christopher A.},
  file = {/Users/pgiarrusso/Zotero/storage/PVCUTITS/Bauer_Stone - 2009 - RZ - a Tool for Bringing Constructive and Computable Mathematics Closer to Programming Practice.pdf;/Users/pgiarrusso/Zotero/storage/JM5QT7TN/17.html}
}
% == BibLateX quality report for Bauer2009rz:
% 'issn': not a valid ISSN

@article{Hyland1988small,
  title = {A small complete category},
  volume = {40},
  issn = {0168-0072},
  url = {http://www.sciencedirect.com/science/article/pii/0168007288900188},
  doi = {10.1016/0168-0072(88)90018-8},
  number = {2},
  journaltitle = {Annals of Pure and Applied Logic},
  shortjournal = {Annals of Pure and Applied Logic},
  urldate = {2014-11-14},
  date = {1988-11},
  pages = {135-165},
  author = {Hyland, J. M. E.},
  file = {/Users/pgiarrusso/Zotero/storage/HGT4I4DT/0168007288900188.html}
}

@article{Scott1976data,
  title = {Data Types as Lattices},
  volume = {5},
  issn = {0097-5397},
  url = {http://epubs.siam.org/doi/abs/10.1137/0205037},
  doi = {10.1137/0205037},
  abstract = {The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of “partial” objects. In this report these spaces are modeled in one universal domain \$\{$\backslash$bf P\} $\backslash$omega  \$, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.,  The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of “partial” objects. In this report these spaces are modeled in one universal domain \$\{$\backslash$bf P\} $\backslash$omega  \$, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.},
  number = {3},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  urldate = {2014-11-14},
  date = {1976-09-01},
  pages = {522-587},
  author = {Scott, D.},
  file = {/Users/pgiarrusso/Zotero/storage/AIWQQ376/0205037.html}
}

@incollection{Hyland1982effective,
  title = {The Effective Topos},
  volume = {Volume 110},
  isbn = {0049-237X},
  url = {http://www.sciencedirect.com/science/article/pii/S0049237X09701296},
  abstract = {This chapter describes the most accessible of the series of toposes that can be constructed from notions of realizability: it is that based on the original notion of recursive realizability and presents the abstract approach to recursive realizability in some detail. The chapter introduces effective topos and discusses the notion of a negative formula that arises naturally in the theory of sheaves. The chapter presents features of effective topos, where the power-set matters: uniformity principles and properties of j-operators. The chapter recommends that while constructing a topos from a tripos, one must add new subobjects of the sets one has started with to represent the nonstandard predicates and take quotients of these by the nonstandard equivalence relations.},
  booktitle = {Studies in Logic and the Foundations of Mathematics},
  series = {The L. E. J. Brouwer Centenary Symposium Proceedings of the Conference held in Noordwijkerhout},
  publisher = {{Elsevier}},
  urldate = {2014-11-14},
  date = {1982},
  pages = {165-216},
  author = {Hyland, J. M. E.},
  editor = {{A.S. Troelstra and D. van Dalen}},
  file = {/Users/pgiarrusso/Zotero/storage/EJAPWK4E/S0049237X09701296.html}
}
% == BibLateX quality report for Hyland1982effective:
% 'isbn': not a valid ISBN

@article{Miranda-Perea2009selective,
  title = {Selective Memoization with Box Types},
  volume = {256},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066109004563},
  doi = {10.1016/j.entcs.2009.11.006},
  abstract = {Memoization is a useful technique to eliminate computational redundancy. A memo function remembers all the arguments to which it has been applied, together with their corresponding results, by storing them in a table. This table is consulted before each functional call to determine if the particular argument is in it. If so, the call is skipped and the stored result is returned; otherwise the call is performed and its result added to the table. Acar, Belloch and Harper present a framework to apply memoization selectively, that is, enabling the programmer to determine precisely the dependences between the input and the result of a function. This framework is efficient and yields programs whose performance can be analyzed using standard techniques. The language, implemented as an SML library, is based on a modal type system which allows the programmer to reveal the true data input/output dependences in a program. However, the modality seems to be an ad-hoc choice for the implementation. In this paper we develop selective memoization, using instead box types, corresponding to the necessitation modality □. We also include non-memoized functions, and provide full proofs of type safeness and soundness of the dynamic semantics with respect to an effect-free system which is later translated into the very well-known language PCF.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the Fourth Workshop on Logical and Semantic Frameworks, with Applications (LSFA 2009)},
  urldate = {2014-11-15},
  date = {2009-12-02},
  pages = {67-85},
  keywords = {functional programming,adaptive computation,box types,modal types,selective memoization,type safeness},
  author = {Miranda-Perea, Favio Ezequiel and González-Huesca, Lourdes Del Carmen},
  file = {/Users/pgiarrusso/Zotero/storage/959IDMPC/Miranda-Perea_González-Huesca - 2009 - Selective Memoization with Box Types.pdf;/Users/pgiarrusso/Zotero/storage/BN9QM3PV/S1571066109004563.html}
}

@inproceedings{Ziliani2013mtac,
  location = {{New York, NY, USA}},
  title = {Mtac: A Monad for Typed Tactic Programming in Coq},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500579},
  doi = {10.1145/2500365.2500579},
  shorttitle = {Mtac},
  abstract = {Effective support for custom proof automation is essential for large scale interactive proof development. However, existing languages for automation via *tactics* either (a) provide no way to specify the behavior of tactics within the base logic of the accompanying theorem prover, or (b) rely on advanced type-theoretic machinery that is not easily integrated into established theorem provers. We present Mtac, a lightweight but powerful extension to Coq that supports dependently-typed tactic programming. Mtac tactics have access to all the features of ordinary Coq programming, as well as a new set of typed tactical primitives. We avoid the need to touch the trusted kernel typechecker of Coq by encapsulating uses of these new tactical primitives in a *monad*, and instrumenting Coq so that it executes monadic tactics during type inference.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2014-11-15},
  date = {2013},
  pages = {87--100},
  keywords = {coq,interactive theorem proving,Monads,custom proof automation,tactics,typed meta-programming},
  author = {Ziliani, Beta and Dreyer, Derek and Krishnaswami, Neelakantan R. and Nanevski, Aleksandar and Vafeiadis, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/ZQPD3TIP/Ziliani et al - 2013 - Mtac - A Monad for Typed Tactic Programming in Coq.pdf}
}

@incollection{Abbott2003derivatives,
  langid = {english},
  title = {Derivatives of Containers},
  isbn = {978-3-540-40332-6 978-3-540-44904-1},
  url = {http://link.springer.com/chapter/10.1007/3-540-44904-3_2},
  abstract = {We are investigating McBride’s idea that the type of one-hole contexts are the formal derivative of a functor from a categorical perspective. Exploiting our recent work on containers we are able to characterise derivatives by a universal property and show that the laws of calculus including a rule for initial algebras as presented by McBride hold — hence the differentiable containers include those generated by polynomials and least fixpoints. Finally, we discuss abstract containers (i.e. quotients of containers) — this includes a container which plays the role of e x in calculus by being its own derivative.},
  number = {2701},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-16},
  date = {2003-01-01},
  pages = {16-30},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Abbott, Michael and Altenkirch, Thorsten and Ghani, Neil and McBride, Conor},
  editor = {Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/4REBBFPK/Abbott-Altenkirch-Ghani-McBride - 2003 - Derivatives of Containers.pdf;/Users/pgiarrusso/Zotero/storage/KMJDFHJW/3-540-44904-3_2.html}
}
% == BibLateX quality report for Abbott2003derivatives:
% 'isbn': not a valid ISBN

@article{Abbott2005data,
  title = {∂ for Data: Differentiating Data Structures},
  volume = {65},
  url = {http://iospress.metapress.com/content/JJ599WW44R90MAA0},
  shorttitle = {∂ for Data},
  abstract = {This paper and our conference paper (Abbott, Altenkirch, Ghani, and McBride, 2003b) explain and analyse the notion of the derivative of a data structure as the type of its one-hole contexts based on the central observation made by McBride (2001). To make the idea precise we need a generic notion of a data type, which leads to the notion of a container, introduced in (Abbott, Altenkirch, and Ghani, 2003a) and investigated extensively in (Abbott, 2003). Using containers we can provide a notion of linear map which is the concept missing from McBride's first analysis. We verify the usual laws of differential calculus including the chain rule and establish laws for initial algebras and terminal coalgebras.},
  number = {1},
  journaltitle = {Fundamenta Informaticae},
  urldate = {2014-11-16},
  date = {2005-01-01},
  pages = {1-28},
  author = {Abbott, Michael and Altenkirch, Thorsten and McBride, Conor and Ghani, Neil},
  file = {/Users/pgiarrusso/Zotero/storage/CKGB4BXD/Abbott-Altenkirch-McBride-Ghani - 2005 - ∂ for Data - Differentiating Data Structures.pdf;/Users/pgiarrusso/Zotero/storage/JG82PD9J/JJ599WW44R90MAA0.html}
}

@incollection{Reynolds2009using,
  langid = {english},
  title = {Using Category Theory to Design Programming Languages},
  isbn = {978-3-642-00589-3 978-3-642-00590-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-00590-9_5},
  abstract = {In a 1980 paper entitled “Using Category Theory to Design Conversions and Generic Operators”, the author showed how the concepts of category theory can guide the design of a programming language to avoid anomalies in the interaction of implicit conversions and generic operators.},
  number = {5502},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-18},
  date = {2009-01-01},
  pages = {62-63},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Software Engineering/Programming and Operating Systems,Theory of Computation},
  author = {Reynolds, John C.},
  editor = {Castagna, Giuseppe},
  file = {/Users/pgiarrusso/Zotero/storage/7Q9BNVNC/978-3-642-00590-9_5.html}
}
% == BibLateX quality report for Reynolds2009using:
% 'isbn': not a valid ISBN

@inproceedings{Murray2011steno,
  location = {{New York, NY, USA}},
  title = {Steno: Automatic Optimization of Declarative Queries},
  isbn = {978-1-4503-0663-8},
  url = {http://doi.acm.org/10.1145/1993498.1993513},
  doi = {10.1145/1993498.1993513},
  shorttitle = {Steno},
  abstract = {Declarative queries enable programmers to write data manipulation code without being aware of the underlying data structure implementation. By increasing the level of abstraction over imperative code, they improve program readability and, crucially, create opportunities for automatic parallelization and optimization. For example, the Language Integrated Query (LINQ) extensions to C\# allow the same declarative query to process in-memory collections, and datasets that are distributed across a compute cluster. However, our experiments show that the serial performance of declarative code is several times slower than the equivalent hand-optimized code, because it is implemented using run-time abstractions---such as iterators---that incur overhead due to virtual function calls and superfluous instructions. To address this problem, we have developed Steno, which uses a combination of novel and well-known techniques to generate code for declarative queries that is almost as efficient as hand-optimized code. Steno translates a declarative LINQ query into type-specialized, inlined and loop-based imperative code. It eliminates chains of iterators from query execution, and optimizes nested queries. We have implemented Steno for uniprocessor, multiprocessor and distributed computing platforms, and show that, for a real-world distributed job, it can almost double the speed of end-to-end execution.},
  booktitle = {Proceedings of the 32Nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '11},
  publisher = {{ACM}},
  urldate = {2014-11-19},
  date = {2011},
  pages = {121--131},
  keywords = {abstract machines,query optimization},
  author = {Murray, Derek Gordon and Isard, Michael and Yu, Yuan},
  file = {/Users/pgiarrusso/Zotero/storage/UXHIC3K2/Murray et al - 2011 - Steno - Automatic Optimization of Declarative Queries.pdf}
}

@incollection{Omar2014safely,
  langid = {english},
  title = {Safely Composable Type-Specific Languages},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-662-44202-9_5},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  number = {8586},
  booktitle = {ECOOP 2014 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-19},
  date = {2014-01-01},
  pages = {105-130},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Computer Communication Networks,bidirectional typechecking,extensible languages,hygiene,parsing},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  file = {/Users/pgiarrusso/Zotero/storage/R7ZEUSVE/Omar et al - 2014 - Safely Composable Type-Specific Languages.pdf;/Users/pgiarrusso/Zotero/storage/VRWWAXBW/978-3-662-44202-9_5.html}
}
% == BibLateX quality report for Omar2014safely:
% 'isbn': not a valid ISBN

@inproceedings{Curien2000duality,
  location = {{New York, NY, USA}},
  title = {The Duality of Computation},
  isbn = {1-58113-202-6},
  url = {http://doi.acm.org/10.1145/351240.351262},
  doi = {10.1145/351240.351262},
  abstract = {We present the \&mu; -calculus, a syntax for \&lambda;-calculus + control operators exhibiting symmetries such as program/context and call-by-name/call-by-value. This calculus is derived from implicational Gentzen's sequent calculus LK, a key classical logical system in proof theory. Under the Curry-Howard correspondence between proofs and programs, we can see LK, or more precisely a formulation called LK\&mu; , as a syntax-directed system of simple types for \&mu; -calculus. For \&mu; -calculus, choosing a call-by-name or call-by-value discipline for reduction amounts to choosing one of the two possible symmetric orientations of a critical pair. Our analysis leads us to revisit the question of what is a natural syntax for call-by-value functional computation. We define a translation of \&lambda;\&mu;-calculus into \&mu; -calculus and two dual translations back to \&lambda;-calculus, and we recover known CPS translations by composing these translations.},
  booktitle = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '00},
  publisher = {{ACM}},
  urldate = {2014-11-20},
  date = {2000},
  pages = {233--243},
  author = {Curien, Pierre-Louis and Herbelin, Hugo},
  file = {/Users/pgiarrusso/Zotero/storage/JQUA5U65/Curien_Herbelin - 2000 - The Duality of Computation.pdf}
}

@article{Cervesato2003linear,
  langid = {english},
  title = {A Linear Spine Calculus},
  volume = {13},
  issn = {0955-792X, 1465-363X},
  url = {http://logcom.oxfordjournals.org/content/13/5/639},
  doi = {10.1093/logcom/13.5.639},
  abstract = {We present the spine calculus S→⊸\&⊤ as an efficient representation for the linear λ-calculus λ→⊸\&⊤ which includes unrestricted functions (→) linear functions (⊸) additive pairing (\&) and additive unit (⊤). S→⊸\&⊤ enhances the representation of Church's simply typed λ-calculus by enforcing extensionality and by incorporating linear constructs. This approach permits procedures such as unification to retain the efficient head access that characterizes first-order term languages without the overhead of performing η-conversions at run time. Applications lie in proof search, logic programming, and logical frameworks based on linear type theories. It is also related to foundational work on term assignment calculi for presentations of the sequent calculus. We define the spine calculus, give translations of λ→⊸\&⊤ into S→⊸\&⊤ and vice versa, prove their soundness and completeness with respect to typing and reductions, and show that the typable fragment of the spine calculus is strongly normalizing and admits unique canonical, i.e. βη-normal, forms.},
  number = {5},
  journaltitle = {Journal of Logic and Computation},
  shortjournal = {J Logic Computation},
  urldate = {2014-11-20},
  date = {2003-01-10},
  pages = {639-688},
  keywords = {Linear lambda calculus; term assignment systems; uniform provability.},
  author = {Cervesato, Iliano and Pfenning, Frank},
  file = {/Users/pgiarrusso/Zotero/storage/DDKF9IXH/Cervesato_Pfenning - 2003 - A Linear Spine Calculus.pdf;/Users/pgiarrusso/Zotero/storage/H3G2JVFG/639.html}
}
% == BibLateX quality report for Cervesato2003linear:
% 'issn': not a valid ISSN

@incollection{McLaughlin2008imogen,
  langid = {english},
  title = {Imogen: Focusing the Polarized Inverse Method for Intuitionistic Propositional Logic},
  isbn = {978-3-540-89438-4 978-3-540-89439-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-89439-1_12},
  shorttitle = {Imogen},
  abstract = {In this paper we describe Imogen, a theorem prover for intuitionistic propositional logic using the focused inverse method. We represent fine-grained control of the search behavior by polarizing the input formula. In manipulating the polarity of atoms and subformulas, we can often improve the search time by several orders of magnitude. We tested our method against seven other systems on the propositional fragment of the ILTP library. We found that our prover outperforms all other provers on a substantial subset of the library.},
  number = {5330},
  booktitle = {Logic for Programming, Artificial Intelligence, and Reasoning},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-21},
  date = {2008-01-01},
  pages = {174-181},
  keywords = {Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {McLaughlin, Sean and Pfenning, Frank},
  editor = {Cervesato, Iliano and Veith, Helmut and Voronkov, Andrei},
  file = {/Users/pgiarrusso/Zotero/storage/FX8WCG62/McLaughlin_Pfenning - 2008 - Imogen - Focusing the Polarized Inverse Method for Intuitionistic Propositional Logic.pdf;/Users/pgiarrusso/Zotero/storage/H9ZACNTD/978-3-540-89439-1_12.html}
}
% == BibLateX quality report for McLaughlin2008imogen:
% 'isbn': not a valid ISBN

@inproceedings{Kammar2012algebraic,
  location = {{New York, NY, USA}},
  title = {Algebraic Foundations for Effect-dependent Optimisations},
  isbn = {978-1-4503-1083-3},
  url = {http://doi.acm.org/10.1145/2103656.2103698},
  doi = {10.1145/2103656.2103698},
  abstract = {We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity).},
  booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '12},
  publisher = {{ACM}},
  urldate = {2014-11-21},
  date = {2012},
  pages = {349--360},
  keywords = {computational effects,Denotational semantics,algebraic theory of effects,call-by-push-value,code transformations,compiler optimisations,domain theory,inequational logic,relevant and affine monads,sum and tensor,type and effect systems,universal algebra},
  author = {Kammar, Ohad and Plotkin, Gordon D.},
  file = {/Users/pgiarrusso/Zotero/storage/GJGNWJSR/Kammar_Plotkin - 2012 - Algebraic Foundations for Effect-dependent Optimisations.pdf}
}

@article{Barbanera1994prooffunctional,
  langid = {english},
  title = {Proof-functional connectives and realizability},
  volume = {33},
  issn = {0933-5846, 1432-0665},
  url = {http://link.springer.com/article/10.1007/BF01203032},
  doi = {10.1007/BF01203032},
  abstract = {The meaning of a formula built out of proof-functional connectives depends in an essential way upon the intensional aspect of the proofs of the component subformulas. We study three such connectives, strong equivalence (where the two directions of the equivalence are established by mutually inverse maps), strong conjunction (where the two components of the conjunction are established by the same proof) and relevant implication (where the implication is established by an identity map). For each of these connectives we give a type assignment system, a realizability semantics, and a completeness theorem. This form of completeness implies the semantic completeness of the type assignment system.},
  number = {3},
  journaltitle = {Archive for Mathematical Logic},
  shortjournal = {Arch Math Logic},
  urldate = {2014-11-21},
  date = {1994-05-01},
  pages = {189-211},
  keywords = {Mathematical Logic and Foundations,03F07,03F55,Algebra,Mathematics; general},
  author = {Barbanera, Franco and Martini, Simone},
  file = {/Users/pgiarrusso/Zotero/storage/WHI7SAZJ/BF01203032.html}
}
% == BibLateX quality report for Barbanera1994prooffunctional:
% 'issn': not a valid ISSN

@article{Amadio1991recursion,
  title = {Recursion over realizability structures},
  volume = {91},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/089054019190074C},
  doi = {10.1016/0890-5401(91)90074-C},
  abstract = {Realizability structures play a major role in the metamathematics of intuitionistic systems and they are a basic tool in the extraction of the computational content of constructive proofs. Besides their rich categorical structure and effectiveness properties provide a privileged mathematical setting for the semantics of data types of programming languages. In this paper we emphasize the modelling of recursive definitions of programs and types. A realizability model for a language including Girard's system F and an operator of recursion on types is given and some of its local properties are studied.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2014-11-21},
  date = {1991-03},
  pages = {55-85},
  author = {Amadio, Roberto M.},
  file = {/Users/pgiarrusso/Zotero/storage/4F7REMSF/089054019190074C.html}
}

@incollection{McCarthy1981history,
  location = {{New York, NY, USA}},
  title = {History of LISP},
  isbn = {0-12-745040-8},
  url = {http://doi.acm.org/10.1145/800025.1198360},
  booktitle = {History of Programming Languages I},
  publisher = {{ACM}},
  urldate = {2014-11-21},
  date = {1981},
  pages = {173--185},
  author = {McCarthy, John},
  editor = {Wexelblat, Richard L.},
  file = {/Users/pgiarrusso/Zotero/storage/DF2PB448/McCarthy - 1981 - History of LISP.pdf}
}

@inproceedings{Longley1999when,
  location = {{New York, NY, USA}},
  title = {When is a Functional Program Not a Functional Program?},
  isbn = {1-58113-111-9},
  url = {http://doi.acm.org/10.1145/317636.317775},
  doi = {10.1145/317636.317775},
  abstract = {In an impure functional language, there are programs whose behaviour is completely functional (in that they behave extensionally on inputs), but the functions they compute cannot be written in the purely functional fragment of the language. That is, the class of programs with functional behaviour is more expressive than the usual class of pure functional programs. In this paper we introduce this extended class of "functional" programs by means of examples in Standard ML, and explore what they might have to offer to programmers and language implementors.After reviewing some theoretical background, we present some examples of functions of the above kind, and discuss how they may be implemented. We then consider two possible programming applications for these functions: the implementation of a search algorithm, and an algorithm for exact real-number integration. We discuss the advantages and limitations of this style of programming relative to other approaches. We also consider the increased scope for compiler optimizations that these functions would offer.},
  booktitle = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '99},
  publisher = {{ACM}},
  urldate = {2014-11-21},
  date = {1999},
  pages = {1--7},
  author = {Longley, John},
  file = {/Users/pgiarrusso/Zotero/storage/XP96WVGP/Longley - 1999 - When is a Functional Program Not a Functional Program.pdf}
}

@article{Longley1999matching,
  title = {Matching typed and untyped realizability},
  volume = {23},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104001057},
  doi = {10.1016/S1571-0661(04)00105-7},
  abstract = {Realizability interpretations of logics are given by saying what it means for computational objects of some kind to realize logical formulae. The computational objects in question might be drawn from an untyped universe of computation, such as a partial combinatory algebra, or they might be typed objects such as terms of a PCF-style programming language. In some instances, one can show that a particular untyped realizability interpretation matches a particular typed one, in the sense that they give the same set of realizable formulae. In this case, we have a very good fit indeed between the typed language and the untyped realizability model — we refer to this condition as (constructive) logical full abstraction.

We give some examples of this situation for a variety of extensions of PCF. Of particular interest are some models that are logically fully abstract for typed languages including non-functional features. Our results establish connections between what is computable in various programming languages and what is true inside various realizability toposes. We consider some examples of logical formulae to illustrate these ideas, in particular their application to exact real-number computability.},
  number = {1},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Tutorial Workshop on Realizability Semantics and Applications (associated to FLoC'99, the 1999 Federated Logic Conference)},
  urldate = {2014-11-21},
  date = {1999},
  pages = {74-100},
  author = {Longley, John},
  file = {/Users/pgiarrusso/Zotero/storage/EXHSFXMU/S1571066104001057.html}
}

@article{Bernadet2013simple,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.3832},
  primaryClass = {cs},
  title = {A simple presentation of the effective topos},
  url = {http://arxiv.org/abs/1307.3832},
  abstract = {We propose for the Effective Topos an alternative construction: a realisability framework composed of two levels of abstraction. This construction simplifies the proof that the Effective Topos is a topos (equipped with natural numbers), which is the main issue that this paper addresses. In this our work can be compared to Frey's monadic tripos-to-topos construction. However, no topos theory or even category theory is here required for the construction of the framework itself, which provides a semantics for higher-order type theories, supporting extensional equalities and the axiom of unique choice.},
  urldate = {2014-11-21},
  date = {2013-07-15},
  keywords = {Computer Science - Logic in Computer Science},
  author = {Bernadet, Alexis and Graham-Lengrand, Stéphane},
  file = {/Users/pgiarrusso/Zotero/storage/U995SC49/Bernadet_Graham-Lengrand - 2013 - A simple presentation of the effective topos.pdf;/Users/pgiarrusso/Zotero/storage/VHQIXANT/1307.html}
}
% == BibLateX quality report for Bernadet2013simple:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Pitts1987polymorphism,
  langid = {english},
  title = {Polymorphism is set theoretic, constructively},
  isbn = {978-3-540-18508-6 978-3-540-48006-8},
  url = {http://link.springer.com/chapter/10.1007/3-540-18508-9_18},
  number = {283},
  booktitle = {Category Theory and Computer Science},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-22},
  date = {1987-01-01},
  pages = {12-39},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Pitts, A. M.},
  editor = {Pitt, David H. and Poigné, Axel and Rydeheard, David E.},
  file = {/Users/pgiarrusso/Zotero/storage/3CW6IV9N/Pitts - 1987 - Polymorphism is set theoretic, constructively.pdf;/Users/pgiarrusso/Zotero/storage/JZ8DXHTH/3-540-18508-9_18.html}
}
% == BibLateX quality report for Pitts1987polymorphism:
% 'isbn': not a valid ISBN

@article{Fulop2012survey,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1203.4912},
  primaryClass = {cs},
  title = {A survey of proof nets and matrices for substructural logics},
  url = {http://arxiv.org/abs/1203.4912},
  abstract = {This paper is a survey of two kinds of "compressed" proof schemes, the $\backslash$emph\{matrix method\} and $\backslash$emph\{proof nets\}, as applied to a variety of logics ranging along the substructural hierarchy from classical all the way down to the nonassociative Lambek system. A novel treatment of proof nets for the latter is provided. Descriptions of proof nets and matrices are given in a uniform notation based on sequents, so that the properties of the schemes for the various logics can be easily compared.},
  urldate = {2014-11-24},
  date = {2012-03-22},
  keywords = {Computer Science - Logic in Computer Science},
  author = {Fulop, Sean A.},
  file = {/Users/pgiarrusso/Zotero/storage/FR5B38RU/Fulop - 2012 - A survey of proof nets and matrices for substructural logics.pdf;/Users/pgiarrusso/Zotero/storage/MGUQNA3F/1203.html}
}
% == BibLateX quality report for Fulop2012survey:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Demetrescu2014reactive,
  title = {Reactive Imperative Programming with Dataflow Constraints},
  volume = {37},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/2623200},
  doi = {10.1145/2623200},
  abstract = {Dataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes in their environment. In this article, we show that one-way dataflow constraints, largely explored in the context of interactive applications, can be seamlessly integrated in any imperative language and can be used as a general paradigm for writing performance-critical reactive applications that require efficient incremental computations. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as “reactive.” Reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically re-executed every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend on the values of other variables. The constraint-solving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using off-the-shelf compilers. We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, data structure repair, and software visualization. The performance of our implementation is compared to problem-specific change propagation algorithms, as well as to language-centric approaches such as self-adjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice.},
  number = {1},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2014-11-24},
  date = {2014-11},
  pages = {3:1--3:53},
  keywords = {incremental computation,constraint solving,data structure repair,dataflow programming,imperative programming,observer design pattern,one-way dataflow constraints,reactive programming,software visualization},
  author = {Demetrescu, Camil and Finocchi, Irene and Ribichini, Andrea}
}
% == BibLateX quality report for Demetrescu2014reactive:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@article{Bauerprogramming,
  title = {Programming with algebraic effects and handlers},
  issn = {2352-2208},
  url = {http://www.sciencedirect.com/science/article/pii/S2352220814000194},
  doi = {10.1016/j.jlamp.2014.02.001},
  abstract = {Eff is a programming language based on the algebraic approach to computational effects, in which effects are viewed as algebraic operations and effect handlers as homomorphisms from free algebras. Eff supports first-class effects and handlers through which we may easily define new computational effects, seamlessly combine existing ones, and handle them in novel ways. We give a denotational semantics of Eff and discuss a prototype implementation based on it. Through examples we demonstrate how the standard effects are treated in Eff, and how Eff supports programming techniques that use various forms of delimited continuations, such as backtracking, breadth-first search, selection functionals, cooperative multi-threading, and others.},
  journaltitle = {Journal of Logical and Algebraic Methods in Programming},
  shortjournal = {Journal of Logical and Algebraic Methods in Programming},
  urldate = {2014-11-24},
  author = {Bauer, Andrej and Pretnar, Matija},
  file = {/Users/pgiarrusso/Zotero/storage/HKGFDF6R/Bauer_Pretnar - Programming with algebraic effects and handlers.pdf;/Users/pgiarrusso/Zotero/storage/72PH3PK6/S2352220814000194.html}
}
% == BibLateX quality report for Bauerprogramming:
% Exactly one of 'date' / 'year' must be present

@incollection{Plotkin2009handlers,
  langid = {english},
  title = {Handlers of Algebraic Effects},
  isbn = {978-3-642-00589-3 978-3-642-00590-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-00590-9_7},
  abstract = {We present an algebraic treatment of exception handlers and, more generally, introduce handlers for other computational effects representable by an algebraic theory. These include nondeterminism, interactive input/output, concurrency, state, time, and their combinations; in all cases the computation monad is the free-model monad of the theory. Each such handler corresponds to a model of the theory for the effects at hand. The handling construct, which applies a handler to a computation, is based on the one introduced by Benton and Kennedy, and is interpreted using the homomorphism induced by the universal property of the free model. This general construct can be used to describe previously unrelated concepts from both theory and practice.},
  number = {5502},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-24},
  date = {2009-01-01},
  pages = {80-94},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Software Engineering/Programming and Operating Systems,Theory of Computation},
  author = {Plotkin, Gordon and Pretnar, Matija},
  editor = {Castagna, Giuseppe},
  file = {/Users/pgiarrusso/Zotero/storage/43WWK699/Plotkin_Pretnar - 2009 - Handlers of Algebraic Effects.pdf;/Users/pgiarrusso/Zotero/storage/BED8EUVE/978-3-642-00590-9_7.html}
}
% == BibLateX quality report for Plotkin2009handlers:
% 'isbn': not a valid ISBN

@inproceedings{Voigtlander2009free,
  location = {{New York, NY, USA}},
  title = {Free Theorems Involving Type Constructor Classes: Functional Pearl},
  isbn = {978-1-60558-332-7},
  url = {http://doi.acm.org/10.1145/1596550.1596577},
  doi = {10.1145/1596550.1596577},
  shorttitle = {Free Theorems Involving Type Constructor Classes},
  abstract = {Free theorems are a charm, allowing the derivation of useful statements about programs from their (polymorphic) types alone. We show how to reap such theorems not only from polymorphism over ordinary types, but also from polymorphism over type constructors restricted by class constraints. Our prime application area is that of monads, which form the probably most popular type constructor class of Haskell. To demonstrate the broader scope, we also deal with a transparent way of introducing difference lists into a program, endowed with a neat and general correctness proof.},
  booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '09},
  publisher = {{ACM}},
  urldate = {2014-11-24},
  date = {2009},
  pages = {173--184},
  keywords = {parametricity,relational},
  author = {Voigtländer, Janis},
  file = {/Users/pgiarrusso/Zotero/storage/FM6XAUJV/Voigtländer - 2009 - Free Theorems Involving Type Constructor Classes - Functional Pearl.pdf;/Users/pgiarrusso/Zotero/storage/WTP5U5R7/Voigtländer - 2009 - Free Theorems Involving Type Constructor Classes - Functional Pearl.pdf}
}

@inproceedings{Fiore1999abstract,
  title = {Abstract syntax and variable binding},
  doi = {10.1109/LICS.1999.782615},
  abstract = {We develop a theory of abstract syntax with variable binding. To every binding signature we associate a category of models consisting of variable sets endowed with compatible algebra and substitution structures. The syntax generated by the signature is the initial model. This gives a notion of initial algebra semantics encompassing the traditional one; besides compositionality, it automatically verifies the semantic substitution lemma},
  eventtitle = {14th Symposium on Logic in Computer Science, 1999. Proceedings},
  booktitle = {14th Symposium on Logic in Computer Science, 1999. Proceedings},
  date = {1999},
  pages = {193-202},
  keywords = {Algebra,abstract syntax,Carbon capture and storage,category of models,compatible algebra,Computer languages,Concrete,initial algebra semantics,Integral equations,Logic,process algebra,Production,Proposals,semantic substitution lemma,substitution structures,Tree graphs,variable binding},
  author = {Fiore, M. and Plotkin, G. and Turi, D.},
  file = {/Users/pgiarrusso/Zotero/storage/BF8ANTWG/Fiore et al - 1999 - Abstract syntax and variable binding.pdf;/Users/pgiarrusso/Zotero/storage/4WSQVAX2/login.html}
}
% == BibLateX quality report for Fiore1999abstract:
% ? Unsure about the formatting of the booktitle

@incollection{Filinski2001normalization,
  langid = {english},
  title = {Normalization by Evaluation for the Computational Lambda-Calculus},
  isbn = {978-3-540-41960-0 978-3-540-45413-7},
  url = {http://link.springer.com/chapter/10.1007/3-540-45413-6_15},
  abstract = {We show how a simple semantic characterization of normalization by evaluation for the λβη-calculus can be extended to a similar construction for normalization of terms in the computational λ-calculus. Specifically, we show that a suitable residualizing interpretation of base types, constants, and computational effects allows us to extract a syntactic normal form from a term’s denotation. The required interpretation can itself be constructed as the meaning of a suitable functional program in an ML-like language, leading directly to a practical normalization algorithm. The results extend easily to product and sum types, and can be seen as a formal basis for call-by-value type-directed partial evaluation.},
  number = {2044},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-24},
  date = {2001-01-01},
  pages = {151-165},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Filinski, Andrzej},
  editor = {Abramsky, Samson},
  file = {/Users/pgiarrusso/Zotero/storage/E2ZZ5RI7/3-540-45413-6_15.html}
}
% == BibLateX quality report for Filinski2001normalization:
% 'isbn': not a valid ISBN

@article{Moggi1991notions,
  title = {Notions of computation and monads},
  volume = {93},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/0890540191900524},
  doi = {10.1016/0890-5401(91)90052-4},
  abstract = {The λ-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with λ-terms. However, if one goes further and uses βη-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations, that provide a correct basis for proving equivalence of programs for a wide range of notions of computation.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  series = {Selections from 1989 IEEE Symposium on Logic in Computer Science},
  urldate = {2014-11-24},
  date = {1991-07},
  pages = {55-92},
  author = {Moggi, Eugenio},
  file = {/Users/pgiarrusso/Zotero/storage/TTD9B9NJ/Moggi - 1991 - Notions of computation and monads.pdf;/Users/pgiarrusso/Zotero/storage/JF8EK4K6/0890540191900524.html;/Users/pgiarrusso/Zotero/storage/SRPIGHBW/0890540191900524.html}
}

@inproceedings{Moggi1989computational,
  title = {Computational lambda-calculus and monads},
  doi = {10.1109/LICS.1989.39155},
  abstract = {The λ-calculus is considered a useful mathematical tool in the study of programming languages. However, if one uses βη-conversion to prove equivalence of programs, then a gross simplification is introduced. The author gives a calculus based on a categorical semantics for computations, which provides a correct basis for proving equivalence of programs, independent from any specific computational model},
  eventtitle = {, Fourth Annual Symposium on Logic in Computer Science, 1989. LICS '89, Proceedings},
  booktitle = {, Fourth Annual Symposium on Logic in Computer Science, 1989. LICS '89, Proceedings},
  date = {1989-06},
  pages = {14-23},
  keywords = {Monads,Calculus,Computer science,formal logic,Mathematical model,Computer languages,categorical semantics,computational lambda-calculus,Contracts,equivalence of programs,formal languages,Logic programming,Mathematical programming,mathematical tool,programming languages,prove,Reasoning about programs,βη-conversion,λ-Calculus},
  author = {Moggi, E.},
  file = {/Users/pgiarrusso/Zotero/storage/3EQXUQCE/Moggi - 1989 - Computational lambda-calculus and monads.pdf;/Users/pgiarrusso/Zotero/storage/MEVTIM29/login.html}
}
% == BibLateX quality report for Moggi1989computational:
% ? Unsure about the formatting of the booktitle

@article{Maraist1995callbyname,
  title = {Call-by-name, Call-by-value, Call-by-need, and the Linear Lambda Calculus},
  volume = {1},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104000222},
  doi = {10.1016/S1571-0661(04)00022-2},
  abstract = {Girard described two translations of intuitionistic logic into linear logic, one where A → B maps to (!A) –○ B, and another where it maps to !(A –○ B). We detail the action of these translations on terms, and show that the first corresponds to a call-by-name calculus, while the second corresponds to call-by-value. We further show that if the target of the translation is taken to be an affine calculus, where ! controls contraction but weakening is allowed everywhere, then the second translation corresponds to a call-by-need calculus, as recently defined by Ariola, Felleisen, Maraist, Odersky and Wadler. Thus the different calling mechanisms can be explained in terms of logical translations, bringing them into the scope of the Curry-Howard isomorphism.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {MFPS XI, Mathematical Foundations of Programming Semantics, Eleventh Annual Conference},
  urldate = {2014-11-24},
  date = {1995},
  pages = {370-392},
  author = {Maraist, John and Odersky, Martin and Turner, David N. and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/M464USPW/Maraist et al - 1995 - Call-by-name, Call-by-value, Call-by-need, and the Linear Lambda Calculus.pdf;/Users/pgiarrusso/Zotero/storage/5JN66RF7/S1571066104000222.html}
}

@inproceedings{Benton1996linear,
  title = {Linear logic, monads and the lambda calculus},
  doi = {10.1109/LICS.1996.561458},
  abstract = {Models of intuitionistic linear logic also provide models of Moggi's computational metalanguage. We use the adjoint presentation of these models and the associated adjoint calculus to show that three translations, due mainly to Moggi, of the lambda calculus into the computational metalanguage (direct, call-by-name and call-by-value) correspond exactly to three translations, due mainly to Girard, of intuitionistic logic into intuitionistic linear logic. We also consider extending these results to languages with recursion},
  eventtitle = {, Eleventh Annual IEEE Symposium on Logic in Computer Science, 1996. LICS '96. Proceedings},
  booktitle = {, Eleventh Annual IEEE Symposium on Logic in Computer Science, 1996. LICS '96. Proceedings},
  date = {1996-07},
  pages = {420-431},
  keywords = {Monads,lambda calculus,Calculus,formal logic,linear logic,Logic,Proposals,adjoint calculus,adjoint presentation,call-by-name,call-by-value,Computational modeling,Ear,intuitionistic linear logic,Laboratories,Moggi's computational metalanguage},
  author = {Benton, N. and Wadler, P.},
  file = {/Users/pgiarrusso/Zotero/storage/5KUXH26E/login.html}
}
% == BibLateX quality report for Benton1996linear:
% ? Unsure about the formatting of the booktitle

@article{Benton1998computational,
  title = {Computational types from a logical perspective},
  volume = {8},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796898002998},
  doi = {null},
  abstract = {Moggi\&apos;s computational lambda calculus is a metalanguage for denotational semantics which arose from the observation that many different notions of computation have the categorical structure of a strong monad on a cartesian closed category. In this paper we show that the computational lambda calculus also arises naturally as the term calculus corresponding (by the Curry–Howard correspondence) to a novel intuitionistic modal propositional logic. We give natural deduction, sequent calculus and Hilbert-style presentations of this logic and prove strong normalisation and confluence results.},
  number = {02},
  journaltitle = {Journal of Functional Programming},
  urldate = {2014-11-24},
  date = {1998-03},
  pages = {177--193},
  author = {Benton, P. N. and Bierman, G. M. and De Paiva, V. C. V.},
  file = {/Users/pgiarrusso/Zotero/storage/2IRIIT7U/Benton et al - 1998 - Computational types from a logical perspective.pdf;/Users/pgiarrusso/Zotero/storage/VX2D3HTQ/displayAbstract.html}
}

@article{Coquand2013isomorphism,
  title = {Isomorphism is equality},
  volume = {24},
  issn = {0019-3577},
  url = {http://www.sciencedirect.com/science/article/pii/S0019357713000694},
  doi = {10.1016/j.indag.2013.09.002},
  abstract = {The setting of this work is dependent type theory extended with the univalence axiom. We prove that, for a large class of algebraic structures, isomorphic instances of a structure are equal—in fact, isomorphism is in bijective correspondence with equality. The class of structures includes monoids whose underlying types are “sets”, and also posets where the underlying types are sets and the ordering relations are pointwise “propositional”. For monoids on sets equality coincides with the usual notion of isomorphism from universal algebra, and for posets of the kind mentioned above equality coincides with order isomorphism.},
  number = {4},
  journaltitle = {Indagationes Mathematicae},
  shortjournal = {Indagationes Mathematicae},
  series = {In memory of N.G. (Dick) de Bruijn (1918–2012)},
  urldate = {2014-11-24},
  date = {2013-11-15},
  pages = {1105-1120},
  keywords = {Dependent type theory,proof assistants,Univalence},
  author = {Coquand, Thierry and Danielsson, Nils Anders},
  file = {/Users/pgiarrusso/Zotero/storage/WW4XJSUB/Coquand_Danielsson - 2013 - Isomorphism is equality.pdf;/Users/pgiarrusso/Zotero/storage/WQHPDJJ6/S0019357713000694.html}
}

@inproceedings{Millstein2002modular,
  location = {{New York, NY, USA}},
  title = {Modular Typechecking for Hierarchically Extensible Datatypes and Functions},
  isbn = {1-58113-487-8},
  url = {http://doi.acm.org/10.1145/581478.581489},
  doi = {10.1145/581478.581489},
  abstract = {One promising approach for adding object-oriented (OO) facilities to functional languages like ML is to generalize the existing datatype and function constructs to be hierarchical and extensible, so that datatype variants simulate classes and function cases simulate methods. This approach allows existing datatypes to be easily extended with both new operations and new variants, resolving a long-standing conflict between the functional and OO styles. However, previous designs based on this approach have been forced to give up modular typechecking, requiring whole-program checks to ensure type safety. We describe Extensible ML (eml), an ML-like language that supports hierarchical, extensible datatypes and functions while preserving purely modular typechecking. To achieve this result, eml's type system imposes a few requirements on datatype and function extensibility, but eml is still able to express both traditional functional and OO idioms. We have formalized a core version of eml and proven the associated type system sound, and we have developed a prototype interpreter for the language.},
  booktitle = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '02},
  publisher = {{ACM}},
  urldate = {2014-11-25},
  date = {2002},
  pages = {110--122},
  keywords = {extensible datatypes,extensible functions,modular typechecking},
  author = {Millstein, Todd and Bleckner, Colin and Chambers, Craig},
  file = {/Users/pgiarrusso/Zotero/storage/MRS3KUSV/Millstein et al - 2002 - Modular Typechecking for Hierarchically Extensible Datatypes and Functions.pdf}
}

@article{Fiore2002objects,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0212377},
  title = {Objects of Categories as Complex Numbers},
  url = {http://arxiv.org/abs/math/0212377},
  abstract = {In many everyday categories (sets, spaces, modules, ...) objects can be both added and multiplied. The arithmetic of such objects is a challenge because there is usually no subtraction. We prove a family of cases of the following principle: if an arithmetic statement about the objects can be proved by pretending that they are complex numbers, then there also exists an honest proof.},
  urldate = {2014-11-27},
  date = {2002-12-30},
  keywords = {Mathematics - Category Theory,Mathematics - Commutative Algebra,Mathematics - Rings and Algebras},
  author = {Fiore, Marcelo and Leinster, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/NUBRAW5P/Fiore_Leinster - 2002 - Objects of Categories as Complex Numbers.pdf;/Users/pgiarrusso/Zotero/storage/NUBKN2X5/0212377.html}
}
% == BibLateX quality report for Fiore2002objects:
% Unexpected field 'archivePrefix'
% Missing required field 'journaltitle'

@inproceedings{McBride2008clowns,
  location = {{New York, NY, USA}},
  title = {Clowns to the Left of Me, Jokers to the Right (Pearl): Dissecting Data Structures},
  isbn = {978-1-59593-689-9},
  url = {http://doi.acm.org/10.1145/1328438.1328474},
  doi = {10.1145/1328438.1328474},
  shorttitle = {Clowns to the Left of Me, Jokers to the Right (Pearl)},
  abstract = {This paper introduces a small but useful generalisation to the 'derivative' operation on datatypes underlying Huet's notion of 'zipper', giving a concrete representation to one-hole contexts in data which is undergoing transformation. This operator, 'dissection', turns a container-like functor into a bifunctor representing a one-hole context in which elements to the left of the hole are distinguished in type from elements to its right. I present dissection here as a generic program, albeit for polynomial functors only. The notion is certainly applicable more widely, but here I prefer to concentrate on its diverse applications. For a start, map-like operations over the functor and fold-like operations over the recursive data structure it induces can be expressed by tail recursion alone. Further, the derivative is readily recovered from the dissection. Indeed, it is the dissection structure which delivers Huet's operations for navigating zippers. The original motivation for dissection was to define 'division', capturing the notion of leftmost hole, canonically distinguishing values with no elements from those with at least one. Division gives rise to an isomorphism corresponding to the remainder theorem in algebra. By way of a larger example, division and dissection are exploited to give a relatively efficient generic algorithm for abstracting all occurrences of one term from another in a first-order syntax. The source code for the paper is available online and compiles with recent extensions to the Glasgow Haskell Compiler.},
  booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '08},
  publisher = {{ACM}},
  urldate = {2014-11-27},
  date = {2008},
  pages = {287--295},
  keywords = {generic programming,datatype,differentiation,dissection,division,iteration,polynomial,stack,tail recursion,traversal,zipper},
  author = {McBride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/BVJQJNIK/McBride - 2008 - Clowns to the Left of Me, Jokers to the Right (Pearl) - Dissecting Data Structures.pdf}
}

@article{Moggi1999monads,
  title = {Monads, Shapely Functors and Traversals},
  volume = {29},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105803160},
  doi = {10.1016/S1571-0661(05)80316-0},
  abstract = {This paper demonstrates the potential for combining the polytypic and monadic programming styles, by introducing a new kind of combinator, called a traversal. The natural setting for defining traversals is the class of shapely data types. This result reinforces the view that shapely data types form a natural domain for polytypism: they include most of the data types of interest, while to exceed them would sacrifice a very smooth interaction between polytypic and monadic programming.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {CTCS '99, Conference on Category Theory and Computer Science},
  urldate = {2014-11-27},
  date = {1999},
  pages = {187-208},
  keywords = {functional/monadic/polytypic programming,shape theory},
  author = {Moggi, E. and Bellè, G. and Jay, C. B.},
  file = {/Users/pgiarrusso/Zotero/storage/XQDGEX2A/Moggi et al - 1999 - Monads, Shapely Functors and Traversals.pdf;/Users/pgiarrusso/Zotero/storage/P9HK827I/S1571066105803160.html}
}

@incollection{Abbott2003categories,
  langid = {english},
  title = {Categories of Containers},
  isbn = {978-3-540-00897-2 978-3-540-36576-1},
  url = {http://link.springer.com/chapter/10.1007/3-540-36576-1_2},
  abstract = {We introduce the notion of containers as a mathematical formalisation of the idea that many important datatypes consist of templates where data is stored. We show that containers have good closure properties under a variety of constructions including the formation of initial algebras and final coalgebras. We also show that containers include strictly positive types and shapely types but that there are containers which do not correspond to either of these. Further, we derive a representation result classifying the nature of polymorphic functions between containers. We finish this paper with an application to the theory of shapely types and refer to a forthcoming paper which applies this theory to differentiable types.},
  number = {2620},
  booktitle = {Foundations of Software Science and Computation Structures},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-27},
  date = {2003-01-01},
  pages = {23-38},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Abbott, Michael and Altenkirch, Thorsten and Ghani, Neil},
  editor = {Gordon, Andrew D.},
  file = {/Users/pgiarrusso/Zotero/storage/ZRWUQPA4/3-540-36576-1_2.html}
}
% == BibLateX quality report for Abbott2003categories:
% 'isbn': not a valid ISBN

@inproceedings{Leroy1995applicative,
  location = {{New York, NY, USA}},
  title = {Applicative Functors and Fully Transparent Higher-order Modules},
  isbn = {0-89791-692-1},
  url = {http://doi.acm.org/10.1145/199448.199476},
  doi = {10.1145/199448.199476},
  abstract = {we present a variety of the Standard ML module system where parameterized abstract types (i.e. functors returning generative types) map provably equal arguments to compatible abstract types, instead of generating distinct types at each applications as in Standard ML. This extension solves the full transparency problem (how to give syntactic signatures for higher-order functors that express exactly their propagation of type equations), and also provides better support for non-closed code fragments.},
  booktitle = {Proceedings of the 22Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '95},
  publisher = {{ACM}},
  urldate = {2014-11-29},
  date = {1995},
  pages = {142--153},
  author = {Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/4MBNTUHT/Leroy - 1995 - Applicative Functors and Fully Transparent Higher-order Modules.pdf}
}

@incollection{Hinze2012kan,
  langid = {english},
  title = {Kan Extensions for Program Optimisation Or: Art and Dan Explain an Old Trick},
  isbn = {978-3-642-31112-3 978-3-642-31113-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-31113-0_16},
  shorttitle = {Kan Extensions for Program Optimisation Or},
  abstract = {Many program optimisations involve transforming a program in direct style to an equivalent program in continuation-passing style. This paper investigates the theoretical underpinnings of this transformation in the categorical setting of monads. We argue that so-called absolute Kan Extensions underlie this program optimisation. It is known that every Kan extension gives rise to a monad, the codensity monad, and furthermore that every monad is isomorphic to a codensity monad. The end formula for Kan extensions then induces an implementation of the monad, which can be seen as the categorical counterpart of continuation-passing style. We show that several optimisations are instances of this scheme: Church representations and implementation of backtracking using success and failure continuations, among others. Furthermore, we develop the calculational properties of Kan extensions, powers and ends. In particular, we propose a two-dimensional notation based on string diagrams that aims to support effective reasoning with Kan extensions.},
  number = {7342},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-11-29},
  date = {2012-01-01},
  pages = {324-362},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Haskell,Mathematical Logic and Formal Languages,adjunction,backtracking,Church representation,codensity monad,CPS,Discrete Mathematics in Computer Science,end,Kan extension,Math Applications in Computer Science,power,string diagram},
  author = {Hinze, Ralf},
  editor = {Gibbons, Jeremy and Nogueira, Pablo},
  file = {/Users/pgiarrusso/Zotero/storage/RBR9JN7X/Hinze - 2012 - Kan Extensions for Program Optimisation Or - Art and Dan Explain an Old Trick.pdf;/Users/pgiarrusso/Zotero/storage/VVE9NIHW/978-3-642-31113-0_16.html}
}
% == BibLateX quality report for Hinze2012kan:
% 'isbn': not a valid ISBN

@article{Berger2012typed,
  title = {Typed vs. Untyped Realizability},
  volume = {286},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066112000357},
  doi = {10.1016/j.entcs.2012.08.005},
  abstract = {We study the domain-theoretic semantics of a Church-style typed λ-calculus with constructors, pattern matching and recursion, and show that it is closely related to the semantics of its untyped counterpart. The motivation for this study comes from program extraction from proofs via realizability where one has the choice of extracting typed or untyped terms from proofs. Our result shows that under a certain regularity condition, the choice is irrelevant. The regularity condition is that in every use of a fixed point type fix α.ρ, α occurs only positively in ρ.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the 28th Conference on the Mathematical Foundations of Programming Semantics (MFPS XXVIII)},
  urldate = {2014-11-29},
  date = {2012-09-24},
  pages = {57-71},
  keywords = {logical relation,Program extraction,realizability,Scott domain,typed and untyped λ-calculus},
  author = {Berger, Ulrich and Hou, Tie},
  file = {/Users/pgiarrusso/Zotero/storage/R8E6DMWN/Berger_Hou - 2012 - Typed vs. Untyped Realizability.pdf;/Users/pgiarrusso/Zotero/storage/5WA9JNNF/S1571066112000357.html}
}

@inproceedings{Johann2004free,
  location = {{New York, NY, USA}},
  title = {Free Theorems in the Presence of Seq},
  isbn = {1-58113-729-X},
  url = {http://doi.acm.org/10.1145/964001.964010},
  doi = {10.1145/964001.964010},
  abstract = {Parametric polymorphism constrains the behavior of pure functional programs in a way that allows the derivation of interesting theorems about them solely from their types, i.e., virtually for free. Unfortunately, the standard parametricity theorem fails for nonstrict languages supporting a polymorphic strict evaluation primitive like Haskell's seq. Contrary to the folklore surrounding seq and parametricity, we show that not even quantifying only over strict and bottom-reflecting relations in the \$$\backslash$forall\$-clause of the underlying logical relation --- and thus restricting the choice of functions with which such relations are instantiated to obtain free theorems to strict and total ones --- is sufficient to recover from this failure. By addressing the subtle issues that arise when propagating up the type hierarchy restrictions imposed on a logical relation in order to accommodate the strictness primitive, we provide a parametricity theorem for the subset of Haskell corresponding to a Girard-Reynolds-style calculus with fixpoints, algebraic datatypes, and seq. A crucial ingredient of our approach is the use of an asymmetric logical relation, which leads to "inequational" versions of free theorems enriched by preconditions guaranteeing their validity in the described setting. Besides the potential to obtain corresponding preconditions for standard equational free theorems by combining some new inequational ones, the latter also have value in their own right, as is exemplified with a careful analysis of seq's impact on familiar program transformations.},
  booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '04},
  publisher = {{ACM}},
  urldate = {2014-12-03},
  date = {2004},
  pages = {99--110},
  keywords = {Haskell,logical relations,Denotational semantics,parametricity,controlling strict evaluation,correctness proofs,program transformations,short cut fusion,theorems for free},
  author = {Johann, Patricia and Voigtländer, Janis},
  file = {/Users/pgiarrusso/Zotero/storage/IQJZQ6MC/Johann_Voigtländer - 2004 - Free Theorems in the Presence of Seq.pdf}
}

@inproceedings{Kohlbecker1986hygienic,
  location = {{New York, NY, USA}},
  title = {Hygienic Macro Expansion},
  isbn = {0-89791-200-4},
  url = {http://doi.acm.org/10.1145/319838.319859},
  doi = {10.1145/319838.319859},
  booktitle = {Proceedings of the 1986 ACM Conference on LISP and Functional Programming},
  series = {LFP '86},
  publisher = {{ACM}},
  urldate = {2014-12-04},
  date = {1986},
  pages = {151--161},
  author = {Kohlbecker, Eugene and Friedman, Daniel P. and Felleisen, Matthias and Duba, Bruce},
  file = {/Users/pgiarrusso/Zotero/storage/KSAGB8QE/Kohlbecker et al - 1986 - Hygienic Macro Expansion.pdf}
}

@inproceedings{PeytonJones1998bridging,
  location = {{New York, NY, USA}},
  title = {Bridging the Gulf: A Common Intermediate Language for ML and Haskell},
  isbn = {0-89791-979-3},
  url = {http://doi.acm.org/10.1145/268946.268951},
  doi = {10.1145/268946.268951},
  shorttitle = {Bridging the Gulf},
  booktitle = {Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '98},
  publisher = {{ACM}},
  urldate = {2014-12-04},
  date = {1998},
  pages = {49--61},
  author = {Peyton Jones, Simon and Shields, Mark and Launchbury, John and Tolmach, Andrew},
  file = {/Users/pgiarrusso/Zotero/storage/AGJ7DZA7/corrigendum.pdf}
}

@article{Barthe1998monadic,
  title = {Monadic Type Systems: Pure Type Systems for Impure Settings (Preliminary Report)},
  volume = {10},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105806917},
  doi = {10.1016/S1571-0661(05)80691-7},
  shorttitle = {Monadic Type Systems},
  abstract = {Pure type systems and computational monads are two parameterized frameworks that have proved to be quite useful in both theoretical and practical applications. We join the foundational concepts of both of these to obtain monadic type systems. Essentially, monadic type systems inherit the parameterized higher-order type structure of pure type systems and the monadic term and type structure used to capture computational effects in the theory of computational monads. We demonstrate that monadic type systems nicely characterize previous work and suggest how they can support several new theoretical and practical applications.

A technical foundation for monadic type systems is laid by recasting and scaling up the main results from pure type systems (confluence, subject reduction, strong normalisation for particular classes of systems, etc.) and from operational presentations of computational monads (notions of operational equivalence based on applicative similarity, co-induction proof techniques).

We demonstrate the use of monadic type systems with case studies of several call-by-value and call-by-name systems. Our framework allows to capture the restriction to value polymorphism in the type structure and is flexible enough to accommodate extensions of the type system, e.g., with higher-order polymorphism. The theoretical foundations make monadic type systems well-suited as a typed intermediate language for compilation and specialization of higher-order, strict and non-strict functional programs. The monadic structure guarantees sound compile-time optimizations and the parameterized type structure guarantees sufficient expressiveness.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {HOOTS II, Second Workshop on Higher-Order Operational Techniques in Semantics},
  urldate = {2014-12-05},
  date = {1998},
  pages = {54-120},
  keywords = {functional programming,polymorphism,Monads,pure type systems,operational semantics},
  author = {Barthe, Gilles and Hatcliff, John and Thiemann, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/KIZZD9KV/S1571066105806917.html}
}

@inproceedings{Shao1998implementing,
  location = {{New York, NY, USA}},
  title = {Implementing Typed Intermediate Languages},
  isbn = {1-58113-024-4},
  url = {http://doi.acm.org/10.1145/289423.289460},
  doi = {10.1145/289423.289460},
  abstract = {Recent advances in compiler technology have demonstrated the benefits of using strongly typed intermediate languages to compile richly typed source languages (e.g., ML). A type-preserving compiler can use types to guide advanced optimizations and to help generate provably secure mobile code. Types, unfortunately, are very hard to represent and manipulate efficiently; a naive implementation can easily add exponential overhead to the compilation and execution of a program. This paper describes our experience with implementing the FLINT typed intermediate language in the SML/NJ production compiler. We observe that a type-preserving compiler will not scale to handle large types unless all of its type-preserving stages preserve the asymptotic time and space usage in representing and manipulating types. We present a series of novel techniques for achieving this property and give empirical evidence of their effectiveness.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2014-12-05},
  date = {1998},
  pages = {313--323},
  author = {Shao, Zhong and League, Christopher and Monnier, Stefan}
}

@incollection{Jacobs1996objects,
  langid = {english},
  title = {Objects And Classes, Co-Algebraically  (2)},
  isbn = {978-1-4612-8625-7 978-1-4613-1437-0},
  url = {http://link.springer.com/chapter/10.1007/978-1-4613-1437-0_5},
  abstract = {The co-algebraic perspective on objects and classes in object-oriented programming is elaborated: classes are described as co-algebras, which may occur as models (implementations) of co-algebraic specifications. These specifications are much like deferred (or virtual) classes with assertions in Eiffel. An object belonging to a class is an element of the state space of the class, as co-algebra. We show how terminal co-algebras of co-algebraic specifications give rise to canonical models (in which all observationally indistinguishable objects are identified). We further describe operational semantics for objects, with an associated notion of bisimulation (for objects in classes modeling the same specification), expressing observational indistinguishability.},
  number = {370},
  booktitle = {Object Orientation with Parallelism and Persistence},
  series = {The Kluwer International Series in Engineering and Computer Science},
  publisher = {{Springer US}},
  urldate = {2014-12-05},
  date = {1996-01-01},
  pages = {83-103},
  keywords = {Programming Languages; Compilers; Interpreters,Data Structures; Cryptology and Information Theory,Software Engineering/Programming and Operating Systems},
  author = {Jacobs, Bart},
  editor = {Freitag, Burkhard and Jones, Cliff B. and Lengauer, Christian and Schek, Hans-Jörg},
  file = {/Users/pgiarrusso/Zotero/storage/CH62ERI6/Jacobs - 1996 - Objects And Classes, Co-Algebraically.pdf;/Users/pgiarrusso/Zotero/storage/SV4B94TS/978-1-4613-1437-0_5.html}
}
% == BibLateX quality report for Jacobs1996objects:
% 'isbn': not a valid ISBN

@incollection{Jacobs1996objectsa,
  location = {{Norwell, MA, USA}},
  title = {Objects and classes, co-algebraically (1)},
  isbn = {0-7923-9770-3},
  url = {http://dl.acm.org/citation.cfm?id=261075.261479},
  booktitle = {Object Orientation with Parallelism and Persistence},
  publisher = {{Kluwer Academic Publishers}},
  urldate = {2014-12-05},
  date = {1996},
  pages = {83--103},
  author = {Jacobs, Bart},
  editor = {Freitag, Burkhard and Jones, Cliff B. and Lengauer, Christian and Schek, Hans-Jörg}
}

@inproceedings{Royce1987managing,
  location = {{Los Alamitos, CA, USA}},
  title = {Managing the Development of Large Software Systems: Concepts and Techniques},
  isbn = {0-89791-216-0},
  url = {http://dl.acm.org/citation.cfm?id=41765.41801},
  shorttitle = {Managing the Development of Large Software Systems},
  booktitle = {Proceedings of the 9th International Conference on Software Engineering},
  series = {ICSE '87},
  publisher = {{IEEE Computer Society Press}},
  urldate = {2014-12-09},
  date = {1987},
  pages = {328--338},
  author = {Royce, W. W.},
  file = {/Users/pgiarrusso/Zotero/storage/AVFNVCS9/Royce - 1987 - Managing the Development of Large Software Systems - Concepts and Techniques.pdf}
}

@article{Cheney2014database,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.1675},
  primaryClass = {cs},
  title = {Database Queries that Explain their Work},
  url = {http://arxiv.org/abs/1408.1675},
  doi = {10.1145/2643135.2643143},
  abstract = {Provenance for database queries or scientific workflows is often motivated as providing explanation, increasing understanding of the underlying data sources and processes used to compute the query, and reproducibility, the capability to recompute the results on different inputs, possibly specialized to a part of the output. Many provenance systems claim to provide such capabilities; however, most lack formal definitions or guarantees of these properties, while others provide formal guarantees only for relatively limited classes of changes. Building on recent work on provenance traces and slicing for functional programming languages, we introduce a detailed tracing model of provenance for multiset-valued Nested Relational Calculus, define trace slicing algorithms that extract subtraces needed to explain or recompute specific parts of the output, and define query slicing and differencing techniques that support explanation. We state and prove correctness properties for these techniques and present a proof-of-concept implementation in Haskell.},
  urldate = {2014-12-10},
  date = {2014-08-07},
  keywords = {Computer Science - Programming Languages,Computer Science - Databases,D.3.0,D.3.3},
  author = {Cheney, James and Ahmed, Amal and Acar, Umut A.},
  file = {/Users/pgiarrusso/Zotero/storage/UVCA8ZTU/Cheney et al - 2014 - Database Queries that Explain their Work.pdf;/Users/pgiarrusso/Zotero/storage/9MTE59WF/1408.html}
}
% == BibLateX quality report for Cheney2014database:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Blelloch1995parallelism,
  location = {{New York, NY, USA}},
  title = {Parallelism in Sequential Functional Languages},
  isbn = {0-89791-719-7},
  url = {http://doi.acm.org/10.1145/224164.224210},
  doi = {10.1145/224164.224210},
  booktitle = {Proceedings of the Seventh International Conference on Functional Programming Languages and Computer Architecture},
  series = {FPCA '95},
  publisher = {{ACM}},
  urldate = {2014-12-12},
  date = {1995},
  pages = {226--237},
  author = {Blelloch, Guy and Greiner, John}
}

@inproceedings{Lamping1990algorithm,
  location = {{New York, NY, USA}},
  title = {An Algorithm for Optimal Lambda Calculus Reduction},
  isbn = {0-89791-343-4},
  url = {http://doi.acm.org/10.1145/96709.96711},
  doi = {10.1145/96709.96711},
  abstract = {We present an algorithm for lambda expression reduction that avoids any copying that could later cause duplication of work. It is optimal in the sense defined by Lévy. The basis of the algorithm is a graphical representation of the kinds of commonality that can arise from substitutions; the idea can be adapted to represent other kinds of expressions besides lambda expressions. The algorithm is also well suited to parallel implementations, consisting of a fixed set of local graph rewrite rules.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '90},
  publisher = {{ACM}},
  urldate = {2014-12-12},
  date = {1990},
  pages = {16--30},
  author = {Lamping, John}
}

@inproceedings{Gonthier1992geometry,
  location = {{New York, NY, USA}},
  title = {The Geometry of Optimal Lambda Reduction},
  isbn = {0-89791-453-8},
  url = {http://doi.acm.org/10.1145/143165.143172},
  doi = {10.1145/143165.143172},
  abstract = {Lamping discovered an optimal graph-reduction implementation of the \&lgr;-calculus. Simultaneously, Girard invented the geometry of interaction, a mathematical foundation for operational semantics. In this paper, we connect and explain the geometry of interaction and Lamping's graphs. The geometry of interaction provides a suitable semantic basis for explaining and improving Lamping's system. On the other hand, graphs similar to Lamping's provide a concrete representation of the geometry of interaction. Together, they offer a new understanding of computation, as well as ideas for efficient and correct implementations.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '92},
  publisher = {{ACM}},
  urldate = {2014-12-12},
  date = {1992},
  pages = {15--26},
  author = {Gonthier, Georges and Abadi, Martín and Lévy, Jean-Jacques}
}

@inproceedings{Mitschke2014i3ql,
  location = {{New York, NY, USA}},
  title = {I3QL: Language-integrated Live Data Views},
  isbn = {978-1-4503-2585-1},
  url = {http://doi.acm.org/10.1145/2660193.2660242},
  doi = {10.1145/2660193.2660242},
  shorttitle = {I3QL},
  abstract = {An incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) real-time systems. This paper presents i3QL, a general-purpose programming language for specifying incremental computations. i3QL provides a declarative SQL-like syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for non-incremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes user-defined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.},
  booktitle = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&\#38; Applications},
  series = {OOPSLA '14},
  publisher = {{ACM}},
  urldate = {2014-12-12},
  date = {2014},
  pages = {417--432},
  keywords = {Scala,incremental computation,reactive programming},
  author = {Mitschke, Ralf and Erdweg, Sebastian and Köhler, Mirko and Mezini, Mira and Salvaneschi, Guido},
  file = {/Users/pgiarrusso/Zotero/storage/ZRGJTFBP/Mitschke et al - 2014 - I3QL - Language-integrated Live Data Views.pdf}
}

@inproceedings{Goguen1989what,
  title = {What is Unification? A Categorical View of Substitution, Equation and Solution},
  shorttitle = {What is Unification?},
  abstract = {From a general perspective, a substitution is a transformation from one space to another, an equation is a pair of such substitutions, and a solution  to an equation is a substitution that yields the same value when composed with (i.e., when substituted into) the substitutions that constitute the given equation. In some special cases, solutions are called unifiers. Other examples include Scott domain equations, unification grammars, type inference, and differential equations. The intuition that the composition of substitutions should be associative when defined, and should have identities, motivates a general concept of substitution system based on category theory. Notions of morphism, congruence, and quotient are given for substitution systems, each with the expected properties, and some general cardinality bounds are proved for most general solution sets (which are minimal sets of solutions with the property that any other solution is a substitution instance of one in the set). The notions of equation and solution are also generalized to systems of equations, i.e., to constraint solving, and applied t oclarify the notions of "compositionality" and "unification" in linguistic unification grammar. This paper is self-contained as regards category theory, and indeed, could be used as an introductory tutorial on that subject.},
  booktitle = {Resolution of Equations in Algebraic Structures, Volume 1: Algebraic Techniques},
  publisher = {{Academic}},
  date = {1989},
  pages = {217--261},
  author = {Goguen, Joseph A.},
  file = {/Users/pgiarrusso/Zotero/storage/BUMKKICN/Goguen - 1989 - What is Unification - A Categorical View of Substitution, Equation and Solution.pdf;/Users/pgiarrusso/Zotero/storage/W2HNSIIS/summary.html}
}

@article{Rodin2007categorical,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0707.3745},
  primaryClass = {math},
  title = {On Categorical Theory-Building: Beyond the Formal},
  url = {http://arxiv.org/abs/0707.3745},
  shorttitle = {On Categorical Theory-Building},
  abstract = {I propose a notion of theory motivated by Category theory.},
  urldate = {2014-12-13},
  date = {2007-07-25},
  keywords = {Mathematics - Category Theory,00A30,Mathematics - History and Overview},
  author = {Rodin, Andrei},
  file = {/Users/pgiarrusso/Zotero/storage/WH5H555E/Rodin - 2007 - On Categorical Theory-Building - Beyond the Formal.pdf;/Users/pgiarrusso/Zotero/storage/DDIH9MTF/0707.html}
}
% == BibLateX quality report for Rodin2007categorical:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Palmgren2005internalising,
  langid = {english},
  title = {Internalising modified realisability in constructive type theory},
  volume = {1},
  issn = {18605974},
  url = {http://www.lmcs-online.org/ojs/viewarticle.php?id=38},
  doi = {10.2168/LMCS-1(2:2)2005},
  number = {2},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2014-12-14},
  date = {2005-10-05},
  author = {Palmgren, Erik},
  editor = {Setzer, Anton},
  file = {/Users/pgiarrusso/Zotero/storage/PS46QAM4/viewarticle.html}
}

@article{Stefik2013empirical,
  title = {An Empirical Investigation into Programming Language Syntax},
  volume = {13},
  issn = {1946-6226},
  url = {http://doi.acm.org/10.1145/2534973},
  doi = {10.1145/2534973},
  abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.},
  number = {4},
  journaltitle = {Trans. Comput. Educ.},
  urldate = {2014-12-15},
  date = {2013-11},
  pages = {19:1--19:40},
  keywords = {programming languages,Novice Programmers,Syntax},
  author = {Stefik, Andreas and Siebert, Susanna}
}
% == BibLateX quality report for Stefik2013empirical:
% ? Possibly abbreviated journal title Trans. Comput. Educ.

@article{Gross2014experience,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.7694},
  primaryClass = {cs, math},
  title = {Experience Implementing a Performant Category-Theory Library in Coq},
  url = {http://arxiv.org/abs/1401.7694},
  abstract = {We describe our experience implementing a broad category-theory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coq's logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful.},
  urldate = {2014-12-15},
  date = {2014-01-29},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  author = {Gross, Jason and Chlipala, Adam and Spivak, David I.},
  file = {/Users/pgiarrusso/Zotero/storage/8ZJWCND2/Gross et al - 2014 - Experience Implementing a Performant Category-Theory Library in Coq.pdf;/Users/pgiarrusso/Zotero/storage/XUJWTUWA/1401.html}
}
% == BibLateX quality report for Gross2014experience:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Blanchette2014truly,
  langid = {english},
  title = {Truly Modular (Co)datatypes for Isabelle/HOL},
  isbn = {978-3-319-08969-0 978-3-319-08970-6},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-08970-6_7},
  abstract = {We extended Isabelle/HOL with a pair of definitional commands for datatypes and codatatypes. They support mutual and nested (co)recursion through well-behaved type constructors, including mixed recursion–corecursion, and are complemented by syntaxes for introducing primitively (co)recursive functions and by a general proof method for reasoning coinductively. As a case study, we ported Isabelle’s Coinductive library to use the new commands, eliminating the need for tedious ad hoc constructions.},
  number = {8558},
  booktitle = {Interactive Theorem Proving},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2014-12-15},
  date = {2014-01-01},
  pages = {93-110},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Mathematical Logic and Formal Languages,Systems and Data Security},
  author = {Blanchette, Jasmin Christian and Hölzl, Johannes and Lochbihler, Andreas and Panny, Lorenz and Popescu, Andrei and Traytel, Dmitriy},
  editor = {Klein, Gerwin and Gamboa, Ruben},
  file = {/Users/pgiarrusso/Zotero/storage/FC6Q839G/978-3-319-08970-6_7.html}
}
% == BibLateX quality report for Blanchette2014truly:
% 'isbn': not a valid ISBN

@inproceedings{Hatcliff1994generic,
  location = {{New York, NY, USA}},
  title = {A Generic Account of Continuation-passing Styles},
  isbn = {0-89791-636-0},
  url = {http://doi.acm.org/10.1145/174675.178053},
  doi = {10.1145/174675.178053},
  abstract = {We unify previous work on the continuation-passing style (CPS) transformations in a generic framework based on Moggi's computational meta-language. This framework is used to obtain CPS transformations for a variety of evaluation strategies and to characterize the corresponding administrative reductions and inverse transformations. We establish generic formal connections between operational semantics and equational theories. Formal properties of transformations for specific evaluation orders follow as corollaries.
Essentially, we factor transformations through Moggi's computational m
eta-language. Mapping \&lgr;-terms into the meta-language captures computation properties (e.g., partiality, strictness) and evaluation order explicitly in both the term and the type structure of the meta-language. The CPS transformation is then obtained by applying a generic transformation from terms and types in the meta-language to CPS terms and types, based on a typed term representation of the continuation monad. We prove an adequacy property for the generic transformation and establish an equational correspondence between the meta-language and CPS terms.
These generic results generalize Plotkin's seminal theorems, subsume more recent results, and enable new uses of CPS transformations and their inverses. We discuss how to aply these results to compilation.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '94},
  publisher = {{ACM}},
  urldate = {2014-12-17},
  date = {1994},
  pages = {458--471},
  author = {Hatcliff, John and Danvy, Olivier},
  file = {/Users/pgiarrusso/Zotero/storage/ZJA764M3/Hatcliff_Danvy - 1994 - A Generic Account of Continuation-passing Styles.pdf}
}

@inproceedings{Sperber1997two,
  location = {{New York, NY, USA}},
  title = {Two for the Price of One: Composing Partial Evaluation and Compilation},
  isbn = {0-89791-907-6},
  url = {http://doi.acm.org/10.1145/258915.258935},
  doi = {10.1145/258915.258935},
  shorttitle = {Two for the Price of One},
  abstract = {One of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a source-to-source transformation for high-level languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several meta-computation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a run-time code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreter-based experiments with a source-to-source version of the partial evaluator before building a realistic compiler which generates object code automatically.},
  booktitle = {Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation},
  series = {PLDI '97},
  publisher = {{ACM}},
  urldate = {2014-12-19},
  date = {1997},
  pages = {215--225},
  author = {Sperber, Michael and Thiemann, Peter}
}

@inproceedings{Sperber1996realistic,
  location = {{New York, NY, USA}},
  title = {Realistic Compilation by Partial Evaluation},
  isbn = {0-89791-795-2},
  url = {http://doi.acm.org/10.1145/231379.231419},
  doi = {10.1145/231379.231419},
  abstract = {Two key steps in the compilation of strict functional languages are the conversion of higher-order functions to data structures (closures) and the transformation to tail-recursive style. We show how to perform both steps at once by applying first-order offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C compilers. In addition, we have integrated various optimizations such as constant propagation, higher-order removal, and arity raising simply by modifying the underlying interpreter. Purely first-order methods suffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation.},
  booktitle = {Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation},
  series = {PLDI '96},
  publisher = {{ACM}},
  urldate = {2014-12-19},
  date = {1996},
  pages = {206--214},
  keywords = {compilation of higher-order functional languages,partial evaluation,semantics-directed compiler generation},
  author = {Sperber, Michael and Thiemann, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/FZC7VH2Q/Sperber_Thiemann - 1996 - Realistic Compilation by Partial Evaluation.pdf}
}

@inproceedings{Lerner2003automatically,
  location = {{New York, NY, USA}},
  title = {Automatically Proving the Correctness of Compiler Optimizations},
  isbn = {1-58113-662-5},
  url = {http://doi.acm.org/10.1145/781131.781156},
  doi = {10.1145/781131.781156},
  abstract = {We describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.},
  booktitle = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation},
  series = {PLDI '03},
  publisher = {{ACM}},
  urldate = {2014-12-19},
  date = {2003},
  pages = {220--231},
  keywords = {automated correctness proofs,compiler optimization},
  author = {Lerner, Sorin and Millstein, Todd and Chambers, Craig},
  file = {/Users/pgiarrusso/Zotero/storage/7ZEUEP84/Lerner et al - 2003 - Automatically Proving the Correctness of Compiler Optimizations.pdf}
}

@article{Stanier2013intermediate,
  title = {Intermediate Representations in Imperative Compilers: A Survey},
  volume = {45},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/2480741.2480743},
  doi = {10.1145/2480741.2480743},
  shorttitle = {Intermediate Representations in Imperative Compilers},
  abstract = {Compilers commonly translate an input program into an intermediate representation (IR) before optimizing it and generating code. Over time there have been a number of different approaches to designing and implementing IRs. Different IRs have varying benefits and drawbacks. In this survey, we highlight key developments in the area of IR for imperative compilers, group them by a taxonomy and timeline, and comment on the divide between academic research and real-world compiler technology. We conclude that mainstream compilers, especially in the multicore era, could benefit from further IR innovations.},
  number = {3},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2014-12-19},
  date = {2013-07},
  pages = {26:1--26:27},
  keywords = {optimization,Compilers,intermediate representations},
  author = {Stanier, James and Watson, Des},
  file = {/Users/pgiarrusso/Zotero/storage/RE3U5R8S/Stanier_Watson - 2013 - Intermediate Representations in Imperative Compilers - A Survey.pdf}
}
% == BibLateX quality report for Stanier2013intermediate:
% ? Possibly abbreviated journal title ACM Comput. Surv.

@article{Lupei2014incremental,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.4320},
  primaryClass = {cs},
  title = {Incremental View Maintenance for Nested-Relational Databases},
  url = {http://arxiv.org/abs/1412.4320},
  abstract = {Incremental view maintenance is an essential tool for speeding up the processing of large, locally changing workloads. Its fundamental challenge is to ensure that changes are propagated from input to output more efficiently than via recomputation. We formalize this requirement for positive nested relational algebra (NRA+) on bags and we propose a transformation deriving deltas for any expression in the language. The main difficulty in maintaining nested queries lies in the inability to express within NRA+ the efficient updating of inner bags, i.e., without completely replacing the tuples that contain them. To address this problem, we first show how to efficiently incrementalize IncNRA+, a large fragment of NRA+ whose deltas never generate inner bag updates. We then provide a semantics-preserving transformation that takes any nested query into a collection of IncNRA+ queries. This constitutes the first static solution for the efficient incremental processing of languages with nested collections. Furthermore, we show that the state-of-the-art technique of recursive IVM, originally developed for positive relational algebra with aggregation, also extends to nested queries. Finally, we generalize our static approach for the efficient incrementalization of NRA+ to a family of simply-typed lambda calculi, given that its primitives are themselves efficiently incrementalizable.},
  urldate = {2014-12-20},
  date = {2014-12-14},
  keywords = {Computer Science - Databases},
  author = {Lupei, Daniel and Koch, Christoph and Tannen, Val},
  file = {/Users/pgiarrusso/Zotero/storage/F9BUNHPP/Lupei et al - 2014 - Incremental View Maintenance for Nested-Relational Databases.pdf;/Users/pgiarrusso/Zotero/storage/FJMHM53K/1412.html}
}
% == BibLateX quality report for Lupei2014incremental:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Stepp2011equalitybased,
  langid = {english},
  title = {Equality-Based Translation Validator for LLVM},
  isbn = {978-3-642-22109-5 978-3-642-22110-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-22110-1_59},
  abstract = {We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.},
  number = {6806},
  booktitle = {Computer Aided Verification},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-12-21},
  date = {2011-01-01},
  pages = {737-742},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Stepp, Michael and Tate, Ross and Lerner, Sorin},
  editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
  file = {/Users/pgiarrusso/Zotero/storage/MZTDZFIB/Stepp et al - 2011 - Equality-Based Translation Validator for LLVM.pdf;/Users/pgiarrusso/Zotero/storage/33T5QZ33/978-3-642-22110-1_59.html}
}
% == BibLateX quality report for Stepp2011equalitybased:
% 'isbn': not a valid ISBN

@inproceedings{Filinski2011comprehensive,
  location = {{New York, NY, USA}},
  title = {Towards a Comprehensive Theory of Monadic Effects},
  isbn = {978-1-4503-0865-6},
  url = {http://doi.acm.org/10.1145/2034773.2034775},
  doi = {10.1145/2034773.2034775},
  abstract = {It has been more than 20 years since monads were proposed as a unifying concept for computational effects, in both formal semantics and functional programs. Over that period, there has been substantial incremental progress on several fronts within the ensuing research area, including denotational, operational, and axiomatic characterizations of effects; principles and frameworks for combining effects; prescriptive vs. descriptive effect-type systems; specification vs. implementation of effects; and realizations of effect-related theoretical constructions in practical functional languages, both eager and lazy. Yet few would confidently claim that programs with computational effects are by now as well understood, and as thoroughly supported by formal reasoning techniques, as types and terms in purely functional settings. This talk outlines (one view of) the landscape of effectful functional programming, and attempts to assess our collective progress towards the goal of a broad yet coherent theory of monadic effects. We are not quite there yet, but intriguingly, many potential ingredients of such a theory have been repeatedly discovered and developed, with only minor variations, in seemingly unrelated contexts. Some stronger-than-expected ties between the research topics mentioned above also instill hope that there is indeed a natural, comprehensive theory of monadic effects, waiting to be fully explicated.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '11},
  publisher = {{ACM}},
  urldate = {2014-12-23},
  date = {2011},
  pages = {1--1},
  keywords = {Monads,computational effects},
  author = {Filinski, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/BBNHDTDT/Filinski - 2011 - Towards a Comprehensive Theory of Monadic Effects.pdf}
}

@article{Maraist1998callbyneed,
  title = {The call-by-need lambda calculus},
  volume = {8},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796898003037},
  doi = {null},
  abstract = {We present a calculus that captures the operational semantics of call-by-need. The call-by-need lambda calculus is confluent, has a notion of standard reduction, and entails the same observational equivalence relation as the call-by-name calculus. The system can be formulated with or without explicit let bindings, admits useful notions of marking and developments, and has a straightforward operational interpretation.},
  number = {03},
  journaltitle = {Journal of Functional Programming},
  urldate = {2014-12-24},
  date = {1998-05},
  pages = {275--317},
  author = {Maraist, John and Odersky, Martin and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/K7P8D5H4/Maraist et al - 1998 - The call-by-need lambda calculus.pdf;/Users/pgiarrusso/Zotero/storage/JESJGEIT/displayAbstract.html}
}

@article{Ariola1997callbyneed,
  title = {The call-by-need lambda calculus},
  volume = {7},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796897002724},
  doi = {null},
  abstract = {Plotkin (1975) showed that the lambda calculus is a good model of the evaluation process for call-by-name functional programs. Reducing programs to constants or lambda abstractions according to the leftmost-outermost strategy exactly mirrors execution on an abstract machine like Landin\&apos;s SECD machine. The machine-based evaluator returns a constant or the token closure if and only if the standard reduction sequence starting at the same program will end in the same constant or in some lambda abstraction. However, the calculus does not capture the sharing of the evaluation of arguments that lazy implementations use to speed up the execution. More precisely, a lazy implementation evaluates procedure arguments only when needed and then only once. All other references to the formal procedure parameter re-use the value of the first argument evaluation. The mismatch between the operational semantics of the lambda calculus and the actual behavior of the prototypical implementation is a major obstacle for compiler writers. Unlike implementors of the leftmost-outermost strategy or of a call-by-value language, implementors of lazy systems cannot easily explain the behavior of their evaluator in terms of source level syntax. Hence, they often cannot explain why a certain syntactic transformation ‘works’ and why another doesn\&apos;t. In this paper we develop an equational characterization of the most popular lazy implementation technique – traditionally called ‘call-by-need’ – and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than Plotkin\&apos;s call-by-name lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g. the call-by-need continuation passing transformation and the realization of sharing via assignments. Some of this material first appeared in a paper presented at the 1995 ACM Conference on the Principles of Programming Languages. The paper was a joint effort with Maraist, Odersky and Wadler, who had independently developed a different equational characterization of call-by-need. We contrast our work with that of Maraist et al. in the body of this paper where appropriate.},
  number = {03},
  journaltitle = {Journal of Functional Programming},
  urldate = {2014-12-24},
  date = {1997-05},
  pages = {265--301},
  author = {Ariola, Zena M. and Felleisen, Matthias},
  file = {/Users/pgiarrusso/Zotero/storage/NQGHGAWV/Ariola_Felleisen - 1997 - The call-by-need lambda calculus.pdf;/Users/pgiarrusso/Zotero/storage/AEP32IPA/displayAbstract.html}
}

@inproceedings{Ariola1995callbyneed,
  location = {{New York, NY, USA}},
  title = {A Call-by-need Lambda Calculus},
  isbn = {0-89791-692-1},
  url = {http://doi.acm.org/10.1145/199448.199507},
  doi = {10.1145/199448.199507},
  abstract = {The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the call-by-need continuation passing transformation and the realization of sharing via assignments.},
  booktitle = {Proceedings of the 22Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '95},
  publisher = {{ACM}},
  urldate = {2014-12-24},
  date = {1995},
  pages = {233--246},
  author = {Ariola, Zena M. and Maraist, John and Odersky, Martin and Felleisen, Matthias and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/UKANXCW7/Ariola et al - 1995 - A Call-by-need Lambda Calculus.pdf}
}

@inproceedings{Filinski1994representing,
  location = {{New York, NY, USA}},
  title = {Representing Monads},
  isbn = {0-89791-636-0},
  url = {http://doi.acm.org/10.1145/174675.178047},
  doi = {10.1145/174675.178047},
  abstract = {We show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a call-by-value language with “composable continuations”. As part of the development, we extend Meyer and Wand's characterization of the relationship between continuation-passing and direct style to one for continuation-passing vs. general “monadic” style. We further show that the composable-continuations construct can itself be represented using ordinary, non-composable first-class continuations and a single piece of state. Thus, in the presence of two specific computational effects - storage and escapes - any expressible monadic structure (e.g., nondeterminism as represented by the list monad) can be added as a purely definitional extension, without requiring a reinterpretation of the whole language. The paper includes an implementation of the construction (in Standard ML with some New Jersey extensions) and several examples.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '94},
  publisher = {{ACM}},
  urldate = {2014-12-24},
  date = {1994},
  pages = {446--457},
  author = {Filinski, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/HT33WM3Q/Filinski - 1994 - Representing Monads.pdf}
}

@article{Danvy1992representing,
  title = {Representing Control: a Study of the CPS Transformation},
  volume = {2},
  issn = {1469-8072},
  url = {http://journals.cambridge.org/article_S0960129500001535},
  doi = {10.1017/S0960129500001535},
  shorttitle = {Representing Control},
  abstract = {This paper investigates the transformation of λν-terms into continuation-passing style (CPS). We show that by appropriate η-expansion of Fisher and Plotkin's two-pass equational specification of the CPS transform, we can obtain a static and context-free separation of the result terms into “essential” and “administrative” constructs. Interpreting the former as syntax builders and the latter as directly executable code, We obtain a simple and efficient one-pass transformation algorithm, easily extended to conditional expressions, recursive definitions, and similar constructs. This new transformation algorithm leads to a simpler proof of Plotkin's simulation and indifference results.We go on to show how CPS-based control operators similar to, more general then, Scheme's call/cc can be naturally accommodated by the new transformation algorithm. To demonstrate the expressive power of these operators, we use them to present an equivalent but even more concise formulation of the efficient CPS transformation algorithm. Finally, we relate the fundamental ideas underlying this derivation to similar concepts from other work on program manipulation; we derive a one-pass CPS transformation of λn-terms; and we outline some promising areas for future research.},
  number = {04},
  journaltitle = {Mathematical Structures in Computer Science},
  urldate = {2014-12-24},
  date = {1992-12},
  pages = {361--391},
  author = {Danvy, Oliver and Filinski, Andrzex},
  file = {/Users/pgiarrusso/Zotero/storage/VATRD9CI/Danvy_Filinski - 1992 - Representing Control - a Study of the CPS Transformation.pdf;/Users/pgiarrusso/Zotero/storage/PTHS994R/displayAbstract.html}
}

@article{Felleisen1992revised,
  title = {The revised report on the syntactic theories of sequential control and state},
  volume = {103},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397592900147},
  doi = {10.1016/0304-3975(92)90014-7},
  abstract = {The syntactic theories of control and state are conservative extensions of the λυ-calculus for equational reasoning about imperative programming facilities in higher-order languages. Unlike the simple λυ-calculus, the extended theories are mixtures of equivalence relations and compatible congruence relations on the term language, which significantly complicates the reasoning process. In this paper we develop fully compatible equational theories of the same imperative higher-order programming languages. The new theories subsume the original calculi of control and state and satisfy the usual Church–Rosser and Standardization Theorems. With the new calculi, equational reasoning about imperative programs becomes as simple as reasoning about functional programs.},
  number = {2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2014-12-25},
  date = {1992-09-14},
  pages = {235-271},
  author = {Felleisen, Matthias and Hieb, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/XCGHGC6T/0304397592900147.html}
}

@article{Felleisen1987syntactic,
  title = {A syntactic theory of sequential control},
  volume = {52},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397587901095},
  doi = {10.1016/0304-3975(87)90109-5},
  abstract = {Sequential control operators like J and call/cc are often found in implementations of the λ-calculus as a programming language. Their semantics is always defined by the evaluation function of an abstract machine. We show that, given such a machine semantics, one can derive an algebraic extension of the λυ-calculus. The extended calculus satisfies the diamond property and contains a Church-Rosser subcalculus. This underscores that the interpretation of control operators is to a certain degree independent of a specific evaluation strategy. We also prove a standardization theorem and use it to investigate the correspondence between the machine and the calculus. Together, the calculus and the rewriting machine form a syntactic theory of control, which provides a natural basis for reasoning about programs with nonfunctional control operators.},
  number = {3},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2014-12-25},
  date = {1987},
  pages = {205-237},
  author = {Felleisen, Matthias and Friedman, Daniel P. and Kohlbecker, Eugene and Duba, Bruce},
  file = {/Users/pgiarrusso/Zotero/storage/FPFSBXD9/0304397587901095.html}
}

@inproceedings{Wadler2003callbyvalue,
  location = {{New York, NY, USA}},
  title = {Call-by-value is Dual to Call-by-name},
  isbn = {1-58113-756-7},
  url = {http://doi.acm.org/10.1145/944705.944723},
  doi = {10.1145/944705.944723},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \& are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).},
  booktitle = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '03},
  publisher = {{ACM}},
  urldate = {2014-12-25},
  date = {2003},
  pages = {189--201},
  keywords = {sequent calculus,lambda calculus,Logic,Curry-Howard correspondence,De Morgan dual,lambda mu calculus,natural deduction},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/9BHXGZ2B/Wadler - 2003 - Call-by-value is Dual to Call-by-name.pdf}
}

@inproceedings{Kennedy2007compiling,
  location = {{New York, NY, USA}},
  title = {Compiling with Continuations, Continued},
  isbn = {978-1-59593-815-2},
  url = {http://doi.acm.org/10.1145/1291151.1291179},
  doi = {10.1145/1291151.1291179},
  abstract = {We present a series of CPS-based intermediate languages suitable for functional language compilation, arguing that they have practical benefits over direct-style languages based on A-normal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANF-based languages, inlining involves a re-normalization step that rearranges let expressions and possibly introduces a new 'join point' function, and in monadic languages, commuting conversions must be applied; in contrast, inlining in our CPS language is a simple substitution of variables for variables. We present a contification transformation implemented by simple rewrites on the intermediate language. Exceptions are modelled using so-called 'double-barrelled' CPS. Subtyping on exception constructors then gives a very straightforward effect analysis for exceptions. We also show how a graph-based representation of CPS terms can be implemented extremely efficiently, with linear-time term simplification.},
  booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '07},
  publisher = {{ACM}},
  urldate = {2014-12-25},
  date = {2007},
  pages = {177--190},
  keywords = {Monads,continuations,functional programming languages,continuation passing style,optimizing compilation},
  author = {Kennedy, Andrew},
  file = {/Users/pgiarrusso/Zotero/storage/36CEQQUG/Kennedy - 2007 - Compiling with Continuations, Continued.pdf;/Users/pgiarrusso/Zotero/storage/7CZ7KX84/Kennedy - 2007 - Compiling with Continuations, Continued.pdf}
}

@article{Barbanera1996symmetric,
  title = {A Symmetric Lambda Calculus for Classical Program Extraction},
  volume = {125},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540196900255},
  doi = {10.1006/inco.1996.0025},
  abstract = {We introduce aλ-calculus with symmetric reduction rules and “classical” types, i.e., types corresponding to formulas of classical propositional logic. The strong normalization property is proved to hold for such a calculus, as well as for its extension to a system equivalent to Peano arithmetic. A theorem on the shape of terms in normal form is also proved, making it possible to get recursive functions out of proofs ofΠ02formulas, i.e., those corresponding to program specifications.},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2014-12-26},
  date = {1996-03-15},
  pages = {103-117},
  author = {Barbanera, Franco and Berardi, Stefano},
  file = {/Users/pgiarrusso/Zotero/storage/FGD4A7BF/Barbanera_Berardi - 1996 - A Symmetric Lambda Calculus for Classical Program Extraction.pdf;/Users/pgiarrusso/Zotero/storage/A9U6G3QN/S0890540196900255.html}
}

@article{Sabry1997reflection,
  title = {A Reflection on Call-by-value},
  volume = {19},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/267959.269968},
  doi = {10.1145/267959.269968},
  abstract = {One way to model a sound and complete translation from a source calculus into a target calculus is with an adjoint or a Galois connection. In the special case of a reflection, one also has that the target calculus is isomorphic to a subset of the source. We show that three widely studied translations form reflections. We use as our source language Moggi's computational lambda calculus, which is an extension of Plotkin's call-by-value calculus. We show that Plotkin's CPS translation, Moggi's monad translation, and Girard's translation to linear logic can all be regarded as reflections form this source language, and we put forward the computational lambda calculus as a model of call-by-value computation that improves on the traditional  call-by-value calculus. Our work strengthens Plotkin's and Moggi's original results and improves on recent work based on equational correspondence, which uses equations rather than reductions.},
  number = {6},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2014-12-27},
  date = {1997-11},
  pages = {916--941},
  keywords = {continuations,_tablet,category theory,compiling,Galois connections},
  author = {Sabry, Amr and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/6RCP3C32/Sabry-Wadler - 1997 - A Reflection on Call-by-value.pdf}
}
% == BibLateX quality report for Sabry1997reflection:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@article{Okasaki1994callbyneed,
  langid = {english},
  title = {Call-by-need and continuation-passing style},
  volume = {7},
  issn = {0892-4635, 1573-0557},
  url = {http://link.springer.com/article/10.1007/BF01019945},
  doi = {10.1007/BF01019945},
  abstract = {This paper examines the transformation of call-by-need λ terms into continuation-passing style (CPS). It begins by presenting a simple transformation of call-by-need λ terms into program graphs and a reducer for such graphs. From this, an informal derivation is carried out, resulting in a translation from λ terms into self-reducing program graphs, where the graphs are represented as CPS terms involving storage operations. Though informal, the derivation proceeds in simple steps, and the resulting translation is taken to be our canonical CPS transformation for call-by-need λ terms. In order to define the CPS transformation more formally, two alternative presentations are given. The first takes the form of a continuation semantics for the call-by-need language. The second presentation follows Danvy and Hatcliff's two-stage decomposition of the call-by-name CPS transformation, resulting in a similar two-stage CPS transformation for call-by-need. Finally, a number of practical matters are considered, including an improvement to eliminate the so-called administrative redexes, as well as to avoid unnecessary memoization and take advantage of strictness information. These improvements make it feasible to consider potential applications in compilers for call-by-need programming languages.},
  number = {1},
  journaltitle = {LISP and Symbolic Computation},
  shortjournal = {LISP and Symbolic Computation},
  urldate = {2014-12-27},
  date = {1994-01-01},
  pages = {57-81},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),functional programming,Lazy Evaluation,Software Engineering/Programming and Operating Systems,continuations,Numeric Computing,Call-by-need,Continuation-passing Style},
  author = {Okasaki, Chris and Lee, Peter and Tarditi, David},
  file = {/Users/pgiarrusso/Zotero/storage/8ZRVM9G3/Okasaki et al - 1994 - Call-by-need and continuation-passing style.pdf;/Users/pgiarrusso/Zotero/storage/XQHMT6KV/BF01019945.html}
}
% == BibLateX quality report for Okasaki1994callbyneed:
% 'issn': not a valid ISSN

@article{Shivers2004higherorder,
  title = {Higher-order Control-flow Analysis in Retrospect: Lessons Learned, Lessons Abandoned},
  volume = {39},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/989393.989421},
  doi = {10.1145/989393.989421},
  shorttitle = {Higher-order Control-flow Analysis in Retrospect},
  abstract = {Traditional flow analysis techniques, such as the ones typically employed by optimising Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique --- control flow analysis --- which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.The techniques presented in this paper are backed up by working code. They are applicable not only to Scheme, but also to related languages, such as Common Lisp and ML.},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-30},
  date = {2004-04},
  pages = {257--269},
  author = {Shivers, Olin}
}
% == BibLateX quality report for Shivers2004higherorder:
% ? Possibly abbreviated journal title SIGPLAN Not.

@article{Flanagan2004essence,
  title = {The Essence of Compiling with Continuations},
  volume = {39},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/989393.989443},
  doi = {10.1145/989393.989443},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the "continuation"). Since the naïve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.A thorough analysis of the abstract machine for CPS terms shows that the actions of the code generator invert the naïve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-30},
  date = {2004-04},
  pages = {502--514},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias}
}
% == BibLateX quality report for Flanagan2004essence:
% ? Possibly abbreviated journal title SIGPLAN Not.

@article{Kranz2004orbit,
  title = {Orbit: An Optimizing Compiler for Scheme},
  volume = {39},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/989393.989414},
  doi = {10.1145/989393.989414},
  shorttitle = {Orbit},
  abstract = {Orbit was an optimizing compiler for T, a dialect of Scheme. Its aggressive use of CPS conversion, novel closure representations, and efficient code generation strategies made it the best compiler for a Scheme dialect at the time and for many years to come. The design of T and Orbit directly spawned six PhD theses and one Masters thesis, and influenced many other projects as well, including SML of New Jersey.},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-30},
  date = {2004-04},
  pages = {175--191},
  author = {Kranz, David and Kelsey, Richard and Rees, Jonathan and Hudak, Paul and Philbin, James and Adams, Norman}
}
% == BibLateX quality report for Kranz2004orbit:
% ? Possibly abbreviated journal title SIGPLAN Not.

@article{Lee2004optimizing,
  title = {Optimizing ML with Run-time Code Generation},
  volume = {39},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/989393.989448},
  doi = {10.1145/989393.989448},
  abstract = {We describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Run-time code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-31},
  date = {2004-04},
  pages = {540--553},
  author = {Lee, Peter and Leone, Mark}
}
% == BibLateX quality report for Lee2004optimizing:
% ? Possibly abbreviated journal title SIGPLAN Not.

@article{Tarditi2004til,
  title = {TIL: A Type-directed, Optimizing Compiler for ML},
  volume = {39},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/989393.989449},
  doi = {10.1145/989393.989449},
  shorttitle = {TIL},
  abstract = {The goal of the TIL project was to explore the use of Typed Intermediate Languages to produce high-performance native code from Standard ML (SML). We believed that existing SML compilers were doing a good job of conventional functional language optimizations, as one might find in a LISP compiler, but that inadequate use was made of the rich type information present in the source language. Our goal was to show that we could get much greater performance by propagating type information through to the back end of the compiler, without sacrificing the advantages afforded by loop-oriented and other optimizations.We also confirmed that using typed intermediate languages dramatically improved the reliability and maintainability of the compiler itself. In particular, we were able to use the type system to express critical invariants, and enforce those invariants through type checking. In this respect, TIL introduced and popularized the notion of a certifying compiler, which attaches a checkable certificate of safety to its generated code. In turn, this led directly to the idea of certified object code, inspiring the development of Proof-Carrying Code and Typed Assembly Language as certified object code formats.},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-31},
  date = {2004-04},
  pages = {554--567},
  author = {Tarditi, David and Morrisett, Greg and Cheng, Perry and Stone, Chris and Harper, Robert and Lee, Peter}
}
% == BibLateX quality report for Tarditi2004til:
% ? Possibly abbreviated journal title SIGPLAN Not.

@inproceedings{Kammar2013handlers,
  location = {{New York, NY, USA}},
  title = {Handlers in Action},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500590},
  doi = {10.1145/2500365.2500590},
  abstract = {Plotkin and Pretnar's handlers for algebraic effects occupy a sweet spot in the design space of abstractions for effectful computation. By separating effect signatures from their implementation, algebraic effects provide a high degree of modularity, allowing programmers to express effectful programs independently of the concrete interpretation of their effects. A handler is an interpretation of the effects of an algebraic computation. The handler abstraction adapts well to multiple settings: pure or impure, strict or lazy, static types or dynamic types. This is a position paper whose main aim is to popularise the handler abstraction. We give a gentle introduction to its use, a collection of illustrative examples, and a straightforward operational semantics. We describe our Haskell implementation of handlers in detail, outline the ideas behind our OCaml, SML, and Racket implementations, and present experimental results comparing handlers with existing code.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2014-12-31},
  date = {2013},
  pages = {145--158},
  keywords = {Haskell,Monads,continuations,modularity,algebraic effects,effect handlers,effect typing},
  author = {Kammar, Ohad and Lindley, Sam and Oury, Nicolas},
  file = {/Users/pgiarrusso/Zotero/storage/BKTDJVIH/Kammar et al - 2013 - Handlers in Action.pdf}
}

@article{Appel1998ssa,
  title = {SSA is Functional Programming},
  volume = {33},
  issn = {0362-1340},
  url = {http://doi.acm.org/10.1145/278283.278285},
  doi = {10.1145/278283.278285},
  number = {4},
  journaltitle = {SIGPLAN Not.},
  urldate = {2014-12-31},
  date = {1998-04},
  pages = {17--20},
  author = {Appel, Andrew W.},
  file = {/Users/pgiarrusso/Zotero/storage/8DADPC46/Appel - 1998 - SSA is Functional Programming.pdf}
}
% == BibLateX quality report for Appel1998ssa:
% ? Possibly abbreviated journal title SIGPLAN Not.

@article{Levy2003adjunction,
  title = {Adjunction Models For Call-By-Push-Value With Stacks},
  volume = {69},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066104805681},
  doi = {10.1016/S1571-0661(04)80568-1},
  abstract = {Call-by-push-value (CBPV) is a new paradigm, which has been claimed to provide the semantic primitives from which call-by-value and call-by-name are built. We present its operational semantics in the form of a Felleisen-Friedman style CK-machine, and see how this machine suggests a new term judgement of stacks. When augmented with this judgement, CBPV has an elegant categorical semantics based on adjunctions.

We describe this categorical semantics incrementally. First, we introduce locally indexed categories and the opGrothendieck construction, and use these to give the basic structure for interpreting the 3 judgements: values, stacks and computations. Then we look at the universal property required to interpret each type constructor. We define a model to be a strong adjunction with countable coproducts, countable products and exponentials.

We justify this definition in two ways. First, we see that it has a wide range of instances: we give examples for divergence, storage, erratic choice, continuations etc., in each case decomposing Moggi's strong monad into a strong adjunction.

For the second justification, we start by giving equational laws for CBPV+stacks. This requires some additional pattern-matching constructs, but they do not affect the set of computations. We then show that the categories of theories and of models are equivalent.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {CTCS'02, Category Theory and Computer Science},
  urldate = {2015-01-02},
  date = {2003-02},
  pages = {248-271},
  author = {Levy, Paul Blain},
  file = {/Users/pgiarrusso/Zotero/storage/29SP9SDB/Levy - 2003 - Adjunction Models For Call-By-Push-Value With Stacks.pdf;/Users/pgiarrusso/Zotero/storage/EVBQBMB9/S1571066104805681.html}
}

@incollection{Wells2002essence,
  langid = {english},
  title = {The Essence of Principal Typings},
  isbn = {978-3-540-43864-9 978-3-540-45465-6},
  url = {http://link.springer.com/chapter/10.1007/3-540-45465-9_78},
  abstract = {Let S be some type system. A typing in S for a typable term M is the collection of all of the information other than M which appears in the final judgement of a proof derivation showing that M is typable. For example, suppose there is a derivation in S ending with the judgement A ⊢ M: τ meaning that M has result type τ when assuming the types of free variables are given by A. Then (A, τ) is a typing for M. A principal typing in S for a term M is a typing for M which somehow represents all other possible typings in S for M. It is important not to confuse this with a weaker notion in connection with the Hindley/Milner type system often called “principal types”. Previous definitions of principal typings for specific type systems have involved various syntactic operations on typings such as substitution of types for type variables, expansion, lifting, etc. This paper presents a new general definition of principal typings which does not depend on the details of any particular type system. This paper shows that the new general definition correctly generalizes previous system-dependent definitions. This paper explains why the new definition is the right one. Furthermore, the new definition is used to prove that certain polymorphic type systems using ∀-quantifiers, namely System F and the Hindley/Milner system, do not have principal typings. All proofs can be found in a longer version available at the author’s home page.},
  number = {2380},
  booktitle = {Automata, Languages and Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-03},
  date = {2002-01-01},
  pages = {913-925},
  keywords = {Mathematics of Computing,Data Structures; Cryptology and Information Theory,Software Engineering/Programming and Operating Systems,Theory of Computation,Computer Graphics},
  author = {Wells, J. B.},
  editor = {Widmayer, Peter and Eidenbenz, Stephan and Triguero, Francisco and Morales, Rafael and Conejo, Ricardo and Hennessy, Matthew},
  file = {/Users/pgiarrusso/Zotero/storage/BBA2QVZA/Wells - 2002 - The Essence of Principal Typings.pdf;/Users/pgiarrusso/Zotero/storage/C4M928GF/3-540-45465-9_78.html}
}
% == BibLateX quality report for Wells2002essence:
% 'isbn': not a valid ISBN

@inproceedings{Jim1996what,
  location = {{New York, NY, USA}},
  title = {What Are Principal Typings and What Are They Good for?},
  isbn = {0-89791-769-3},
  url = {http://doi.acm.org/10.1145/237721.237728},
  doi = {10.1145/237721.237728},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '96},
  publisher = {{ACM}},
  urldate = {2015-01-03},
  date = {1996},
  pages = {42--53},
  author = {Jim, Trevor},
  file = {/Users/pgiarrusso/Zotero/storage/V22H3Z3P/Jim - 1996 - What Are Principal Typings and What Are They Good for.pdf}
}

@inproceedings{Wadler1989how,
  location = {{New York, NY, USA}},
  title = {How to Make Ad-hoc Polymorphism Less Ad Hoc},
  isbn = {0-89791-294-2},
  url = {http://doi.acm.org/10.1145/75277.75283},
  doi = {10.1145/75277.75283},
  abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the “eqtype variables” of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '89},
  publisher = {{ACM}},
  urldate = {2015-01-04},
  date = {1989},
  pages = {60--76},
  author = {Wadler, P. and Blott, S.}
}

@inproceedings{Bahr2015generalising,
  location = {{New York, NY, USA}},
  title = {Generalising Tree Traversals to DAGs: Exploiting Sharing Without the Pain},
  isbn = {978-1-4503-3297-2},
  url = {http://doi.acm.org/10.1145/2678015.2682539},
  doi = {10.1145/2678015.2682539},
  shorttitle = {Generalising Tree Traversals to DAGs},
  abstract = {We present a recursion scheme based on attribute grammars that can be transparently applied to trees and acyclic graphs. Our recursion scheme allows the programmer to implement a tree traversal and then apply it to compact graph representations of trees instead. The resulting graph traversals avoid recomputation of intermediate results for shared nodes -- even if intermediate results are used in different contexts. Consequently, this approach leads to asymptotic speedup proportional to the compression provided by the graph representation. In general, however, this sharing of intermediate results is not sound. Therefore, we complement our implementation of the recursion scheme with a number of correspondence theorems that ensure soundness for various classes of traversals. We illustrate the practical applicability of the implementation as well as the complementing theory with a number of examples.},
  booktitle = {Proceedings of the 2015 Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM '15},
  publisher = {{ACM}},
  urldate = {2015-01-05},
  date = {2015},
  pages = {27--38},
  keywords = {Haskell,attribute grammars,graph traversal,sharing},
  author = {Bahr, Patrick and Axelsson, Emil},
  file = {/Users/pgiarrusso/Zotero/storage/FK7CMCSA/Bahr_Axelsson - 2015 - Generalising Tree Traversals to DAGs - Exploiting Sharing Without the Pain.pdf}
}

@inproceedings{Odersky1997pizza,
  location = {{New York, NY, USA}},
  title = {Pizza into Java: Translating Theory into Practice},
  isbn = {0-89791-853-3},
  url = {http://doi.acm.org/10.1145/263699.263715},
  doi = {10.1145/263699.263715},
  shorttitle = {Pizza into Java},
  booktitle = {Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '97},
  publisher = {{ACM}},
  urldate = {2015-01-05},
  date = {1997},
  pages = {146--159},
  author = {Odersky, Martin and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/9HHEGDGF/Odersky_Wadler - 1997 - Pizza into Java - Translating Theory into Practice.pdf}
}

@incollection{Chang2012callbyneed,
  langid = {english},
  title = {The Call-by-Need Lambda Calculus, Revisited},
  isbn = {978-3-642-28868-5 978-3-642-28869-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-28869-2_7},
  abstract = {The existing call-by-need λ describe lazy evaluation via equational logics. A programmer can use these logics to safely ascertain whether one term is behaviorally equivalent to another or to determine the value of a lazy program. However, neither of the existing calculi models evaluation in a way that matches lazy implementations. Both calculi suffer from the same two problems. First, the calculi never discard function calls, even after they are completely resolved. Second, the calculi include re-association axioms even though these axioms are merely administrative steps with no counterpart in any implementation. In this paper, we present an alternative axiomatization of lazy evaluation using a single axiom. It eliminates both the function call retention problem and the extraneous re-association axioms. Our axiom uses a grammar of contexts to describe the exact notion of a needed computation. Like its predecessors, our new calculus satisfies consistency and standardization properties and is thus suitable for reasoning about behavioral equivalence. In addition, we establish a correspondence between our semantics and Launchbury’s natural semantics.},
  number = {7211},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-01-05},
  date = {2012-01-01},
  pages = {128-147},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,lambda calculus,Computer Communication Networks,Call-by-need,laziness},
  author = {Chang, Stephen and Felleisen, Matthias},
  editor = {Seidl, Helmut},
  file = {/Users/pgiarrusso/Zotero/storage/K2DNHI9E/Chang_Felleisen - 2012 - The Call-by-Need Lambda Calculus, Revisited.pdf;/Users/pgiarrusso/Zotero/storage/AJCBKXWH/978-3-642-28869-2_7.html}
}
% == BibLateX quality report for Chang2012callbyneed:
% 'isbn': not a valid ISBN

@online{Hofmann1995extensional,
  title = {Extensional concepts in intensional type theory},
  url = {http://www.lfcs.inf.ed.ac.uk/reports/95/ECS-LFCS-95-327/index.html},
  abstract = {Theories of dependent types have been proposed as a foundation of constructive mathematics and as a framework in which to construct certified programs. In these applications an important role is played by identity types which internalise equality and therefore are essential for accommodating proofs and programs in the same formal system.

This thesis attempts to reconcile the two different ways that type theories deal with identity types. In extensional type theory the propositional equality induced by the identity types is identified with definitional equality, i.e. conversion. This renders type-checking and well-formedness of propositions undecidable and leads to non-termination in the presence of universes. In intensional type theory propositional equality is coarser than definitional equality, the latter being confined to definitional expansion and normalisation. Then type-checking and well-formedness are decidable, and this variant is therefore adopted by most implementations.

However, the identity type in intensional type theory is not powerful enough for formalisation of mathematics and program development. Notably, it does not identify pointwise equal functions (functional extensionality) and provides no means of redefining equality on a type as a given relation, i.e. quotient types. We call such capabilities extensional concepts. Other extensional concepts of interest are uniqueness of proofs and more specifically of equality proofs, subset types, and propositional extensionality---the identification of equivalent propositions. In this work we investigate to what extent these extensional concepts may be added to intensional type theory without sacrificing decidability and existence of canonical forms. The method we use is the translation of identity types into equivalence relations defined by induction on the type structure. In this way type theory with extensional concepts can be understood as a high-level language for working with equivalence relations instead of equality. Such translations of type theory into itself turn out to be best described using categorical models of type theory.

We thus begin with a thorough treatment of categorical models with particular emphasis on the interpretation of type-theoretic syntax in such models. We then show how pairs of types and predicates can be organised into a model of type theory in which subset types are available and in which any two proofs of a proposition are equal. This model has applications in the areas of program extraction from proofs and modules for functional programs. For us its main purpose is to clarify the idea of syntactic translations via categorical model constructions.

The main result of the thesis consists of the construction of two models in which functional extensionality and quotient types are available. In the first one types are modelled by types together with proposition-valued partial equivalence relations. This model is rather simple and in addition provides subset types and propositional extensionality. However, it does not furnish proper dependent types such as vectors or matrices. We try to overcome this disadvantage by using another model based on families of type-valued equivalence relations which is however much more complicated and validates certain conversion rules only up to propositional equality.

We illustrate the use of these models by several small examples taken from both formalised mathematics and program development.

We also establish various syntactic properties of propositional equality including a proof of the undecidability of typing in extensional type theory and a correspondence between derivations in extensional type theory and terms in intensional type theory with extensional concepts added. Furthermore we settle affirmatively the hitherto open question of the independence of unicity of equality proofs in intensional type theory which implies that the addition of pattern matching to intensional type theory does not yield a conservative extension.},
  urldate = {2015-05-04},
  date = {1995},
  author = {Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/AHYDRRZB/Hofmann - 1995 - Extensional concepts in intensional type theory.pdf;/Users/pgiarrusso/Zotero/storage/UDGRQ7W2/index.html}
}

@article{Buss2002prospects,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0205003},
  title = {The prospects for mathematical logic in the twenty-first century},
  url = {http://arxiv.org/abs/cs/0205003},
  abstract = {The four authors present their speculations about the future developments of mathematical logic in the twenty-first century. The areas of recursion theory, proof theory and logic for computer science, model theory, and set theory are discussed independently.},
  urldate = {2015-05-05},
  date = {2002-05-03},
  keywords = {Computer Science - Logic in Computer Science,A.1,F.0,I.2.0},
  author = {Buss, Samuel R. and Kechris, Alexander S. and Pillay, Anand and Shore, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/95CK5U4Q/Buss et al - 2002 - The prospects for mathematical logic in the twenty-first century.pdf;/Users/pgiarrusso/Zotero/storage/KC8BNH3A/0205003.html}
}
% == BibLateX quality report for Buss2002prospects:
% Unexpected field 'archivePrefix'
% Missing required field 'journaltitle'

@incollection{Hofmann1997syntaxa,
  title = {Syntax and Semantics of Dependent Types},
  isbn = {978-0-511-52661-9},
  url = {http://dx.doi.org/10.1017/CBO9780511526619.004},
  booktitle = {Semantics and Logics of Computation},
  series = {Publications of the Newton Institute},
  publisher = {{Cambridge University Press}},
  date = {1997},
  author = {Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/HSRHBJN4/Hofmann - 1997 - Syntax and Semantics of Dependent Types.pdf;/Users/pgiarrusso/Zotero/storage/3MSCBXN8/chapter.html}
}
% == BibLateX quality report for Hofmann1997syntaxa:
% Missing required field 'editor'

@inproceedings{Pugh1989incremental,
  location = {{New York, NY, USA}},
  title = {Incremental Computation via Function Caching},
  isbn = {0-89791-294-2},
  url = {http://doi.acm.org/10.1145/75277.75305},
  doi = {10.1145/75277.75305},
  booktitle = {Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '89},
  publisher = {{ACM}},
  urldate = {2015-05-07},
  date = {1989},
  pages = {315--328},
  author = {Pugh, W. and Teitelbaum, T.}
}

@inproceedings{Krishnaswami2013internalizing,
  location = {{Dagstuhl, Germany}},
  title = {Internalizing Relational Parametricity in the Extensional Calculus of Constructions},
  volume = {23},
  isbn = {978-3-939897-60-6},
  url = {http://drops.dagstuhl.de/opus/volltexte/2013/4212},
  doi = {http://dx.doi.org/10.4230/LIPIcs.CSL.2013.432},
  booktitle = {Computer Science Logic 2013 (CSL 2013)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2013},
  pages = {432-451},
  keywords = {_tablet},
  author = {Krishnaswami, Neelakantan R. and Dreyer, Derek},
  editor = {Rocca, Simona Ronchi Della},
  file = {/Users/pgiarrusso/Zotero/storage/V75RRRSA/Krishnaswami-Dreyer - 2013 - Internalizing Relational Parametricity in the Extensional Calculus of Constructions.pdf},
  urn = {urn:nbn:de:0030-drops-42125}
}
% == BibLateX quality report for Krishnaswami2013internalizing:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Bainbridge1990functorial,
  title = {Functorial polymorphism},
  volume = {70},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397590901517},
  doi = {10.1016/0304-3975(90)90151-7},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Special Issue Fourth Workshop on Mathematical Foundations of Programming Semantics, Boulder, CO, May 1988},
  urldate = {2015-05-15},
  date = {1990-01-15},
  pages = {35-64},
  author = {Bainbridge, E. S. and Freyd, P. J. and Scedrov, A. and Scott, P. J.},
  file = {/Users/pgiarrusso/Zotero/storage/V4XD36U6/0304397590901517.html}
}

@inproceedings{Dunphy2004parametric,
  title = {Parametric limits},
  doi = {10.1109/LICS.2004.1319618},
  abstract = {We develop a categorical model of polymorphic lambda calculi using the notion of parametric limits, which extend the notion of limits in categories to reflexive graphs of categories. We show that a number of parametric models of polymorphism can be captured in this way. We also axiomatize the structure of reflexive graphs needed for modelling parametric polymorphism based on ideas of fibrations, and show that it leads to proofs of representation results such as the initial algebra and final coalgebra properties one expects in polymorphic lambda calculi.},
  eventtitle = {Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004},
  booktitle = {Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004},
  date = {2004-07},
  pages = {242-251},
  keywords = {lambda calculus,Computer science,parametric polymorphism,reflexive graphs,relational parametricity,Logic,categorical model,fibrations,final coalgebra properties,initial algebra,parametric limits,parametric models,polymorphic lambda calculi,representation proofs,topology},
  author = {Dunphy, B. and Reddy, U.S.},
  file = {/Users/pgiarrusso/Zotero/storage/VI6M4FKP/Dunphy_Reddy - 2004 - Parametric limits.pdf;/Users/pgiarrusso/Zotero/storage/TRCK6UXA/freeabs_all.html}
}

@article{OHearn1995parametricity,
  title = {Parametricity and Local Variables},
  volume = {42},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/210346.210425},
  doi = {10.1145/210346.210425},
  abstract = {We propose that the phenomenon of local state may be understood in terms of Strachey's concept of parametric (i.e., uniform) polymorphism. The intuitive basis for our proposal is the following analogy: a non-local procedure is independent of locally-declared variables in the same way that a parametrically polymorphic function is independent of types to which it is instantiated.A connection between parametricity and representational abstraction was first suggested by J.C. Reynolds. Reynolds used logical relations to formalize this connection in languages with type variables and user-defined types. We use relational parametricity to construct a model for an Algol-like language in which interactions between local and non-local entities satisfy certain relational criteria.  Reasoning about local variables essentially involved proving properties of polymorphic functions. The new model supports straightforward validations of all the test equivalences that have been proposed in the literature for local-variable semantics, and encompasses standard methods of reasoning about data representations. It is not known whether our techniques yield fully abstract semantics. A model based on partial equivalence relations on the natural numbers is also briefly examined.},
  number = {3},
  journaltitle = {J. ACM},
  urldate = {2015-05-15},
  date = {1995-05},
  pages = {658--709},
  keywords = {logical relations,local state,parametric polymorphism,algol-like languages},
  author = {O'Hearn, P. W. and Tennent, R. D.}
}
% == BibLateX quality report for OHearn1995parametricity:
% ? Possibly abbreviated journal title J. ACM

@incollection{McBride2009lets,
  langid = {english},
  title = {Let’s See How Things Unfold: Reconciling the Infinite with the Intensional (Extended Abstract)},
  isbn = {978-3-642-03740-5 978-3-642-03741-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-03741-2_9},
  shorttitle = {Let’s See How Things Unfold},
  abstract = {Coinductive types model infinite structures unfolded on demand, like politicians’ excuses: for each attack, there is a defence but no likelihood of resolution. Representing such evolving processes coinductively is often more attractive than representing them as functions from a set of permitted observations, such as projections or finite approximants, as it can be tricky to ensure that observations are meaningful and consistent. As programmers and reasoners, we need coinductive definitions in our toolbox, equipped with appropriate computational and logical machinery. Lazy functional languages like Haskell [18] exploit call-by-need computation to over-approximate the programming toolkit for coinductive data: in a sense, all data is coinductive and delivered on demand, or not at all if the programmer has failed to ensure the productivity of a program.},
  number = {5728},
  booktitle = {Algebra and Coalgebra in Computer Science},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-05-18},
  date = {2009},
  pages = {113-126},
  keywords = {Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages,Symbolic and Algebraic Manipulation,Discrete Mathematics in Computer Science,Models and Principles},
  author = {McBride, Conor},
  editor = {Kurz, Alexander and Lenisa, Marina and Tarlecki, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/MWI8GK6S/McBride - 2009 - Let’s See How Things Unfold - Reconciling the Infinite with the Intensional (Extended Abstract).pdf;/Users/pgiarrusso/Zotero/storage/4AT88WN9/978-3-642-03741-2_9.html}
}
% == BibLateX quality report for McBride2009lets:
% 'isbn': not a valid ISBN

@incollection{Atkey2009syntax,
  langid = {english},
  title = {Syntax for Free: Representing Syntax with Binding Using Parametricity},
  isbn = {978-3-642-02272-2 978-3-642-02273-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-02273-9_5},
  shorttitle = {Syntax for Free},
  abstract = {We show that, in a parametric model of polymorphism, the type ∀ α. ((α→α) →α) →(α→α→α) →α is isomorphic to closed de Bruijn terms. That is, the type of closed higher-order abstract syntax terms is isomorphic to a concrete representation. To demonstrate the proof we have constructed a model of parametric polymorphism inside the Coq proof assistant. The proof of the theorem requires parametricity over Kripke relations. We also investigate some variants of this representation.},
  number = {5608},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-05-19},
  date = {2009},
  pages = {35-49},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematics of Computing,Symbolic and Algebraic Manipulation,Mathematical Logic and Foundations,Computing Methodologies},
  author = {Atkey, Robert},
  editor = {Curien, Pierre-Louis},
  file = {/Users/pgiarrusso/Zotero/storage/JX6G3EBF/Atkey - 2009 - Syntax for Free - Representing Syntax with Binding Using Parametricity.pdf;/Users/pgiarrusso/Zotero/storage/EH2ZD6ED/10.html}
}
% == BibLateX quality report for Atkey2009syntax:
% 'isbn': not a valid ISBN

@inproceedings{Rompf2015go,
  location = {{Dagstuhl, Germany}},
  title = {Go Meta! A Case for Generative Programming and DSLs in Performance Critical Systems},
  volume = {32},
  isbn = {978-3-939897-80-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5029},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.238},
  booktitle = {1st Summit on Advances in Programming Languages (SNAPL 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {238-261},
  author = {Rompf, Tiark and Brown, Kevin J. and Lee, HyoukJoong and Sujeeth, Arvind K. and Jonnalagedda, Manohar and Amin, Nada and Ofenbeck, Georg and Stojanov, Alen and Klonatos, Yannis and Dashti, Mohammad and Koch, Christoph and Püschel, Markus and Olukotun, Kunle},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/QTP82E7C/Rompf et al - 2015 - Go Meta! A Case for Generative Programming and DSLs in Performance Critical Systems.pdf},
  urn = {urn:nbn:de:0030-drops-50295}
}
% == BibLateX quality report for Rompf2015go:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{DeVito2015design,
  location = {{Dagstuhl, Germany}},
  title = {The Design of Terra: Harnessing the Best Features of High-Level and Low-Level Languages},
  volume = {32},
  isbn = {978-3-939897-80-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5018},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.79},
  abstract = {Applications are often written using a combination of high-level and low-level languages since it allows performance critical parts to be carefully optimized, while other parts can be written more productively. This approach is used in web development, game programming, and in build systems for applications themselves. However, most languages were not designed with interoperability in mind, resulting in glue code and duplicated features that add complexity. We propose a two-language system where both languages were designed to interoperate. Lua is used for our high-level language since it was originally designed with interoperability in mind. We create a new low-level language, Terra, that we designed to interoperate with Lua. It is embedded in Lua, and meta-programmed from it, but has a low level of abstraction suited for writing high-performance code. We discuss important design decisions - compartmentalized runtimes, glue-free interoperation, and meta-programming features - that enable Lua and Terra to be more powerful than the sum of their parts.},
  booktitle = {1st Summit on Advances in Programming Languages (SNAPL 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {79-89},
  author = {DeVito, Zachary and Hanrahan, Pat},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/5J5ISD24/DeVito_Hanrahan - 2015 - The Design of Terra - Harnessing the Best Features of High-Level and Low-Level Languages.pdf},
  urn = {urn:nbn:de:0030-drops-50186}
}
% == BibLateX quality report for DeVito2015design:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Wang2015draining,
  location = {{Dagstuhl, Germany}},
  title = {Draining the Swamp: Micro Virtual Machines as Solid Foundation for Language Development},
  volume = {32},
  isbn = {978-3-939897-80-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5034},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.321},
  booktitle = {1st Summit on Advances in Programming Languages (SNAPL 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {321-336},
  author = {Wang, Kunshan and Lin, Yi and Blackburn, Stephen M. and Norrish, Michael and Hosking, Antony L.},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/M8BAU2E5/Wang et al - 2015 - Draining the Swamp - Micro Virtual Machines as Solid Foundation for Language Development.pdf},
  urn = {urn:nbn:de:0030-drops-50341}
}
% == BibLateX quality report for Wang2015draining:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Acar2015coupling,
  location = {{Dagstuhl, Germany}},
  title = {Coupling Memory and Computation for Locality Management},
  volume = {32},
  isbn = {978-3-939897-80-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5012},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.1},
  booktitle = {1st Summit on Advances in Programming Languages (SNAPL 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {1-14},
  author = {Acar, Umut A. and Blelloch, Guy and Fluet, Matthew and Muller, Stefan K. and Raghunathan, Ram},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/PEJT4HGH/Acar et al - 2015 - Coupling Memory and Computation for Locality Management.pdf},
  urn = {urn:nbn:de:0030-drops-50121}
}
% == BibLateX quality report for Acar2015coupling:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Heydon2000caching,
  location = {{New York, NY, USA}},
  title = {Caching Function Calls Using Precise Dependencies},
  isbn = {1-58113-199-2},
  url = {http://doi.acm.org/10.1145/349299.349341},
  doi = {10.1145/349299.349341},
  abstract = {This paper describes the implementation of a purely functional
programming language for building software systems. In this language,
external tools like compilers and linkers are invoked by function calls. Because some function calls are extremely expensive, it is obviously important to reuse the results of previous function calls whenever possible. Caching a function call requires the language interpreter to record all values on which the function call depends. For optimal caching, it is important to record precise dependencies that are both dynamic and fine-grained. The paper sketches how we compute such dependencies, describes the implementation of an efficient function cache, and evaluates our implementation's performance.},
  booktitle = {Proceedings of the ACM SIGPLAN 2000 Conference on Programming Language Design and Implementation},
  series = {PLDI '00},
  publisher = {{ACM}},
  urldate = {2015-05-21},
  date = {2000},
  pages = {311--320},
  author = {Heydon, Allan and Levin, Roy and Yu, Yuan},
  file = {/Users/pgiarrusso/Zotero/storage/W9XVSPDP/Heydon et al - 2000 - Caching Function Calls Using Precise Dependencies.pdf}
}

@article{Amadio1993subtyping,
  title = {Subtyping Recursive Types},
  volume = {15},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/155183.155231},
  doi = {10.1145/155183.155231},
  abstract = {We investigate the interactions of subtyping and recursive types, in a simply typed \&lgr;-calculus. The two fundamental questions here are whether two (recursive)types are in the subtype relation and whether a term has a type. To address the first question, we relate various definitions of type equivalence and subtyping that are induced by a model, an ordering on infinite trees, an algorithm, and a set of type rules. We show soundness and completeness among the rules, the algorithm, and the tree semantics. We also prove soundness and a restricted form of completeness for the model. To address the second question, we show that to every pair of types in the subtype relation we can associate a term whose denotation is the uniquely determined coercion map between the two types. Moreover, we derive an algorithm that, when given a term with implicit coercions, can infer its least type whenever possible.},
  number = {4},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2015-05-26},
  date = {1993-09},
  pages = {575--631},
  keywords = {recursive types,subtyping,coercions,Lambda-calculus,partial-equivalence relations,regular trees,tree orderings,type equivalence,typechecking algorithm},
  author = {Amadio, Roberto M. and Cardelli, Luca}
}
% == BibLateX quality report for Amadio1993subtyping:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@incollection{Reynolds1974theory,
  langid = {english},
  title = {Towards a theory of type structure},
  isbn = {978-3-540-06859-4 978-3-540-37819-8},
  url = {http://link.springer.com/chapter/10.1007/3-540-06859-7_148},
  number = {19},
  booktitle = {Programming Symposium},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-05-27},
  date = {1974},
  pages = {408-425},
  keywords = {Computer Science; general},
  author = {Reynolds, John C.},
  editor = {Robinet, Prof B.},
  file = {/Users/pgiarrusso/Zotero/storage/3WRH5EH5/10.html}
}
% == BibLateX quality report for Reynolds1974theory:
% 'isbn': not a valid ISBN

@article{Seely1984locally,
  title = {Locally cartesian closed categories and type theory},
  volume = {95},
  issn = {1469-8064},
  url = {http://journals.cambridge.org/article_S0305004100061284},
  doi = {10.1017/S0305004100061284},
  abstract = {It is well known that for much of the mathematics of topos theory, it is in fact sufficient to use a category C whose slice categories C/A are cartesian closed. In such a category, the notion of a ‘generalized set’, for example an ‘A-indexed set’, is represented by a morphism B → A of C, i.e. by an object of C/A. The point about such a category C is that C is a C-indexed category, and more, is a hyper-doctrine, so that it has a full first order logic associated with it. This logic has some peculiar aspects. For instance, the types are the objects of C and the terms are the morphisms of C. For a given type A, the predicates with a free variable of type A are morphisms into A, and ‘proofs’ are morphisms over A. We see here a certain ‘ambiguity’ between the notions of type, predicate, and term, of object and proof: a term of type A is a morphism into A, which is a predicate over A; a morphism 1 → A can be viewed either as an object of type A or as a proof of the proposition A.},
  number = {01},
  journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
  urldate = {2015-05-29},
  date = {1984-01},
  pages = {33--48},
  author = {Seely, R. a. G.},
  file = {/Users/pgiarrusso/Zotero/storage/RMVS4WAC/displayAbstract.html}
}

@incollection{Huet1986cartesian,
  langid = {english},
  title = {Cartesian closed categories and lambda-calculus},
  isbn = {978-3-540-17184-3 978-3-540-47253-7},
  url = {http://link.springer.com/chapter/10.1007/3-540-17184-3_43},
  number = {242},
  booktitle = {Combinators and Functional Programming Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-05-29},
  date = {1986},
  pages = {123-135},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Computation by Abstract Devices},
  author = {Huet, Gérard},
  editor = {Cousineau, Guy and Curien, Pierre-Louis and Robinet, Bernard},
  file = {/Users/pgiarrusso/Zotero/storage/K3JTAP4U/10.html}
}
% == BibLateX quality report for Huet1986cartesian:
% 'isbn': not a valid ISBN

@inproceedings{Ulrich2015flatter,
  location = {{New York, NY, USA}},
  title = {The Flatter, the Better: Query Compilation Based on the Flattening Transformation},
  isbn = {978-1-4503-2758-9},
  url = {http://doi.acm.org/10.1145/2723372.2735359},
  doi = {10.1145/2723372.2735359},
  shorttitle = {The Flatter, the Better},
  abstract = {We demonstrate the insides and outs of a query compiler based on the flattening transformation, a translation technique designed by the programming language community to derive efficient data-parallel implementations from iterative programs. Flattening admits the straightforward formulation of intricate query logic including deeply nested loops over (possibly ordered) data or the construction of rich data structures. To demonstrate the level of expressiveness that can be achieved, we will bring a compiler frontend that accepts queries embedded into the Haskell programming language. Compilation via flattening takes places in a series of simple steps all of which will be made tangible by the demonstration. The final output is a program of lifted primitive operations which existing query engines can efficiently implement. We provide backends based on PostgreSQL and VectorWise to make this point however, most set-oriented or data-parallel engines could benefit from a flattening-based query compiler.},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '15},
  publisher = {{ACM}},
  urldate = {2015-06-03},
  date = {2015},
  pages = {1421--1426},
  keywords = {flattening,list comprehensions,nested data parallelism},
  author = {Ulrich, Alexander and Grust, Torsten},
  file = {/Users/pgiarrusso/Zotero/storage/RFHXBI7F/Ulrich_Grust - 2015 - The Flatter, the Better - Query Compilation Based on the Flattening Transformation.pdf}
}

@inproceedings{Ager2003functional,
  location = {{New York, NY, USA}},
  title = {A Functional Correspondence Between Evaluators and Abstract Machines},
  isbn = {1-58113-705-2},
  url = {http://doi.acm.org/10.1145/888251.888254},
  doi = {10.1145/888251.888254},
  abstract = {We bridge the gap between functional evaluators and abstract machines for the λ-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization.We illustrate this approach by deriving Krivine's abstract machine from an ordinary call-by-name evaluator and by deriving an ordinary call-by-value evaluator from Felleisen et al.'s CEK machine. The first derivation is strikingly simpler than what can be found in the literature. The second one is new. Together, they show that Krivine's abstract machine and the CEK machine correspond to the call-by-name and call-by-value facets of an ordinary evaluator for the λ-calculus.We then reveal the denotational content of Hannan and Miller's CLS machine and of Landin's SECD machine. We formally compare the corresponding evaluators and we illustrate some degrees of freedom in the design spaces of evaluators and of abstract machines for the λ-calculus with computational effects.Finally, we consider the Categorical Abstract Machine and the extent to which it is more of a virtual machine than an abstract machine.},
  booktitle = {Proceedings of the 5th ACM SIGPLAN International Conference on Principles and Practice of Declaritive Programming},
  series = {PPDP '03},
  publisher = {{ACM}},
  urldate = {2015-06-04},
  date = {2003},
  pages = {8--19},
  keywords = {abstract machines,closure conversion,defunctionalization,interpreters,transformation into continuation-passing style (CPS)},
  author = {Ager, Mads Sig and Biernacki, Dariusz and Danvy, Olivier and Midtgaard, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/ZDKJDTCF/Ager et al - 2003 - A Functional Correspondence Between Evaluators and Abstract Machines.pdf}
}

@article{Curien2014revisiting,
  title = {Revisiting the categorical interpretation of dependent type theory},
  volume = {546},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397514001789},
  doi = {10.1016/j.tcs.2014.03.003},
  abstract = {We show that Hofmann's and Curien's interpretations of Martin-Löf's type theory, which were both designed to cure a mismatch between syntax and semantics in Seely's original interpretation in locally cartesian closed categories, are related via a natural isomorphism. As an outcome, we obtain a new proof of the coherence theorem needed to show the soundness after all of Seely's interpretation.},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Models of Interaction: Essays in Honour of Glynn Winskel},
  urldate = {2015-06-05},
  date = {2014-08-21},
  pages = {99-119},
  keywords = {type theory,dependent types,categorical semantics,Coherence,Grothendieck fibrations,Locally cartesian closed categories,Monoidal categories},
  author = {Curien, Pierre-Louis and Garner, Richard and Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/IADDTT67/Curien et al - 2014 - Revisiting the categorical interpretation of dependent type theory.pdf;/Users/pgiarrusso/Zotero/storage/EXZZK9T4/S0304397514001789.html}
}

@article{Harper2007mechanizing,
  title = {Mechanizing metatheory in a logical framework},
  volume = {17},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796807006430},
  doi = {10.1017/S0956796807006430},
  abstract = {The LF logical framework codifies a methodology for representing deductive systems, such as programming languages and logics, within a dependently typed λ-calculus. In this methodology, the syntactic and deductive apparatus of a system is encoded as the canonical forms of associated LF types; an encoding is correct (adequate) if and only if it defines a compositional bijection between the apparatus of the deductive system and the associated canonical forms. Given an adequate encoding, one may establish metatheoretic properties of a deductive system by reasoning about the associated LF representation. The Twelf implementation of the LF logical framework is a convenient and powerful tool for putting this methodology into practice. Twelf supports both the representation of a deductive system and the mechanical verification of proofs of metatheorems about it. The purpose of this article is to provide an up-to-date overview of the LF λ-calculus, the LF methodology for adequate representation, and the Twelf methodology for mechanizing metatheory. We begin by defining a variant of the original LF language, called Canonical LF, in which only canonical forms (long βη-normal forms) are permitted. This variant is parameterized by a subordination relation, which enables modular reasoning about LF representations. We then give an adequate representation of a simply typed λ-calculus in Canonical LF, both to illustrate adequacy and to serve as an object of analysis. Using this representation, we formalize and verify the proofs of some metatheoretic results, including preservation, determinacy, and strengthening. Each example illustrates a significant aspect of using LF and Twelf for formalized metatheory.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-06-10},
  date = {2007},
  pages = {613--673},
  author = {Harper, Robert and Licata, Daniel R.},
  file = {/Users/pgiarrusso/Zotero/storage/KF77ZU78/Harper_Licata - 2007 - Mechanizing metatheory in a logical framework.pdf;/Users/pgiarrusso/Zotero/storage/SSI6B4SP/displayAbstract.html}
}

@inproceedings{Kiselyov2009embedded,
  location = {{Berlin, Heidelberg}},
  title = {Embedded Probabilistic Programming},
  isbn = {978-3-642-03033-8},
  url = {http://dx.doi.org/10.1007/978-3-642-03034-5_17},
  doi = {10.1007/978-3-642-03034-5_17},
  abstract = {Two general techniques for implementing a domain-specific language (DSL) with less overhead are the $<$em$>$finally-tagless$<$/em$>$ embedding of object programs and the $<$em$>$direct-style$<$/em$>$ representation of side effects. We use these techniques to build a DSL for $<$em$>$probabilistic programming$<$/em$>$ , for expressing countable probabilistic models and performing exact inference and importance sampling on them. Our language is embedded as an ordinary OCaml library and represents probability distributions as ordinary OCaml programs. We use delimited continuations to reify probabilistic programs as lazy search trees, which inference algorithms may traverse without imposing any interpretive overhead on deterministic parts of a model. We thus take advantage of the existing OCaml implementation to achieve competitive performance and ease of use. Inference algorithms can easily be embedded in probabilistic programs themselves.},
  booktitle = {Proceedings of the IFIP TC 2 Working Conference on Domain-Specific Languages},
  series = {DSL '09},
  publisher = {{Springer-Verlag}},
  urldate = {2015-06-16},
  date = {2009},
  pages = {360--384},
  author = {Kiselyov, Oleg and Shan, Chung-Chieh},
  file = {/Users/pgiarrusso/Zotero/storage/VTXJRHJ5/Kiselyov_Shan - 2009 - Embedded Probabilistic Programming.pdf}
}
% == BibLateX quality report for Kiselyov2009embedded:
% 'isbn': not a valid ISBN

@inproceedings{Jonnalagedda2015foldbased,
  location = {{New York, NY, USA}},
  title = {Fold-based Fusion As a Library: A Generative Programming Pearl},
  isbn = {978-1-4503-3626-0},
  url = {http://doi.acm.org/10.1145/2774975.2774981},
  doi = {10.1145/2774975.2774981},
  shorttitle = {Fold-based Fusion As a Library},
  abstract = {Fusion is a program optimisation technique commonly implemented using special-purpose compiler support. In this paper, we present an alternative approach, implementing fold-based fusion as a standalone library. We use staging to compose operations on folds; the operations are partially evaluated away, yielding code that does not construct unnecessary intermediate data structures. The technique extends to partitioning and grouping of collections.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Symposium on Scala},
  series = {SCALA 2015},
  publisher = {{ACM}},
  urldate = {2015-06-16},
  date = {2015},
  pages = {41--50},
  keywords = {deforestation,multi-stage programming,fold,fusion,Program optimisation},
  author = {Jonnalagedda, Manohar and Stucki, Sandro},
  file = {/Users/pgiarrusso/Zotero/storage/I389PABX/Jonnalagedda_Stucki - 2015 - Fold-based Fusion As a Library - A Generative Programming Pearl.pdf}
}

@inproceedings{Fischer2009purely,
  location = {{New York, NY, USA}},
  title = {Purely Functional Lazy Non-deterministic Programming},
  isbn = {978-1-60558-332-7},
  url = {http://doi.acm.org/10.1145/1596550.1596556},
  doi = {10.1145/1596550.1596556},
  abstract = {Functional logic programming and probabilistic programming have demonstrated the broad benefits of combining laziness (non-strict evaluation with sharing of the results) with non-determinism. Yet these benefits are seldom enjoyed in functional programming, because the existing features for non-strictness, sharing, and non-determinism in functional languages are tricky to combine. We present a practical way to write purely functional lazy non-deterministic programs that are efficient and perspicuous. We achieve this goal by embedding the programs into existing languages (such as Haskell, SML, and OCaml) with high-quality implementations, by making choices lazily and representing data with non-deterministic components, by working with custom monadic data types and search strategies, and by providing equational laws for the programmer to reason about their code.},
  booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '09},
  publisher = {{ACM}},
  urldate = {2015-06-16},
  date = {2009},
  pages = {11--22},
  keywords = {Monads,continuations,call-time choice,side effects},
  author = {Fischer, Sebastian and Kiselyov, Oleg and Shan, Chung-chieh},
  file = {/Users/pgiarrusso/Zotero/storage/8K8RTRRN/Fischer et al - 2009 - Purely Functional Lazy Non-deterministic Programming.pdf}
}

@article{Liang2015satbased,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05198},
  primaryClass = {cs},
  title = {SAT-based Analysis of Large Real-world Feature Models is Easy},
  url = {http://arxiv.org/abs/1506.05198},
  abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known non-backtracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions.},
  urldate = {2015-06-19},
  date = {2015-06-17},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  author = {Liang, Jia Hui and Ganesh, Vijay and Raman, Venkatesh and Czarnecki, Krzysztof},
  file = {/Users/pgiarrusso/Zotero/storage/ADCRU8BD/Liang et al - 2015 - SAT-based Analysis of Large Real-world Feature Models is Easy.pdf;/Users/pgiarrusso/Zotero/storage/WJC4H5UK/1506.html}
}
% == BibLateX quality report for Liang2015satbased:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Im2013contractive,
  langid = {english},
  title = {Contractive Signatures with Recursive Types, Type Parameters, and Abstract Types},
  isbn = {978-3-642-39211-5 978-3-642-39212-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-39212-2_28},
  abstract = {Although theories of equivalence or subtyping for recursive types have been extensively investigated, sophisticated interaction between recursive types and abstract types has gained little attention. The key idea behind type theories for recursive types is to use syntactic contractiveness, meaning every μ-bound variable occurs only under a type constructor such as → or ∗. This syntactic contractiveness guarantees the existence of the unique solution of recursive equations and thus has been considered necessary for designing a sound theory for recursive types. However, in an advanced type system, such as OCaml, with recursive types, type parameters, and abstract types, we cannot easily define the syntactic contractiveness of types. In this paper, we investigate a sound type system for recursive types, type parameters, and abstract types. In particular, we develop a new semantic notion of contractiveness for types and signatures using mixed induction and coinduction, and show that our type system is sound with respect to the standard call-by-value operational semantics, which eliminates signature sealings. Moreover we show that while non-contractive types in signatures lead to unsoundness of the type system, they may be allowed in modules. We have also formalized the whole system and its type soundness proof in Coq.},
  number = {7966},
  booktitle = {Automata, Languages, and Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-06-20},
  date = {2013},
  pages = {299-311},
  keywords = {Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Computer Communication Networks,Information Systems Applications (incl. Internet),Discrete Mathematics in Computer Science,Information Storage and Retrieval},
  author = {Im, Hyeonseung and Nakata, Keiko and Park, Sungwoo},
  editor = {Fomin, Fedor V. and Freivalds, Rūsiņš and Kwiatkowska, Marta and Peleg, David},
  file = {/Users/pgiarrusso/Zotero/storage/9WJB2699/Im et al - 2013 - Contractive Signatures with Recursive Types, Type Parameters, and Abstract Types.pdf;/Users/pgiarrusso/Zotero/storage/Z2SZMUZE/978-3-642-39212-2_28.html}
}
% == BibLateX quality report for Im2013contractive:
% 'isbn': not a valid ISBN

@article{Hedges2014monad,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2058},
  title = {Monad Transformers for Backtracking Search},
  volume = {153},
  issn = {2075-2180},
  url = {http://arxiv.org/abs/1406.2058},
  doi = {10.4204/EPTCS.153.3},
  abstract = {This paper extends Escardo and Oliva's selection monad to the selection monad transformer, a general monadic framework for expressing backtracking search algorithms in Haskell. The use of the closely related continuation monad transformer for similar purposes is also discussed, including an implementation of a DPLL-like SAT solver with no explicit recursion. Continuing a line of work exploring connections between selection functions and game theory, we use the selection monad transformer with the nondeterminism monad to obtain an intuitive notion of backward induction for a certain class of nondeterministic games.},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  urldate = {2015-06-20},
  date = {2014-06-05},
  pages = {31-50},
  keywords = {Computer Science - Programming Languages},
  author = {Hedges, Jules},
  file = {/Users/pgiarrusso/Zotero/storage/B6HBIGXJ/Hedges - 2014 - Monad Transformers for Backtracking Search.pdf;/Users/pgiarrusso/Zotero/storage/CK5WPI8P/1406.html}
}
% == BibLateX quality report for Hedges2014monad:
% Unexpected field 'archivePrefix'

@incollection{Meyer1985continuation,
  langid = {english},
  title = {Continuation semantics in typed lambda-calculi},
  isbn = {978-3-540-15648-2 978-3-540-39527-0},
  url = {http://link.springer.com/chapter/10.1007/3-540-15648-8_17},
  abstract = {This paper reports preliminary work on the semantics of the continuation transform. Previous work on the semantics of continuations has concentrated on untyped lambda-calculi and has used primarily the mechanism of inclusive predicates. Such predicates are easy to understand on atomic values, but they become obscure on functional values. In the case of the typed lambda-calculus, we show that such predicates can be replaced by retractions. The main theorem states that the meaning of a closed term is a retraction of the meaning of the corresponding continuationized term.},
  number = {193},
  booktitle = {Logics of Programs},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-06-22},
  date = {1985},
  pages = {219-224},
  keywords = {Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Meyer, Albert R. and Wand, Mitchell},
  editor = {Parikh, Rohit},
  file = {/Users/pgiarrusso/Zotero/storage/CAH6BFB2/Meyer_Wand - 1985 - Continuation semantics in typed lambda-calculi.pdf;/Users/pgiarrusso/Zotero/storage/XAPBNSFK/3-540-15648-8_17.html}
}
% == BibLateX quality report for Meyer1985continuation:
% 'isbn': not a valid ISBN

@incollection{Cicek2015refinement,
  langid = {english},
  title = {Refinement Types for Incremental Computational Complexity},
  isbn = {978-3-662-46668-1 978-3-662-46669-8},
  url = {http://link.springer.com/chapter/10.1007/978-3-662-46669-8_17},
  abstract = {With recent advances, programs can be compiled to efficiently respond to incremental input changes. However, there is no language-level support for reasoning about the time complexity of incremental updates. Motivated by this gap, we present CostIt, a higher-order functional language with a lightweight refinement type system for proving asymptotic bounds on incremental computation time. Type refinements specify which parts of inputs and outputs may change, as well as dynamic stability, a measure of time required to propagate changes to a program’s execution trace, given modified inputs. We prove our type system sound using a new step-indexed cost semantics for change propagation and demonstrate the precision and generality of our technique through examples.},
  number = {9032},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-06-22},
  date = {2015-04-11},
  pages = {406-431},
  keywords = {Artificial Intelligence (incl. Robotics),Algorithm Analysis and Problem Complexity,Information Systems Applications (incl. Internet),Information Storage and Retrieval,Data Mining and Knowledge Discovery,Database Management},
  author = {Çiçek, Ezgi and Garg, Deepak and Acar, Umut},
  editor = {Vitek, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/X2WBIVV9/Çiçek et al - 2015 - Refinement Types for Incremental Computational Complexity.pdf;/Users/pgiarrusso/Zotero/storage/987AR7G9/978-3-662-46669-8_17.html}
}
% == BibLateX quality report for Cicek2015refinement:
% 'isbn': not a valid ISBN

@inproceedings{Filinski2010monads,
  location = {{New York, NY, USA}},
  title = {Monads in Action},
  isbn = {978-1-60558-479-9},
  url = {http://doi.acm.org/10.1145/1706299.1706354},
  doi = {10.1145/1706299.1706354},
  abstract = {In functional programming, monadic characterizations of computational effects are normally understood denotationally: they describe how an effectful program can be systematically expanded or translated into a larger, pure program, which can then be evaluated according to an effect-free semantics. Any effect-specific operations expressible in the monad are also given purely functional definitions, but these definitions are only directly executable in the context of an already translated program. This approach thus takes an inherently Church-style view of effects: the nominal meaning of every effectful term in the program depends crucially on its type. We present here a complementary, operational view of monadic effects, in which an effect definition directly induces an imperative behavior of the new operations expressible in the monad. This behavior is formalized as additional operational rules for only the new constructs; it does not require any structural changes to the evaluation judgment. Specifically, we give a small-step operational semantics of a prototypical functional language supporting programmer-definable, layered effects, and show how this semantics naturally supports reasoning by familiar syntactic techniques, such as showing soundness of a Curry-style effect-type system by the progress+preservation method.},
  booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '10},
  publisher = {{ACM}},
  urldate = {2015-06-22},
  date = {2010},
  pages = {483--494},
  keywords = {Monads,computational effects,modular semantics},
  author = {Filinski, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/VUZ5P29Q/Filinski - 2010 - Monads in Action.pdf}
}

@inproceedings{Filinski1999representing,
  location = {{New York, NY, USA}},
  title = {Representing Layered Monads},
  isbn = {1-58113-095-3},
  url = {http://doi.acm.org/10.1145/292540.292557},
  doi = {10.1145/292540.292557},
  abstract = {There has already been considerable research on constructing modular, monad-based specifications of computational effects (state, exceptions, nondeterminism, etc.) in programming languages. We present a simple framework in this tradition, based on a Church-style effect-typing system for an ML-like language. The semantics of this language is formally defined by a series of monadic translations, each one expanding away a layer of effects. Such a layered specification is easy to reason about, but its direct implementation (whether by parameterized interpretation or by actual translation) is often prohibitively inefficient.By exploiting deeper semantic properties of monads, however, it is also possible to derive a vastly more efficient implementation: we show that each layer of effects can be uniformly simulated by continuation-passing, and further that multiple such layers can themselves be simulated by a standard semantics for call/cc and mutable state. Thus, even multi-effect programs can be executed in Scheme or SML/NJ at full native speed, generalizing an earlier single-effect result. As an example, we show how a simple resumption-based semantics of concurrency allows us to directly simulate a shared-state program across all possible dynamic interleavings of execution threads.},
  booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '99},
  publisher = {{ACM}},
  urldate = {2015-06-23},
  date = {1999},
  pages = {175--188},
  author = {Filinski, Andrzej}
}

@article{Reynolds1974relation,
  title = {On the Relation Between Direct and Continuation Semantics},
  url = {http://repository.cmu.edu/compsci/1288},
  journaltitle = {Computer Science Department},
  date = {1974-01-01},
  author = {Reynolds, John},
  file = {/Users/pgiarrusso/Zotero/storage/J5JKUDP7/1288.html}
}

@article{Crary2007syntactic,
  title = {Syntactic Logical Relations for Polymorphic and Recursive Types},
  volume = {172},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066107000825},
  doi = {10.1016/j.entcs.2007.02.010},
  abstract = {The method of logical relations assigns a relational interpretation to types that expresses operational invariants satisfied by all terms of a type. The method is widely used in the study of typed languages, for example to establish contextual equivalences of terms. The chief difficulty in using logical relations is to establish the existence of a suitable relational interpretation. We extend work of Pitts and Birkedal and Harper on constructing relational interpretations of types to polymorphism and recursive types, and apply it to establish parametricity and representation independence properties in a purely operational setting. We argue that, once the existence of a relational interpretation has been established, it is straightforward to use it to establish properties of interest.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
  urldate = {2015-06-23},
  date = {2007-04-01},
  pages = {259-299},
  keywords = {polymorphism,Type structure,operational semantics,data abstraction,lambda calculus and related systems,logics of programs,Operational semantics,type structure},
  author = {Crary, Karl and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/56QLZKX2/Crary-Harper - 2007 - Syntactic Logical Relations for Polymorphic and Recursive Types.pdf;/Users/pgiarrusso/Zotero/storage/8XFY8ASG/S1571066107000825.html;/Users/pgiarrusso/Zotero/storage/IZ47H2DC/S1571066107000825.html}
}

@inproceedings{PeytonJones1993imperative,
  location = {{New York, NY, USA}},
  title = {Imperative Functional Programming},
  isbn = {0-89791-560-7},
  url = {http://doi.acm.org/10.1145/158511.158524},
  doi = {10.1145/158511.158524},
  abstract = {We present a new model, based on monads, for performing input/output in a non-strict, purely functional language. It is composable, extensible, efficient, requires no extensions to the type system, and extends smoothly to incorporate mixed-language working and in-place array updates.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '93},
  publisher = {{ACM}},
  urldate = {2015-06-23},
  date = {1993},
  pages = {71--84},
  author = {Peyton Jones, Simon L. and Wadler, Philip}
}

@article{Filinski2007relations,
  title = {On the relations between monadic semantics},
  volume = {375},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397506009169},
  doi = {10.1016/j.tcs.2006.12.027},
  abstract = {We present a simple computational metalanguage with general recursive types and multiple notions of effects, through which a variety of concrete denotational semantics can be conveniently factored, by suitably interpreting the effects as monads. We then propose a methodology for relating two such interpretations of the metalanguage, with the aim of showing that the semantics they induce agree for complete programs. As a prototypical instance of such a relation, we use the framework to show agreement between a direct and a continuation semantics of the simple, untyped functional language from Reynolds’s original paper on the subject.},
  number = {1–3},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Festschrift for John C. Reynolds’s 70th birthday},
  urldate = {2015-06-23},
  date = {2007-05-01},
  pages = {41-75},
  keywords = {recursive types,Monads,continuations,logical relations,Computational metalanguage},
  author = {Filinski, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/H3W94N2J/S0304397506009169.html}
}

@inproceedings{Mycroft1984polymorphic,
  location = {{London, UK, UK}},
  title = {Polymorphic Type Schemes and Recursive Definitions},
  isbn = {3-540-12925-1},
  url = {http://dl.acm.org/citation.cfm?id=647326.721798},
  booktitle = {Proceedings of the 6th Colloquium on International Symposium on Programming},
  publisher = {{Springer-Verlag}},
  urldate = {2015-06-27},
  date = {1984},
  pages = {217--228},
  author = {Mycroft, Alan}
}
% == BibLateX quality report for Mycroft1984polymorphic:
% 'isbn': not a valid ISBN

@incollection{Setzer2014unnesting,
  langid = {english},
  title = {Unnesting of Copatterns},
  isbn = {978-3-319-08917-1 978-3-319-08918-8},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-08918-8_3},
  abstract = {Inductive data such as finite lists and trees can elegantly be defined by constructors which allow programmers to analyze and manipulate finite data via pattern matching. Dually, coinductive data such as streams can be defined by observations such as head and tail and programmers can synthesize infinite data via copattern matching. This leads to a symmetric language where finite and infinite data can be nested. In this paper, we compile nested pattern and copattern matching into a core language which only supports simple non-nested (co)pattern matching. This core language may serve as an intermediate language of a compiler. We show that this translation is conservative, i.e. the multi-step reduction relation in both languages coincides for terms of the original language. Furthermore, we show that the translation preserves strong and weak normalisation: a term of the original language is strongly/weakly normalising in one language if and only if it is so in the other. In the proof we develop more general criteria which guarantee that extensions of abstract reduction systems are conservative and preserve strong or weak normalisation.},
  number = {8560},
  booktitle = {Rewriting and Typed Lambda Calculi},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2015-06-29},
  date = {2014-07-14},
  pages = {31-45},
  keywords = {Logics and Meanings of Programs,pattern matching,Mathematical Logic and Formal Languages,Mathematics of Computing,Symbolic and Algebraic Manipulation,Mathematical Logic and Foundations,Computing Methodologies,abstract reduction system,algebraic data types,ARS,coalgebras,codata,conservative extension,copattern matching,strong normalisation,weak normalisation},
  author = {Setzer, Anton and Abel, Andreas and Pientka, Brigitte and Thibodeau, David},
  editor = {Dowek, Gilles},
  file = {/Users/pgiarrusso/Zotero/storage/AW5UDC8V/Setzer et al - 2014 - Unnesting of Copatterns.pdf;/Users/pgiarrusso/Zotero/storage/SK4TMSCV/978-3-319-08918-8_3.html}
}
% == BibLateX quality report for Setzer2014unnesting:
% 'isbn': not a valid ISBN

@inproceedings{Im2011syntactic,
  location = {{New York, NY, USA}},
  title = {A Syntactic Type System for Recursive Modules},
  isbn = {978-1-4503-0940-0},
  url = {http://doi.acm.org/10.1145/2048066.2048141},
  doi = {10.1145/2048066.2048141},
  abstract = {A practical type system for ML-style recursive modules should address at least two technical challenges. First, it needs to solve the double vision problem, which refers to an inconsistency between external and internal views of recursive modules. Second, it needs to overcome the tension between practical decidability and expressivity which arises from the potential presence of cyclic type definitions caused by recursion between modules. Although type systems in previous proposals solve the double vision problem and are also decidable, they fail to typecheck common patterns of recursive modules, such as functor fixpoints, that are essential to the expressivity of the module system and the modular development of recursive modules. This paper proposes a novel type system for recursive modules that solves the double vision problem and typechecks common patterns of recursive modules including functor fixpoints. First, we design a type system with a type equivalence based on weak bisimilarity, which does not lend itself to practical implementation in general, but accommodates a broad range of cyclic type definitions. Then, we identify a practically implementable fragment using a type equivalence based on type normalization, which is expressive enough to typecheck typical uses of recursive modules. Our approach is purely syntactic and the definition of the type system is ready for use in an actual implementation.},
  booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '11},
  publisher = {{ACM}},
  urldate = {2015-06-30},
  date = {2011},
  pages = {993--1012},
  keywords = {abstract types,recursion,type systems,modules,weak bisimulations},
  author = {Im, Hyeonseung and Nakata, Keiko and Garrigue, Jacques and Park, Sungwoo},
  file = {/Users/pgiarrusso/Zotero/storage/6XWLYRKW/Im-Nakata-Garrigue-Park - 2011 - A Syntactic Type System for Recursive Modules.pdf}
}

@article{Dezani-Ciancaglini2003infinitary,
  title = {Infinitary lambda calculus and discrimination of Berarducci trees},
  volume = {298},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397502008095},
  doi = {10.1016/S0304-3975(02)00809-5},
  abstract = {We propose an extension of lambda calculus for which the Berarducci trees equality coincides with observational equivalence, when we observe rootstable or rootactive behavior of terms. In one direction the proof is an adaptation of the classical Böhm out technique. In the other direction the proof is based on confluence for strongly converging reductions in this extension.},
  number = {2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Australasian Computing Theory},
  urldate = {2015-07-01},
  date = {2003-04-08},
  pages = {275-302},
  keywords = {Berarducci trees,Böhm out technique,Infinitary lambda calculus,Observational equivalence},
  author = {Dezani-Ciancaglini, Mariangiola and Severi, Paula and de Vries, Fer-Jan},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/9IN62IW8/S0304397502008095.html}
}

@incollection{Czajka2014coinductive,
  langid = {english},
  title = {A Coinductive Confluence Proof for Infinitary Lambda-Calculus},
  isbn = {978-3-319-08917-1 978-3-319-08918-8},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-08918-8_12},
  abstract = {We give a coinductive proof of confluence, up to equivalence of root-active subterms, of infinitary lambda-calculus. We also show confluence of Böhm reduction (with respect to root-active terms) in infinitary lambda-calculus. In contrast to previous proofs, our proof makes heavy use of coinduction and does not employ the notion of descendants.},
  number = {8560},
  booktitle = {Rewriting and Typed Lambda Calculi},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2015-07-01},
  date = {2014-07-14},
  pages = {164-178},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematics of Computing,Symbolic and Algebraic Manipulation,Mathematical Logic and Foundations,Computing Methodologies},
  author = {Czajka, Łukasz},
  editor = {Dowek, Gilles},
  file = {/Users/pgiarrusso/Zotero/storage/4FHD8I6U/Czajka - 2014 - A Coinductive Confluence Proof for Infinitary Lambda-Calculus.pdf;/Users/pgiarrusso/Zotero/storage/PXAMNAE5/978-3-319-08918-8_12.html}
}
% == BibLateX quality report for Czajka2014coinductive:
% 'isbn': not a valid ISBN

@article{Gapeyev2002recursive,
  title = {Recursive subtyping revealed},
  volume = {12},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796802004318},
  doi = {10.1017/S0956796802004318},
  abstract = {Algorithms for checking subtyping between recursive types lie at the core of many programming language implementations. But the fundamental theory of these algorithms and how they relate to simpler declarative specifications is not widely understood, due in part to the difficulty of the available introductions to the area. This tutorial paper offers an ‘end-to-end’ introduction to recursive types and subtyping algorithms, from basic theory to efficient implementation, set in the unifying mathematical framework of coinduction.},
  number = {06},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-07-02},
  date = {2002},
  pages = {511--548},
  author = {Gapeyev, Vladimir and Levin, Michael Y. and Pierce, Benjamin C.},
  file = {/Users/pgiarrusso/Zotero/storage/8VPVF9RA/Gapeyev et al - 2002 - Recursive subtyping revealed.pdf;/Users/pgiarrusso/Zotero/storage/5NRUHTTG/displayAbstract.html}
}

@incollection{Brandt1997coinductive,
  langid = {english},
  title = {Coinductive axiomatization of recursive type equality and subtyping},
  isbn = {978-3-540-62688-6 978-3-540-68438-1},
  url = {http://link.springer.com/chapter/10.1007/3-540-62688-3_29},
  abstract = {We present new sound and complete axiomatizations of type equality and subtype inequality for a first-order type language with regular recursive types. The rules are motivated by coinductive characterizations of type containment and type equality via simulation and bisimulation, respectively. The main novelty of the axiomatization is the fixpoint rule (or coinduction principle), which has the form where P is either a type equality τ=τ′ or type containment τ≤τ′. We define what it means for a proof (formal derivation) to be formally contractive and show that the fixpoint rule is sound if the proof of the premise A, P ⊢ P is contractive. (A proof of A, P ⊢ P using the assumption axiom is, of course, not contractive.) The fixpoint rule thus allows us to capture a coinductive relation in the fundamentally inductive framework of inference systems. The new axiomatizations are “leaner” than previous axiomatizations, particularly so for type containment since no separate axiomatization of type equality is required, as in Amadio and Cardelli's axiomatization. They give rise to a natural operational interpretation of proofs as coercions. In particular, the fixpoint rule corresponds to definition by recursion. Finally, the axiomatization is closely related to (known) efficient algorithms for deciding type equality and type containment. These can be modified to not only decide type equality and type containment, but also construct proofs in our axiomatizations efficiently. In connection with the operational interpretation of proofs as coercions this gives efficient (O(n 2) time) algorithms for constructing efficient coercions from a type to any of its supertypes or isomorphic types.},
  number = {1210},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-07-02},
  date = {1997},
  pages = {63-81},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Brandt, Michael and Henglein, Fritz},
  editor = {de Groote, Philippe and Hindley, J. Roger},
  file = {/Users/pgiarrusso/Zotero/storage/2EDMPITT/Brandt_Henglein - 1997 - Coinductive axiomatization of recursive type equality and subtyping.pdf;/Users/pgiarrusso/Zotero/storage/AAJ25SEE/3-540-62688-3_29.html}
}
% == BibLateX quality report for Brandt1997coinductive:
% 'isbn': not a valid ISBN

@article{Brandt1998coinductive,
  title = {Coinductive Axiomatization of Recursive Type Equality and Subtyping},
  volume = {33},
  issn = {0169-2968},
  url = {http://dl.acm.org/citation.cfm?id=2379036.2379037},
  abstract = {We present new sound and complete axiomatizations of type equality and subtype inequality for a first-order type language with regular recursive types. The rules are motivated by coinductive characterizations of type containment and type equality via simulation and bisimulation, respectively. The main novelty of the axiomatization is the fixpoint rule (or coinduction principle). It states that from A,P \$\$$\backslash$vdash\$\$ P one may deduce A \$\$$\backslash$vdash\$\$ P, where P is either a type equality τ = τ' or type containment τ ≤ τ' and the proof of the premise must be contractive in a sense we define in this paper. In particular, a proof of A, P \$\$$\backslash$vdash\$\$ P using the assumption axiom is not contractive. The fixpoint rule embodies a finitary coinduction principle and thus allows us to capture a coinductive relation in the fundamentally inductive framework of inference systems. The new axiomatizations are more concise than previous axiomatizations, particularly so for type containment since no separate axiomatization of type equality is required, as in Amadio and Cardelli's axiomatization. They give rise to a natural operational interpretation of proofs as coercions. In particular, the fixpoint rule corresponds to definition by recursion. Finally, the axiomatization is closely related to (known) efficient algorithms for deciding type equality and type containment. These can be modified to not only decide type equality and type containment, but also construct proofs in our axiomatizations efficiently. In connection with the operational interpretation of proofs as coercions this gives efficient (O(n 2) time) algorithms for constructing efficient coercions from a type to any of its supertypes or isomorphic types. More generally, we show how adding the fixpoint rule makes it possible to characterize inductively a set that is coinductively defined as the kernel (greatest fixed point) of an inference system.},
  number = {4},
  journaltitle = {Fundam. Inf.},
  urldate = {2015-07-02},
  date = {1998-12},
  pages = {309--338},
  keywords = {coinduction,subtyping,axiomatization,coercion,fixpoint,inference rule,inference system,operational interpretation,recursive type,type equality},
  author = {Brandt, Michael and Henglein, Fritz},
  file = {/Users/pgiarrusso/Zotero/storage/3VM6MC4C/Brandt_Henglein - 1998 - Coinductive Axiomatization of Recursive Type Equality and Subtyping.pdf}
}
% == BibLateX quality report for Brandt1998coinductive:
% ? Possibly abbreviated journal title Fundam. Inf.

@article{Cardone1991type,
  title = {Type inference with recursive types: Syntax and semantics},
  volume = {92},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/0890540191900203},
  doi = {10.1016/0890-5401(91)90020-3},
  shorttitle = {Type inference with recursive types},
  abstract = {In this paper we study type inference systems for λ-calculus with a recursion operator over types. The main syntactical properties, notably the existence of principal type schemes, are proved to hold when recursive types are viewed as finite notations for infinite (regular) type expressions representing their infinite unfoldings. Exploiting the approximation structure of a model for the untyped language of terms, types are interpreted as limits of sequences of their approximations. We show that the interpretation is essentially unique and that two types have equal interpretation if and only if their infinite unfoldings are identical. Finally, a completeness theorem is proved to hold w.r.t. the specific model we consider for a natural (infinitary) extension of the type inference system.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2015-07-02},
  date = {1991-05},
  pages = {48-80},
  author = {Cardone, Felice and Coppo, Mario},
  file = {/Users/pgiarrusso/Zotero/storage/XE4JJUPV/0890540191900203.html}
}

@incollection{StoyanArtificialIntelligenceMathematical1991,
  location = {{San Diego, CA, USA}},
  title = {Artificial Intelligence and Mathematical Theory of Computation},
  isbn = {0-12-450010-2},
  url = {http://dl.acm.org/citation.cfm?id=132218.132242},
  publisher = {{Academic Press Professional, Inc.}},
  urldate = {2014-11-01},
  date = {1991},
  pages = {409--426},
  author = {Stoyan, Herbert},
  editor = {Lifschitz, Vladimir}
}
% == BibLateX quality report for StoyanArtificialIntelligenceMathematical1991:
% Missing required field 'booktitle'

@article{Kellsearch,
  title = {In Search of Types},
  author = {Kell, Stephen},
  file = {/Users/pgiarrusso/Zotero/storage/WD2DTXUC/Kell - In Search of Types.pdf}
}
% == BibLateX quality report for Kellsearch:
% Exactly one of 'date' / 'year' must be present
% Missing required field 'journaltitle'

@article{Boylandvariable,
  title = {Variable Arity for LF},
  url = {http://www.cs.uwm.edu/~boyland/papers/lf+variable.pdf},
  author = {Boyland, John Tang and Zhao, Tian},
  file = {/Users/pgiarrusso/Zotero/storage/2KMQCGRD/Boyland and Zhao - Variable Arity for LF.pdf}
}
% == BibLateX quality report for Boylandvariable:
% Exactly one of 'date' / 'year' must be present
% Missing required field 'journaltitle'

@article{Voelterprojecting,
  title = {Projecting a Modular Future},
  author = {Voelter, Markus and Warmer, Jos and Kolb, Bernd},
  file = {/Users/pgiarrusso/Zotero/storage/WIWFS4DZ/Voelter et al. - Projecting a Modular Future.pdf}
}
% == BibLateX quality report for Voelterprojecting:
% Exactly one of 'date' / 'year' must be present
% Missing required field 'journaltitle'

@report{Jones1993partial,
  title = {Partial evaluation for dictionary-free overloading},
  institution = {{Research Report YALEU/DCS/RR-959, Yale University, New Haven, Connecticut, USA}},
  date = {1993},
  author = {Jones, Mark P},
  file = {/Users/pgiarrusso/Zotero/storage/B24PIKMV/Jones - 1993 - Partial evaluation for dictionary-free overloading.pdf}
}
% == BibLateX quality report for Jones1993partial:
% Missing required field 'type'

@video{rattodiEuropa,
  title = {Il ratto di Europa by Scuola Superiore di Catania},
  url = {http://new.livestream.com/accounts/5791652/events/2996456},
  abstract = {Watch Scuola Superiore di Catania's Il ratto di Europa on Livestream.com. Il seminario, tenuto da Andrea Scavo, ex allievo SSC, è organizzato dall'associazione Alumni della Scuola Superiore di Catania.},
  urldate = {2014-05-19},
  file = {/Users/pgiarrusso/Zotero/storage/R99JZGHB/2996456.html}
}
% == BibLateX quality report for rattodiEuropa:
% Unexpected field 'title'

@video{courseraBriefHistoryHumankind2013,
  title = {A Brief History of Humankind with Dr. Yuval Noah Harari},
  url = {http://www.youtube.com/watch?v=xQ4eg7Zj5OY&feature=youtube_gdata_player},
  abstract = {The course A Brief History of Humankind by Dr. Yuval Noah Harari from Hebrew University of Jerusalem will be offered free of charge to everyone on the Coursera platform. Sign up at http://www.coursera.org/course/humankind.},
  urldate = {2014-04-28},
  date = {2013-01-25},
  editora = {{coursera}},
  editoratype = {collaborator}
}
% == BibLateX quality report for courseraBriefHistoryHumankind2013:
% Unexpected field 'title'
% Unexpected field 'editora'
% Unexpected field 'editoratype'

@online{CenterforHistoryandNewMediaZoteroQuickStart,
  title = {Zotero Quick Start Guide},
  url = {http://zotero.org/support/quick_start_guide},
  author = {{Center for History and New Media}}
}
% == BibLateX quality report for CenterforHistoryandNewMediaZoteroQuickStart:
% Exactly one of 'date' / 'year' must be present

@article{Poll2000coalgebraic,
  title = {A Coalgebraic Semantics of Subtyping},
  volume = {33},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066105803524},
  doi = {10.1016/S1571-0661(05)80352-4},
  abstract = {Subtyping is a central notion in object-oriented programming. In this paper we investigate how the coalgebraic semantics of objects accounts for subtyping. We show that different characterisations of so-called behavioural subtyping found in the literature can conveniently be expressed in coalgebraic terms. We define subtyping between coalgebras and subtyping between coalgebraic specifications, and show that the latter is sound and complete w.r.t. the former. We also illustrate the subtle difference between the notions of subtyping and refinement.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {CMCS'2000, Coalgebraic Methods in Computer Science},
  urldate = {2014-08-06},
  date = {2000},
  pages = {276-293},
  author = {Poll, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/2FNTGXSD/Poll - 2000 - A Coalgebraic Semantics of Subtyping.pdf;/Users/pgiarrusso/Zotero/storage/68AHH7F2/S1571066105803524.html}
}

@inproceedings{Oliveira2008visitor,
  location = {{New York, NY, USA}},
  title = {The Visitor Pattern As a Reusable, Generic, Type-safe Component},
  isbn = {978-1-60558-215-3},
  url = {http://doi.acm.org/10.1145/1449764.1449799},
  doi = {10.1145/1449764.1449799},
  abstract = {The VISITOR design pattern shows how to separate the structure of an object hierarchy from the behaviour of traversals over that hierarchy. The pattern is very flexible; this very flexibility makes it difficult to capture the pattern as anything more formal than prose, pictures and prototypes. We show how to capture the essence of the VISITOR pattern as a reusable software library, by using advanced type system features appearing in modern object-oriented languages such as Scala. We preserve type-safety statically and modularly: no reflection or similar mechanisms are used and modules can be independently compiled. The library is generic, in two senses: not only is it parametrised by both the return type and the shape of the object hierarchy, but also it allows a number of implementation choices (internal versus external control, imperative versus functional behaviour, orthogonal aspects such as tracing and memoisation) to be specified by parameters rather than fixed in early design decisions. Finally, we propose a generalised datatype-like notation,on top of our visitor library: this provides a convenient functional decomposition style in object-oriented languages.},
  booktitle = {Proceedings of the 23rd ACM SIGPLAN Conference on Object-oriented Programming Systems Languages and Applications},
  series = {OOPSLA '08},
  publisher = {{ACM}},
  urldate = {2014-08-06},
  date = {2008},
  pages = {439--456},
  keywords = {visitor pattern,traversal,algebraic datatypes,design patterns,program extensibility,software components},
  author = {Oliveira, Bruno C.d.S. and Wang, Meng and Gibbons, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/85IGM4RT/Oliveira et al. - 2008 - The Visitor Pattern As a Reusable, Generic, Type-s.pdf;/Users/pgiarrusso/Zotero/storage/JKFGDADK/Oliveira et al. - 2008 - The Visitor Pattern As a Reusable, Generic, Type-s.pdf}
}

@inproceedings{Qi2009sharing,
  location = {{New York, NY, USA}},
  title = {Sharing Classes Between Families},
  isbn = {978-1-60558-392-1},
  url = {http://doi.acm.org/10.1145/1542476.1542508},
  doi = {10.1145/1542476.1542508},
  abstract = {Class sharing is a new language mechanism for building extensible software systems. Recent work has separately explored two different kinds of extensibility: first, family inheritance, in which an entire family of related classes can be inherited, and second, adaptation, in which existing objects are extended in place with new behavior and state. Class sharing integrates these two kinds of extensibility mechanisms. With little programmer effort, objects of one family can be used as members of another, while preserving relationships among objects. Therefore, a family of classes can be adapted in place with new functionality spanning multiple classes. Object graphs can evolve from one family to another, adding or removing functionality even at run time. Several new mechanisms support this flexibility while ensuring type safety. Class sharing has been implemented as an extension to Java, and its utility for evolving and extending software is demonstrated with realistic systems.},
  booktitle = {Proceedings of the 2009 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '09},
  publisher = {{ACM}},
  urldate = {2014-08-06},
  date = {2009},
  pages = {281--292},
  keywords = {family inheritance,masked types,views},
  author = {Qi, Xin and Myers, Andrew C.},
  file = {/Users/pgiarrusso/Zotero/storage/R5IN3VBT/Qi and Myers - 2009 - Sharing Classes Between Families.pdf}
}

@inproceedings{Bastin2014scaladyno,
  location = {{New York, NY, USA}},
  title = {ScalaDyno: Making Name Resolution and Type Checking Fault-tolerant},
  isbn = {978-1-4503-2868-5},
  url = {http://doi.acm.org/10.1145/2637647.2637649},
  doi = {10.1145/2637647.2637649},
  shorttitle = {ScalaDyno},
  abstract = {The ScalaDyno compiler plugin allows fast prototyping with the Scala programming language, in a way that combines the benefits of both statically and dynamically typed languages. Static name resolution and type checking prevent partially-correct code from being compiled and executed. Yet, allowing programmers to test critical paths in a program without worrying about the consistency of the entire code base is crucial to fast prototyping and agile development. This is where ScalaDyno comes in: it allows partially-correct programs to be compiled and executed, while shifting compile-time errors to program runtime. The key insight in ScalaDyno is that name and type errors affect limited areas of the code, which can be replaced by instructions to output the respective errors at runtime. This allows byte code generation and execution for partially correct programs, thus allowing Python or JavaScript-like fast prototyping in Scala. This is all done without sacrificing name resolution, full type checking and optimizations for the correct parts of the code -- they are still performed, but without getting in the way of agile development. Finally, for release code or sensitive refactoring, runtime errors can be disabled, thus allowing full static name resolution and type checking typical of the Scala compiler.},
  booktitle = {Proceedings of the Fifth Annual Scala Workshop},
  series = {SCALA '14},
  publisher = {{ACM}},
  urldate = {2014-08-06},
  date = {2014},
  pages = {1--5},
  keywords = {dynamic typing,Scala,deferred type errors},
  author = {Bastin, Cédric and Ureche, Vlad and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/WFPFG9IG/Bastin et al - 2014 - ScalaDyno - Making Name Resolution and Type Checking Fault-tolerant.pdf}
}

@inproceedings{Lindley2012embedding,
  location = {{New York, NY, USA}},
  title = {Embedding F},
  isbn = {978-1-4503-1576-0},
  url = {http://doi.acm.org/10.1145/2364394.2364402},
  doi = {10.1145/2364394.2364402},
  abstract = {This millennium has seen a great deal of research into embedded domain-specific languages. Primarily, such languages are simply-typed. Focusing on System F, we demonstrate how to embed polymorphic domain specific languages in Haskell and OCaml. We exploit recent language extensions including kind polymorphism and first-class modules.},
  booktitle = {Proceedings of the 8th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '12},
  publisher = {{ACM}},
  urldate = {2014-08-07},
  date = {2012},
  pages = {45--56},
  keywords = {Haskell,polymorphism,higher-order abstract syntax,domain specific languages,ocaml},
  author = {Lindley, Sam},
  file = {/Users/pgiarrusso/Zotero/storage/FHFU4KJK/Lindley - 2012 - Embedding F.pdf}
}

@inproceedings{McBride2010outrageous,
  location = {{New York, NY, USA}},
  title = {Outrageous but Meaningful Coincidences: Dependent Type-safe Syntax and Evaluation},
  isbn = {978-1-4503-0251-7},
  url = {http://doi.acm.org/10.1145/1863495.1863497},
  doi = {10.1145/1863495.1863497},
  shorttitle = {Outrageous but Meaningful Coincidences},
  abstract = {Tagless interpreters for well-typed terms in some object language are a standard example of the power and benefit of precise indexing in types, whether with dependent types, or generalized algebraic datatypes. The key is to reflect object language types as indices (however they may be constituted) for the term datatype in the host language, so that host type coincidence ensures object type coincidence. Whilst this technique is widespread for simply typed object languages, dependent types have proved a tougher nut with nontrivial computation in type equality. In their type-safe representations, Danielsson [2006] and Chapman [2009] succeed in capturing the equality rules, but at the cost of representing equality derivations explicitly within terms. This article constructs a type-safe representation for a dependently typed object language, dubbed KIPLING, whose computational type equality just appropriates that of its host, Agda. The KIPLING interpreter example is not merely de rigeur - it is key to the construction. At the heart of the technique is that key component of generic programming, the universe.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '10},
  publisher = {{ACM}},
  urldate = {2014-08-08},
  date = {2010},
  pages = {1--12},
  keywords = {generic programming,dependent types},
  author = {McBride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/J5I5KU9E/McBride - 2010 - Outrageous but Meaningful Coincidences - Dependent Type-safe Syntax and Evaluation.pdf}
}

@article{Reynolds2000meaning,
  title = {The Meaning of Types From Intrinsic to Extrinsic Semantics},
  url = {http://repository.cmu.edu/compsci/1291},
  journaltitle = {BRICS Report Series},
  date = {2000-12-01},
  author = {Reynolds, John},
  file = {/Users/pgiarrusso/Zotero/storage/S2T4FXGP/Reynolds - 2000 - The Meaning of Types From Intrinsic to Extrinsic Semantics.pdf;/Users/pgiarrusso/Zotero/storage/SU5RRHWS/1291.html}
}

@incollection{Reynolds2003what,
  langid = {english},
  title = {What do types mean? — From intrinsic to extrinsic semantics},
  isbn = {978-1-4419-2964-8 978-0-387-21798-7},
  url = {http://link.springer.com/chapter/10.1007/978-0-387-21798-7_15},
  shorttitle = {What do types mean?},
  abstract = {A definition of a typed language is said to be “intrinsic” if it assigns meanings to typings rather than arbitrary phrases, so that ill-typed phrases are meaningless. In contrast, a definition is said to be “extrinsic” if all phrases have meanings that are independent of their typings, while typings represent properties of these meanings. For a simply typed lambda calculus, extended with integers, recursion, and conditional expressions, we give an intrinsic denotational semantics and a denotational semantics of the underlying untyped language. We then estab?lish a logical relations theorem between these two semantics, and show that the logical relations can be “bracketed” by retractions between the domains of the two semantics. From these results, we derive an extrinsic semantics that uses partial equivalence relations.},
  booktitle = {Programming Methodology},
  series = {Monographs in Computer Science},
  publisher = {{Springer New York}},
  urldate = {2014-08-09},
  date = {2003-01-01},
  pages = {309-327},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  author = {Reynolds, John C.},
  editor = {McIver, Annabelle and Morgan, Carroll},
  file = {/Users/pgiarrusso/Zotero/storage/SBPSQW72/Reynolds - 2003 - What do types mean - — From intrinsic to extrinsic semantics.pdf;/Users/pgiarrusso/Zotero/storage/JSNK7Q8F/978-0-387-21798-7_15.html}
}
% == BibLateX quality report for Reynolds2003what:
% 'isbn': not a valid ISBN

@inproceedings{Morris1973types,
  location = {{New York, NY, USA}},
  title = {Types Are Not Sets},
  url = {http://doi.acm.org/10.1145/512927.512938},
  doi = {10.1145/512927.512938},
  abstract = {The title is not a statement of fact, of course, but an opinion about how language designers should think about types. There has been a natural tendency to look to mathematics for a consistent, precise notion of what types are. The point of view there is extensional: a type is a subset of the universe of values. While this approach may have served its purpose quite adequately in mathematics, defining programming language types in this way ignores some vital ideas. Some interesting developments following the extensional approach are the ALGOL-68 type system [vW], Scott's theory [S], and Reynolds' system [R]. While each of these lend valuable insight to programming languages, I feel they miss an important aspect of types.Rather than worry about what types are I shall focus on the role of type checking. Type checking seems to serve two distinct purposes: authentication and secrecy. Both are useful when a programmer undertakes to implement a class of abstract objects to be used by many other programmers. He usually proceeds by choosing a representation for the objects in terms of other objects and then writes the required operations to manipulate them.},
  booktitle = {Proceedings of the 1st Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '73},
  publisher = {{ACM}},
  urldate = {2014-08-10},
  date = {1973},
  pages = {120--124},
  author = {Morris, Jr., James H.},
  file = {/Users/pgiarrusso/Zotero/storage/8CHI8KXE/Morris - 1973 - Types Are Not Sets.pdf}
}

@inproceedings{Jones1994dictionaryfree,
  title = {Dictionary-Free Overloading by Partial Evaluation (2)},
  booktitle = {PEPM},
  date = {1994},
  pages = {107-117},
  author = {Jones, Mark P.},
  file = {/Users/pgiarrusso/Zotero/storage/3KHPJT96/Jones - 1994 - Dictionary-Free Overloading by Partial Evaluation.pdf},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/pepm/1994}
}
% == BibLateX quality report for Jones1994dictionaryfree:
% Unexpected field 'bibsource'
% ? Unsure about the formatting of the booktitle

@book{1994pepm94,
  title = {PEPM'94 - ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, Walt Disney World Vilage, Orlando, Florida, USA, 25 June 1994, Proceedings. Technical Report 94/9},
  publisher = {{University of Melbourne, Australia, Department of Computer Science}},
  date = {1994},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
% == BibLateX quality report for 1994pepm94:
% Unexpected field 'bibsource'
% Missing required field 'author'

@article{Jones1995dictionaryfree,
  langid = {english},
  title = {Dictionary-free overloading by partial evaluation (1)},
  volume = {8},
  issn = {0892-4635, 1573-0557},
  url = {http://link.springer.com/article/10.1007/BF01019005},
  doi = {10.1007/BF01019005},
  abstract = {One of the most novel features in the functional programming language Haskell is the system oftype classes used to support a combination of overloading and polymorphism. Current implementations of type class overloading are based on the use ofdictionary values, passed as extra parameters to overloaded functions. Unfortunately, this can have a significant effect on run-time performance, for example, by reducing the effectiveness of important program analyses and optimizations. This paper describes how a simple partial evaluator can be used to avoid the need for dictionary values at run-time by generating specialized versions of overloaded functions. This eliminates the run-time costs of overloading. Furthermore, and somewhat surprisingly given the presence of multiple versions of some functions, for all of the examples that we have tried so far, specialization actually leads to a reduction in the size of compiled programs.},
  number = {3},
  journaltitle = {LISP and Symbolic Computation},
  shortjournal = {Lisp and Symbolic Computation},
  urldate = {2014-08-10},
  date = {1995-09-01},
  pages = {229-248},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Haskell,Software Engineering/Programming and Operating Systems,Numeric Computing,partial evaluation,specialization,type class overloading},
  author = {Jones, Mark P.},
  file = {/Users/pgiarrusso/Zotero/storage/QCU23JUX/10.html}
}
% == BibLateX quality report for Jones1995dictionaryfree:
% 'issn': not a valid ISSN

@inproceedings{Chapman2010gentle,
  location = {{New York, NY, USA}},
  title = {The Gentle Art of Levitation},
  isbn = {978-1-60558-794-3},
  url = {http://doi.acm.org/10.1145/1863543.1863547},
  doi = {10.1145/1863543.1863547},
  abstract = {We present a closed dependent type theory whose inductive types are given not by a scheme for generative declarations, but by encoding in a universe. Each inductive datatype arises by interpreting its description - a first-class value in a datatype of descriptions. Moreover, the latter itself has a description. Datatype-generic programming thus becomes ordinary programming. We show some of the resulting generic operations and deploy them in particular, useful ways on the datatype of datatype descriptions itself. Simulations in existing systems suggest that this apparently self-supporting setup is achievable without paradox or infinite regress.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '10},
  publisher = {{ACM}},
  urldate = {2014-08-11},
  date = {2010},
  pages = {3--14},
  keywords = {data structure,type systems,Monads,proof assistants,metaprogramming},
  author = {Chapman, James and Dagand, Pierre-Évariste and McBride, Conor and Morris, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/98HTCR2I/Chapman et al. - 2010 - The Gentle Art of Levitation.pdf}
}

@incollection{Altenkirch2003generic,
  langid = {english},
  title = {Generic Programming within Dependently Typed Programming},
  isbn = {978-1-4757-5320-2 978-0-387-35672-3},
  url = {http://link.springer.com/chapter/10.1007/978-0-387-35672-3_1},
  abstract = {We show how higher kinded generic programming can be represented faithfully within a dependently typed programming system. This development has been implemented using the Oleg system. The present work can be seen as evidence for our thesis that extensions of type systems can be done by programming within a dependently typed language, using data as codes for types.},
  number = {115},
  booktitle = {Generic Programming},
  series = {IFIP — The International Federation for Information Processing},
  publisher = {{Springer US}},
  urldate = {2014-08-11},
  date = {2003-01-01},
  pages = {1-20},
  keywords = {Programming Languages; Compilers; Interpreters,Data Structures,Mathematics of Computing,Software Engineering/Programming and Operating Systems,Theory of Computation},
  author = {Altenkirch, Thorsten and Mcbride, Conor},
  editor = {Gibbons, Jeremy and Jeuring, Johan},
  file = {/Users/pgiarrusso/Zotero/storage/NV6GFRXI/978-0-387-35672-3_1.html}
}
% == BibLateX quality report for Altenkirch2003generic:
% 'isbn': not a valid ISBN

@report{Dijkstra2008efficient,
  title = {Efficient Functional Unification and Substitution},
  number = {UU-CS-2008-027},
  institution = {{Department of Information and Computing Sciences, Utrecht University}},
  date = {2008},
  author = {Dijkstra, Atze and Middelkoop, Arie and Swierstra, S. Doaitse},
  file = {/Users/pgiarrusso/Zotero/storage/3B6CZ3RI/Dijkstra et al. - 2008 - Efficient Functional Unification and Substitution.pdf},
  pubcat = {techreport},
  urlpdf = {http://www.cs.uu.nl/research/techreps/repo/CS-2008/2008-027.pdf}
}
% == BibLateX quality report for Dijkstra2008efficient:
% Unexpected field 'pubcat'
% Unexpected field 'urlpdf'
% Missing required field 'type'

@inproceedings{Cousineau2007embedding,
  location = {{Berlin, Heidelberg}},
  title = {Embedding Pure Type Systems in the Lambda-pi-calculus Modulo},
  isbn = {978-3-540-73227-3},
  url = {http://dl.acm.org/citation.cfm?id=1770203.1770212},
  abstract = {The lambda-Pi-calculus allows to express proofs of minimal predicate logic. It can be extended, in a very simple way, by adding computation rules. This leads to the lambda-Pi-calculus modulo. We show in this paper that this simple extension is surprisingly expressive and, in particular, that all functional Pure Type Systems, such as the system F, or the Calculus of Constructions, can be embedded in it. And, moreover, that this embedding is conservative under termination hypothesis.},
  booktitle = {Proceedings of the 8th International Conference on Typed Lambda Calculi and Applications},
  series = {TLCA'07},
  publisher = {{Springer-Verlag}},
  urldate = {2014-08-11},
  date = {2007},
  pages = {102--117},
  author = {Cousineau, Denis and Dowek, Gilles}
}
% == BibLateX quality report for Cousineau2007embedding:
% 'isbn': not a valid ISBN

@inproceedings{Barthe2003pure,
  location = {{New York, NY, USA}},
  title = {Pure Patterns Type Systems},
  isbn = {1-58113-628-5},
  url = {http://doi.acm.org/10.1145/604131.604152},
  doi = {10.1145/604131.604152},
  abstract = {We introduce a new framework of algebraic pure type systems in which we consider rewrite rules as lambda terms with patterns and rewrite rule application as abstraction application with built-in matching facilities. This framework, that we call "Pure Pattern Type Systems", is particularly well-suited for the foundations of programming (meta)languages and proof assistants since it provides in a fully unified setting higher-order capabilities and pattern matching ability together with powerful type systems. We prove some standard properties like confluence and subject reduction for the case of a syntactic theory and under a syntactical restriction over the shape of patterns. We also conjecture the strong normalization of typable terms. This work should be seen as a contribution to a formal connection between logics and rewriting, and a step towards new proof engines based on the Curry-Howard isomorphism.},
  booktitle = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '03},
  publisher = {{ACM}},
  urldate = {2014-08-11},
  date = {2003},
  pages = {250--261},
  keywords = {Curry-Howard,pure type systems,Lambda-calculus,logics,matching,patterns,rewriting},
  author = {Barthe, Gilles and Cirstea, Horatiu and Kirchner, Claude and Liquori, Luigi},
  file = {/Users/pgiarrusso/Zotero/storage/W36XAKDC/Barthe et al - 2003 - Pure Patterns Type Systems.pdf}
}

@incollection{Scherr2014implicit,
  langid = {english},
  title = {Implicit Staging of EDSL Expressions: A Bridge between Shallow and Deep Embedding},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-662-44202-9_16},
  shorttitle = {Implicit Staging of EDSL Expressions},
  abstract = {Common implementation approaches for embedding DSLs in general-purpose host languages force developers to choose between a shallow (single-staged) embedding which offers seamless usage, but limits DSL developers, or a deep (multi-staged) embedding which offers freedom to optimize at will, but is less seamless to use and incurs additional runtime overhead. We propose a metaprogrammatic approach for extracting domain-specific programs from user programs for custom processing. This allows for similar optimization options as deep embedding, while still allowing for seamless embedded usage. We have implemented a simplified instance of this approach in a prototype framework for Java-embedded EDSL expressions, which relies on load-time reflection for improved deployability and usability.},
  number = {8586},
  booktitle = {ECOOP 2014 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-11},
  date = {2014-01-01},
  pages = {385-410},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,java,Computer Communication Networks,programming languages,metaprogramming,DSL},
  author = {Scherr, Maximilian and Chiba, Shigeru},
  editor = {Jones, Richard},
  file = {/Users/pgiarrusso/Zotero/storage/9RBPTHDD/Scherr_Chiba - 2014 - Implicit Staging of EDSL Expressions - A Bridge between Shallow and Deep Embedding.pdf;/Users/pgiarrusso/Zotero/storage/E2AS7BPK/978-3-662-44202-9_16.html}
}
% == BibLateX quality report for Scherr2014implicit:
% 'isbn': not a valid ISBN

@inproceedings{Qi2010homogeneous,
  location = {{New York, NY, USA}},
  title = {Homogeneous Family Sharing},
  isbn = {978-1-4503-0203-6},
  url = {http://doi.acm.org/10.1145/1869459.1869502},
  doi = {10.1145/1869459.1869502},
  abstract = {Recent work has introduced class sharing as a mechanism for adapting a family of related classes with new functionality. This paper introduces homogeneous family sharing, implemented in the J\&h language, in which the sharing mechanism is lifted from class-level sharing to true family-level sharing. Compared to the original (heterogeneous) class sharing mechanism, homogeneous family sharing provides useful new functionality and substantially reduces the annotation burden on programmers by eliminating the need for masked types and sharing declarations. This is achieved through a new mechanism, shadow classes, which permit homogeneous sharing of all related classes in shared families. The new sharing mechanism has a straightforward semantics, which is formalized in the J\&h calculus. The soundness of the J\&h type system is proved. The J\&h language is implemented as an extension to the J\& language. To demonstrate the effectiveness of family sharing, the Polyglot compiler framework is ported to J\&h.},
  booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '10},
  publisher = {{ACM}},
  urldate = {2014-08-11},
  date = {2010},
  pages = {520--538},
  keywords = {family inheritance,views,shadow classes},
  author = {Qi, Xin and Myers, Andrew C.},
  file = {/Users/pgiarrusso/Zotero/storage/2HPMPDTV/Qi and Myers - 2010 - Homogeneous Family Sharing.pdf}
}

@article{Poll2001coalgebraic,
  title = {A Coalgebraic Semantics of Subtyping},
  volume = {35},
  doi = {10.1051/ita:2001109},
  abstract = {Coalgebras have been proposed as formal basis for the semantics of objects in the sense of object-oriented programming. This paper shows that this semantics provides a smooth interpretation for subtyping, a central notion in object-oriented programming. We show that different characterisations of behavioural subtyping found in the literature can conveniently be expressed in coalgebraic terms. We also investigate the subtle difference between behavioural subtyping and refinement.},
  number = {01},
  journaltitle = {RAI},
  date = {2001},
  pages = {61-81},
  author = {Poll, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/VVAMA5TI/Poll - 2001 - A Coalgebraic Semantics of Subtyping.html}
}

@report{Elliott2009denotational,
  title = {Denotational design with type class morphisms (extended version)},
  url = {http://conal.net/papers/type-class-morphisms},
  abstract = {Type classes provide a mechanism for varied implementations of standard interfaces. Many of these interfaces are founded in mathematical tradition and so have regularity not only of types but also of properties (laws) that must hold. Types and properties give strong guidance to the library implementor, while leaving freedom as well. Some of this remaining freedom is in how the implementation works, and some is in what it accomplishes.

To give additional guidance to the what, without impinging on the how, this paper proposes a principle of type class morphisms (TCMs), which further refines the compositional style of denotational semantics. The TCM idea is simply that the instance’s meaning is the meaning’s instance. This principle determines the meaning of each type class instance, and hence defines correctness of implementation. It also serves to transfer laws about a type’s semantic model, such as the class laws, to hold for the type itself. In some cases, it also provides a systematic guide to implementation, and in some cases, valuable design feedback.

The paper is illustrated with several examples of types, meanings, and morphisms.},
  number = {2009-01},
  institution = {{LambdaPix}},
  date = {2009-03},
  author = {Elliott, Conal},
  file = {/Users/pgiarrusso/Zotero/storage/WUD7C769/Elliott - 2009 - Denotational design with type class morphisms (extended version).pdf}
}
% == BibLateX quality report for Elliott2009denotational:
% Missing required field 'type'

@inproceedings{Elliott2009pushpull,
  location = {{New York, NY, USA}},
  title = {Push-pull Functional Reactive Programming},
  isbn = {978-1-60558-508-6},
  url = {http://doi.acm.org/10.1145/1596638.1596643},
  doi = {10.1145/1596638.1596643},
  abstract = {Functional reactive programming (FRP) has simple and powerful semantics, but has resisted efficient implementation. In particular, most past implementations have used demand-driven sampling, which accommodates FRP's continuous time semantics and fits well with the nature of functional programming. Consequently, values are wastefully recomputed even when inputs don't change, and reaction latency can be as high as the sampling period. This paper presents a way to implement FRP that combines data- and demand-driven evaluation, in which values are recomputed only when necessary, and reactions are nearly instantaneous. The implementation is rooted in a new simple formulation of FRP and its semantics and so is easy to understand and reason about. On the road to a new implementation, we'll meet some old friends (monoids, functors, applicative functors, monads, morphisms, and improving values) and make some new friends (functional future values, reactive normal form, and concurrent "unambiguous choice").},
  booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '09},
  publisher = {{ACM}},
  urldate = {2014-08-19},
  date = {2009},
  pages = {25--36},
  keywords = {functional reactive programming,concurrency,data-driven,demand-driven,semantics},
  author = {Elliott, Conal M.},
  file = {/Users/pgiarrusso/Zotero/storage/CSU69SAH/Elliott - 2009 - Push-pull Functional Reactive Programming.pdf}
}

@report{Elliott2008simply,
  title = {Simply efficient functional reactivity},
  url = {http://conal.net/papers/simply-reactive/},
  abstract = {Functional reactive programming (FRP) has simple and powerful semantics, but has resisted efficient implementation. In particular, most past implementations have used demand-driven sampling, which accommodates FRP's continuous time semantics and fits well with the nature of functional programming. Consequently, values are wastefully recomputed even when inputs don't change, and reaction latency can be as high as the sampling period.

This paper presents a way to implement FRP that combines data- and demand-driven evaluation, in which values are recomputed only when necessary, and reactions are nearly instantaneous. The implementation is rooted in a new simple formulation of FRP and its semantics and so is easy to understand and reason about.

On the road to efficiency and simplicity, we'll meet some old friends (monoids, functors, applicative functors, monads, morphisms, and improving values) and make some new friends (functional future values, reactive normal form, and concurrent "unambiguous choice").},
  number = {2008-01},
  institution = {{LambdaPix}},
  date = {2008-04},
  author = {Elliott, Conal},
  file = {/Users/pgiarrusso/Zotero/storage/AMU54FXC/Elliott - 2008 - Simply efficient functional reactivity.pdf}
}
% == BibLateX quality report for Elliott2008simply:
% Missing required field 'type'

@misc{Gowerstwo,
  title = {The Two Cultures of Mathematics},
  url = {https://www.dpmms.cam.ac.uk/~wtg10/2cultures.pdf},
  abstract = {This article was for an IMU volume entitled Mathematics: Frontiers and Perspectives. It is basically a defence of combinatorics against some of the criticisms commonly made of it. Judging from some of the reaction to the article, it is perhaps worth saying that when I quote the views of `theoreticians', it is because I agree with them. My aim is not to dispute their criteria for what constitutes worthwhile mathematics, but rather to show that combinatorics meets these criteria (suitably, and I hope reasonably, interpreted).},
  urldate = {2014-08-23},
  author = {Gowers, W. T.},
  file = {/Users/pgiarrusso/Zotero/storage/628MTKNT/Gowers - The Two Cultures of Mathematics.pdf},
  note = {Info found at https://www.dpmms.cam.ac.uk/\textasciitilde{}wtg10/papers.html}
}
% == BibLateX quality report for Gowerstwo:
% Exactly one of 'date' / 'year' must be present

@article{Kamin1998research,
  title = {Research on Domain-specific Embedded Languages and Program Generators},
  volume = {14},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S157106610580235X},
  doi = {10.1016/S1571-0661(05)80235-X},
  abstract = {Embedding is the process of implementing a language by defining functions in an existing “host” Language; the host language with these added functions is the new language. As a consequence; the new language comes equipped with all the features of the host language, with no additional work on the part of the language designer. Embedding works particularly well when the host language is a functional language.

We describe several examples of embedded languages. The first is a language for specifying simple pictures. The others are program generators, that is, languages used to specify programs in other languages. In all of these examples, the host language is Standard ML; in the program generating languages, the target language is C+plus;. The power obtained from the host language is the main emphasis of our presentation.

The author gratefully acknowledges the support provided by the Oregon Graduate Institute, where he was on sabbatical during the preparation of this paper.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {US-Brazil Joint Workshops on the Formal Foundations of Software Systems},
  urldate = {2014-08-25},
  date = {1998},
  pages = {149-168},
  author = {Kamin, Samuel N.},
  file = {/Users/pgiarrusso/Zotero/storage/QZCSMIM7/Kamin - 1998 - Research on Domain-specific Embedded Languages and Program Generators.pdf;/Users/pgiarrusso/Zotero/storage/UC9ZMCFH/S157106610580235X.html}
}

@incollection{Shivers1996universal,
  langid = {english},
  title = {A universal scripting framework or Lambda: The ultimate “little language”},
  isbn = {978-3-540-62031-0 978-3-540-49626-7},
  url = {http://link.springer.com/chapter/10.1007/BFb0027798},
  shorttitle = {A universal scripting framework or Lambda},
  abstract = {The “little languages” approach to systems programming is flawed: inefficient, fragile, error-prone, inexpressive, and difficult to compose. A better solution is to embed task-specific sublanguages within a powerful, syntactically extensible, universal language, such as Scheme. I demonstrate two such embeddings that have been implemented in scsh, a Scheme programming environment for Unix systems programming. The first embedded language is a highlevel process-control notation; the second provides for Awk-like processing. Embedding systems in this way is a powerful technique: for example, although the embedded Awk system was implemented with 7\% of the code required for the standard C-based Awk, it is significantly more expressive than its C counterpart.},
  number = {1179},
  booktitle = {Concurrency and Parallelism, Programming, Networking, and Security},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-25},
  date = {1996-01-01},
  pages = {254-265},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Computer Communication Networks},
  author = {Shivers, Olin},
  editor = {Jaffar, Joxan and Yap, Roland H. C.},
  file = {/Users/pgiarrusso/Zotero/storage/9BU7AHID/Shivers - 1996 - A universal scripting framework or Lambda - The ultimate “little language”.pdf;/Users/pgiarrusso/Zotero/storage/588V8MJZ/BFb0027798.html}
}
% == BibLateX quality report for Shivers1996universal:
% 'isbn': not a valid ISBN

@inproceedings{Boehm2008foundations,
  location = {{New York, NY, USA}},
  title = {Foundations of the C++ Concurrency Memory Model},
  isbn = {978-1-59593-860-2},
  url = {http://doi.acm.org/10.1145/1375581.1375591},
  doi = {10.1145/1375581.1375591},
  abstract = {Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work. We give no semantics to programs with data races. There are no benign C++ data races. We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock. This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.},
  booktitle = {Proceedings of the 2008 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '08},
  publisher = {{ACM}},
  urldate = {2014-08-29},
  date = {2008},
  pages = {68--78},
  keywords = {C++,data race,memory consistency,memory model,sequential consistency,trylock},
  author = {Boehm, Hans-J. and Adve, Sarita V.},
  file = {/Users/pgiarrusso/Zotero/storage/IBU2DIG8/Boehm_Adve - 2008 - Foundations of the C++ Concurrency Memory Model.pdf}
}

@incollection{Vaziri2014stream,
  langid = {english},
  title = {Stream Processing with a Spreadsheet},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  url = {http://link.springer.com/chapter/10.1007/978-3-662-44202-9_15},
  abstract = {Continuous data streams are ubiquitous and represent such a high volume of data that they cannot be stored to disk, yet it is often crucial for them to be analyzed in real-time. Stream processing is a programming paradigm that processes these immediately, and enables continuous analytics. Our objective is to make it easier for analysts, with little programming experience, to develop continuous analytics applications directly. We propose enhancing a spreadsheet, a pervasive tool, to obtain a programming platform for stream processing. We present the design and implementation of an enhanced spreadsheet that enables visualizing live streams, live programming to compute new streams, and exporting computations to be run on a server where they can be shared with other users, and persisted beyond the life of the spreadsheet. We formalize our core language, and present case studies that cover a range of stream processing applications.},
  number = {8586},
  booktitle = {ECOOP 2014 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-29},
  date = {2014-01-01},
  pages = {360-384},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Computer Communication Networks},
  author = {Vaziri, Mandana and Tardieu, Olivier and Rabbah, Rodric and Suter, Philippe and Hirzel, Martin},
  editor = {Jones, Richard},
  file = {/Users/pgiarrusso/Zotero/storage/ZJ2642RX/Vaziri et al - 2014 - Stream Processing with a Spreadsheet.pdf;/Users/pgiarrusso/Zotero/storage/EADHN62V/978-3-662-44202-9_15.html}
}
% == BibLateX quality report for Vaziri2014stream:
% 'isbn': not a valid ISBN

@inproceedings{Vytiniotis2012equality,
  location = {{New York, NY, USA}},
  title = {Equality Proofs and Deferred Type Errors: A Compiler Pearl},
  isbn = {978-1-4503-1054-3},
  url = {http://doi.acm.org/10.1145/2364527.2364554},
  doi = {10.1145/2364527.2364554},
  shorttitle = {Equality Proofs and Deferred Type Errors},
  abstract = {The Glasgow Haskell Compiler is an optimizing compiler that expresses and manipulates first-class equality proofs in its intermediate language. We describe a simple, elegant technique that exploits these equality proofs to support deferred type errors. The technique requires us to treat equality proofs as possibly-divergent terms; we show how to do so without losing either soundness or the zero-overhead cost model that the programmer expects.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '12},
  publisher = {{ACM}},
  urldate = {2014-08-30},
  date = {2012},
  pages = {341--352},
  keywords = {system fc,deferred type errors,type equalities},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Magalhães, José Pedro},
  file = {/Users/pgiarrusso/Zotero/storage/27CRVQ2K/Vytiniotis et al - 2012 - Equality Proofs and Deferred Type Errors - A Compiler Pearl.pdf}
}

@inproceedings{Hammer2014adapton,
  location = {{New York, NY, USA}},
  title = {Adapton: Composable, Demand-driven Incremental Computation},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594324},
  doi = {10.1145/2594291.2594324},
  shorttitle = {Adapton},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered. To address these problems, we present λiccdd, a core calculus that applies a demand-driven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. λiccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures λiccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing λiccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms state-of-the-art IC approaches.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '14},
  publisher = {{ACM}},
  urldate = {2014-09-01},
  date = {2014},
  pages = {156--166},
  keywords = {self-adjusting computation,laziness,call-by-push-value (CBPV),demanded computation graph (DCG) incremental computation,thunks},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  file = {/Users/pgiarrusso/Zotero/storage/KW5ITCRT/Hammer et al. - 2014 - Adapton Composable, Demand-driven Incremental Com.pdf}
}

@inproceedings{Hammer2014adaptona,
  title = {Adapton: Composable, Demand-driven Incremental Computation (Extended version)},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594324},
  shorttitle = {Adapton},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered. To address these problems, we present λiccdd, a core calculus that applies a demand-driven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. λiccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures λiccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing λiccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms state-of-the-art IC approaches.},
  publisher = {{ACM}},
  urldate = {2014-09-01},
  date = {2014},
  keywords = {self-adjusting computation,laziness,call-by-push-value (CBPV),demanded computation graph (DCG) incremental computation,thunks},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  file = {/Users/pgiarrusso/Zotero/storage/XTKI4GXQ/Hammer et al. - 2014 - Adapton Composable, Demand-driven Incremental Com.pdf}
}
% == BibLateX quality report for Hammer2014adaptona:
% Missing required field 'booktitle'

@article{Michie1968memo,
  langid = {english},
  title = {“Memo” Functions and Machine Learning},
  volume = {218},
  url = {http://www.nature.com/nature/journal/v218/n5136/abs/218019a0.html},
  doi = {10.1038/218019a0},
  abstract = {It would be useful if computers could learn from experience and thus automatically improve the efficiency of their own programs during execution. A simple but effective rote-learning facility can be provided within the framework of a suitable programming language.},
  number = {5136},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2014-09-01},
  date = {1968-04-06},
  pages = {19-22},
  keywords = {read},
  author = {Michie, Donald},
  file = {/Users/pgiarrusso/Zotero/storage/RPKD7PDM/Michie - 1968 - “Memo” Functions and Machine Learning.pdf;/Users/pgiarrusso/Zotero/storage/WDNJTC33/218019a0.html}
}

@incollection{Levy1999callbypushvalue,
  langid = {english},
  title = {Call-by-Push-Value: A Subsuming Paradigm},
  isbn = {978-3-540-65763-7 978-3-540-48959-7},
  url = {http://link.springer.com/chapter/10.1007/3-540-48959-2_17},
  shorttitle = {Call-by-Push-Value},
  abstract = {Call-by-push-value is a new paradigm that subsumes the call-by-name and call-by-value paradigms, in the following sense: both operational and denotational semantics for those paradigms can be seen as arising, via translations that we will provide, from similar semantics for call-by-observable. To explain call-by-observable, we first discuss general operational ideas, especially the distinction between values and computations, using the principle that “a value is, a computation does”. Using an example program, we see that the lambda-calculus primitives can be understood as push/pop commands for an operand-stack. We provide operational and denotational semantics for a range of computational effects and show their agreement. We hence obtain semantics for call-by-name and call-by-value, of which some are familiar, some are new and some were known but previously appeared mysterious.},
  number = {1581},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-09-01},
  date = {1999-01-01},
  pages = {228-243},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {Levy, Paul Blain},
  editor = {Girard, Jean-Yves},
  file = {/Users/pgiarrusso/Zotero/storage/9W4PRTR3/10.html}
}
% == BibLateX quality report for Levy1999callbypushvalue:
% 'isbn': not a valid ISBN

@incollection{Svenningsson2013combining,
  langid = {english},
  title = {Combining Deep and Shallow Embedding for EDSL},
  isbn = {978-3-642-40446-7 978-3-642-40447-4},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-40447-4_2},
  abstract = {When compiling embedded languages it is natural to use an abstract syntax tree to represent programs. This is known as a deep embedding and it is a rather cumbersome technique compared to other forms of embedding, typically leading to more code and being harder to extend. In shallow embeddings, language constructs are mapped directly to their semantics which yields more flexible and succinct implementations. But shallow embeddings are not well-suited for compiling embedded languages. We present a technique to combine deep and shallow embedding in the context of compiling embedded languages in order to provide the benefits of both techniques. In particular it helps keeping the deep embedding small and it makes extending the embedded language much easier. Our technique also has some unexpected but welcome knock-on effects. It provides fusion of functions to remove intermediate results for free without any additional effort. It also helps to give the embedded language a more natural programming interface.},
  number = {7829},
  booktitle = {Trends in Functional Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-09-02},
  date = {2013-01-01},
  pages = {21-36},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Mathematical Logic and Formal Languages},
  author = {Svenningsson, Josef and Axelsson, Emil},
  editor = {Loidl, Hans-Wolfgang and Peña, Ricardo},
  file = {/Users/pgiarrusso/Zotero/storage/VRKSKUSG/Svenningsson_Axelsson - 2013 - Combining Deep and Shallow Embedding for EDSL.pdf;/Users/pgiarrusso/Zotero/storage/BEHGV6ZK/978-3-642-40447-4_2.html}
}
% == BibLateX quality report for Svenningsson2013combining:
% 'isbn': not a valid ISBN

@inproceedings{Slesarenko2014firstclass,
  location = {{New York, NY, USA}},
  title = {First-class Isomorphic Specialization by Staged Evaluation},
  isbn = {978-1-4503-3042-8},
  url = {http://doi.acm.org/10.1145/2633628.2633632},
  doi = {10.1145/2633628.2633632},
  abstract = {The state of the art approach for reducing complexity in software development is to use abstraction mechanisms of programming languages such as modules, types, higher-order functions etc. and develop high-level frameworks and domain-specific abstractions. Abstraction mechanisms, however, along with simplicity, introduce also execution overhead and often lead to significant performance degradation. Avoiding abstractions in favor of performance, on the other hand, increases code complexity and cost of maintenance. We develop a systematic approach and formalized framework for implementing software components with a first-class specialization capability. We show how to extend a higher-order functional language with abstraction mechanisms carefully designed to provide automatic and guaranteed elimination of abstraction overhead. We propose staged evaluation as a new method of program staging and show how it can be implemented as zipper-based traversal of program terms where one-hole contexts are generically constructed from the abstract syntax of the language. We show how generic programming techniques together with staged evaluation lead to a very simple yet powerful method of isomorphic specialization which utilizes first-class definitions of isomorphisms between data types to provide guarantee of abstraction elimination. We give a formalized description of the isomorphic specialization algorithm and show how it can be implemented as a set of term rewriting rules using active patterns and staged evaluation. We implemented our approach as a generic programming framework with first-class staging, term rewriting and isomorphic specialization and show in our evaluation that the proposed capabilities give rise to a new paradigm to develop domain-specific software components without abstraction penalty.},
  booktitle = {Proceedings of the 10th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '14},
  publisher = {{ACM}},
  urldate = {2014-09-02},
  date = {2014},
  pages = {35--46},
  keywords = {domain-specific languages,staging,generic programming,multi-stage programming,polytypic programming,specialization,DSL,isomorphisms},
  author = {Slesarenko, Alexander and Filippov, Alexander and Romanov, Alexey},
  file = {/Users/pgiarrusso/Zotero/storage/54M7QEFW/Slesarenko et al - 2014 - First-class Isomorphic Specialization by Staged Evaluation.pdf}
}

@inproceedings{Atkey2009unembedding,
  location = {{New York, NY, USA}},
  title = {Unembedding Domain-specific Languages},
  isbn = {978-1-60558-508-6},
  url = {http://doi.acm.org/10.1145/1596638.1596644},
  doi = {10.1145/1596638.1596644},
  abstract = {Higher-order abstract syntax provides a convenient way of embedding domain-specific languages, but is awkward to analyse and manipulate directly. We explore the boundaries of higher-order abstract syntax. Our key tool is the unembedding of embedded terms as de Bruijn terms, enabling intensional analysis. As part of our solution we present techniques for separating the definition of an embedded program from its interpretation, giving modular extensions of the embedded language, and different ways to encode the types of the embedded language.},
  booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '09},
  publisher = {{ACM}},
  urldate = {2014-09-02},
  date = {2009},
  pages = {37--48},
  keywords = {domain-specific languages,type classes,higher-order abstract syntax,unembedding},
  author = {Atkey, Robert and Lindley, Sam and Yallop, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/WAEICJC6/Atkey et al - 2009 - Unembedding Domain-specific Languages.pdf}
}

@article{Benton2012strongly,
  langid = {english},
  title = {Strongly Typed Term Representations in Coq},
  volume = {49},
  issn = {0168-7433, 1573-0670},
  url = {http://link.springer.com/article/10.1007/s10817-011-9219-0},
  doi = {10.1007/s10817-011-9219-0},
  abstract = {There are two approaches to formalizing the syntax of typed object languages in a proof assistant or programming language. The extrinsic approach is to first define a type that encodes untyped object expressions and then make a separate definition of typing judgements over the untyped terms. The intrinsic approach is to make a single definition that captures well-typed object expressions, so ill-typed expressions cannot even be expressed. Intrinsic encodings are attractive and naturally enforce the requirement that metalanguage operations on object expressions, such as substitution, respect object types. The price is that the metalanguage types of intrinsic encodings and operations involve non-trivial dependency, adding significant complexity. This paper describes intrinsic-style formalizations of both simply-typed and polymorphic languages, and basic syntactic operations thereon, in the Coq proof assistant. The Coq types encoding object-level variables (de Bruijn indices) and terms are indexed by both type and typing environment. One key construction is the boot-strapping of definitions and lemmas about the action of substitutions in terms of similar ones for a simpler notion of renamings. In the simply-typed case, this yields definitions that are free of any use of type equality coercions. In the polymorphic case, some substitution operations do still require type coercions, which we at least partially tame by uniform use of heterogeneous equality.},
  number = {2},
  journaltitle = {Journal of Automated Reasoning},
  shortjournal = {J Autom Reasoning},
  urldate = {2014-09-02},
  date = {2012-08-01},
  pages = {141-159},
  keywords = {Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages,Symbolic and Algebraic Manipulation,Mathematical Logic and Foundations,_tablet,de Bruijn indices,The Coq proof assistant,Typed object languages},
  author = {Benton, Nick and Hur, Chung-Kil and Kennedy, Andrew J. and McBride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/HHCVBAAM/Benton-Hur-Kennedy-McBride - 2012 - Strongly Typed Term Representations in Coq.pdf;/Users/pgiarrusso/Zotero/storage/KZJCKU56/s10817-011-9219-0.html}
}
% == BibLateX quality report for Benton2012strongly:
% 'issn': not a valid ISSN

@article{Baez2009physics,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0903.0340},
  primaryClass = {quant-ph},
  title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
  url = {http://arxiv.org/abs/0903.0340},
  shorttitle = {Physics, Topology, Logic and Computation},
  abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology: namely, a linear operator behaves very much like a "cobordism". Similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of "closed symmetric monoidal category". We assume no prior knowledge of category theory, proof theory or computer science.},
  urldate = {2014-09-13},
  date = {2009-03-02},
  keywords = {Mathematics - Category Theory,Quantum Physics},
  author = {Baez, John C. and Stay, Mike},
  file = {/Users/pgiarrusso/Zotero/storage/P8R7Z326/Baez_Stay - 2009 - Physics, Topology, Logic and Computation - A Rosetta Stone.pdf;/Users/pgiarrusso/Zotero/storage/ZHBSHMZ6/0903.html}
}
% == BibLateX quality report for Baez2009physics:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Hoare1989notes,
  langid = {english},
  title = {Notes on an Approach to Category Theory for Computer Scientists},
  isbn = {978-3-642-74886-8 978-3-642-74884-4},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-74884-4_9},
  abstract = {These notes have been designed for the benefit of theoretical computer scientists who are not sure whether they want to study category theory, and who are too busy to devote the long period of continuous study required to master the subject from standard texts. The notes are therefore structured into three independent chapters, of which the earlier ones are the simplest and most clearly relevant to computer science. Each chapter introduces a number of essential categorical concepts, illustrates them by examples intended to be familiar to computer scientists, and presents theorems describing their most important properties. Each chapter may therefore be studied at widely separated intervals of time; further, the material of each chapter is organised so that there is no need to finish one chapter (or even one section) before starting the next. Finally, the reader who decides to abandon the study of category theory before completing the notes will still have obtained benefit from the effort expended.},
  number = {55},
  booktitle = {Constructive Methods in Computing Science},
  series = {NATO ASI Series},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-09-13},
  date = {1989-01-01},
  pages = {245-305},
  keywords = {Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Hoare, C. a. R.},
  editor = {Broy, Manfred},
  file = {/Users/pgiarrusso/Zotero/storage/6W9TNFIE/978-3-642-74884-4_9.html}
}
% == BibLateX quality report for Hoare1989notes:
% 'isbn': not a valid ISBN

@article{Mogelberg2009relational,
  langid = {english},
  title = {Relational Parametricity for Computational Effects},
  volume = {5},
  issn = {18605974},
  url = {http://lmcs-online.org/ojs/viewarticle.php?id=449&layout=abstract},
  doi = {10.2168/LMCS-5(3:7)2009},
  number = {3},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2014-09-13},
  date = {2009-08-09},
  keywords = {_tablet},
  author = {Møgelberg, Rasmus and Simpson, Alex},
  editor = {Tennent, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/FHDWK3ZN/Møgelberg-Simpson - 2009 - Relational Parametricity for Computational Effects.pdf;/Users/pgiarrusso/Zotero/storage/NKPE59BM/viewarticle.html}
}

@inproceedings{Gvero2013complete,
  location = {{New York, NY, USA}},
  title = {Complete Completion Using Types and Weights},
  isbn = {978-1-4503-2014-6},
  url = {http://doi.acm.org/10.1145/2491956.2462192},
  doi = {10.1145/2491956.2462192},
  abstract = {Developing modern software typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. As the basis of our technique we use type inhabitation for lambda calculus terms in long normal form. We introduce a succinct representation for type judgements that merges types into equivalence classes to reduce the search space, then reconstructs any desired number of solutions on demand. Furthermore, we introduce a method to rank solutions based on weights derived from a corpus of code. We implemented the algorithm and deployed it as a plugin for the Eclipse IDE for Scala. We show that the techniques we incorporated greatly increase the effectiveness of the approach. Our evaluation benchmarks are code examples from programming practice; we make them available for future comparisons.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '13},
  publisher = {{ACM}},
  urldate = {2014-09-15},
  date = {2013},
  pages = {27--38},
  keywords = {code completion,program synthesis,ranking,type inhabitation,type-driven synthesis},
  author = {Gvero, Tihomir and Kuncak, Viktor and Kuraj, Ivan and Piskac, Ruzica},
  file = {/Users/pgiarrusso/Zotero/storage/2DUVQRBT/Gvero et al. - 2013 - Complete Completion Using Types and Weights.pdf}
}

@inproceedings{Tate2009equality,
  location = {{New York, NY, USA}},
  title = {Equality Saturation: A New Approach to Optimization},
  isbn = {978-1-60558-379-2},
  url = {http://doi.acm.org/10.1145/1480881.1480915},
  doi = {10.1145/1480881.1480915},
  shorttitle = {Equality Saturation},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '09},
  publisher = {{ACM}},
  urldate = {2014-09-15},
  date = {2009},
  pages = {264--276},
  keywords = {compiler optimization,equality reasoning,intermediate representation},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  file = {/Users/pgiarrusso/Zotero/storage/B7ZACQF2/Tate et al - 2009 - Equality Saturation - A New Approach to Optimization.pdf}
}

@inproceedings{Lassen2008typed,
  title = {Typed Normal Form Bisimulation for Parametric Polymorphism},
  doi = {10.1109/LICS.2008.26},
  abstract = {This paper presents a new bisimulation theory for parametric polymorphism which enables straight forward co-inductive proofs of program equivalences involving existential types. The theory is an instance of typed normal form bisimulation and demonstrates the power of this recent framework for modeling typed lambda calculi as labelled transition systems.We develop our theory for a continuation-passing style calculus, Jump-With-Argument, where normal form bisimulation takes a simple form. We equip the calculus with both existential and recursive types. An "ultimate pattern matching theorem" enables us to define bisimilarity and we show it to be a congruence. We apply our theory to proving program equivalences, type isomorphisms and genericity.},
  eventtitle = {23rd Annual IEEE Symposium on Logic in Computer Science, 2008. LICS '08},
  booktitle = {23rd Annual IEEE Symposium on Logic in Computer Science, 2008. LICS '08},
  date = {2008-06},
  pages = {341-352},
  keywords = {pattern matching,recursive types,type theory,lambda calculus,Calculus,Computer science,theorem proving,parametric polymorphism,Computer languages,Logic,Reasoning about programs,bisimulation,bisimulation equivalence,bisimulation theory,coinductive proofs,continuation-passing style calculus,Counting circuits,equivalence classes,Filling,lambda calculi,LTS,Power system modeling,program equivalences,recursive functions,Robustness,type isomorphisms,typed lambda calculus,typed normal form bisimulation,ultimate pattern matching theorem},
  author = {Lassen, S.B. and Levy, P.B.},
  file = {/Users/pgiarrusso/Zotero/storage/PGCBHDRF/Lassen and Levy - 2008 - Typed Normal Form Bisimulation for Parametric Poly.pdf;/Users/pgiarrusso/Zotero/storage/HFXUKFG9/freeabs_all.html}
}
% == BibLateX quality report for Lassen2008typed:
% ? Unsure about the formatting of the booktitle

@article{Reddy1996global,
  langid = {english},
  title = {Global state considered unnecessary: An introduction to object-based semantics},
  volume = {9},
  issn = {0892-4635, 1573-0557},
  url = {http://link.springer.com/article/10.1007/BF01806032},
  doi = {10.1007/BF01806032},
  shorttitle = {Global state considered unnecessary},
  abstract = {Semantics of imperative programming languages is traditionally described in terms of functions on global states. We propose here a novel approach based on a notion ofobjects and characterize them in terms of their observable behavior. States are regarded as part of the internal structure of objects and play no role in the observable behavior. It is shown that this leads to considerable accuracy in the semantic modelling of locality and single-threadedness properties of objects.},
  number = {1},
  journaltitle = {LISP and Symbolic Computation},
  shortjournal = {Lisp and Symbolic Computation},
  urldate = {2014-09-15},
  date = {1996-02-01},
  pages = {7-76},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Software Engineering/Programming and Operating Systems,Numeric Computing,Denotational semantics,Imperative programs,Objects,Syntactic control of interference},
  author = {Reddy, Uday S.},
  file = {/Users/pgiarrusso/Zotero/storage/WV3VWR24/BF01806032.html}
}
% == BibLateX quality report for Reddy1996global:
% 'issn': not a valid ISSN

@book{OHearn2013algollike,
  title = {Algol-like Languages},
  isbn = {1-4757-3853-6 978-1-4757-3853-7},
  abstract = {In 1959 John Backus presented a paper on a proposed international algebraic language which evolved into ALGOL 60. This set of two volumes aims to review the attempts over recent years to use programming languages based on ALGOL 60, using Backus' original document as an introduction.},
  publisher = {{Birkh\&\#228;user Basel}},
  date = {2013},
  author = {O'Hearn, Peter and Tennent, Robert}
}
% == BibLateX quality report for OHearn2013algollike:
% 'isbn': not a valid ISBN

@article{Leinster2010informal,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1012.5647},
  primaryClass = {math},
  title = {An informal introduction to topos theory},
  url = {http://arxiv.org/abs/1012.5647},
  abstract = {This short expository text is for readers who are confident in basic category theory but know little or nothing about toposes. It is based on some impromptu talks given to a small group of category theorists.},
  urldate = {2014-09-17},
  date = {2010-12-27},
  keywords = {Mathematics - Category Theory,Mathematics - Logic,Mathematics - Algebraic Geometry},
  author = {Leinster, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/FA6QEPVM/Leinster - 2010 - An informal introduction to topos theory.pdf;/Users/pgiarrusso/Zotero/storage/4HB7WG6U/Leinster - 2010 - An informal introduction to topos theory.html}
}
% == BibLateX quality report for Leinster2010informal:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Dreyer2008mixin,
  location = {{New York, NY, USA}},
  title = {Mixin' Up the ML Module System},
  isbn = {978-1-59593-919-7},
  url = {http://doi.acm.org/10.1145/1411204.1411248},
  doi = {10.1145/1411204.1411248},
  abstract = {ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent MLstyle data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '08},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2008},
  pages = {307--320},
  keywords = {abstract data types,ML modules,type systems,_tablet,hierarchical composability,mixin modules,recursive modules},
  author = {Dreyer, Derek and Rossberg, Andreas},
  file = {/Users/pgiarrusso/Zotero/storage/RCU32WWJ/Dreyer-Rossberg - 2008 - Mixin' Up the ML Module System.pdf}
}

@article{Rossberg2013mixin,
  title = {Mixin' Up the ML Module System},
  volume = {35},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/2450136.2450137},
  doi = {10.1145/2450136.2450137},
  abstract = {ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent ML-style data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role. We provide a declarative type system for MixML, including two important extensions: higher-order modules, and modules as first-class values. We also present a sound and complete, three-pass type-checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG---namely, a polymorphic lambda calculus with single-assignment references and recursive type generativity---which employs a linear type and kind system to track definedness of term and type imports.},
  number = {1},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2014-09-20},
  date = {2013-04},
  pages = {2:1--2:84},
  keywords = {abstract data types,ML modules,type systems,hierarchical composability,mixin modules,recursive modules},
  author = {Rossberg, Andreas and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/HXFK557E/Rossberg_Dreyer - 2013 - Mixin' Up the ML Module System.pdf}
}
% == BibLateX quality report for Rossberg2013mixin:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@inproceedings{Dietz1987two,
  location = {{New York, NY, USA}},
  title = {Two Algorithms for Maintaining Order in a List},
  isbn = {0-89791-221-7},
  url = {http://doi.acm.org/10.1145/28395.28434},
  doi = {10.1145/28395.28434},
  abstract = {The order maintenance problem is that of maintaining a list under a sequence of Insert and Delete operations, while answering Order queries (determine which of two elements comes first in the list). We give two new algorithms for this problem. The first algorithm matches the O(1) amortized time per operation of the best previously known algorithm, and is much simpler. The second algorithm permits all operations to be performed in O(1) worst-case time.},
  booktitle = {Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing},
  series = {STOC '87},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {1987},
  pages = {365--372},
  author = {Dietz, P. and Sleator, D.},
  file = {/Users/pgiarrusso/Zotero/storage/EUQAKC38/Dietz_Sleator - 1987 - Two Algorithms for Maintaining Order in a List.pdf}
}

@inproceedings{McCreight2010certified,
  location = {{New York, NY, USA}},
  title = {A Certified Framework for Compiling and Executing Garbage-collected Languages},
  isbn = {978-1-60558-794-3},
  url = {http://doi.acm.org/10.1145/1863543.1863584},
  doi = {10.1145/1863543.1863584},
  abstract = {We describe the design, implementation, and use of a machine-certified framework for correct compilation and execution of programs in garbage-collected languages. Our framework extends Leroy's Coq-certified Compcert compiler and Cminor intermediate language. We add: (i) a new intermediate language, GCminor, that includes primitives for allocating memory in a garbage-collected heap and for specifying GC roots; (ii) a precise, low-level specification for a Cminor library for garbage collection; and (iii) a proven semantics-preserving translation from GCminor to Cminor plus the GC library. GCminor neatly encapsulates the interface between mutator and collector code, while remaining simple and flexible enough to be used with a wide variety of source languages and collector styles. Front ends targeting GCminor can be implemented using any compiler technology and any desired degree of verification, including full semantics preservation, type preservation, or informal trust. As an example application of our framework, we describe a compiler for Haskell that translates the Glasgow Haskell Compiler's Core intermediate language to GCminor. To support a simple but useful memory safety argument for this compiler, the front end uses a novel combination of type preservation and runtime checks, which is of independent interest.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '10},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2010},
  pages = {273--284},
  keywords = {certified compilation,garbage collection,haskell compilation,program proof,the Coq theorem prover},
  author = {McCreight, Andrew and Chevalier, Tim and Tolmach, Andrew},
  file = {/Users/pgiarrusso/Zotero/storage/9TXU8UEW/McCreight et al - 2010 - A Certified Framework for Compiling and Executing Garbage-collected Languages.pdf}
}

@inproceedings{Piumarta2011open,
  location = {{New York, NY, USA}},
  title = {Open, Extensible Composition Models},
  isbn = {978-1-4503-0892-2},
  url = {http://doi.acm.org/10.1145/2068776.2068778},
  doi = {10.1145/2068776.2068778},
  abstract = {Simple functional languages like LISP are useful for exploring novel semantics and composition mechanisms. That usefulness can be limited by the assumptions built into the evaluator about the structure of data and the meaning of expressions. These assumptions create difficulties when a program introduces a composition mechanism that differs substantially from the built-in mechanism of function application. We explore how an evaluator can be constructed to eliminate most built-in assumptions about meaning, and show how new composition mechanisms can be introduced easily and seamlessly into the language it evaluates.},
  booktitle = {Proceedings of the 1st International Workshop on Free Composition},
  series = {FREECO '11},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2011},
  pages = {2:1--2:5},
  author = {Piumarta, Ian},
  file = {/Users/pgiarrusso/Zotero/storage/NTWJ33CG/Piumarta - 2011 - Open, Extensible Composition Models.pdf}
}

@article{Tate2011equality,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1012.1802},
  title = {Equality Saturation: A New Approach to Optimization},
  volume = {7},
  issn = {18605974},
  url = {http://arxiv.org/abs/1012.1802},
  doi = {10.2168/LMCS-7(1:10)2011},
  shorttitle = {Equality Saturation},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  number = {1},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2014-09-20},
  date = {2011-03-28},
  keywords = {Computer Science - Programming Languages,D.3.4},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  file = {/Users/pgiarrusso/Zotero/storage/QMCKP66Q/Tate et al. - 2011 - Equality Saturation A New Approach to Optimizatio.html}
}
% == BibLateX quality report for Tate2011equality:
% Unexpected field 'archivePrefix'

@inproceedings{Tate2010generating,
  location = {{New York, NY, USA}},
  title = {Generating Compiler Optimizations from Proofs},
  isbn = {978-1-60558-479-9},
  url = {http://doi.acm.org/10.1145/1706299.1706345},
  doi = {10.1145/1706299.1706345},
  abstract = {We present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proof-carrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules. We present a category-theoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in debugging type errors. Finally, we show experimentally that our technique enables programmers to train a compiler with application-specific optimizations by providing concrete examples of original programs and the desired transformed programs. We also show how it enables a compiler to learn efficient-to-run optimizations from expensive-to-run super-optimizers.},
  booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '10},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2010},
  pages = {389--402},
  keywords = {compiler optimization,explanation-based learning,proof generalization},
  author = {Tate, Ross and Stepp, Michael and Lerner, Sorin},
  file = {/Users/pgiarrusso/Zotero/storage/PIXBHVIG/Tate et al - 2010 - Generating Compiler Optimizations from Proofs.pdf}
}

@inproceedings{Gill2009typesafe,
  location = {{New York, NY, USA}},
  title = {Type-safe Observable Sharing in Haskell},
  isbn = {978-1-60558-508-6},
  url = {http://doi.acm.org/10.1145/1596638.1596653},
  doi = {10.1145/1596638.1596653},
  abstract = {Haskell is a great language for writing and supporting embedded Domain Specific Languages (DSLs). Some form of observable sharing is often a critical capability for allowing so-called deep DSLs to be compiled and processed. In this paper, we describe and explore uses of an IO function for reification which allows direct observation of sharing.},
  booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '09},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2009},
  pages = {117--128},
  keywords = {DSL compilation,observable sharing},
  author = {Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/MQA6I43T/Gill - 2009 - Type-safe Observable Sharing in Haskell.pdf}
}

@inproceedings{Gibbons2014folding,
  location = {{New York, NY, USA}},
  title = {Folding Domain-specific Languages: Deep and Shallow Embeddings (Functional Pearl)},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628138},
  doi = {10.1145/2628136.2628138},
  shorttitle = {Folding Domain-specific Languages},
  abstract = {A domain-specific language can be implemented by embedding within a general-purpose host language. This embedding may be deep or shallow, depending on whether terms in the language construct syntactic or semantic representations. The deep and shallow styles are closely related, and intimately connected to folds; in this paper, we explore that connection.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2014},
  pages = {339--347},
  keywords = {domain-specific languages,deep and shallow embedding,folds},
  author = {Gibbons, Jeremy and Wu, Nicolas},
  file = {/Users/pgiarrusso/Zotero/storage/IEZU3J6G/Gibbons_Wu - 2014 - Folding Domain-specific Languages - Deep and Shallow Embeddings (Functional Pearl).pdf}
}

@inproceedings{Schoepe2014selinq,
  location = {{New York, NY, USA}},
  title = {SeLINQ: Tracking Information Across Application-database Boundaries},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628151},
  doi = {10.1145/2628136.2628151},
  shorttitle = {SeLINQ},
  abstract = {The root cause for confidentiality and integrity attacks against computing systems is insecure information flow. The complexity of modern systems poses a major challenge to secure end-to-end information flow, ensuring that the insecurity of a single component does not render the entire system insecure. While information flow in a variety of languages and settings has been thoroughly studied in isolation, the problem of tracking information across component boundaries has been largely out of reach of the work so far. This is unsatisfactory because tracking information across component boundaries is necessary for end-to-end security. This paper proposes a framework for uniform tracking of information flow through both the application and the underlying database. Key enabler of the uniform treatment is recent work by Cheney et al., which studies database manipulation via an embedded language-integrated query language (with Microsoft's LINQ on the backend). Because both the host language and the embedded query languages are functional F\#-like languages, we are able to leverage information-flow enforcement for functional languages to obtain information-flow control for databases "for free", synergize it with information-flow control for applications and thus guarantee security across application-database boundaries. We develop the formal results in the form of a security type system that includes a treatment of algebraic data types and pattern matching, and establish its soundness. On the practical side, we implement the framework and demonstrate its usefulness in a case study with a realistic movie rental database.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2014},
  pages = {25--38},
  keywords = {end-to-end security,information flow,language-integrated queries,static analysis},
  author = {Schoepe, Daniel and Hedin, Daniel and Sabelfeld, Andrei},
  file = {/Users/pgiarrusso/Zotero/storage/73SNNMQ2/Schoepe et al - 2014 - SeLINQ - Tracking Information Across Application-database Boundaries.pdf}
}

@inproceedings{Mulligan2014lem,
  location = {{New York, NY, USA}},
  title = {Lem: Reusable Engineering of Real-world Semantics},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628143},
  doi = {10.1145/2628136.2628143},
  shorttitle = {Lem},
  abstract = {Recent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of real-world processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many person-years effort and maintained over an extended period, cannot be used by those familiar with another. We introduce Lem, a language for engineering reusable large-scale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations - akin to compilation, but subject to the constraint of producing usable and human-readable code for each target. Lem's effectiveness is demonstrated by its use in practice.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2014},
  pages = {175--188},
  keywords = {proof assistants,lem,real-world semantics,specification languages},
  author = {Mulligan, Dominic P. and Owens, Scott and Gray, Kathryn E. and Ridge, Tom and Sewell, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/XCTNZUFB/Mulligan et al - 2014 - Lem - Reusable Engineering of Real-world Semantics.pdf}
}

@inproceedings{Pottier2014hindleymilner,
  location = {{New York, NY, USA}},
  title = {Hindley-milner Elaboration in Applicative Style: Functional Pearl},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628145},
  doi = {10.1145/2628136.2628145},
  shorttitle = {Hindley-milner Elaboration in Applicative Style},
  abstract = {Type inference - the problem of determining whether a program is well-typed - is well-understood. In contrast, elaboration - the task of constructing an explicitly-typed representation of the program - seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2014},
  pages = {203--212},
  keywords = {polymorphism,elaboration,type inference,constraints},
  author = {Pottier, François},
  file = {/Users/pgiarrusso/Zotero/storage/JGUWSB5H/Pottier - 2014 - Hindley-milner Elaboration in Applicative Style - Functional Pearl.pdf}
}

@inproceedings{Breitner2014safe,
  location = {{New York, NY, USA}},
  title = {Safe Zero-cost Coercions for Haskell},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628141},
  doi = {10.1145/2628136.2628141},
  abstract = {Generative type abstractions -- present in Haskell, OCaml, and other languages -- are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  urldate = {2014-09-20},
  date = {2014},
  pages = {189--202},
  keywords = {Haskell,coercion,newtype deriving,type class},
  author = {Breitner, Joachim and Eisenberg, Richard A. and Peyton Jones, Simon and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/HVSPIPRP/Breitner et al - 2014 - Safe Zero-cost Coercions for Haskell.pdf}
}

@inproceedings{Puech2012safe,
  langid = {english},
  location = {{Philadelphia, United States}},
  title = {Safe Incremental Type Checking},
  url = {http://hal.archives-ouvertes.fr/hal-00650341},
  abstract = {We study the problem of verifying the well-typing of terms, not in a batch fashion, as it is usually the case for typed languages, but incrementally, that is by sequentially modifying a term, and re- verifying each time only a smaller amount of information than the whole term, still ensuring that it is well-typed.},
  booktitle = {TLDI 2012 - Seventh ACM SIGPLAN Workshop on Types in Language Design and Implementation},
  urldate = {2014-09-21},
  date = {2012-01},
  keywords = {type checking,incrementality,logical framework,ver- sion control},
  author = {Puech, Matthias and Régis-Gianas, Yann},
  file = {/Users/pgiarrusso/Zotero/storage/BW8NSP94/Puech_Régis-Gianas - 2012 - Safe Incremental Type Checking.pdf;/Users/pgiarrusso/Zotero/storage/39UKMS9T/Puech and Régis-Gianas - 2012 - Safe Incremental Type Checking.html},
  note = {2 pages}
}
% == BibLateX quality report for Puech2012safe:
% ? Unsure about the formatting of the booktitle

@incollection{Pfenning2002logical,
  langid = {english},
  title = {Logical Frameworks—A Brief Introduction},
  isbn = {978-1-4020-0608-1 978-94-010-0413-8},
  url = {http://link.springer.com/chapter/10.1007/978-94-010-0413-8_5},
  abstract = {A logical framework is a meta-language for the formalization of deductive systems. We provide a brief introduction to logical frameworks and their methodology, concentrating on LF. We use first-order logic as the running example to illustrate the representations of syntax, natural deductions, and proof transformations. We also sketch a recent formulation of LF centered on the notion of canonical form, and show how it affects proofs of adequacy of encodings.},
  number = {62},
  booktitle = {Proof and System-Reliability},
  series = {NATO Science Series},
  publisher = {{Springer Netherlands}},
  urldate = {2014-09-22},
  date = {2002-01-01},
  pages = {137-166},
  keywords = {type theory,Theory of Computation,logical frameworks,Mathematical Logic and Foundations,Logic,Computing Methodologies},
  author = {Pfenning, Frank},
  editor = {Schwichtenberg, Helmut and Steinbrüggen, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/SCHS9F3R/Pfenning - 2002 - Logical Frameworks—A Brief Introduction.pdf;/Users/pgiarrusso/Zotero/storage/AF7K2952/978-94-010-0413-8_5.html}
}
% == BibLateX quality report for Pfenning2002logical:
% 'isbn': not a valid ISBN

@inproceedings{Adams2010scrap,
  location = {{New York, NY, USA}},
  title = {Scrap Your Zippers: A Generic Zipper for Heterogeneous Types},
  isbn = {978-1-4503-0251-7},
  url = {http://doi.acm.org/10.1145/1863495.1863499},
  doi = {10.1145/1863495.1863499},
  shorttitle = {Scrap Your Zippers},
  abstract = {The zipper type provides the ability to efficiently edit tree-shaped data in a purely functional setting by providing constant time edits at a focal point in an immutable structure. It is used in a number of applications and is widely applicable for manipulating tree-shaped data structures. The traditional zipper suffers from two major limitations, however. First, it operates only on homogeneous types. That is to say, every node the zipper visits must have the same type. In practice, many tree-shaped types do not satisfy this condition, and thus cannot be handled by the traditional zipper. Second, the traditional zipper involves a significant amount of boilerplate code. A custom implementation must be written for each type the zipper traverses. This is error prone and must be updated whenever the type being traversed changes. The generic zipper presented in this paper overcomes these limitations. It operates over any type and requires no boilerplate code to be written by the user. The only restriction is that the types traversed must be instances of the Data class from the Scrap your Boilerplate framework},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '10},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {2010},
  pages = {13--24},
  keywords = {generic programming,heterogeneous types,scrap your boilerplate,zippers},
  author = {Adams, Michael D.},
  file = {/Users/pgiarrusso/Zotero/storage/7TIKEGE7/Adams - 2010 - Scrap Your Zippers - A Generic Zipper for Heterogeneous Types.pdf}
}

@inproceedings{Axelsson2012generic,
  location = {{New York, NY, USA}},
  title = {A Generic Abstract Syntax Model for Embedded Languages},
  isbn = {978-1-4503-1054-3},
  url = {http://doi.acm.org/10.1145/2364527.2364573},
  doi = {10.1145/2364527.2364573},
  abstract = {Representing a syntax tree using a data type often involves having many similar-looking constructors. Functions operating on such types often end up having many similar-looking cases. Different languages often make use of similar-looking constructions. We propose a generic model of abstract syntax trees capable of representing a wide range of typed languages. Syntactic constructs can be composed in a modular fashion enabling reuse of abstract syntax and syntactic processing within and across languages. Building on previous methods of encoding extensible data types in Haskell, our model is a pragmatic solution to Wadler's "expression problem". Its practicality has been confirmed by its use in the implementation of the embedded language Feldspar.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '12},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {2012},
  pages = {323--334},
  keywords = {generic programming,embedded domain-specific languages,the expression problem},
  author = {Axelsson, Emil},
  file = {/Users/pgiarrusso/Zotero/storage/QRUXPAKP/Axelsson - 2012 - A Generic Abstract Syntax Model for Embedded Languages.pdf}
}

@inproceedings{Hamana2011foundation,
  location = {{New York, NY, USA}},
  title = {A Foundation for GADTs and Inductive Families: Dependent Polynomial Functor Approach},
  isbn = {978-1-4503-0861-8},
  url = {http://doi.acm.org/10.1145/2036918.2036927},
  doi = {10.1145/2036918.2036927},
  shorttitle = {A Foundation for GADTs and Inductive Families},
  abstract = {Every Algebraic Datatype (ADT) is characterised as the initial algebra of a polynomial functor on sets. This paper extends the characterisation to the case of more advanced datatypes: Generalised Algebraic Datatypes (GADTs) and Inductive Families. Specifically, we show that GADTs and Inductive Families are characterised as initial algebras of dependent polynomial functors. The theoretical tool we use throughout is an abstract notion of polynomial between sets together with its associated general form of polynomial functor between categories of indexed sets introduced by Gambino and Hyland. In the context of ADTs, this fundamental result is the basis for various generic functional programming techniques. To establish the usefulness of our approach for such developments in the broader context of inductively defined dependent types, we apply the theory to construct zippers for Inductive Families.},
  booktitle = {Proceedings of the Seventh ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '11},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {2011},
  pages = {59--70},
  keywords = {dependent types,categorical semantics},
  author = {Hamana, Makoto and Fiore, Marcelo},
  file = {/Users/pgiarrusso/Zotero/storage/7HH8IVWS/Hamana_Fiore - 2011 - A Foundation for GADTs and Inductive Families - Dependent Polynomial Functor Approach.pdf}
}

@inproceedings{Jones1995mix,
  location = {{New York, NY, USA}},
  title = {Mix Ten Years Later},
  isbn = {0-89791-720-0},
  url = {http://doi.acm.org/10.1145/215465.215468},
  doi = {10.1145/215465.215468},
  booktitle = {Proceedings of the 1995 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation},
  series = {PEPM '95},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1995},
  pages = {24--38},
  author = {Jones, Neil D.}
}

@inproceedings{Danvy1997lambdadropping,
  location = {{New York, NY, USA}},
  title = {Lambda-dropping: Transforming Recursive Equations into Programs with Block Structure},
  isbn = {0-89791-917-3},
  url = {http://doi.acm.org/10.1145/258993.259007},
  doi = {10.1145/258993.259007},
  shorttitle = {Lambda-dropping},
  abstract = {Lambda-lifting a functional program transforms it into a set of recursive equations. We present the symmetric transformation: lambda-dropping. Lambda-dropping a set of recursive equations restores block structure and lexical scope.For lack of scope, recursive equations must carry around all the parameters that any of their callees might possibly need. Both lambda-lifting and lambda-dropping thus require one to compute a transitive closure over the call graph:\&bull; for lambda-lifting: to establish the Def/Use path of each free variable (these free variables are then added as parameters to each of the functions in the call path);\&bull; for lambda-dropping: to establish the Def/Use path of each parameter (parameters whose use occurs in the same scope as their definition do not need to be passed along in the call path).Without free variables, a program is scope-insensitive. Its blocks are then free to float (for lambda-lifting) or to sink (for lambda-dropping) along the vertices of the scope tree.We believe lambda-lifting and lambda-dropping are interesting per se, both in principle and in practice, but our prime application is partial evaluation: except for Malmkj\&aelig;r and \&Oslash;rb\&aelig;k's case study presented at PEPM '95, most polyvariant specializers for procedural programs operate on recursive equations. To this end, in a pre-processing phase, they lambda-lift source programs into recursive equations, As a result, residual programs are also expressed as recursive equations, often with dozens of parameters, which most compilers do not handle efficiently. Lambda-dropping in a post-processing phase restores their block structure and lexical scope thereby significantly reducing both the compile time and the run time of residual programs.},
  booktitle = {Proceedings of the 1997 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation},
  series = {PEPM '97},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1997},
  pages = {90--106},
  author = {Danvy, Olivier and Schultz, Ulrik P.}
}

@report{Leone1993deferred,
  location = {{Pittsburgh, PA, USA}},
  title = {Deferred Compilation: The Automation of Run-Time Code Generation},
  shorttitle = {Deferred Compilation},
  abstract = {This paper describes deferred compilation, an alternative and complement to compile-time program analysis and optimization. By deferring aspects of compilation to run time, exact information about programs can be exploited, leading to greater opportunities for code improvement. This is in contrast to the use of static analyses, which are inherently conservative. Deferred compilation automates the translation of ordinary programs into native machine code that performs fast optimization and native-code generation at run time. Automation is obtained through the use of a compile-time staging analysis, which determines the portions of a program that may be safely and profitably compiled at run time. Fast run-time optimization is obtained by trading space for time: compile-time specialization yields numerous run-time code generators, each customized to optimize a small portion of the source program based on run-time information. Implementation strategies developed for a prototype compiler are discussed, and the results of preliminary experiments demonstrating significant overall speedup are presented.},
  institution = {{Carnegie Mellon University}},
  date = {1993},
  author = {Leone, Mark and Lee, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/WD2Z49RT/Leone_Lee - 1993 - Deferred Compilation - The Automation of Run-Time Code Generation.pdf}
}
% == BibLateX quality report for Leone1993deferred:
% Missing required field 'type'

@inproceedings{Consel1993tutorial,
  location = {{New York, NY, USA}},
  title = {Tutorial Notes on Partial Evaluation},
  isbn = {0-89791-560-7},
  url = {http://doi.acm.org/10.1145/158511.158707},
  doi = {10.1145/158511.158707},
  abstract = {The last years have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes survey the field and present a critical assessment of the state of the art.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '93},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1993},
  pages = {493--501},
  author = {Consel, Charles and Danvy, Olivier},
  file = {/Users/pgiarrusso/Zotero/storage/9JXSXQTR/Consel_Danvy - 1993 - Tutorial Notes on Partial Evaluation.pdf}
}

@article{Futamura1999partial,
  langid = {english},
  title = {Partial Evaluation of Computation Process--An Approach to a Compiler-Compiler},
  volume = {12},
  issn = {1388-3690, 1573-0557},
  url = {http://link.springer.com/article/10.1023/A%3A1010095604496},
  doi = {10.1023/A:1010095604496},
  abstract = {This paper reports the relationship between formal description of semantics (i.e., interpreter) of a programming language and an actual compiler. The paper also describes a method to automatically generate an actual compiler from a formal description which is, in some sense, the partial evaluation of a computation process. The compiler-compiler inspired by this method differs from conventional ones in that the compiler-compiler based on our method can describe an evaluation procedure (interpreter) in defining the semantics of a programming language, while the conventional one describes a translation process.},
  number = {4},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order and Symbolic Computation},
  urldate = {2014-10-02},
  date = {1999-12-01},
  pages = {381-391},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),program transformation,Futamura projections,Software Engineering/Programming and Operating Systems,Numeric Computing,partial evaluation,compiler,interpreter},
  author = {Futamura, Yoshihiko},
  file = {/Users/pgiarrusso/Zotero/storage/VPNQ2ZUS/Futamura - 1999 - Partial Evaluation of Computation Process--An Approach to a Compiler-Compiler.pdf;/Users/pgiarrusso/Zotero/storage/F3Q5QG8E/A1010095604496.html}
}
% == BibLateX quality report for Futamura1999partial:
% 'issn': not a valid ISSN

@inproceedings{Friedman1984reification,
  location = {{New York, NY, USA}},
  title = {Reification: Reflection Without Metaphysics},
  isbn = {0-89791-142-3},
  url = {http://doi.acm.org/10.1145/800055.802051},
  doi = {10.1145/800055.802051},
  shorttitle = {Reification},
  abstract = {We consider how the data structures of an interpreter may be made available to the program it is running, and how the program may alter its interpreter's structures. We refer to these processes as reification and reflection. We show how these processes may be considered as an extension of the fexpr concept in which not only the form and the environment, but also the continuation, are made available to the body of the procedure. We show how such a construct can be used to effectively add an unlimited variety of “special forms” to a very small base language. We consider some trade-offs in how interpreter objects are reified. Our version of this construct is similar to one in 3-Lisp [Smith 82, 84], but is independent of the rest of 3-Lisp. In particular, it does not rely on the notion of a tower of interpreters.},
  booktitle = {Proceedings of the 1984 ACM Symposium on LISP and Functional Programming},
  series = {LFP '84},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1984},
  pages = {348--355},
  author = {Friedman, Daniel P. and Wand, Mitchell},
  file = {/Users/pgiarrusso/Zotero/storage/TQ6EFRH3/Friedman_Wand - 1984 - Reification - Reflection Without Metaphysics.pdf}
}

@inproceedings{Weise1993programmable,
  location = {{New York, NY, USA}},
  title = {Programmable Syntax Macros},
  isbn = {0-89791-598-4},
  url = {http://doi.acm.org/10.1145/155090.155105},
  doi = {10.1145/155090.155105},
  abstract = {Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP (the C preprocessor), syntax macros operate on Abstract Syntax Trees (ASTs). Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, difficult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems by having the macro language be a  minimal extension of the programming language, by introducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro definition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros.},
  booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
  series = {PLDI '93},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1993},
  pages = {156--165},
  author = {Weise, Daniel and Crew, Roger},
  file = {/Users/pgiarrusso/Zotero/storage/A85IQIQV/Weise_Crew - 1993 - Programmable Syntax Macros.pdf;/Users/pgiarrusso/Zotero/storage/CCPC4DIR/Weise_Crew - 1993 - Programmable Syntax Macros.ps}
}

@article{Wand1988mystery,
  langid = {english},
  title = {The mystery of the tower revealed: A nonreflective description of the reflective tower},
  volume = {1},
  issn = {0892-4635, 1573-0557},
  url = {http://link.springer.com/article/10.1007/BF01806174},
  doi = {10.1007/BF01806174},
  shorttitle = {The mystery of the tower revealed},
  abstract = {In an important series of papers [8, 9], Brian Smith has discussed the nature of programs that know about their text and the context in which they are executed. He called this kind of knowledgereflection. Smith proposed a programming language, called 3-LISP, which embodied such self-knowledge in the domain of metacircular interpreters. Every 3-LISP program is interpreted by a metacircular interpreter, also written in 3-LISP. This gives rise to a picture of an infinite tower of metacircular interpreters, each being interpreted by the one above it. Such a metaphor poses a serious challenge for conventional modes of understanding of programming languages. In our earlier work on reflection [4], we showed how a useful species of reflection could be modeled without the use of towers. In this paper, we give a semantic account of the reflective tower. This account is self-contained in the sense that it does not employ reflection to explain reflection.},
  number = {1},
  journaltitle = {LISP and Symbolic Computation},
  shortjournal = {Lisp and Symbolic Computation},
  urldate = {2014-10-02},
  date = {1988-06-01},
  pages = {11-38},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Software Engineering/Programming and Operating Systems,Numeric Computing},
  author = {Wand, Mitchell and Friedman, Daniel P.},
  file = {/Users/pgiarrusso/Zotero/storage/SSR2JT4T/Wand_Friedman - 1988 - The mystery of the tower revealed - A nonreflective description of the reflective tower.pdf;/Users/pgiarrusso/Zotero/storage/6F2QAJVN/BF01806174.html}
}
% == BibLateX quality report for Wand1988mystery:
% 'issn': not a valid ISSN

@inproceedings{Demers1995reflection,
  title = {Reflection in logic, functional and object-oriented programming: a Short Comparative Study},
  shorttitle = {Reflection in logic, functional and object-oriented programming},
  abstract = {Reflection is a wide-ranging concept that has been studied independently in many different areas of science in general, and computer science in particular. Even in the sub-area of programming languages, it has been applied to different paradigms, especially the logic, functional and objectoriented ones. Partly because of different past influences, but also because researchers in these communities scarcely talk to each others, concepts have evolved separately, sometimes to the point where it is hard for people in one community to recognize similarities in the work of others, not to speak about cross-fertilization among them. In this paper, we propose a synthesis covering mainly the application of computation reflection to programming languages. We compare the different approaches and try to identify similar concepts hidden behind different names or constructs. We also point out the different emphasis that has been given to different concepts in each of them. We do not claim neither comp...},
  booktitle = {In IJCAI '95 Workshop on Reflection and Metalevel Architectures and their Applications in AI},
  date = {1995},
  pages = {29--38},
  author = {Demers, François-Nicola and Malenfant, Jacques},
  file = {/Users/pgiarrusso/Zotero/storage/F77ZI66N/Demers_Malenfant - 1995 - Reflection in logic, functional and object-oriented programming - a Short Comparative Study.pdf;/Users/pgiarrusso/Zotero/storage/PM6UGWS3/Demers_Malenfant - 1995 - Reflection in logic, functional and object-oriented programming - a Short Comparative Study.pdf;/Users/pgiarrusso/Zotero/storage/WTFPD2JV/summary.html}
}

@inproceedings{Meyerovich2009flapjax,
  location = {{New York, NY, USA}},
  title = {Flapjax: A Programming Language for Ajax Applications},
  isbn = {978-1-60558-766-0},
  url = {http://doi.acm.org/10.1145/1640089.1640091},
  doi = {10.1145/1640089.1640091},
  shorttitle = {Flapjax},
  abstract = {This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces. Flapjax provides two key features that simplify writing these applications. First, it provides event streams, a uniform abstraction for communication within a program as well as with external Web services. Second, the language itself is reactive: it automatically tracks data dependencies and propagates updates along those dataflows. This allows developers to write reactive interfaces in a declarative and compositional style. Flapjax is built on top of JavaScript. It runs on unmodified browsers and readily interoperates with existing JavaScript code. It is usable as either a programming language (that is compiled to JavaScript) or as a JavaScript library, and is designed for both uses. This paper presents the language, its design decisions, and illustrative examples drawn from several working Flapjax applications.},
  booktitle = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '09},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {2009},
  pages = {1--20},
  keywords = {functional reactive programming,javascript,web programming},
  author = {Meyerovich, Leo A. and Guha, Arjun and Baskin, Jacob and Cooper, Gregory H. and Greenberg, Michael and Bromfield, Aleks and Krishnamurthi, Shriram},
  file = {/Users/pgiarrusso/Zotero/storage/5NRIRHXH/Meyerovich et al - 2009 - Flapjax - A Programming Language for Ajax Applications.pdf}
}

@article{Davies2001modal,
  title = {A Modal Analysis of Staged Computation},
  volume = {48},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/382780.382785},
  doi = {10.1145/382780.382785},
  abstract = {We show that a type system based on the intuitionistic modal logic S4  provides an expressive framework for specifying and analyzing computation stages in the context of typed \&lgr;-calculi and functional languages. We directly demonstrate the sense in which our   l→□e -calculus captures staging, and also give a conservative embeddng of Nielson and Nielson's two-level functional language in our functional language Mini-ML  □ , thus proving  that binding-time correctness is equivalent to modal correctness on this fragment. In addition,   Mini-ML□    can also express immediate evaluation and sharing of code across multiple stages, thus supporting run-time code generation as well as partial evaluation.},
  number = {3},
  journaltitle = {J. ACM},
  urldate = {2014-10-02},
  date = {2001-05},
  pages = {555--604},
  keywords = {binding times,run-time code generation,staged computation},
  author = {Davies, Rowan and Pfenning, Frank},
  file = {/Users/pgiarrusso/Zotero/storage/T8VCWK7X/Davies_Pfenning - 2001 - A Modal Analysis of Staged Computation.pdf}
}
% == BibLateX quality report for Davies2001modal:
% ? Possibly abbreviated journal title J. ACM

@inproceedings{Jones1997henk,
  title = {Henk: A Typed Intermediate Language},
  shorttitle = {Henk},
  abstract = {There is growing interest in the use of richly-typed intermediate languages in sophisticated compilers for higher-order, typed source languages. These intermediate languages are typically stratified, involving terms, types, and kinds. As the sophistication of the type system increases, these three levels begin to look more and more similar, so an attractive approach is to use a single syntax, and a single data type in the compiler, to represent all three. The theory of so-called pure type systems makes precisely such an identification. This paper describes Henk, a new typed intermediate language based closely on a particular pure type system, the lambda cube. On the way we give a tutorial introduction to the lambda cube. 1 Overview  Many compilers can be divided into three main stages. The  front end translates the source language into an intermediate  language; the middle end transforms the intermediatelanguage into a more efficient form; and the back end translates the intermediate l...},
  booktitle = {In Proc. First Int'l Workshop on Types in Compilation},
  date = {1997},
  author = {Jones, Simon Peyton and Meijer, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/HWKVBA86/Jones_Meijer - 1997 - Henk - A Typed Intermediate Language.pdf;/Users/pgiarrusso/Zotero/storage/XENJPNQS/summary.html}
}
% == BibLateX quality report for Jones1997henk:
% ? Unsure about the formatting of the booktitle

@inproceedings{Bawden1999quasiquotation,
  title = {Quasiquotation in Lisp (2)},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.2566},
  abstract = {Quasiquotation is the technology commonly used in Lisp to write program-generating
programs. This paper explains how quasiquotation works, why it works well, and what its limitations
are. A brief history of quasiquotation is included.},
  eventtitle = {Partial Evaluation and Semantic-Based Program Manipulation},
  urldate = {2014-10-02},
  date = {1999},
  pages = {4-12},
  keywords = {metaprogramming,lisp,quasiquote,quote},
  author = {Bawden, Alan}
}
% == BibLateX quality report for Bawden1999quasiquotation:
% Missing required field 'booktitle'

@inproceedings{Bawden1999quasiquotationa,
  title = {Quasiquotation in Lisp (1)},
  abstract = {Quasiquotation is the technology commonly used in Lisp to write program-generating programs. In this paper I will review the history and development of this technology, and explain why it works so well in practice.},
  booktitle = {O. Danvy, Ed., University of Aarhus, Dept. of Computer Science},
  date = {1999},
  pages = {88--99},
  author = {Bawden, Alan},
  file = {/Users/pgiarrusso/Zotero/storage/KTTS4WNN/Bawden - 1999 - Quasiquotation in Lisp.pdf;/Users/pgiarrusso/Zotero/storage/W2WP4UR5/summary.html}
}
% == BibLateX quality report for Bawden1999quasiquotationa:
% ? Unsure about the formatting of the booktitle

@incollection{Herman2008theory,
  langid = {english},
  title = {A Theory of Hygienic Macros},
  isbn = {978-3-540-78738-9 978-3-540-78739-6},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-78739-6_4},
  abstract = {Hygienic macro systems, such as Scheme’s, automatically rename variables to prevent unintentional variable capture—in short, they “just work.” Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  number = {4960},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-10-02},
  date = {2008-01-01},
  pages = {48-62},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Data Structures,Mathematical Logic and Formal Languages},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  file = {/Users/pgiarrusso/Zotero/storage/VMI5QABV/Herman_Wand - 2008 - A Theory of Hygienic Macros.pdf;/Users/pgiarrusso/Zotero/storage/T9WHTJTX/978-3-540-78739-6_4.html}
}
% == BibLateX quality report for Herman2008theory:
% 'isbn': not a valid ISBN

@inproceedings{Sheard2002template,
  location = {{New York, NY, USA}},
  title = {Template Meta-programming for Haskell},
  isbn = {1-58113-605-6},
  url = {http://doi.acm.org/10.1145/581690.581691},
  doi = {10.1145/581690.581691},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  booktitle = {Proceedings of the 2002 ACM SIGPLAN Workshop on Haskell},
  series = {Haskell '02},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {2002},
  pages = {1--16},
  keywords = {meta programming,templates},
  author = {Sheard, Tim and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/6N4FGT26/Sheard_Jones - 2002 - Template Meta-programming for Haskell.pdf}
}

@inproceedings{Consel1990binding,
  location = {{New York, NY, USA}},
  title = {Binding Time Analysis for High Order Untyped Functional Languages},
  isbn = {0-89791-368-X},
  url = {http://doi.acm.org/10.1145/91556.91668},
  doi = {10.1145/91556.91668},
  abstract = {When some inputs of a program are known at compile-time, certain expressions can be processed statically; this is the basis of the notion of partial evaluation. Identifying these early computations can be determined independently of the actual values of the input by a static analysis called binding time analysis. Then, to process a program, one simply follows the binding time information: evaluate compile-time expressions and defer the others to run-time.
Using abstract interpretation, we present a binding time analysis for an untyped functional language which provides an effective treatment of both higher order functions and data structures. To our knowledge it is the first such analysis. It has been implemented and is used in a partial evaluator for a side-effect free dialect of Scheme. The analysis is general enough, however, to be valid for non-strict typed functional languages such as Haskell. Our approach and the system we have developed solve and go beyond the open problem of partially evaluating higher order functions described in [3] since we also provide a method to handle data structures.
Our analysis improves on previous work [5, 15, 4] in that: (1) it treats both higher order functions and data structures, (2) it does not impose syntactic restrictions on the program being processed, and (3) it does not require a preliminary phase to collect the set of possible functions that may occur at each site of application.},
  booktitle = {Proceedings of the 1990 ACM Conference on LISP and Functional Programming},
  series = {LFP '90},
  publisher = {{ACM}},
  urldate = {2014-10-02},
  date = {1990},
  pages = {264--272},
  author = {Consel, Charles},
  file = {/Users/pgiarrusso/Zotero/storage/TRRS9FNC/Consel - 1990 - Binding Time Analysis for High Order Untyped Functional Languages.pdf}
}

@incollection{Linger2004bindingtime,
  langid = {english},
  title = {Binding-Time Analysis for MetaML via Type Inference and Constraint Solving},
  isbn = {978-3-540-21299-7 978-3-540-24730-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-24730-2_22},
  abstract = {The two predominant program specialization techniques, partial evaluation and staged programming, take opposite approaches to automating binding-time analysis (BTA). Despite their common goal, there are no systems integrating both methods. Programmers must choose between the precision of manually placing staging annotations and the convenience of automating such annotation. We present an automatic BTA algorithm for a subset of MetaML. Such an algorithm provides a basis for a system integrating staged programming and partial evaluation because it allows programmers to switch between automatic and manual staging. Our algorithm is based on typing algorithm coupled with arithmetic-constraint solving. The algorithm decorates each subexpression of both a program and its type with numeric variables representing staging-annotations and then generates simple arithmetic constraints that describe the space of all possible stagings of the original program. Benefits of our approach include expressive BTA specifications in the form of stage-annotated types as well as support for polyvariance.},
  number = {2988},
  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-10-02},
  date = {2004-01-01},
  pages = {266-279},
  keywords = {Software Engineering,Logics and Meanings of Programs,Algorithm Analysis and Problem Complexity,Computer Communication Networks},
  author = {Linger, Nathan and Sheard, Tim},
  editor = {Jensen, Kurt and Podelski, Andreas},
  file = {/Users/pgiarrusso/Zotero/storage/UP5722QC/Linger_Sheard - 2004 - Binding-Time Analysis for MetaML via Type Inference and Constraint Solving.pdf;/Users/pgiarrusso/Zotero/storage/E6K5F7UU/978-3-540-24730-2_22.html}
}
% == BibLateX quality report for Linger2004bindingtime:
% 'isbn': not a valid ISBN

@inproceedings{Cook1992interfaces,
  location = {{New York, NY, USA}},
  title = {Interfaces and Specifications for the Smalltalk-80 Collection Classes},
  isbn = {0-201-53372-3},
  url = {http://doi.acm.org/10.1145/141936.141938},
  doi = {10.1145/141936.141938},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages, and Applications},
  series = {OOPSLA '92},
  publisher = {{ACM}},
  urldate = {2014-10-18},
  date = {1992},
  pages = {1--15},
  author = {Cook, William R.}
}

@inproceedings{Black2003applying,
  location = {{New York, NY, USA}},
  title = {Applying Traits to the Smalltalk Collection Classes},
  isbn = {1-58113-712-5},
  url = {http://doi.acm.org/10.1145/949305.949311},
  doi = {10.1145/949305.949311},
  abstract = {Traits are a programming language technology that promote the reuse of methods between unrelated classes. This paper reports on a refactoring of the Smalltalk collections classes using traits. The original collection classes contained much duplication of code; traits let us remove all of it. We also found places where the protocols of the collections lacked uniformity; traits allowed us to correct these non-uniformities without code duplication.Traits also make it possible to reuse fragments of collection code outside of the existing hierarchy; for example, they make it easy to convert other collection-like things into true collections. Our refactoring reduced the number of methods in the collection classes by approximately 10 per cent. More importantly, understandability maintainability and reusability of the code were significantly improved.},
  booktitle = {Proceedings of the 18th Annual ACM SIGPLAN Conference on Object-oriented Programing, Systems, Languages, and Applications},
  series = {OOPSLA '03},
  publisher = {{ACM}},
  urldate = {2014-10-18},
  date = {2003},
  pages = {47--64},
  keywords = {Smalltalk,collection hierarchy,inheritance,mixins,multiple Inheritance,refactoring,reuse,stream classes,traits},
  author = {Black, Andrew P. and Schärli, Nathanael and Ducasse, Stéphane},
  file = {/Users/pgiarrusso/Zotero/storage/674JAJSC/Black et al - 2003 - Applying Traits to the Smalltalk Collection Classes.pdf}
}

@article{Taha2000metaml,
  title = {MetaML and multi-stage programming with explicit annotations},
  volume = {248},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397500000530},
  doi = {10.1016/S0304-3975(00)00053-0},
  abstract = {We introduce MetaML, a practically motivated, statically typed multi-stage programming language. MetaML is a “real” language. We have built an implementation and used it to solve multi-stage problems. MetaML allows the programmer to construct, combine, and execute code fragments in a type-safe manner. Code fragments can contain free variables, but they obey the static-scoping principle. MetaML performs type-checking for all stages once and for all before the execution of the first stage. Certain anomalies with our first MetaML implementation led us to formalize an illustrative subset of the MetaML implementation. We present both a big-step semantics and type system for this subset, and prove the type system's soundness with respect to a big-step semantics. From a software engineering point of view, this means that generators written in the MetaML subset never generate unsafe programs. A type system and semantics for full MetaML is still ongoing work. We argue that multi-stage languages are useful as programming languages in their own right, that they supply a sound basis for high-level program generation technology, and that they should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three-stage example elaborating a number of practical issues. The design of MetaML was based on two main principles that we identified as fundamental for high-level program generation, namely, cross-stage persistence and cross-stage safety. We present these principles, explain the technical problems they give rise to, and how we address with these problems in our implementation.},
  number = {1–2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2014-10-20},
  date = {2000-10-06},
  pages = {211-242},
  keywords = {functional programming,λ-Calculus,High-level program generation,Multi-level languages,Multi-stage languages,Programming language semantics,Type-safety,Type-systems},
  author = {Taha, Walid and Sheard, Tim},
  file = {/Users/pgiarrusso/Zotero/storage/WRU7U9A6/Taha_Sheard - 2000 - MetaML and multi-stage programming with explicit annotations.pdf;/Users/pgiarrusso/Zotero/storage/DV3TKDHQ/S0304397500000530.html}
}

@inproceedings{Chafi2011domainspecific,
  location = {{New York, NY, USA}},
  title = {A Domain-specific Approach to Heterogeneous Parallelism},
  isbn = {978-1-4503-0119-0},
  url = {http://doi.acm.org/10.1145/1941553.1941561},
  doi = {10.1145/1941553.1941561},
  abstract = {Exploiting heterogeneous parallel hardware currently requires mapping application code to multiple disparate programming models. Unfortunately, general-purpose programming models available today can yield high performance but are too low-level to be accessible to the average programmer. We propose leveraging domain-specific languages (DSLs) to map high-level application code to heterogeneous devices. To demonstrate the potential of this approach we present OptiML, a DSL for machine learning. OptiML programs are implicitly parallel and can achieve high performance on heterogeneous hardware with no modification required to the source code. For such a DSL-based approach to be tractable at large scales, better tools are required for DSL authors to simplify language creation and parallelization. To address this concern, we introduce Delite, a system designed specifically for DSLs that is both a framework for creating an implicitly parallel DSL as well as a dynamic runtime providing automated targeting to heterogeneous parallel hardware. We show that OptiML running on Delite achieves single-threaded, parallel, and GPU performance superior to explicitly parallelized MATLAB code in nearly all cases.},
  booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
  series = {PPoPP '11},
  publisher = {{ACM}},
  urldate = {2014-10-20},
  date = {2011},
  pages = {35--46},
  keywords = {domain-specific languages,parallel programming,dynamic optimizations,runtimes},
  author = {Chafi, Hassan and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Atreya, Anand R. and Olukotun, Kunle},
  file = {/Users/pgiarrusso/Zotero/storage/PJCB7CNQ/Chafi et al - 2011 - A Domain-specific Approach to Heterogeneous Parallelism.pdf}
}

@inproceedings{DeVito2013terra,
  location = {{New York, NY, USA}},
  title = {Terra: A Multi-stage Language for High-performance Computing},
  isbn = {978-1-4503-2014-6},
  url = {http://doi.acm.org/10.1145/2491956.2462166},
  doi = {10.1145/2491956.2462166},
  shorttitle = {Terra},
  abstract = {High-performance computing applications, such as auto-tuners and domain-specific languages, rely on generative programming techniques to achieve high performance and portability. However, these systems are often implemented in multiple disparate languages and perform code generation in a separate process from program execution, making certain optimizations difficult to engineer. We leverage a popular scripting language, Lua, to stage the execution of a novel low-level language, Terra. Users can implement optimizations in the high-level language, and use built-in constructs to generate and execute high-performance Terra code. To simplify meta-programming, Lua and Terra share the same lexical environment, but, to ensure performance, Terra code can execute independently of Lua's runtime. We evaluate our design by reimplementing existing multi-language systems entirely in Terra. Our Terra-based auto-tuner for BLAS routines performs within 20\% of ATLAS, and our DSL for stencil computations runs 2.3x faster than hand-written C.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '13},
  publisher = {{ACM}},
  urldate = {2014-10-20},
  date = {2013},
  pages = {105--116},
  keywords = {DSL,staged computation,lua},
  author = {DeVito, Zachary and Hegarty, James and Aiken, Alex and Hanrahan, Pat and Vitek, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/JW5GB462/DeVito et al - 2013 - Terra - A Multi-stage Language for High-performance Computing.pdf}
}

@inproceedings{Clinger1991macros,
  location = {{New York, NY, USA}},
  title = {Macros That Work},
  isbn = {0-89791-419-8},
  url = {http://doi.acm.org/10.1145/99583.99607},
  doi = {10.1145/99583.99607},
  booktitle = {Proceedings of the 18th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '91},
  publisher = {{ACM}},
  urldate = {2014-10-21},
  date = {1991},
  pages = {155--162},
  author = {Clinger, William and Rees, Jonathan},
  file = {/Users/pgiarrusso/Zotero/storage/E4NVKGCG/Clinger_Rees - 1991 - Macros That Work.pdf}
}

@inproceedings{Verwaest2010pinocchio,
  location = {{New York, NY, USA}},
  title = {Pinocchio: Bringing Reflection to Life with First-class Interpreters},
  isbn = {978-1-4503-0203-6},
  url = {http://doi.acm.org/10.1145/1869459.1869522},
  doi = {10.1145/1869459.1869522},
  shorttitle = {Pinocchio},
  abstract = {To support development tools like debuggers, runtime systems need to provide a meta-programming interface to alter their semantics and access internal data. Reflective capabilities are typically fixed by the Virtual Machine (VM). Unanticipated reflective features must either be simulated by complex program transformations, or they require the development of a specially tailored VM. We propose a novel approach to behavioral reflection that eliminates the barrier between applications and the VM by manipulating an explicit tower of first-class interpreters. Pinocchio is a proof-of-concept implementation of our approach which enables radical changes to the interpretation of programs by explicitly instantiating subclasses of the base interpreter. We illustrate the design of Pinocchio through non-trivial examples that extend runtime semantics to support debugging, parallel debugging, and back-in-time object-flow debugging. Although performance is not yet addressed, we also discuss numerous opportunities for optimization, which we believe will lead to a practical approach to behavioral reflection.},
  booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '10},
  publisher = {{ACM}},
  urldate = {2014-10-21},
  date = {2010},
  pages = {774--789},
  keywords = {Smalltalk,behavioral reflection,debugging,metacircularity,object-flow analysis,reflection,virtual machines},
  author = {Verwaest, Toon and Bruni, Camillo and Gurtner, David and Lienhard, Adrian and Niestrasz, Oscar}
}

@article{Rompf2012lightweight,
  title = {Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled DSLs},
  volume = {55},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/2184319.2184345},
  doi = {10.1145/2184319.2184345},
  shorttitle = {Lightweight Modular Staging},
  abstract = {Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources. Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize. We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework. LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
  number = {6},
  journaltitle = {Commun. ACM},
  urldate = {2014-10-21},
  date = {2012-06},
  pages = {121--130},
  author = {Rompf, Tiark and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/CVAAU74P/Rompf_Odersky - 2012 - Lightweight Modular Staging - A Pragmatic Approach to Runtime Code Generation and Compiled DSLs.pdf}
}
% == BibLateX quality report for Rompf2012lightweight:
% ? Possibly abbreviated journal title Commun. ACM

@inproceedings{Erdweg2011sugarj,
  location = {{New York, NY, USA}},
  title = {SugarJ: Library-based Syntactic Language Extensibility},
  isbn = {978-1-4503-0940-0},
  url = {http://doi.acm.org/10.1145/2048066.2048099},
  doi = {10.1145/2048066.2048099},
  shorttitle = {SugarJ},
  abstract = {Existing approaches to extend a programming language with syntactic sugar often leave a bitter taste, because they cannot be used with the same ease as the main extension mechanism of the programming language - libraries. Sugar libraries are a novel approach for syntactically extending a programming language within the language. A sugar library is like an ordinary library, but can, in addition, export syntactic sugar for using the library. Sugar libraries maintain the composability and scoping properties of ordinary libraries and are hence particularly well-suited for embedding a multitude of domain-specific languages into a host language. They also inherit self-applicability from libraries, which means that sugar libraries can provide syntactic extensions for the definition of other sugar libraries. To demonstrate the expressiveness and applicability of sugar libraries, we have developed SugarJ, a language on top of Java, SDF and Stratego, which supports syntactic extensibility. SugarJ employs a novel incremental parsing technique, which allows changing the syntax within a source file. We demonstrate SugarJ by five language extensions, including embeddings of XML and closures in Java, all available as sugar libraries. We illustrate the utility of self-applicability by embedding XML Schema, a metalanguage to define XML languages.},
  booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '11},
  publisher = {{ACM}},
  urldate = {2014-10-21},
  date = {2011},
  pages = {391--406},
  keywords = {language composition,sugarj,Libraries,DSL embedding,language extensibility,syntactic sugar},
  author = {Erdweg, Sebastian and Rendel, Tillmann and Kästner, Christian and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/J45HBA66/Erdweg et al - 2011 - SugarJ - Library-based Syntactic Language Extensibility.pdf}
}

@article{Elliott2003compiling,
  title = {Compiling embedded languages},
  volume = {13},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796802004574},
  doi = {10.1017/S0956796802004574},
  abstract = {Functional languages are particularly well-suited to the interpretive implementations of Domain-Specific Embedded Languages (DSELs). We describe an implemented technique for producing optimizing compilers for DSELs, based on Kamin\&apos;s idea of DSELs for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization results from chains of domain-independent rewrites on basic types. New DSELs are defined directly in terms of the basic syntactic types, plus host language functions and tuples. This definition style makes compilers easy to write and, in fact, almost identical to the simplest embedded interpreters. We illustrate this technique with a language Pan for the computationally intensive domain of image synthesis and manipulation.},
  number = {03},
  journaltitle = {Journal of Functional Programming},
  urldate = {2014-10-21},
  date = {2003-05},
  pages = {455--481},
  author = {Elliott, Conal and Finne, Sigbjørn and De Moor, Oege},
  file = {/Users/pgiarrusso/Zotero/storage/CTS2HDIW/Elliott et al - 2003 - Compiling embedded languages.pdf;/Users/pgiarrusso/Zotero/storage/BBFD3IA5/displayAbstract.html}
}

@inproceedings{Leijen1999domain,
  location = {{New York, NY, USA}},
  title = {Domain Specific Embedded Compilers},
  isbn = {1-58113-255-7},
  url = {http://doi.acm.org/10.1145/331960.331977},
  doi = {10.1145/331960.331977},
  abstract = {Domain-specific embedded languages (DSELs) expressed in higher-order, typed (HOT) languages provide a composable framework for domain-specific abstractions. Such a framework is of greater utility than a collection of stand-alone domain-specific languages. Usually, embedded domain specific languages are build on top of a set of domain specific primitive functions that are ultimately implemented using some form of foreign function call. We sketch a general design pattern/or embedding client-server style services into Haskell using a domain specific embedded compiler for the server's source language. In particular we apply this idea to implement Haskell/DB, a domain specific embdedded compiler that dynamically generates of SQL queries from monad comprehensions, which are then executed on an arbitrary ODBC database server.},
  booktitle = {Proceedings of the 2Nd Conference on Domain-specific Languages},
  series = {DSL '99},
  publisher = {{ACM}},
  urldate = {2014-10-21},
  date = {1999},
  pages = {109--122},
  author = {Leijen, Daan and Meijer, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/HCIE5B6B/Leijen_Meijer - 1999 - Domain Specific Embedded Compilers.pdf}
}

@inproceedings{Hanenberg2014why,
  location = {{New York, NY, USA}},
  title = {Why Do We Know So Little About Programming Languages, and What Would Have Happened if We Had Known More?},
  isbn = {978-1-4503-3211-8},
  url = {http://doi.acm.org/10.1145/2661088.2661102},
  doi = {10.1145/2661088.2661102},
  abstract = {Programming language research in the last decades was mainly driven by mathematical methods (such as formal semantics, correctness proofs, type soundness proofs, etc.) or run-time arguments based on benchmark tests. This happened despite the frequent discussion over programming language usability. We have now been through decade after decade of one language after another domainating the field, forcing companies to switch languages and migrate libraries. Now that Javascript seems to be the next language to dominate, people start to ask old questions anew. The first goal of this talk is to discuss why the application of empirical methods is (still) relatively rare in PL research, and to discuss what could be done in empirical methods to make them a substantial part of PL research. The second goal is to speculate about the possible effects that concrete empirical knowledge could have had on the programming language community. For example, what would have happened to programming languages if current knowledge would have been available 30 years ago? What if knowledge about programming languages from the year 2050 would be available today?},
  booktitle = {Proceedings of the 10th ACM Symposium on Dynamic Languages},
  series = {DLS '14},
  publisher = {{ACM}},
  urldate = {2014-10-22},
  date = {2014},
  pages = {1--1},
  keywords = {programming languages,controlled experiments,empirical studies},
  author = {Hanenberg, Stefan},
  file = {/Users/pgiarrusso/Zotero/storage/NHHTGH68/Hanenberg - 2014 - Why Do We Know So Little About Programming Languages, and What Would Have Happened if We Had Known More.pdf}
}

@inproceedings{Stefik2014programming,
  location = {{New York, NY, USA}},
  title = {The Programming Language Wars: Questions and Responsibilities for the Programming Language Community},
  isbn = {978-1-4503-3210-1},
  url = {http://doi.acm.org/10.1145/2661136.2661156},
  doi = {10.1145/2661136.2661156},
  shorttitle = {The Programming Language Wars},
  abstract = {The discipline of computer science has a long and complicated history with computer programming languages. Historically, inventors have created language products for a wide variety of reasons, from attempts at making domain specific tasks easier or technical achievements, to economic, social, or political reasons. As a consequence, the modern programming language industry now has a large variety of incompatible programming languages, each of which with unique syntax, semantics, toolsets, and often their own standard libraries, lifetimes, and costs. In this paper, we suggest that the programming language wars, a term which describes the broad divergence and impact of language designs, including often pseudo-scientific claims made that they are good or bad, may be negatively impacting the world. This broad problem, which is almost completely ignored in computer science, needs to be acted upon by the community.},
  booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \&\#38; Software},
  series = {Onward! '14},
  publisher = {{ACM}},
  urldate = {2014-10-22},
  date = {2014},
  pages = {283--299},
  keywords = {evidence standards,stability of the academic literature,the programming language wars},
  author = {Stefik, Andreas and Hanenberg, Stefan},
  file = {/Users/pgiarrusso/Zotero/storage/S5A3VDVU/Stefik_Hanenberg - 2014 - The Programming Language Wars - Questions and Responsibilities for the Programming Language Community.pdf}
}

@inproceedings{Cook1990inheritance,
  location = {{New York, NY, USA}},
  title = {Inheritance is Not Subtyping},
  isbn = {0-89791-343-4},
  url = {http://doi.acm.org/10.1145/96709.96721},
  doi = {10.1145/96709.96721},
  abstract = {In typed object-oriented languages the subtype relation is typically based on the inheritance hierarchy. This approach, however, leads either to insecure type-systems or to restrictions on inheritance that make it less flexible than untyped Smalltalk inheritance. We present a new typed model of inheritance that allows more of the flexibility of Smalltalk inheritance within a statically-typed system. Significant features of our analysis are the introduction of polymorphism into the typing of inheritance and the uniform application of inheritance to objects, classes and types. The resulting notion of type inheritance allows us to show that the type of an inherited object is an inherited type but not always a subtype.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '90},
  publisher = {{ACM}},
  urldate = {2014-10-23},
  date = {1990},
  pages = {125--135},
  author = {Cook, William R. and Hill, Walter and Canning, Peter S.},
  file = {/Users/pgiarrusso/Zotero/storage/247VQ9S7/Cook et al - 1990 - Inheritance is Not Subtyping.pdf}
}

@article{Seidel2014type,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.5370},
  primaryClass = {cs},
  title = {Type Targeted Testing},
  url = {http://arxiv.org/abs/1410.5370},
  abstract = {We present a new technique called type targeted testing, which translates precise refinement types into comprehensive test-suites. The key insight behind our approach is that through the lens of SMT solvers, refinement types can also be viewed as a high-level, declarative, test generation technique, wherein types are converted to SMT queries whose models can be decoded into concrete program inputs. Our approach enables the systematic and exhaustive testing of implementations from high-level declarative specifications, and furthermore, provides a gradual path from testing to full verification. We have implemented our approach as a Haskell testing tool called TARGET, and present an evaluation that shows how TARGET can be used to test a wide variety of properties and how it compares against state-of-the-art testing approaches.},
  urldate = {2014-10-25},
  date = {2014-10-20},
  keywords = {Computer Science - Programming Languages},
  author = {Seidel, Eric L. and Vazou, Niki and Jhala, Ranjit},
  file = {/Users/pgiarrusso/Zotero/storage/34EBBEW2/Seidel et al - 2014 - Type Targeted Testing.pdf;/Users/pgiarrusso/Zotero/storage/6HTHMH3D/1410.html}
}
% == BibLateX quality report for Seidel2014type:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Acharya2014rise,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.2217},
  primaryClass = {cs},
  title = {Rise of the Rest: The Growing Impact of Non-Elite Journals},
  url = {http://arxiv.org/abs/1410.2217},
  shorttitle = {Rise of the Rest},
  abstract = {In this paper, we examine the evolution of the impact of non-elite journals. We attempt to answer two questions. First, what fraction of the top-cited articles are published in non-elite journals and how has this changed over time. Second, what fraction of the total citations are to non-elite journals and how has this changed over time. We studied citations to articles published in 1995-2013. We computed the 10 most-cited journals and the 1000 most-cited articles each year for all 261 subject categories in Scholar Metrics. We marked the 10 most-cited journals in a category as the elite journals for the category and the rest as non-elite. There are two conclusions from our study. First, the fraction of top-cited articles published in non-elite journals increased steadily over 1995-2013. While the elite journals still publish a substantial fraction of high-impact articles, many more authors of well-regarded papers in diverse research fields are choosing other venues. The number of top-1000 papers published in non-elite journals for the representative subject category went from 149 in 1995 to 245 in 2013, a growth of 64\%. Looking at broad research areas, 4 out of 9 areas saw at least one-third of the top-cited articles published in non-elite journals in 2013. For 6 out of 9 areas, the fraction of top-cited papers published in non-elite journals for the representative subject category grew by 45\% or more. Second, now that finding and reading relevant articles in non-elite journals is about as easy as finding and reading articles in elite journals, researchers are increasingly building on and citing work published everywhere. Considering citations to all articles, the percentage of citations to articles in non-elite journals went from 27\% in 1995 to 47\% in 2013. Six out of nine broad areas had at least 50\% of citations going to articles published in non-elite journals in 2013.},
  urldate = {2014-11-04},
  date = {2014-10-08},
  keywords = {Computer Science - Digital Libraries},
  author = {Acharya, Anurag and Verstak, Alex and Suzuki, Helder and Henderson, Sean and Iakhiaev, Mikhail and Lin, Cliff Chiung Yu and Shetty, Namit},
  file = {/Users/pgiarrusso/Zotero/storage/FWZ3V6W9/Acharya et al - 2014 - Rise of the Rest - The Growing Impact of Non-Elite Journals.pdf;/Users/pgiarrusso/Zotero/storage/C5B6V3MB/1410.html}
}
% == BibLateX quality report for Acharya2014rise:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Crary2009syntactic,
  location = {{New York, NY, USA}},
  title = {A Syntactic Account of Singleton Types via Hereditary Substitution},
  isbn = {978-1-60558-529-1},
  url = {http://doi.acm.org/10.1145/1577824.1577829},
  doi = {10.1145/1577824.1577829},
  abstract = {We give a syntactic proof of decidability and consistency of equivalence for the singleton type calculus, which lies at the foundation of modern module systems such as that of ML. Unlike existing proofs, which work by constructing a model, our syntactic proof makes few demands on the underlying proof theory and mathematical foundation. Consequently, it can be --- and has been --- entirely formulated in the Twelf meta-logic, and provides an important piece of a Twelf-formalized type-safety proof for Standard ML. The proof works by translation of the singleton type calculus into a canonical presentation, adapted from work on logical frameworks, in which equivalent terms are written identically. Canonical forms are not preserved under standard substitution, so we employ an alternative definition of substitution called hereditary substitution, which contracts redices that arise during substitution.},
  booktitle = {Proceedings of the Fourth International Workshop on Logical Frameworks and Meta-Languages: Theory and Practice},
  series = {LFMTP '09},
  publisher = {{ACM}},
  urldate = {2014-11-10},
  date = {2009},
  pages = {21--29},
  keywords = {singleton types,logical frameworks,mechanized metatheory,_tablet},
  author = {Crary, Karl},
  file = {/Users/pgiarrusso/Zotero/storage/BXQAI3BT/Crary - 2009 - A Syntactic Account of Singleton Types via Hereditary Substitution.pdf}
}

@article{Stone2006extensional,
  title = {Extensional Equivalence and Singleton Types},
  volume = {7},
  issn = {1529-3785},
  url = {http://doi.acm.org/10.1145/1183278.1183281},
  doi = {10.1145/1183278.1183281},
  abstract = {We study the \&lamda;ΠΣS≤ calculus, which contains singleton types S(M) classifying terms of base type provably equivalent to the term M. The system includes dependent types for pairs and functions (Σ and Π) and a subtyping relation induced by regarding singletons as subtypes of the base type. The decidability of type checking for this language is non-obvious, since to type check we must be able to determine equivalence of well-formed terms. But in the presence of singleton types, the provability of an equivalence judgment Γ ⊢ M1 \&eqiv;M2 : A can depend both on the typing context Γ and on the particular type A at which M1 and M2 are compared.We show how to prove decidability of term equivalence, hence of type checking, in \&lamda;ΠΣS≤ by exhibiting a type-directed algorithm for directly computing normal forms. The correctness of normalization is shown using an unusual variant of Kripke logical relations organized around sets; rather than defining a logical equivalence relation, we work directly with (subsets of) the corresponding equivalence classes.We then provide a more efficient algorithm for checking type equivalence without constructing normal forms. We also show that type checking, subtyping, and all other judgments of the system are decidable.The \&lamda;ΠΣS≤ calculus models type constructors and kinds in the intermediate language used by the TILT compiler for Standard ML to implement the SML module system. The decidability of \&lamda;ΠΣS≤ term equivalence allows us to show decidability of type checking for TILT's intermediate language. We also obtain a consistency result that allows us to prove type safety for the intermediate language. The algorithms derived here form the core of the type checker used for internal type checking in TILT.},
  number = {4},
  journaltitle = {ACM Trans. Comput. Logic},
  urldate = {2014-11-10},
  date = {2006-10},
  pages = {676--722},
  keywords = {singleton types,logical relations,_tablet,equivalence algorithms},
  author = {Stone, Christopher A. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/VJE542RR/Stone-Harper - 2006 - Extensional Equivalence and Singleton Types.pdf}
}
% == BibLateX quality report for Stone2006extensional:
% ? Possibly abbreviated journal title ACM Trans. Comput. Logic

@inproceedings{Visser2014separation,
  location = {{New York, NY, USA}},
  title = {Separation of Concerns in Language Definition},
  isbn = {978-1-4503-2772-5},
  url = {http://doi.acm.org/10.1145/2584469.2584662},
  doi = {10.1145/2584469.2584662},
  abstract = {Effectively applying linguistic abstraction to emerging domains of computation requires the ability to rapidly develop software languages. However, a software language is a complex software system in its own right and can take significant effort to design and implement. We are currently investigating a radical separation of concerns in language definition by designing high-level declarative meta-languages specialized to the various concerns of language definition that can be used as the single source of production quality (incremental) semantic operations and as a model for reasoning about language properties.},
  booktitle = {Proceedings of the 13th International Conference on Modularity},
  series = {MODULARITY '14},
  publisher = {{ACM}},
  urldate = {2014-04-29},
  date = {2014},
  pages = {1--2},
  keywords = {spoofax,language design,language work- bench,meta-languages,nabl,sdf3,stratego},
  author = {Visser, Eelco},
  file = {/Users/pgiarrusso/Zotero/storage/BGV64ZTQ/Visser - 2014 - Separation of Concerns in Language Definition.pdf}
}

@article{Koch2014dbtoaster,
  langid = {english},
  title = {DBToaster: higher-order delta processing for dynamic, frequently fresh views},
  volume = {23},
  issn = {1066-8888, 0949-877X},
  url = {http://link.springer.com/article/10.1007/s00778-013-0348-4},
  doi = {10.1007/s00778-013-0348-4},
  shorttitle = {DBToaster},
  abstract = {Applications ranging from algorithmic trading to scientific data analysis require real-time analytics based on views over databases receiving thousands of updates each second. Such views have to be kept fresh at millisecond latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this article, we present the DBToaster system, which keeps materialized views of standard SQL queries continuously fresh as data changes very rapidly. This is achieved by a combination of aggressive compilation techniques and DBToaster’s original recursive finite differencing technique which materializes a query and a set of its higher-order deltas as views. These views support each other’s incremental maintenance, leading to a reduced overall view maintenance cost. DBToaster supports tens of thousands of complete view refreshes per second for a wide range of queries.},
  number = {2},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  urldate = {2014-04-30},
  date = {2014-04-01},
  pages = {253-278},
  keywords = {Database Management,Compilation,Database queries,Incremental view maintenance,Materialized views},
  author = {Koch, Christoph and Ahmad, Yanif and Kennedy, Oliver and Nikolic, Milos and Nötzli, Andres and Lupei, Daniel and Shaikhha, Amir},
  file = {/Users/pgiarrusso/Zotero/storage/MTJ4KRHX/Koch et al - 2014 - DBToaster - higher-order delta processing for dynamic, frequently fresh views.pdf;/Users/pgiarrusso/Zotero/storage/SBA5GKUP/s00778-013-0348-4.html}
}
% == BibLateX quality report for Koch2014dbtoaster:
% 'issn': not a valid ISSN

@inproceedings{Ohshima2013future,
  location = {{New York, NY, USA}},
  title = {Toward the Future of Personal Computing System Construction},
  isbn = {978-1-4503-2602-5},
  url = {http://doi.acm.org/10.1145/2541329.2541346},
  doi = {10.1145/2541329.2541346},
  abstract = {The software for today's personal computing environments has become so complex that no single person can understand an entire system. Our group's early experiences with personal computing led us to understand that the essential model of personal computing can be expressed much more compactly. Our group engaged in a project (called the STEPS project) to materialize that vision over the last six years. Several interesting results came out of the STEPS project. There are various meta-language implementations. A new stream-processing language called Nile was invented. The syntax of Nile allows a fully-featured vector graphics engine called Gezira to be written in a clean, mathematical manner in less than 500 lines of code. We also wrote a dynamic, yet declarative, graphical user interface framework in which we built a universal document editor. From the experience with the STEPS project, we are exploring the next steps; one of the directions is to incorporate the idea of using multiple constraint solvers to build a system. Solvers can help stronger "negotiation" between objects and languages, and help the system to be written in a more declarative manner. Another direction is to take the idea of loose-coupling to the next level; objects should not know about other objects directly but should always negotiate and "find" other objects. As the basic concept for this next stage, we are using the "Internet all the way down" analogy. The Internet has proven itself to be a great engineering feat: it scaled from just four nodes to billions of nodes without ever needing a full shutdown for maintenance. It is also likely that we can take out a random 10\% of the Internet's nodes and it will still keep running. By building a personal computing environment that consists of such nodes, or objects, perhaps we can make it never need to be restarted. Interestingly, J. C. R. Licklider already foresaw the need for program components to discover each other on a huge network of computers. From that viewpoint, what we are trying to do is to carry the vision forward.},
  booktitle = {Proceedings of the 2013 Workshop on Programming Based on Actors, Agents, and Decentralized Control},
  series = {AGERE! '13},
  publisher = {{ACM}},
  urldate = {2014-04-30},
  date = {2013},
  pages = {1--2},
  keywords = {exploratory programming,the internet all the way down model},
  author = {Ohshima, Yoshiki},
  file = {/Users/pgiarrusso/Zotero/storage/B5TWI79K/Ohshima - 2013 - Toward the Future of Personal Computing System Construction.pdf}
}

@article{Turner2007understanding,
  langid = {english},
  title = {Understanding Programming Languages},
  volume = {17},
  issn = {0924-6495, 1572-8641},
  url = {http://link.springer.com/article/10.1007/s11023-007-9062-6},
  doi = {10.1007/s11023-007-9062-6},
  abstract = {We document the influence on programming language semantics of the Platonism/formalism divide in the philosophy of mathematics.},
  number = {2},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  urldate = {2014-04-30},
  date = {2007-07-01},
  pages = {203-216},
  keywords = {Artificial Intelligence (incl. Robotics),Programming language semantics,Denotational,Interdisciplinary Studies,Operational,Philosophy of Mind,Systems Theory; Control},
  author = {Turner, Raymond},
  file = {/Users/pgiarrusso/Zotero/storage/MMBJM9DA/Turner - 2007 - Understanding Programming Languages.pdf;/Users/pgiarrusso/Zotero/storage/TZNW7KX6/10.html}
}
% == BibLateX quality report for Turner2007understanding:
% 'issn': not a valid ISSN

@inproceedings{Bertino1991indexing,
  title = {An indexing techniques for object-oriented databases},
  doi = {10.1109/ICDE.1991.131463},
  abstract = {The basic characteristics of object-oriented data models and query languages are summarized. An indexing technique is presented that supports an efficient evaluation of nested predicates for queries involving class hierarchies. A preliminary comparison of the proposed indexing technique with other techniques is presented. A first extension of this work is to define cost formulas for the proposed index organization and to compare it with other organizations, such as the multi-index and inherited-multi-index organizations. A second extension is to define indexing techniques to support alternative predicates on properties that are semantically equivalent. In addition to these, an important issue concerns indexing support for the use of methods in queries},
  eventtitle = {Seventh International Conference on Data Engineering, 1991. Proceedings},
  booktitle = {Seventh International Conference on Data Engineering, 1991. Proceedings},
  date = {1991-04},
  pages = {160-170},
  keywords = {Indexing,class hierarchies,cost formulas,Database languages,indexing techniques,nested predicates,Object oriented databases,Object oriented modeling,Object oriented programming,object-oriented databases,query languages,Spatial databases,Vehicles},
  author = {Bertino, E.},
  file = {/Users/pgiarrusso/Zotero/storage/ZPRRZ9DD/login.html}
}
% == BibLateX quality report for Bertino1991indexing:
% ? Unsure about the formatting of the booktitle

@article{Bertino1993pathindex,
  title = {Path-index: An approach to the efficient execution of object-oriented queries},
  volume = {10},
  issn = {0169-023X},
  url = {http://www.sciencedirect.com/science/article/pii/0169023X9390017J},
  doi = {10.1016/0169-023X(93)90017-J},
  shorttitle = {Path-index},
  abstract = {This paper addresses the prolem of efficiently evaluating nested predicates in object-oriented databases. In a previous paper [4] we have introduced the notion of path index that associates the values of a nested attribute with the instances of the class root of a given aggregation hierarchy. In that paper we have evaluated the performance of the path index in the case of queries containing a single predicate. In the present paper we consider the use of the path index in the framework of more general queries containing several predicates.},
  number = {1},
  journaltitle = {Data \& Knowledge Engineering},
  shortjournal = {Data \& Knowledge Engineering},
  urldate = {2014-05-02},
  date = {1993-02},
  pages = {1-27},
  keywords = {query optimization,indexing techniques,complex objects,Object-oriented query languages},
  author = {Bertino, E. and Guglielmina, C.},
  file = {/Users/pgiarrusso/Zotero/storage/4D9GP64C/0169023X9390017J.html}
}

@article{Bertino1994index,
  title = {Index Configuration in Object-oriented Databases},
  volume = {3},
  issn = {1066-8888},
  url = {http://dx.doi.org/10.1007/BF01232644},
  doi = {10.1007/BF01232644},
  abstract = {In relational databases, an attribute of a relation can have only a single primitive value, making it cumbersome to model complex objects. The object-oriented paradigm removes this difficulty by introducing the notion of nested objects, which allows the value of an object attribute to be another object or a set of other objects. This means that a class consists of a set of attributes, and the values of the attributes are objects that belong to other classes; that is, the definition of a class forms a hierarchy of classes. All attributes of the nested classes are nested attributes of the root of the hierarchy. A branch of such hierarchy is called a path. In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.},
  number = {3},
  journaltitle = {The VLDB Journal},
  urldate = {2014-05-02},
  date = {1994-07},
  pages = {355--399},
  keywords = {query optimization,index selection,physical database design},
  author = {Bertino, Elisa}
}

@inproceedings{Bertino1992optimization,
  title = {Optimization of object-oriented queries using path indices},
  doi = {10.1109/RIDE.1992.227413},
  abstract = {This paper addresses the problem of efficiently evaluating nested predicates in object-oriented databases. The authors have previously introduced the notion of path index (E. Bertino et al., 1989) that associates the values of a nested attribute with the instances of the class root of a given aggregation hierarchy. They evaluated the performance of the path index in the case of queries containing a single predicate. Here they consider the usage of the path index in the framework of more general queries containing several predicates},
  eventtitle = {Second International Workshop on Research Issues on Data Engineering, 1992: Transaction and Query Processing},
  booktitle = {Second International Workshop on Research Issues on Data Engineering, 1992: Transaction and Query Processing},
  date = {1992-02},
  pages = {140-149},
  keywords = {Indexing,query optimization,Database languages,nested predicates,Object oriented modeling,object-oriented databases,Vehicles,aggregation hierarchy,database theory,Navigation,object-oriented queries,optimisation,path index,query processing},
  author = {Bertino, E. and Guglielmina, C.},
  file = {/Users/pgiarrusso/Zotero/storage/9JA94UAP/login.html}
}

@incollection{Bertino1990optimization,
  title = {Optimization of queries using nested indices},
  isbn = {978-3-540-52291-1 978-3-540-46948-3},
  url = {http://link.springer.com/chapter/10.1007/BFb0022163},
  abstract = {The notion of nested object is a basic concept of the object-oriented paradigm. It allows the value of an object attribute to be another object or a set of other objects. This means that a class consists of a set of attributes, and the values of the attributes are objects that belong to other classes; that is, the definition of a class forms a hierarchy of classes. All attributes of the nested classes are nested attributes of the root of the hierarchy. In a previous paper [Bert 89], we have introduced the notion of nested index that associates the values of a nested attribute with the objects instances of the root of the hierarchy. In that paper, we have evaluated the performance of this indexing mechanism in the case of queries containing a single predicate. In the present paper, we consider the usage of nested indices in the framework of more general queries containing several predicates.},
  number = {416},
  booktitle = {Advances in Database Technology — EDBT '90},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-05-02},
  date = {1990-01-01},
  pages = {44-59},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages,Models and Principles,Database Management,Data Storage Representation},
  author = {Bertino, Elisa},
  editor = {Bancilhon, François and Thanos, Constantino and Tsichritzis, Dennis},
  file = {/Users/pgiarrusso/Zotero/storage/9VGJM49D/BFb0022163.html}
}
% == BibLateX quality report for Bertino1990optimization:
% 'isbn': not a valid ISBN

@incollection{Bertino1997objectoriented,
  langid = {english},
  title = {Object-Oriented Databases},
  isbn = {978-1-4613-7856-3 978-1-4615-6227-6},
  url = {http://link.springer.com/chapter/10.1007/978-1-4615-6227-6_1},
  abstract = {There has been a growing acceptance of the object-oriented data model as the basis of next generation database management systems (DBMSs). Both pure object-oriented DBMS (OODBMSs) and object-relational DBMS (ORDBMSs) have been developed based on object-oriented concepts. Object-relational DBMS, in particular, extend the SQL language by incorporating all the concepts of the object-oriented data model. A large number of products for both categories of DBMS is today available. In particular, all major vendors of relational DBMSs are turning their products into ORDBMSs [Nori, 1996].},
  number = {8},
  booktitle = {Indexing Techniques for Advanced Database Systems},
  series = {The Springer International Series on Advances in Database Systems},
  publisher = {{Springer US}},
  urldate = {2014-05-02},
  date = {1997-01-01},
  pages = {1-38},
  keywords = {Data Structures; Cryptology and Information Theory,Information Storage and Retrieval,Geographical Information Systems/Cartography},
  author = {Bertino, Elisa and Ooi, Beng Chin and Sacks-Davis, Ron and Tan, Kian-Lee and Zobel, Justin and Shidlovsky, Boris and Catania, Barbara},
  file = {/Users/pgiarrusso/Zotero/storage/649XUEC6/978-1-4615-6227-6_1.html}
}
% == BibLateX quality report for Bertino1997objectoriented:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@incollection{Cremet2006core,
  title = {A Core Calculus for Scala Type Checking},
  isbn = {978-3-540-37791-7 978-3-540-37793-1},
  url = {http://link.springer.com/chapter/10.1007/11821069_1},
  abstract = {We present a minimal core calculus that captures interesting constructs of the Scala programming language: nested classes, abstract types, mixin composition, and path dependent types. We show that the problems of type assignment and subtyping in this calculus are decidable.},
  number = {4162},
  booktitle = {Mathematical Foundations of Computer Science 2006},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-05-02},
  date = {2006-01-01},
  pages = {1-23},
  keywords = {Logics and Meanings of Programs,Data Structures,Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Discrete Mathematics in Computer Science,Algorithms},
  author = {Cremet, Vincent and Garillot, François and Lenglet, Sergueï and Odersky, Martin},
  editor = {Královič, Rastislav and Urzyczyn, Paweł},
  file = {/Users/pgiarrusso/Zotero/storage/TRX37GWH/11821069_1.html}
}
% == BibLateX quality report for Cremet2006core:
% 'isbn': not a valid ISBN

@article{Leroy2009formal,
  title = {Formal Verification of a Realistic Compiler},
  volume = {52},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/1538788.1538814},
  doi = {10.1145/1538788.1538814},
  abstract = {This paper reports on the development and formal verification (proof of semantic preservation) of CompCert, a compiler from Clight (a large subset of the C programming language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a verified compiler is useful in the context of critical software and its formal verification: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
  number = {7},
  journaltitle = {Commun. ACM},
  urldate = {2014-05-02},
  date = {2009-07},
  pages = {107--115},
  author = {Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/7BEDX9Q8/Leroy - 2009 - Formal Verification of a Realistic Compiler.pdf}
}
% == BibLateX quality report for Leroy2009formal:
% ? Possibly abbreviated journal title Commun. ACM

@inproceedings{Leroy2006formal,
  location = {{New York, NY, USA}},
  title = {Formal Certification of a Compiler Back-end or: Programming a Compiler with a Proof Assistant},
  isbn = {1-59593-027-2},
  url = {http://doi.acm.org/10.1145/1111037.1111042},
  doi = {10.1145/1111037.1111042},
  shorttitle = {Formal Certification of a Compiler Back-end or},
  abstract = {This paper reports on the development and formal certification (proof of semantic preservation) of a compiler from Cminor (a C-like imperative language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a certified compiler is useful in the context of formal methods applied to the certification of critical software: the certification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
  booktitle = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '06},
  publisher = {{ACM}},
  urldate = {2014-05-02},
  date = {2006},
  pages = {42--54},
  keywords = {certified compilation,program proof,the Coq theorem prover,compiler transformations and optimizations,semantic preservation},
  author = {Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/WSQR93KN/Leroy - 2006 - Formal Certification of a Compiler Back-end or - Programming a Compiler with a.pdf}
}
% == BibLateX quality report for Leroy2006formal:
% ? Unsure about the formatting of the booktitle

@article{Sujeeth2014delite,
  title = {Delite: A Compiler Architecture for Performance-Oriented Embedded Domain-Specific Languages},
  volume = {13},
  issn = {1539-9087},
  url = {http://doi.acm.org/10.1145/2584665},
  doi = {10.1145/2584665},
  shorttitle = {Delite},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  issue = {4s},
  journaltitle = {ACM Trans. Embed. Comput. Syst.},
  urldate = {2014-05-03},
  date = {2014-04},
  pages = {134:1--134:25},
  keywords = {domain-specific languages,code generation,language virtualization,multistage programming},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle}
}
% == BibLateX quality report for Sujeeth2014delite:
% ? Possibly abbreviated journal title ACM Trans. Embed. Comput. Syst.

@inproceedings{Shankar2005runtime,
  location = {{New York, NY, USA}},
  title = {Runtime Specialization with Optimistic Heap Analysis},
  isbn = {1-59593-031-0},
  url = {http://doi.acm.org/10.1145/1094811.1094837},
  doi = {10.1145/1094811.1094837},
  abstract = {We describe a highly practical program specializer for Java programs. The specializer is powerful, because it specializes optimistically, using (potentially transient) constants in the heap; it is precise, because it specializes using data structures that are only partially invariant; it is deployable, because it is hidden in a JIT compiler and does not require any user annotations or offline preprocessing; it is simple, because it uses existing JIT compiler ingredients; and it is fast, because it specializes programs in under 1s.These properties are the result of (1) a new algorithm for selecting specializable code fragments, based on a notion of influence; (2) a precise store profile for identifying constant heap locations; and (3) an efficient invalidation mechanism for monitoring optimistic assumptions about heap constants. Our implementation of the specializer in the Jikes RVM has low overhead, selects specialization points that would be chosen manually, and produces speedups ranging from a factor of 1.2 to 6.4, comparable with annotation-guided specializers.},
  booktitle = {Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA '05},
  publisher = {{ACM}},
  urldate = {2014-05-04},
  date = {2005},
  pages = {327--343},
  keywords = {partial evaluation,specialization,dynamic optimization,program analysis},
  author = {Shankar, Ajeet and Sastry, S. Subramanya and Bodík, Rastislav and Smith, James E.},
  file = {/Users/pgiarrusso/Zotero/storage/927KGGJM/Shankar et al - 2005 - Runtime Specialization with Optimistic Heap Analysis.pdf}
}

@inproceedings{Knoblock1996data,
  location = {{New York, NY, USA}},
  title = {Data Specialization},
  isbn = {0-89791-795-2},
  url = {http://doi.acm.org/10.1145/231379.231428},
  doi = {10.1145/231379.231428},
  abstract = {Given a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.},
  booktitle = {Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation},
  series = {PLDI '96},
  publisher = {{ACM}},
  urldate = {2014-05-04},
  date = {1996},
  pages = {215--225},
  author = {Knoblock, Todd B. and Ruf, Erik},
  file = {/Users/pgiarrusso/Zotero/storage/A3WJXD96/Knoblock_Ruf - 1996 - Data Specialization.pdf}
}

@article{Bohm1985automatic,
  title = {Automatic synthesis of typed Λ-programs on term algebras},
  volume = {39},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397585901355},
  doi = {10.1016/0304-3975(85)90135-5},
  abstract = {The notion of iteratively defined functions from and to heterogeneous term algebras is introduced as the solution of a finite set of equations of a special shape.
Such a notion has remarkable consequences: (1) Choosing the second-order typed lamdda-calculus (Λ for short) as a programming language enables one to represent algebra elements and iterative functions by automatic uniform synthesis paradigms, using neither conditional nor recursive constructs. (2) A completeness theorem for Λ-terms with type of degree at most two and a companion corollary for Λ-programs have been proved. (3) A new congruence relation for the last-mentioned Λ-terms which is stronger than Λ-convertibility is introduced and proved to have the meaning of a Λ-program equivalence. Moreover, an extension of the paradigms to the synthesis of functions of higher complexity is considered and exemplified. All the concepts are explained and motivated by examples over integers, list- and tree-structures.},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Third Conference on Foundations of Software Technology and Theoretical Computer Science},
  urldate = {2014-05-10},
  date = {1985},
  pages = {135-154},
  author = {Böhm, Corrado and Berarducci, Alessandro},
  file = {/Users/pgiarrusso/Zotero/storage/764J48ER/Böhm_Berarducci - 1985 - Automatic synthesis of typed Λ-programs on term algebras.pdf;/Users/pgiarrusso/Zotero/storage/EJ3RZQ7Q/0304397585901355.html}
}

@book{Carettefinally,
  title = {Finally Tagless, Partially Evaluated – Tagless Staged Interpreters for Simpler Typed Languages},
  abstract = {We have built the first family of tagless interpretations for a higher-order typed object language in a typed metalanguage (Haskell or ML) that require no dependent types, generalized algebraic data types, or postprocessing to eliminate tags. The statically type-preserving interpretations include an evaluator, a compiler (or staged evaluator), a partial evaluator, and call-by-name and call-by-value CPS transformers. Our main idea is to encode HOAS using cogen functions rather than data constructors. In other words, we represent object terms not in an initial algebra but using the coalgebraic structure of the λ-calculus. Our representation also simulates inductive maps from types to types, which are required for typed partial evaluation and CPS transformations. Our encoding of an object term abstracts over the various ways to interpret it, yet statically assures that the interpreters never get stuck. To achieve self-interpretation and show Jones-optimality, we relate this exemplar of higher-rank and higher-kind polymorphism to plugging a term into a context of let-polymorphic bindings.},
  author = {Carette, Jacques and Kiselyov, Oleg and Shan, Chung-chieh},
  file = {/Users/pgiarrusso/Zotero/storage/R45GSGHX/Carette et al - Finally Tagless, Partially Evaluated – Tagless Staged Interpreters for Simpler.pdf;/Users/pgiarrusso/Zotero/storage/A8Z5V4XP/summary.html}
}
% == BibLateX quality report for Carettefinally:
% Exactly one of 'date' / 'year' must be present

@inproceedings{Hofer2008polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Embedding of DSLs},
  isbn = {978-1-60558-267-2},
  url = {http://doi.acm.org/10.1145/1449913.1449935},
  doi = {10.1145/1449913.1449935},
  abstract = {The influential pure embedding methodology of embedding domain-specific languages (DSLs) as libraries into a general-purpose host language forces the DSL designer to commit to a single semantics. This precludes the subsequent addition of compilation, optimization or domain-specific analyses. We propose polymorphic embedding of DSLs, where many different interpretations of a DSL can be provided as reusable components, and show how polymorphic embedding can be realized in the programming language Scala. With polymorphic embedding, the static type-safety, modularity, composability and rapid prototyping of pure embedding are reconciled with the flexibility attainable by external toolchains.},
  booktitle = {Proceedings of the 7th International Conference on Generative Programming and Component Engineering},
  series = {GPCE '08},
  publisher = {{ACM}},
  urldate = {2014-05-10},
  date = {2008},
  pages = {137--148},
  keywords = {domain-specific languages,Scala,extensibility,algebraic semantics,compositionality,pure embedding},
  author = {Hofer, Christian and Ostermann, Klaus and Rendel, Tillmann and Moors, Adriaan},
  file = {/Users/pgiarrusso/Zotero/storage/XKT3ZPR5/Hofer et al - 2008 - Polymorphic Embedding of DSLs.pdf}
}

@inproceedings{Vytiniotis2013halo,
  location = {{New York, NY, USA}},
  title = {HALO: Haskell to Logic Through Denotational Semantics},
  isbn = {978-1-4503-1832-7},
  url = {http://doi.acm.org/10.1145/2429069.2429121},
  doi = {10.1145/2429069.2429121},
  shorttitle = {HALO},
  abstract = {Even well-typed programs can go wrong in modern functional languages, by encountering a pattern-match failure, or simply returning the wrong answer. An increasingly-popular response is to allow programmers to write contracts that express semantic properties, such as crash-freedom or some useful post-condition. We study the static verification of such contracts. Our main contribution is a novel translation to first-order logic of both Haskell programs, and contracts written in Haskell, all justified by denotational semantics. This translation enables us to prove that functions satisfy their contracts using an off-the-shelf first-order logic theorem prover.},
  booktitle = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '13},
  publisher = {{ACM}},
  urldate = {2014-05-12},
  date = {2013},
  pages = {431--442},
  keywords = {first-order logic,static contract checking},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Claessen, Koen and Rosén, Dan},
  file = {/Users/pgiarrusso/Zotero/storage/DCCF5HN4/Vytiniotis et al - 2013 - HALO - Haskell to Logic Through Denotational Semantics.pdf}
}

@inproceedings{Augusteijn1998sorting,
  title = {Sorting Morphisms (2)},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.3315},
  abstract = {Sorting algorithms can be classified in many different ways. The way presented here is by expressing the algorithms as functional programs and to classify them by means of their recursion patterns. These patterns on their turn can be classified as the natural recursion patterns that destruct or construct a given data-type, the so called cata- and anamorphisms respectively. We show that the selection of the recursion pattern can be seen as the major design decision, in most cases leaving no more room for more decisions in the design of the sorting algorithm. It is also shown that the use of alternative data structures may lead to new sorting algorithms. This presentation also serves as a gentle, light-weight, introduction into the various morphisms.},
  booktitle = {3rd International Summer School on Advanced Functional Programming, volume 1608 of LNCS},
  publisher = {{Springer-Verlag}},
  date = {1998},
  pages = {1--27},
  author = {Augusteijn, Lex},
  file = {/Users/pgiarrusso/Zotero/storage/8V5H9E5T/Augusteijn - 1998 - Sorting Morphisms.pdf;/Users/pgiarrusso/Zotero/storage/WGDZFRW8/summary.html}
}
% == BibLateX quality report for Augusteijn1998sorting:
% ? Unsure about the formatting of the booktitle

@incollection{Augusteijn1999sorting,
  title = {Sorting Morphisms (1)},
  isbn = {978-3-540-66241-9 978-3-540-48506-3},
  url = {http://link.springer.com/chapter/10.1007/10704973_1},
  abstract = {Sorting algorithms can be classified in many different ways. The way presented here is by expressing the algorithms as functional programs and to classify them by means of their recursion patterns. These patterns on their turn can be classified as the natural recursion patterns that destruct or construct a given data-type, the so called cata- and anamorphisms respectively. We show that the selection of the recursion pattern can be seen as the major design decision, in most cases leaving no more room for more decisions in the design of the sorting algorithm. It is also shown that the use of alternative data structures may lead to new sorting algorithms. This presentation also serves as a gentle, light-weight, introduction into the various morphisms.},
  number = {1608},
  booktitle = {Advanced Functional Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-05-13},
  date = {1999-01-01},
  pages = {1-27},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  author = {Augusteijn, Lex},
  editor = {Swierstra, S. Doaitse and Oliveira, José N. and Henriques, Pedro R.},
  file = {/Users/pgiarrusso/Zotero/storage/F7E42SBS/10704973_1.html}
}
% == BibLateX quality report for Augusteijn1999sorting:
% 'isbn': not a valid ISBN

@inproceedings{Zenger2001extensible,
  location = {{New York, NY, USA}},
  title = {Extensible algebraic datatypes with defaults},
  isbn = {1-58113-415-0},
  url = {http://doi.acm.org/10.1145/507635.507665},
  doi = {10.1145/507635.507665},
  abstract = {A major problem for writing extensible software arises when recursively defined datatypes and operations on these types have to be extended simultaneously without modifying existing code. This paper introduces Extensible Algebraic Datatypes with defaults, which promote a simple programming pattern to solve this well-known problem. We show that it is possible to encode extensible algebraic datatypes in an object-oriented language, using a new design pattern for extensible visitors. Extensible algebraic datatypes have been successfully applied in the implementation of an extensible Java compiler. Our technique allows for the reuse of existing components in compiler extensions without the need for any adaptations.},
  booktitle = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '01},
  publisher = {{ACM}},
  urldate = {2014-05-14},
  date = {2001},
  pages = {241--252},
  author = {Zenger, Matthias and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/UNFXK2TP/Zenger_Odersky - 2001 - Extensible Algebraic Datatypes with Defaults.pdf}
}

@article{Qi2009implementation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0911.5203},
  primaryClass = {cs},
  title = {An Implementation of the Language Lambda Prolog Organized around Higher-Order Pattern Unification},
  url = {http://arxiv.org/abs/0911.5203},
  abstract = {This thesis concerns the implementation of Lambda Prolog, a higher-order logic programming language that supports the lambda-tree syntax approach to representing and manipulating formal syntactic objects. Lambda Prolog achieves its functionality by extending a Prolog-like language by using typed lambda terms as data structures that it then manipulates via higher-order unification and some new program-level abstraction mechanisms. These additional features raise new implementation questions that must be adequately addressed for Lambda Prolog to be an effective programming tool. We consider these questions here, providing eventually a virtual machine and compilation based realization. A key idea is the orientation of the computation model of Lambda Prolog around a restricted version of higher-order unification with nice algorithmic properties and appearing to encompass most interesting applications. Our virtual machine embeds a treatment of this form of unification within the structure of the Warren Abstract Machine that is used in traditional Prolog implementations. Along the way, we treat various auxiliary issues such as the low-level representation of lambda terms, the implementation of reduction on such terms and the optimized processing of types in computation. We also develop an actual implementation of Lambda Prolog called Teyjus Version 2. A characteristic of this system is that it realizes an emulator for the virtual machine in the C language a compiler in the OCaml language. We present a treatment of the software issues that arise from this kind of mixing of languages within one system and we discuss issues relevant to the portability of our virtual machine emulator across arbitrary architectures. Finally, we assess the the efficacy of our various design ideas through experiments carried out using the system.},
  urldate = {2014-05-18},
  date = {2009-11-27},
  keywords = {Computer Science - Programming Languages},
  author = {Qi, Xiaochu},
  file = {/Users/pgiarrusso/Zotero/storage/RTWUGQ4H/Qi - 2009 - An Implementation of the Language Lambda Prolog Organized around Higher-Order.pdf;/Users/pgiarrusso/Zotero/storage/GCKCMRJ7/0911.html}
}
% == BibLateX quality report for Qi2009implementation:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Brassel2010transforming,
  title = {Transforming functional logic programs into monadic functional programs},
  abstract = {Abstract. We present a high-level transformation scheme to translate lazy functional logic programs into pure Haskell programs. This transformation is based on a recent proposal to efficiently implement lazy non-deterministic computations in Haskell into monadic style. We build on this work and define a systematic method to transform lazy functional logic programs into monadic programs with explicit sharing. This results in a transformation scheme which produces high-level and flexible target code. For instance, the target code is parametric w.r.t. the concrete evaluation monad. Thus, different monad instances could, for example, define different search strategies (e.g., depth-first, breadth-first, parallel). We formally describe the basic compilation scheme and some useful extensions. 1},
  booktitle = {In Workshop on Functional and (Constraint) Logic Programming, Draft Proceedings},
  date = {2010},
  author = {Braßel, Bernd and Fischer, Sebastian and Hanus, Michael and Reck, Fabian},
  file = {/Users/pgiarrusso/Zotero/storage/FV6I69MQ/Braßel et al - 2010 - Transforming functional logic programs into monadic functional programs.pdf;/Users/pgiarrusso/Zotero/storage/FA48BSW2/summary.html}
}

@inproceedings{Miller2014spores,
  title = {Spores: A Type-Based Foundation for Closures in the Age of Concurrency and Distribution},
  url = {http://infoscience.epfl.ch/record/191239},
  shorttitle = {Spores},
  eventtitle = {European Conference on Object-Oriented Programming (ECOOP"14)},
  urldate = {2014-05-20},
  date = {2014},
  keywords = {type systems,closures,concurrent programming,distributed programming,functions},
  author = {Miller, Heather and Haller, Philipp and Odersky, Martin},
  file = {/Users/pgiarrusso/AeroFS/Downloads/PostPrint - spores_1.pdf;/Users/pgiarrusso/Zotero/storage/8A4PMX9P/Miller et al - 2014 - Spores - A Type-Based Foundation for Closures in the Age of Concurrency and.pdf;/Users/pgiarrusso/Zotero/storage/Q5B8GT4X/Miller et al - 2014 - Spores - A Type-Based Foundation for Closures in the Age of Concurrency and.pdf;/Users/pgiarrusso/Zotero/storage/9RMES5K6/191239.html}
}
% == BibLateX quality report for Miller2014spores:
% Missing required field 'booktitle'

@incollection{Hinze2012generic,
  title = {Generic Programming with Adjunctions},
  isbn = {978-3-642-32201-3 978-3-642-32202-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-32202-0_2},
  abstract = {Adjunctions are among the most important constructions in mathematics. These lecture notes show they are also highly relevant to datatype-generic programming. First, every fundamental datatype—sums, products, function types, recursive types—arises out of an adjunction. The defining properties of an adjunction give rise to well-known laws of the algebra of programming. Second, adjunctions are instrumental in unifying and generalising recursion schemes. We discuss a multitude of basic adjunctions and show that they are directly relevant to programming and to reasoning about programs.},
  number = {7470},
  booktitle = {Generic and Indexed Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-05-23},
  date = {2012-01-01},
  pages = {47-129},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Data Structures,Mathematical Logic and Formal Languages},
  author = {Hinze, Ralf},
  editor = {Gibbons, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/5FECICKJ/Hinze - 2012 - Generic Programming with Adjunctions.pdf;/Users/pgiarrusso/Zotero/storage/WDQSC9UB/978-3-642-32202-0_2.html}
}
% == BibLateX quality report for Hinze2012generic:
% 'isbn': not a valid ISBN

@article{Gill2014domainspecific,
  title = {Domain-specific Languages and Code Synthesis Using Haskell},
  volume = {12},
  issn = {1542-7730},
  url = {http://doi.acm.org/10.1145/2611429.2617811},
  doi = {10.1145/2611429.2617811},
  abstract = {Looking at embedded DSLs},
  number = {4},
  journaltitle = {Queue},
  urldate = {2014-05-23},
  date = {2014-04},
  pages = {30:30--30:43},
  author = {Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/AVMDRDJ7/Gill - 2014 - Domain-specific Languages and Code Synthesis Using Haskell.pdf}
}

@misc{Wellsnotes,
  title = {Notes on the λ-Calculus},
  url = {http://www.cwru.edu/artsci/math/wells/pub/pdf/lambda2009.pdf},
  urldate = {2014-05-24},
  author = {Wells, Charles},
  file = {/Users/pgiarrusso/Zotero/storage/NMMPZF7S/Wells - Notes on the λ-Calculus.pdf}
}
% == BibLateX quality report for Wellsnotes:
% Exactly one of 'date' / 'year' must be present

@thesis{Elliott1990extensions,
  location = {{Pittsburgh, PA, USA}},
  title = {Extensions and Applications of Higher-order Unification},
  institution = {{Carnegie Mellon University}},
  date = {1990},
  author = {Elliott, Conal Mullen},
  file = {/Users/pgiarrusso/Zotero/storage/HB568DIB/Elliott - 1990 - Extensions and Applications of Higher-order Unification.pdf},
  note = {AAI9107557}
}
% == BibLateX quality report for Elliott1990extensions:
% Missing required field 'type'

@incollection{Reynolds1984polymorphism,
  title = {Polymorphism is not set-theoretic},
  isbn = {978-3-540-13346-9 978-3-540-38891-3},
  url = {http://link.springer.com/chapter/10.1007/3-540-13346-1_7},
  abstract = {The polymorphic, or second-order, typed lambda calculus is an extension of the typed lambda calculus in which polymorphic functions can be defined. In this paper, we will prove that the standard set-theoretic model of the ordinary typed lambda calculus cannot be extended to model this language extension.},
  number = {173},
  booktitle = {Semantics of Data Types},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-05-25},
  date = {1984-01-01},
  pages = {145-156},
  keywords = {Programming Languages; Compilers; Interpreters,Mathematical Logic and Formal Languages},
  author = {Reynolds, John C.},
  editor = {Kahn, Gilles and MacQueen, David B. and Plotkin, Gordon},
  file = {/Users/pgiarrusso/Zotero/storage/AJQ6H7GT/Reynolds - 1984 - Polymorphism is not set-theoretic.pdf;/Users/pgiarrusso/Zotero/storage/WA6VG8HA/3-540-13346-1_7.html}
}
% == BibLateX quality report for Reynolds1984polymorphism:
% 'isbn': not a valid ISBN

@article{Jonnalagedda2014staged,
  title = {On Staged Parser Combinators for Efficient Data Processing},
  url = {http://infoscience.epfl.ch/record/198921},
  urldate = {2014-05-25},
  date = {2014},
  keywords = {staging,Dynamic programming,parallel processing,parser combinators},
  author = {Jonnalagedda, Manohar and Coppey, Thierry and Stucki, Sandro and Rompf, Tiark and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/5BUBINUB/Jonnalagedda et al - 2014 - On Staged Parser Combinators for Efficient Data Processing.pdf;/Users/pgiarrusso/Zotero/storage/6EQIAB82/198921.html}
}
% == BibLateX quality report for Jonnalagedda2014staged:
% Missing required field 'journaltitle'

@inproceedings{Gerakios2013forsaking,
  location = {{New York, NY, USA}},
  title = {Forsaking Inheritance: Supercharged Delegation in DelphJ},
  isbn = {978-1-4503-2374-1},
  url = {http://doi.acm.org/10.1145/2509136.2509535},
  doi = {10.1145/2509136.2509535},
  shorttitle = {Forsaking Inheritance},
  abstract = {We propose DelphJ: a Java-based OO language that eschews inheritance completely, in favor of a combination of class morphing and (deep) delegation. Compared to past delegation approaches, the novel aspect of our design is the ability to emulate the best aspects of inheritance while retaining maximum flexibility: using morphing, a class can select any of the methods of its delegatee and export them (if desired) or transform them (e.g., to add extra arguments or modify type signatures), yet without needing to name these methods explicitly and handle them one-by-one. Compared to past work on morphing, our approach adopts and adapts advanced delegation mechanisms, in order to add late binding capabilities and, thus, provide a full substitute of inheritance. Additionally, we explore complex semantic issues in the interaction of delegation with late binding. We present our language design both informally, with numerous examples, and formally in a core calculus.},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&\#38; Applications},
  series = {OOPSLA '13},
  publisher = {{ACM}},
  urldate = {2014-05-25},
  date = {2013},
  pages = {233--252},
  keywords = {delegation,language extensions,meta-programming,morphing,object composition,static reflection},
  author = {Gerakios, Prodromos and Biboudis, Aggelos and Smaragdakis, Yannis},
  file = {/Users/pgiarrusso/Zotero/storage/D2DD7E2C/Gerakios et al - 2013 - Forsaking Inheritance - Supercharged Delegation in DelphJ.pdf}
}

@inproceedings{Clifton2000multijava,
  location = {{New York, NY, USA}},
  title = {MultiJava: Modular Open Classes and Symmetric Multiple Dispatch for Java},
  isbn = {1-58113-200-X},
  url = {http://doi.acm.org/10.1145/353171.353181},
  doi = {10.1145/353171.353181},
  shorttitle = {MultiJava},
  abstract = {We present MultiJava, a backward-compatible extension to Java supporting open classes and symmetric multiple dispatch. Open classes allow one to add to the set of methods that an existing class supports without creating distinct subclasses or editing existing code. Unlike the "Visitor" design pattern, open classes do not require advance planning, and open classes preserve the ability to add new subclasses modularly and safely. Multiple dispatch offers several well-known advantages over the single dispatching of conventional object-oriented languages, including a simple solution to some kinds of "binary method" problems. MultiJava's multiple dispatch retains Java's existing class-based encapsulation properties. We adapt previous theoretical work to allow compilation units to be statically typechecked modularly and safely, ruling out any link-time or run-time type errors. We also present a n compilation scheme that operates modularly and incurs performance overhead only where open classes or multiple dispatching are actually used.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA '00},
  publisher = {{ACM}},
  urldate = {2014-05-27},
  date = {2000},
  pages = {130--145},
  author = {Clifton, Curtis and Leavens, Gary T. and Chambers, Craig and Millstein, Todd},
  file = {/Users/pgiarrusso/Zotero/storage/ZIXQVXSP/Clifton et al. - 2000 - MultiJava Modular Open Classes and Symmetric Mult.pdf}
}

@inproceedings{Mezini2003conquering,
  location = {{New York, NY, USA}},
  title = {Conquering Aspects with Caesar},
  isbn = {1-58113-660-9},
  url = {http://doi.acm.org/10.1145/643603.643613},
  doi = {10.1145/643603.643613},
  abstract = {Join point interception (JPI), is considered an important cornerstone of aspect-oriented languages. However, we claim that JPI alone does not suffice for a modular structuring of aspects. We propose CAESAR, a model for aspect-oriented programming with a higher-level module concept on top of JPI, which enables reuse and componentization of aspects, allows us to use aspects polymorphically, and introduces a novel concept for dynamic aspect deployment.},
  booktitle = {Proceedings of the 2Nd International Conference on Aspect-oriented Software Development},
  series = {AOSD '03},
  publisher = {{ACM}},
  urldate = {2014-05-27},
  date = {2003},
  pages = {90--99},
  author = {Mezini, Mira and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/54KJBJXK/Mezini and Ostermann - 2003 - Conquering Aspects with Caesar.pdf}
}

@inproceedings{Eichberg2014software,
  location = {{New York, NY, USA}},
  title = {A Software Product Line for Static Analyses: The OPAL Framework},
  isbn = {978-1-4503-2919-4},
  url = {http://doi.acm.org/10.1145/2614628.2614630},
  doi = {10.1145/2614628.2614630},
  shorttitle = {A Software Product Line for Static Analyses},
  abstract = {Implementations of static analyses are usually tailored toward a single goal to be efficient, hampering reusability and adaptability of the components of an analysis. To solve these issues, we propose to implement static analyses as highly-configurable software product lines (SPLs). Furthermore, we also discuss an implementation of an SPL for static analyses -- called OPAL -- that uses advanced language features offered by the Scala programming language to get an easily adaptable and (type-)safe software product line. OPAL is a general purpose library for static analysis of Java Bytecode that is already successfully used. We present OPAL and show how a design based on software produce line engineering benefits the implementation of static analyses with the framework.},
  booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on the State of the Art in Java Program Analysis},
  series = {SOAP '14},
  publisher = {{ACM}},
  urldate = {2014-06-03},
  date = {2014},
  pages = {1--6},
  keywords = {abstract interpretation,static analysis,program analysis,design,software product line engineering},
  author = {Eichberg, Michael and Hermann, Ben}
}

@report{Abal40,
  title = {40 Variability Bugs in the Linux Kernel — A Qualitative Study},
  author = {Abal, Iago and Braband, Claus and Wąsowski, Andrzej},
  file = {/Users/pgiarrusso/Zotero/storage/CB5WR8HK/Abal et al - 40 Variability Bugs in the Linux Kernel — A Qualitative Study.pdf}
}
% == BibLateX quality report for Abal40:
% Exactly one of 'date' / 'year' must be present
% Missing required field 'type'
% Missing required field 'institution'

@inproceedings{Angiuli2014homotopical,
  title = {Homotopical Patch Theory},
  abstract = {Homotopy type theory is an extension of Martin-Löf type theory, based on a correspondence with homotopy theory and higher category theory. The propositional equality type becomes proof-relevant, and acts like paths in a space. Higher inductive types are a new class of datatypes which are specified by constructors not only for points but also for paths.  In this paper, we show how patch theory in the style of the Darcs version control system can be developed in homotopy type theory. We reformulate patch theory using the tools of homotopy type theory, and clearly separate formal theories of patches from their interpretation in terms of basic revision control mechanisms.  A patch theory is presented by a higher inductive type. Models of a patch theory are functions from that type, which, because function are functors, automatically preserve the structure of patches. Several standard tools of homotopy theory come into play, demonstrating the use of these methods in a practical programming context.},
  eventtitle = {ICFP},
  date = {2014},
  author = {Angiuli, Carlo and Morehouse, Ed and Licata, Daniel R. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/T7PSDB72/Angiuli et al - 2014 - Homotopical Patch Theory.pdf}
}
% == BibLateX quality report for Angiuli2014homotopical:
% Missing required field 'booktitle'

@inproceedings{Maes1987concepts,
  location = {{New York, NY, USA}},
  title = {Concepts and Experiments in Computational Reflection},
  isbn = {0-89791-247-0},
  url = {http://doi.acm.org/10.1145/38765.38821},
  doi = {10.1145/38765.38821},
  abstract = {This paper brings some perspective to various concepts in computational reflection. A definition of computational reflection is presented, the importance of computational reflection is discussed and the architecture of languages that support reflection is studied. Further, this paper presents a survey of some experiments in reflection which have been performed. Examples of existing procedural, logic-based and rule-based languages with an architecture for reflection are briefly presented. The main part of the paper describes an original experiment to introduce a reflective architecture in an object-oriented language. It stresses the contributions of this language to the field of object-oriented programming and illustrates the new programming style made possible. The examples show that a lot of programming problems that were previously handled on an ad hoc basis, can in a reflective architecture be solved more elegantly.},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages and Applications},
  series = {OOPSLA '87},
  publisher = {{ACM}},
  urldate = {2014-06-05},
  date = {1987},
  pages = {147--155},
  author = {Maes, Pattie},
  file = {/Users/pgiarrusso/Zotero/storage/VFMVNDK8/Maes - 1987 - Concepts and Experiments in Computational Reflection.pdf}
}

@thesis{Mitschke2014scalable,
  title = {Scalable Automated Incrementalization for Real-Time Static Analyses},
  url = {http://tuprints.ulb.tu-darmstadt.de/3815/},
  abstract = {This thesis proposes a framework for easy development of static analyses, whose results are incrementalized to provide instantaneous feedback in an integrated development environment (IDE).

Today, IDEs feature many tools that have static analyses as their foundation to assess software quality and catch correctness problems. 
Yet, these tools often fail to provide instantaneous feedback and are thus restricted to nightly build processes. This precludes developers from fixing issues at their inception time, i.e., when the problem and the developed solution are both still fresh in mind.

In order to provide instantaneous feedback, incrementalization is a well-known technique that utilizes the fact that developers make only small changes to the code and, hence, analysis results can be re-computed fast based on these changes. Yet, incrementalization requires carefully crafted static analyses. Thus, a manual approach to incrementalization is unattractive. Automated incrementalization can alleviate these problems and allows analyses writers to formulate their analyses as queries with the full data set in mind, without worrying over the semantics of incremental changes.

Existing approaches to automated incrementalization utilize standard technologies, such as deductive databases, that provide declarative query languages, yet also require to materialize the full dataset in main-memory, i.e., the memory is permanently blocked by the data required for the analyses. Other standard technologies such as relational databases offer better scalability due to persistence, yet require large transaction times for data. Both technologies are not a perfect match for integrating static analyses into an IDE, since the underlying data, i.e., the code base, is already persisted and managed by the IDE. Hence, transitioning the data into a database is redundant work.

In this thesis a novel approach is proposed that provides a declarative query language and automated incrementalization, yet retains in memory only a necessary minimum of data, i.e., only the data that is required for the incrementalization. The approach allows to declare static analyses as incrementally maintained views, where the underlying formalism for incrementalization is the relational algebra with extensions for object-orientation and recursion. The algebra allows to deduce which data is the necessary minimum for incremental maintenance and indeed shows that many views are self-maintainable, i.e., do not require to materialize memory at all. In addition an optimization for the algebra is proposed that allows to widen the range of self-maintainable views, based on domain knowledge of the underlying data. The optimization works similar to declaring primary keys for databases, i.e., the optimization is declared on the schema of the data, and defines which data is incrementally maintained in the same scope. The scope makes all analyses (views) that correlate only data within the boundaries of the scope self-maintainable.

The approach is implemented as an embedded domain specific language in a general-purpose programming language. The implementation can be understood as a database-like engine with an SQL-style query language and the execution semantics of the relational algebra. As such the system is a general purpose database-like query engine and can be used to incrementalize other domains than static analyses. To evaluate the approach a large variety of static analyses were sampled from real-world tools and formulated as incrementally maintained views in the implemented engine.},
  institution = {{TU Darmstadt}},
  type = {Ph.D. Thesis},
  urldate = {2014-06-07},
  date = {2014-05-05},
  author = {Mitschke, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/U826V2IZ/3815.html}
}

@inproceedings{Dreyer2007modular,
  location = {{New York, NY, USA}},
  title = {Modular Type Classes},
  isbn = {1-59593-575-4},
  url = {http://doi.acm.org/10.1145/1190216.1190229},
  doi = {10.1145/1190216.1190229},
  abstract = {ML modules and Haskell type classes have proven to be highly effective tools for program structuring. Modules emphasize explicit configuration of program components and the use of data abstraction. Type classes emphasize implicit program construction and ad hoc polymorphism. In this paper, we show how the implicitly-typed style of type class programming may be supported within the framework of an explicitly-typed module language by viewing type classes as a particular mode of use of modules. This view offers a harmonious integration of modules and type classes, where type class features, such as class hierarchies and associated types, arise naturally as uses of existing module-language constructs, such as module hierarchies and type components. In addition, programmers have explicit control over which type class instances are available for use by type inference in a given scope. We formalize our approach as a Harper-Stone-style elaboration relation, and provide a sound type inference algorithm as a guide to implementation.},
  booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '07},
  publisher = {{ACM}},
  urldate = {2014-06-07},
  date = {2007},
  pages = {63--70},
  keywords = {type systems,type inference,type classes,modules},
  author = {Dreyer, Derek and Harper, Robert and Chakravarty, Manuel M. T. and Keller, Gabriele},
  file = {/Users/pgiarrusso/Zotero/storage/IJX89CDK/Dreyer et al. - 2007 - Modular Type Classes.pdf}
}

@article{Miller2014selfassembly,
  title = {Self-Assembly: Lightweight Language Extension and Datatype Generic Programming, All-in-One!},
  url = {http://infoscience.epfl.ch/record/199389},
  shorttitle = {Self-Assembly},
  urldate = {2014-06-09},
  date = {2014},
  keywords = {generative programming,meta-programming,datatype generic programming,language extension},
  author = {Miller, Heather and Haller, Philipp and Oliveira, d S. and Bruno, C.},
  file = {/Users/pgiarrusso/Zotero/storage/Q8T39T46/Miller et al - 2014 - Self-Assembly - Lightweight Language Extension and Datatype Generic Programming,.pdf;/Users/pgiarrusso/Zotero/storage/UJJJAUKW/199389.html}
}
% == BibLateX quality report for Miller2014selfassembly:
% Missing required field 'journaltitle'

@article{Palem2014inexactness,
  langid = {english},
  title = {Inexactness and a future of computing},
  volume = {372},
  issn = {1364-503X, 1471-2962},
  url = {http://rsta.royalsocietypublishing.org/content/372/2018/20130281},
  doi = {10.1098/rsta.2013.0281},
  abstract = {As pressures, notably from energy consumption, start impeding the growth and scale of computing systems, inevitably, designers and users are increasingly considering the prospect of trading accuracy or exactness. This paper is a perspective on the progress in embracing this somewhat unusual philosophy of innovating computing systems that are designed to be inexact or approximate, in the interests of realizing extreme efficiencies. With our own experience in designing inexact physical systems including hardware as a backdrop, we speculate on the rich potential for considering inexactness as a broad emerging theme if not an entire domain for investigation for exciting research and innovation. If this emerging trend to pursuing inexactness persists and grows, then we anticipate an increasing need to consider system co-design where application domain characteristics and technology features interplay in an active manner. A noteworthy early example of this approach is our own excursion into tailoring and hence co-designing floating point arithmetic units guided by the needs of stochastic climate models. This approach requires a unified effort between software and hardware designers that does away with the normal clean abstraction layers between the two.},
  number = {2018},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  shortjournal = {Phil. Trans. R. Soc. A},
  urldate = {2014-06-09},
  date = {2014-06-28},
  pages = {20130281},
  keywords = {approximation algorithms,heuristics,inexact computing,information theory,probabilistic CMOS,randomized algorithms},
  author = {Palem, Krishna V.},
  file = {/Users/pgiarrusso/Zotero/storage/JS5QVURW/20130281.html},
  eprinttype = {pmid},
  eprint = {24842028}
}
% == BibLateX quality report for Palem2014inexactness:
% 'issn': not a valid ISSN

@book{Malekievaluation,
  title = {An Evaluation of Vectorizing Compilers},
  abstract = {Abstract—Most of today’s processors include vector units that have been designed to speedup single threaded programs. Although vector instructions can deliver high performance, writing vector code in assembly language or using intrinsics in high level languages is a time consuming and error-prone task. The alternative is to automate the process of vectorization by using vectorizing compilers. This paper evaluates how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II. We evaluated three compilers:},
  author = {Maleki, Saeed and Gao, Yaoqing and Garzarán, María J. and Wong, Tommy and Padua, David A.},
  file = {/Users/pgiarrusso/Zotero/storage/XXQGX6Z2/Maleki et al - An Evaluation of Vectorizing Compilers.pdf;/Users/pgiarrusso/Zotero/storage/FMCWBQV4/summary.html}
}
% == BibLateX quality report for Malekievaluation:
% Exactly one of 'date' / 'year' must be present

@inproceedings{Ricketts2014automating,
  location = {{New York, NY, USA}},
  title = {Automating Formal Proofs for Reactive Systems},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594338},
  doi = {10.1145/2594291.2594338},
  abstract = {Implementing systems in proof assistants like Coq and proving their correctness in full formal detail has consistently demonstrated promise for making extremely strong guarantees about critical software, ranging from compilers and operating systems to databases and web browsers. Unfortunately, these verifications demand such heroic manual proof effort, even for a single system, that the approach has not been widely adopted. We demonstrate a technique to eliminate the manual proof burden for verifying many properties within an entire class of applications, in our case reactive systems, while only expending effort comparable to the manual verification of a single system. A crucial insight of our approach is simultaneously designing both (1) a domain-specific language (DSL) for expressing reactive systems and their correctness properties and (2) proof automation which exploits the constrained language of both programs and properties to enable fully automatic, pushbutton verification. We apply this insight in a deeply embedded Coq DSL, dubbed Reflex, and illustrate Reflex's expressiveness by implementing and automatically verifying realistic systems including a modern web browser, an SSH server, and a web server. Using Reflex radically reduced the proof burden: in previous, similar versions of our benchmarks written in Coq by experts, proofs accounted for over 80\% of the code base; our versions require no manual proofs.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '14},
  publisher = {{ACM}},
  urldate = {2014-06-11},
  date = {2014},
  pages = {452--462},
  keywords = {domain-specific languages,dependent types,interactive proof assistants,proof automation,reactive systems},
  author = {Ricketts, Daniel and Robert, Valentin and Jang, Dongseok and Tatlock, Zachary and Lerner, Sorin},
  file = {/Users/pgiarrusso/Zotero/storage/MTQ2M9AQ/Ricketts et al - 2014 - Automating Formal Proofs for Reactive Systems.pdf}
}

@inproceedings{Xiong2001spl,
  location = {{New York, NY, USA}},
  title = {SPL: A Language and Compiler for DSP Algorithms},
  isbn = {1-58113-414-2},
  url = {http://doi.acm.org/10.1145/378795.378860},
  doi = {10.1145/378795.378860},
  shorttitle = {SPL},
  abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as “hard-wired” systems like FFTW.},
  booktitle = {Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation},
  series = {PLDI '01},
  publisher = {{ACM}},
  urldate = {2014-06-11},
  date = {2001},
  pages = {298--308},
  author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David}
}

@inproceedings{Clifford2014allocation,
  location = {{New York, NY, USA}},
  title = {Allocation Folding Based on Dominance},
  isbn = {978-1-4503-2921-7},
  url = {http://doi.acm.org/10.1145/2602988.2602994},
  doi = {10.1145/2602988.2602994},
  abstract = {Memory management system performance is of increasing importance in today's managed languages. Two lingering sources of overhead are the direct costs of memory allocations and write barriers. This paper introduces it allocation folding, an optimization technique where the virtual machine automatically folds multiple memory allocation operations in optimized code together into a single, larger it allocation group. An allocation group comprises multiple objects and requires just a single bounds check in a bump-pointer style allocation, rather than a check for each individual object. More importantly, all objects allocated in a single allocation group are guaranteed to be contiguous after allocation and thus exist in the same generation, which makes it possible to statically remove write barriers for reference stores involving objects in the same allocation group. Unlike object inlining, object fusing, and object colocation, allocation folding requires no special connectivity or ownership relation between the objects in an allocation group. We present our analysis algorithm to determine when it is safe to fold allocations together and discuss our implementation in V8, an open-source, production JavaScript virtual machine. We present performance results for the Octane and Kraken benchmark suites and show that allocation folding is a strong performance improvement, even in the presence of some heap fragmentation. Additionally, we use four hand-selected benchmarks JPEGEncoder, NBody, Soft3D, and Textwriter where allocation folding has a large impact.},
  booktitle = {Proceedings of the 2014 International Symposium on Memory Management},
  series = {ISMM '14},
  publisher = {{ACM}},
  urldate = {2014-06-12},
  date = {2014},
  pages = {15--24},
  keywords = {javascript,garbage collection,dynamic optimization,memory managment,write barriers},
  author = {Clifford, Daniel and Payer, Hannes and Starzinger, Michael and Titzer, Ben L.},
  file = {/Users/pgiarrusso/Zotero/storage/HERD4QJ5/Clifford et al - 2014 - Allocation Folding Based on Dominance.pdf}
}

@inproceedings{Rossberg2010fing,
  location = {{New York, NY, USA}},
  title = {F-ing Modules},
  isbn = {978-1-60558-891-9},
  url = {http://doi.acm.org/10.1145/1708016.1708028},
  doi = {10.1145/1708016.1708028},
  abstract = {ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being "complex" and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we give a very simple elaboration semantics for a full-featured, higher-order ML-like module language. Our elaboration defines the meaning of module expressions by a straightforward, compositional translation into vanilla System F-ω (the higher-order polymorphic λ-calculus), under plain F-ω typing environments. We thereby show that ML modules are merely a particular mode of use of System F-ω. Our module language supports the usual second-class modules with Standard ML-style generative functors and local module definitions. To demonstrate the versatility of our approach, we further extend the language with the ability to package modules as first-class values---a very simple extension, as it turns out. Our approach also scales to handle OCaml-style applicative functor semantics, but the details are significantly more subtle, so we leave their presentation to a future, expanded version of this paper. Lastly, we report on our experience using the "locally nameless" approach in order to mechanize the soundness of our elaboration semantics in Coq.},
  booktitle = {Proceedings of the 5th ACM SIGPLAN Workshop on Types in Language Design and Implementation},
  series = {TLDI '10},
  publisher = {{ACM}},
  urldate = {2014-06-12},
  date = {2010},
  pages = {89--102},
  keywords = {abstract data types,elaboration,existential types,first-class modules,ML modules,system f,type systems},
  author = {Rossberg, Andreas and Russo, Claudio V. and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/XEC62NMC/Rossberg et al - 2010 - F-ing Modules.pdf}
}

@inproceedings{Zeller2011failure,
  location = {{New York, NY, USA}},
  title = {Failure is a Four-letter Word: A Parody in Empirical Research},
  isbn = {978-1-4503-0709-3},
  url = {http://doi.acm.org/10.1145/2020390.2020395},
  doi = {10.1145/2020390.2020395},
  shorttitle = {Failure is a Four-letter Word},
  abstract = {Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74\% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce "actionable" results.},
  booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
  series = {Promise '11},
  publisher = {{ACM}},
  urldate = {2014-06-13},
  date = {2011},
  pages = {5:1--5:7},
  keywords = {empirical research,parody},
  author = {Zeller, Andreas and Zimmermann, Thomas and Bird, Christian}
}

@article{Scherer2013gadts,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.2903},
  primaryClass = {cs},
  title = {GADTs meet subtyping},
  url = {http://arxiv.org/abs/1301.2903},
  abstract = {While generalized algebraic datatypes ($\backslash$GADTs) are now considered well-understood, adding them to a language with a notion of subtyping comes with a few surprises. What does it mean for a $\backslash$GADT parameter to be covariant? The answer turns out to be quite subtle. It involves fine-grained properties of the subtyping relation that raise interesting design questions. We allow variance annotations in $\backslash$GADT definitions, study their soundness, and present a sound and complete algorithm to check them. Our work may be applied to real-world ML-like languages with explicit subtyping such as OCaml, or to languages with general subtyping constraints.},
  urldate = {2014-06-14},
  date = {2013-01-14},
  keywords = {Computer Science - Programming Languages},
  author = {Scherer, Gabriel and Rémy, Didier},
  file = {/Users/pgiarrusso/Zotero/storage/Q5RRTQJI/Scherer_Rémy - 2013 - GADTs meet subtyping.pdf;/Users/pgiarrusso/Zotero/storage/FH3HG4AN/1301.html},
  note = {22nd European Symposium on Programming (ESOP), Rome : Italy (2013)}
}
% == BibLateX quality report for Scherer2013gadts:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Liu2013intel,
  location = {{New York, NY, USA}},
  title = {The Intel Labs Haskell Research Compiler},
  isbn = {978-1-4503-2383-3},
  url = {http://doi.acm.org/10.1145/2503778.2503779},
  doi = {10.1145/2503778.2503779},
  abstract = {The Glasgow Haskell Compiler (GHC) is a well supported optimizing compiler for the Haskell programming language, along with its own extensions to the language and libraries. Haskell's lazy semantics imposes a runtime model which is in general difficult to implement efficiently. GHC achieves good performance across a wide variety of programs via aggressive optimization taking advantage of the lack of side effects, and by targeting a carefully tuned virtual machine. The Intel Labs Haskell Research Compiler uses GHC as a frontend, but provides a new whole-program optimizing backend by compiling the GHC intermediate representation to a relatively generic functional language compilation platform. We found that GHC's external Core language was relatively easy to use, but reusing GHC's libraries and achieving full compatibility were harder. For certain classes of programs, our platform provides substantial performance benefits over GHC alone, performing 2x faster than GHC with the LLVM backend on selected modern performance-oriented benchmarks; for other classes of programs, the benefits of GHC's tuned virtual machine continue to outweigh the benefits of more aggressive whole program optimization. Overall we achieve parity with GHC with the LLVM backend. In this paper, we describe our Haskell compiler stack, its implementation and optimization approach, and present benchmark results comparing it to GHC.},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '13},
  publisher = {{ACM}},
  urldate = {2014-06-14},
  date = {2013},
  pages = {105--116},
  keywords = {Haskell,compiler optimization,functional language compiler},
  author = {Liu, Hai and Glew, Neal and Petersen, Leaf and Anderson, Todd A.},
  file = {/Users/pgiarrusso/Zotero/storage/IPSMHS7U/Liu et al - 2013 - The Intel Labs Haskell Research Compiler.pdf}
}

@article{Gonthier2013how,
  title = {How to make ad hoc proof automation less ad hoc},
  volume = {23},
  doi = {10.1017/S0956796813000051},
  abstract = {Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself.},
  issue = {Special Issue 04},
  journaltitle = {Journal of Functional Programming},
  date = {2013},
  pages = {357-401},
  author = {Gonthier, Georges and Ziliani, Beta and Nanevski, Aleksandar and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/HV8CXUFW/Gonthier et al - 2013 - How to make ad hoc proof automation less ad hoc.pdf;/Users/pgiarrusso/Zotero/storage/WQM5NP8M/displayAbstract.html}
}

@incollection{Cheney2013theory,
  title = {Toward a Theory of Self-explaining Computation},
  isbn = {978-3-642-41659-0 978-3-642-41660-6},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-41660-6_9},
  abstract = {Provenance techniques aim to increase the reliability of human judgments about data by making its origin and derivation process explicit. Originally motivated by the needs of scientific databases and scientific computation, provenance has also become a major issue for business and government data on the Web. However, so far provenance has been studied only in relatively restrictive settings: typically, for data stored in databases or scientific workflow systems, and processed by query or workflow languages of limited expressiveness. Long-term provenance solutions require an understanding of provenance in other settings, particularly the general-purpose programming or scripting languages that are used to glue different components such as databases, Web services and workflows together. Moreover, what is required is not only an account of mechanisms for recording provenance, but also a theory of what it means for provenance information to explain or justify a computation. In this paper, we begin to outline a such a theory of self-explaining computation. We introduce a model of provenance for a simple imperative language based on operational derivations and explore its properties.},
  number = {8000},
  booktitle = {In Search of Elegance in the Theory and Practice of Computation},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-06-15},
  date = {2013-01-01},
  pages = {193-216},
  keywords = {Programming Languages; Compilers; Interpreters,Logics and Meanings of Programs,Database Management},
  author = {Cheney, James and Acar, Umut A. and Perera, Roly},
  editor = {Tannen, Val and Wong, Limsoon and Libkin, Leonid and Fan, Wenfei and Tan, Wang-Chiew and Fourman, Michael},
  file = {/Users/pgiarrusso/Zotero/storage/PJQQ3W72/Cheney et al - 2013 - Toward a Theory of Self-explaining Computation.pdf;/Users/pgiarrusso/Zotero/storage/KTHPHG2C/978-3-642-41660-6_9.html}
}
% == BibLateX quality report for Cheney2013theory:
% 'isbn': not a valid ISBN

@inproceedings{Perera2012functional,
  location = {{New York, NY, USA}},
  title = {Functional Programs That Explain Their Work},
  isbn = {978-1-4503-1054-3},
  url = {http://doi.acm.org/10.1145/2364527.2364579},
  doi = {10.1145/2364527.2364579},
  abstract = {We present techniques that enable higher-order functional computations to "explain" their work by answering questions about how parts of their output were calculated. As explanations, we consider the traditional notion of program slices, which we show can be inadequate, and propose a new notion: trace slices. We present techniques for specifying flexible and rich slicing criteria based on partial expressions, parts of which have been replaced by holes. We characterise program slices in an algorithm-independent fashion and show that a least slice for a given criterion exists. We then present an algorithm, called unevaluation, for computing least program slices from computations reified as traces. Observing a limitation of program slices, we develop a notion of trace slice as another form of explanation and present an algorithm for computing them. The unevaluation algorithm can be applied to any subtrace of a trace slice to compute a program slice whose evaluation generates that subtrace. This close correspondence between programs, traces, and their slices can enable the programmer to understand a computation interactively, in terms of the programming language in which the computation is expressed. We present an implementation in the form of a tool, discuss some important practical implementation concerns and present some techniques for addressing them.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '12},
  publisher = {{ACM}},
  urldate = {2014-06-15},
  date = {2012},
  pages = {365--376},
  keywords = {debugging,program slicing,provenance},
  author = {Perera, Roly and Acar, Umut A. and Cheney, James and Levy, Paul Blain},
  file = {/Users/pgiarrusso/Zotero/storage/A3CD38VB/Perera et al - 2012 - Functional Programs That Explain Their Work.pdf}
}

@article{Acar2013core,
  title = {A core calculus for provenance},
  volume = {21},
  url = {http://dx.doi.org/10.3233/JCS-130487},
  doi = {10.3233/JCS-130487},
  abstract = {Provenance is an increasing concern due to the ongoing revolution in sharing and processing scientific data on the Web and in other computer systems. It is proposed that many computer systems will need to become provenance-aware in order to provide satisfactory accountability, reproducibility, and trust for scientific or other high-value data. To date, there is not a consensus concerning appropriate formal models or security properties for provenance. In previous work, we introduced a formal framework for provenance security and proposed formal definitions of properties called disclosure and obfuscation. In this article, we study refined notions of positive and negative disclosure and obfuscation in a concrete setting, that of a general-purpose programing language. Previous models of provenance have focused on special-purpose languages such as workflows and database queries. We consider a higher-order, functional language with sums, products, and recursive types and functions, and equip it with a tracing semantics in which traces themselves can be replayed as computations. We present an annotation-propagation framework that supports many provenance views over traces, including standard forms of provenance studied previously. We investigate some relationships among provenance views and develop some partial solutions to the disclosure and obfuscation problems, including correct algorithms for disclosure and positive obfuscation based on trace slicing.},
  number = {6},
  journaltitle = {Journal of Computer Security},
  urldate = {2014-06-15},
  date = {2013-01-01},
  pages = {919-969},
  author = {Acar, Umut A. and Ahmed, Amal and Cheney, James and Perera, Roly},
  file = {/Users/pgiarrusso/Zotero/storage/KZR2VPQB/Acar et al - 2013 - A core calculus for provenance.pdf;/Users/pgiarrusso/Zotero/storage/9SJ2IPUP/q82676p524312663.html}
}

@inproceedings{Hutton1998fold,
  location = {{New York, NY, USA}},
  title = {Fold and Unfold for Program Semantics},
  isbn = {1-58113-024-4},
  url = {http://doi.acm.org/10.1145/289423.289457},
  doi = {10.1145/289423.289457},
  abstract = {In this paper we explain how recursion operators can be used to structure and reason about program semantics within a functional language. In particular, we show how the recursion operator fold can be used to structure denotational semantics, how the dual recursion operator unfold can be used to structure operational semantics, and how algebraic properties of these operators can be used to reason about program semantics. The techniques are explained with the aid of two main examples, the first concerning arithmetic expressions, and the second concerning Milner's concurrent language CCS. The aim of the paper is to give functional programmers new insights into recursion operators, program semantics, and the relationships between them.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2014-06-17},
  date = {1998},
  pages = {280--288},
  author = {Hutton, Graham},
  file = {/Users/pgiarrusso/Zotero/storage/HG7TAUT4/Hutton - 1998 - Fold and Unfold for Program Semantics.pdf}
}

@inproceedings{Luth2002composing,
  location = {{New York, NY, USA}},
  title = {Composing Monads Using Coproducts},
  isbn = {1-58113-487-8},
  url = {http://doi.acm.org/10.1145/581478.581492},
  doi = {10.1145/581478.581492},
  abstract = {Monads are a useful abstraction of computation, as they model diverse computational effects such as stateful computations, exceptions and I/O in a uniform manner. Their potential to provide both a modular semantics and a modular programming style was soon recognised. However, in general, monads proved difficult to compose and so research focused on special mechanisms for their composition such as distributive monads and monad transformers.We present a new approach to this problem which is general in that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning monads and computationally expressive in supporting a canonical recursion operator. In a nutshell, we propose that two monads should be composed by taking their coproduct. Although abstractly this is a simple idea, the actual construction of the coproduct of two monads is non-trivial. We outline this construction, show how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We also discuss its relationship with other ways of combining monads, in particular distributive laws for monads and monad transformers.},
  booktitle = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '02},
  publisher = {{ACM}},
  urldate = {2014-06-22},
  date = {2002},
  pages = {133--144},
  author = {Lüth, Christoph and Ghani, Neil},
  file = {/Users/pgiarrusso/Zotero/storage/65JEHVE7/Lüth_Ghani - 2002 - Composing Monads Using Coproducts.pdf}
}

@inproceedings{Greenman2014getting,
  location = {{New York, NY, USA}},
  title = {Getting F-bounded Polymorphism into Shape},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594308},
  doi = {10.1145/2594291.2594308},
  abstract = {We present a way to restrict recursive inheritance without sacrificing the benefits of F-bounded polymorphism. In particular, we distinguish two new concepts, materials and shapes, and demonstrate through a survey of 13.5 million lines of open-source generic-Java code that these two concepts never actually overlap in practice. With this Material-Shape Separation, we prove that even naïve type-checking algorithms are sound and complete, some of which address problems that were unsolvable even under the existing proposals for restricting inheritance. We illustrate how the simplicity of our design reflects the design intuitions employed by programmers and potentially enables new features coming into demand for upcoming programming languages.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '14},
  publisher = {{ACM}},
  urldate = {2014-06-23},
  date = {2014},
  pages = {89--99},
  keywords = {subtyping,decidability,F-bounded polymorphism,higher-kinded types,joins,materials,separation,shapes,variance},
  author = {Greenman, Ben and Muehlboeck, Fabian and Tate, Ross},
  file = {/Users/pgiarrusso/Zotero/storage/VPPBAQ88/Greenman et al - 2014 - Getting F-bounded Polymorphism into Shape.pdf}
}

@inproceedings{Lammel2005scrap,
  location = {{New York, NY, USA}},
  title = {Scrap Your Boilerplate with Class: Extensible Generic Functions},
  isbn = {1-59593-064-7},
  url = {http://doi.acm.org/10.1145/1086365.1086391},
  doi = {10.1145/1086365.1086391},
  shorttitle = {Scrap Your Boilerplate with Class},
  abstract = {The 'Scrap your boilerplate' approach to generic programming allows the programmer to write generic functions that can traverse arbitrary data structures, and yet have type-specific cases. However, the original approach required all the type-specific cases to be supplied at once, when the recursive knot of generic function definition is tied. Hence, generic functions were closed. In contrast, Haskell's type classes support open, or extensible, functions that can be extended with new type-specific cases as new data types are defined. In this paper, we extend the 'Scrap your boilerplate' approach to support this open style. On the way, we demonstrate the desirability of abstraction over type classes, and the usefulness of recursive dictionarie.},
  booktitle = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '05},
  publisher = {{ACM}},
  urldate = {2014-06-24},
  date = {2005},
  pages = {204--215},
  keywords = {generic programming,typecase,extensibility,type classes,recursive dictionaries},
  author = {Lämmel, Ralf and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/JDZE2HUU/Lämmel_Jones - 2005 - Scrap Your Boilerplate with Class - Extensible Generic Functions.pdf}
}

@inproceedings{Cheney2014query,
  location = {{New York, NY, USA}},
  title = {Query Shredding: Efficient Relational Evaluation of Queries over Nested Multisets},
  isbn = {978-1-4503-2376-5},
  url = {http://doi.acm.org/10.1145/2588555.2612186},
  doi = {10.1145/2588555.2612186},
  shorttitle = {Query Shredding},
  abstract = {Nested relational query languages have been explored extensively, and underlie industrial language-integrated query systems such as Microsoft's LINQ. However, relational databases do not natively support nested collections in query results. This can lead to major performance problems: if programmers write queries that yield nested results, then such systems typically either fail or generate a large number of queries. We present a new approach to query shredding, which converts a query returning nested data to a fixed number of SQL queries. Our approach, in contrast to prior work, handles multiset semantics, and generates an idiomatic SQL:1999 query directly from a normal form for nested queries. We provide a detailed description of our translation and present experiments showing that it offers comparable or better performance than a recent alternative approach on a range of examples.},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '14},
  publisher = {{ACM}},
  urldate = {2014-06-25},
  date = {2014},
  pages = {1027--1038},
  keywords = {language-integrated query,querying nested collections},
  author = {Cheney, James and Lindley, Sam and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/PZW5MIH5/Cheney et al - 2014 - Query Shredding - Efficient Relational Evaluation of Queries over Nested.pdf}
}

@incollection{Lieberman2006feasibility,
  langid = {english},
  title = {Feasibility Studies for Programming in Natural Language},
  isbn = {978-1-4020-4220-1 978-1-4020-5386-3},
  url = {http://link.springer.com/chapter/10.1007/1-4020-5386-X_20},
  abstract = {We think it is time to take another look at an old dream—that one could program a computer by speaking to it in natural language. Programming in natural language might seem impossible, because it would appear to require complete natural language understanding and dealing with the vagueness of human descriptions of programs. Butwe think that several developments might nowmake programming in natural language feasible. First, improved broad coverage natural language parsers and semantic extraction techniques permit partial understanding. Second, mixed-initiative dialogues can be used for meaning disambiguation. And finally, where direct understanding techniques fail, we hope to fall back on Programming by Example, and other techniques for specifying the program in a more fail-soft manner. To assess the feasibility of this project, as a first step, we are studying how non-programming users describe programs in unconstrained natural language.We are exploring how to design dialogs that help the user make precise their intentions for the program, while constraining them as little as possible. Key words. natural language programming, natural language processing, parsing, part-of-speech tagging, computer science education, programming languages, scripting languages, computer games.},
  number = {9},
  booktitle = {End User Development},
  series = {Human-Computer Interaction Series},
  publisher = {{Springer Netherlands}},
  urldate = {2014-06-26},
  date = {2006-01-01},
  pages = {459-473},
  keywords = {Software Engineering,Computer Science; general,User Interfaces and Human Computer Interaction},
  author = {Lieberman, Henry and Liu, Hugo},
  editor = {Lieberman, Henry and Paternò, Fabio and Wulf, Volker},
  file = {/Users/pgiarrusso/Zotero/storage/7VGW6E4U/Lieberman_Liu - 2006 - Feasibility Studies for Programming in Natural Language.pdf;/Users/pgiarrusso/Zotero/storage/GNKUGFXU/1-4020-5386-X_20.html}
}
% == BibLateX quality report for Lieberman2006feasibility:
% 'isbn': not a valid ISBN

@article{Wehr2008subtyping,
  title = {Subtyping existential types},
  url = {http://privat.spinnerstefan.de/publications/Wehr_Subtyping_existential_types_TR.pdf},
  journaltitle = {10th FTfJP, informal proceedings},
  urldate = {2014-06-28},
  date = {2008},
  author = {Wehr, Stefan and Thiemann, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/4FGCUI8Z/Wehr_Thiemann - 2008 - Subtyping existential types.pdf}
}

@incollection{Rytz2012lightweight,
  langid = {english},
  title = {Lightweight Polymorphic Effects},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-31057-7_13},
  abstract = {Type-and-effect systems are a well-studied approach for reasoning about the computational behavior of programs. Nevertheless, there is only one example of an effect system that has been adopted in a wide-spread industrial language: Java’s checked exceptions. We believe that the main obstacle to using effect systems in day-to-day programming is their verbosity, especially when writing functions that are polymorphic in the effect of their argument. To overcome this issue, we propose a new syntactically lightweight technique for writing effect-polymorphic functions. We show its independence from a specific kind of side-effect by embedding it into a generic and extensible framework for checking effects of multiple domains. Finally, we verify the expressiveness and practicality of the system by implementing it for the Scala programming language.},
  number = {7313},
  booktitle = {ECOOP 2012 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-06-29},
  date = {2012-01-01},
  pages = {258-282},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Computer Communication Networks},
  author = {Rytz, Lukas and Odersky, Martin and Haller, Philipp},
  editor = {Noble, James},
  file = {/Users/pgiarrusso/Zotero/storage/4KW6I8XQ/Rytz et al - 2012 - Lightweight Polymorphic Effects.pdf;/Users/pgiarrusso/Zotero/storage/DCQB8XVB/978-3-642-31057-7_13.html}
}
% == BibLateX quality report for Rytz2012lightweight:
% 'isbn': not a valid ISBN

@inproceedings{Hofer2010modular,
  location = {{New York, NY, USA}},
  title = {Modular Domain-specific Language Components in Scala},
  isbn = {978-1-4503-0154-1},
  url = {http://doi.acm.org/10.1145/1868294.1868307},
  doi = {10.1145/1868294.1868307},
  abstract = {Programs in domain-­specific embedded languages (DSELs) can be represented in the host language in different ways, for instance implicitly as libraries, or explicitly in the form of abstract syntax trees. Each of these representations has its own strengths and weaknesses. The implicit approach has good composability properties, whereas the explicit approach allows more freedom in making syntactic program transformations. Traditional designs for DSELs fix the form of representation, which means that it is not possible to choose the best representation for a particular interpretation or transformation. We propose a new design for implementing DSELs in Scala which makes it easy to use different program representations at the same time. It enables the DSL implementor to define modular language components and to compose transformations and interpretations for them.},
  booktitle = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering},
  series = {GPCE '10},
  publisher = {{ACM}},
  urldate = {2014-06-29},
  date = {2010},
  pages = {83--92},
  keywords = {domain-specific languages,Scala,visitor pattern,embedded languages,term representation},
  author = {Hofer, Christian and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/99TGMWMP/Hofer_Ostermann - 2010 - Modular Domain-specific Language Components in Scala.pdf}
}

@inproceedings{Rytz2013flowinsensitive,
  location = {{New York, NY, USA}},
  title = {A Flow-insensitive, Modular Effect System for Purity},
  isbn = {978-1-4503-2042-9},
  url = {http://doi.acm.org/10.1145/2489804.2489808},
  doi = {10.1145/2489804.2489808},
  abstract = {This article presents a modular, flow-insensitive type-and-effect system for purity with lightweight annotations. It does not enforce a global programming discipline and allows arbitrary effects to occur in impure parts of the program. The system is designed to support higher-order languages that mix functional and imperative code like Scala or C\#. We show that it can express purity of non-local programming patterns which involve mutable state such as those used in the Scala collections library. We formalize the type system using a functional language with mutable records and define type and effect soundness.},
  booktitle = {Proceedings of the 15th Workshop on Formal Techniques for Java-like Programs},
  series = {FTfJP '13},
  publisher = {{ACM}},
  urldate = {2014-06-29},
  date = {2013},
  pages = {4:1--4:7},
  keywords = {purity,type-and-effect systems},
  author = {Rytz, Lukas and Amin, Nada and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/4DDBKXFU/Rytz et al - 2013 - A Flow-insensitive, Modular Effect System for Purity.pdf}
}

@inproceedings{Lammel2006software,
  location = {{New York, NY, USA}},
  title = {Software Extension and Integration with Type Classes},
  isbn = {1-59593-237-2},
  url = {http://doi.acm.org/10.1145/1173706.1173732},
  doi = {10.1145/1173706.1173732},
  abstract = {The abilities to extend a software module and to integrate a software module into an existing software system without changing existing source code are fundamental challenges in software engineering and programming-language design. We reconsider these challenges at the level of language expressiveness, by using the language concept of type classes, as it is available in the functional programming language Haskell. A detailed comparison with related work shows that type classes provide a powerful framework in which solutions to known software extension and integration problems can be provided. We also pinpoint several limitations of type classes in this context.},
  booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
  series = {GPCE '06},
  publisher = {{ACM}},
  urldate = {2014-06-30},
  date = {2006},
  pages = {161--170},
  keywords = {Haskell,type classes,expression problem,family polymorphism,framework integration,multiple dispatch,object adapter,software extension,software integration,tyranny of the dominant decomposition},
  author = {Lämmel, Ralf and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/8F5B7DX7/Lämmel_Ostermann - 2006 - Software Extension and Integration with Type Classes.pdf;/Users/pgiarrusso/Zotero/storage/BSZ4V7EN/Lämmel_Ostermann - 2006 - Software Extension and Integration with Type Classes.pdf}
}

@article{Grabmayer2014maximal,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.1460},
  primaryClass = {cs},
  title = {Maximal Sharing in the Lambda Calculus with letrec},
  url = {http://arxiv.org/abs/1401.1460},
  abstract = {Increasing sharing in programs is desirable to compactify the code, and to avoid duplication of reduction work at run-time, thereby speeding up execution. We show how a maximal degree of sharing can be obtained for programs expressed as terms in the lambda calculus with letrec. We introduce a notion of `maximal compactness' for lambda-letrec-terms among all terms with the same infinite unfolding. Instead of defined purely syntactically, this notion is based on a graph semantics. lambda-letrec-terms are interpreted as first-order term graphs so that unfolding equivalence between terms is preserved and reflected through bisimilarity of the term graph interpretations. Compactness of the term graphs can then be compared via functional bisimulation. We describe practical and efficient methods for the following two problems: transforming a lambda-letrec-term into a maximally compact form; and deciding whether two lambda-letrec-terms are unfolding-equivalent. The transformation of a lambda-letrec-term \$L\$ into maximally compact form \$L\_0\$ proceeds in three steps: (i) translate L into its term graph \$G = [[ L ]]\$; (ii) compute the maximally shared form of \$G\$ as its bisimulation collapse \$G\_0\$; (iii) read back a lambda-letrec-term \$L\_0\$ from the term graph \$G\_0\$ with the property \$[[ L\_0 ]] = G\_0\$. This guarantees that \$L\_0\$ and \$L\$ have the same unfolding, and that \$L\_0\$ exhibits maximal sharing. The procedure for deciding whether two given lambda-letrec-terms \$L\_1\$ and \$L\_2\$ are unfolding-equivalent computes their term graph interpretations \$[[ L\_1 ]]\$ and \$[[ L\_2 ]]\$, and checks whether these term graphs are bisimilar. For illustration, we also provide a readily usable implementation.},
  urldate = {2014-07-01},
  date = {2014-01-07},
  keywords = {Computer Science - Programming Languages,D.1.1,F.3.3},
  author = {Grabmayer, Clemens and Rochel, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/HFVIZ3EN/Grabmayer_Rochel - 2014 - Maximal Sharing in the Lambda Calculus with letrec.pdf;/Users/pgiarrusso/Zotero/storage/NBZQ3IS2/1401.html}
}
% == BibLateX quality report for Grabmayer2014maximal:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Chitil1998common,
  langid = {english},
  title = {Common subexpressions are uncommon in lazy functional languages},
  isbn = {978-3-540-64849-9 978-3-540-68528-9},
  url = {http://link.springer.com/chapter/10.1007/BFb0055424},
  abstract = {Common subexpression elimination is a well-known compiler optimisation that saves time by avoiding the repetition of the same computation. In lazy functional languages, referential transparency renders the identification of common subexpressions very simple. More common subexpressions can be recognised because they can be of arbitrary type whereas standard common subexpression elimination only shares primitive values. However, because lazy functional languages decouple program structure from data space allocation and control flow, analysing its effects and deciding under which conditions the elimination of a common subexpression is beneficial proves to be quite difficult. We developed and implemented the transformation for the language Haskell by extending the Glasgow Haskell compiler. On real-world programs the transformation showed nearly no effect. The reason is that common subexpressions whose elimination could speed up programs are uncommon in lazy functional languages.},
  number = {1467},
  booktitle = {Implementation of Functional Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-05},
  date = {1998-01-01},
  pages = {53-71},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Logics and Meanings of Programs},
  author = {Chitil, Olaf},
  editor = {Clack, Chris and Hammond, Kevin and Davie, Tony},
  file = {/Users/pgiarrusso/Zotero/storage/2WFD6URJ/Chitil - 1998 - Common subexpressions are uncommon in lazy functional languages.pdf;/Users/pgiarrusso/Zotero/storage/SKPN67EF/BFb0055424.html}
}
% == BibLateX quality report for Chitil1998common:
% 'isbn': not a valid ISBN

@inproceedings{Miller2013instant,
  location = {{New York, NY, USA}},
  title = {Instant Pickles: Generating Object-oriented Pickler Combinators for Fast and Extensible Serialization},
  isbn = {978-1-4503-2374-1},
  url = {http://doi.acm.org/10.1145/2509136.2509547},
  doi = {10.1145/2509136.2509547},
  shorttitle = {Instant Pickles},
  abstract = {As more applications migrate to the cloud, and as "big data" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to object-oriented programming due to qualities like open class hierarchies and subtyping polymorphism. Furthermore, both functional pickler combinators and popular, Java-based serialization frameworks tend to be tied to a specific pickle format, leaving programmers with no choice of how their data is persisted. In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In addition to high performance and the need for little to no boilerplate, our framework is extensible: using the type class pattern, users can provide both (1) custom, easily interchangeable pickle formats and (2) custom picklers, to override the default behavior of the pickling framework. In benchmarks, we compare scala/pickling with other popular industrial frameworks, and present results on time, memory usage, and size when pickling/unpickling a number of data types used in real-world, large-scale distributed applications and frameworks.},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&\#38; Applications},
  series = {OOPSLA '13},
  publisher = {{ACM}},
  urldate = {2014-07-07},
  date = {2013},
  pages = {183--202},
  keywords = {Scala,distributed programming,meta-programming,pickling,serialization},
  author = {Miller, Heather and Haller, Philipp and Burmako, Eugene and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/XPBAJAQU/Miller et al - 2013 - Instant Pickles - Generating Object-oriented Pickler Combinators for Fast and Extensible Serialization.pdf}
}

@inproceedings{Torlak2014lightweight,
  location = {{New York, NY, USA}},
  title = {A Lightweight Symbolic Virtual Machine for Solver-aided Host Languages},
  isbn = {978-1-4503-2784-8},
  url = {http://doi.acm.org/10.1145/2594291.2594340},
  doi = {10.1145/2594291.2594340},
  abstract = {Solver-aided domain-specific languages (SDSLs) are an emerging class of computer-aided programming systems. They ease the construction of programs by using satisfiability solvers to automate tasks such as verification, debugging, synthesis, and non-deterministic execution. But reducing programming tasks to satisfiability problems involves translating programs to logical constraints, which is an engineering challenge even for domain-specific languages. We have previously shown that translation to constraints can be avoided if SDSLs are implemented by (traditional) embedding into a host language that is itself solver-aided. This paper describes how to implement a symbolic virtual machine (SVM) for such a host language. Our symbolic virtual machine is lightweight because it compiles to constraints only a small subset of the host's constructs, while allowing SDSL designers to use the entire language, including constructs for DSL embedding. This lightweight compilation employs a novel symbolic execution technique with two key properties: it produces compact encodings, and it enables concrete evaluation to strip away host constructs that are outside the subset compilable to constraints. Our symbolic virtual machine architecture is at the heart of Rosette, a solver-aided language that is host to several new SDSLs.},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '14},
  publisher = {{ACM}},
  urldate = {2014-07-07},
  date = {2014},
  pages = {530--541},
  keywords = {solver-aided languages,symbolic virtual machine},
  author = {Torlak, Emina and Bodik, Rastislav},
  file = {/Users/pgiarrusso/Zotero/storage/26Q8TIAX/Torlak_Bodik - 2014 - A Lightweight Symbolic Virtual Machine for Solver-aided Host Languages.pdf}
}

@article{Allen2006innovations,
  title = {Innovations in computational type theory using Nuprl},
  volume = {4},
  issn = {1570-8683},
  url = {http://www.sciencedirect.com/science/article/pii/S1570868305000704},
  doi = {10.1016/j.jal.2005.10.005},
  abstract = {For twenty years the Nuprl (“new pearl”) system has been used to develop software systems and formal theories of computational mathematics. It has also been used to explore and implement computational type theory (CTT)—a formal theory of computation closely related to Martin-Löf's intuitionistic type theory (ITT) and to the calculus of inductive constructions (CIC) implemented in the Coq prover.

This article focuses on the theory and practice underpinning our use of Nuprl for much of the last decade. We discuss innovative elements of type theory, including new type constructors such as unions and dependent intersections, our theory of classes, and our theory of event structures.

We also discuss the innovative architecture of Nuprl as a distributed system and as a transactional database of formal mathematics using the notion of abstract object identifiers. The database has led to an independent project called the Formal Digital Library, FDL, now used as a repository for Nuprl results as well as selected results from HOL, MetaPRL, and PVS. We discuss Howe's set theoretic semantics that is used to relate such disparate theories and systems as those represented by these provers.},
  number = {4},
  journaltitle = {Journal of Applied Logic},
  shortjournal = {Journal of Applied Logic},
  series = {Towards Computer Aided Mathematics},
  urldate = {2014-07-07},
  date = {2006-12},
  pages = {428-469},
  keywords = {tactics,Program extraction,Computational type theory,Dependent intersection types,Formal digital libraries,Logic of events,Martin-Löf type theory,Polymorphic subtyping,Proofs as programs,Union types},
  author = {Allen, S. F. and Bickford, M. and Constable, R. L. and Eaton, R. and Kreitz, C. and Lorigo, L. and Moran, E.},
  file = {/Users/pgiarrusso/Zotero/storage/QTD8QPU6/Allen et al - 2006 - Innovations in computational type theory using Nuprl.pdf;/Users/pgiarrusso/Zotero/storage/7ST64TAJ/S1570868305000704.html}
}

@inproceedings{Sansom1995time,
  location = {{New York, NY, USA}},
  title = {Time and Space Profiling for Non-strict, Higher-order Functional Languages},
  isbn = {0-89791-692-1},
  url = {http://doi.acm.org/10.1145/199448.199531},
  doi = {10.1145/199448.199531},
  abstract = {We present the first profiler for a compiled, non-strict, higher-order, purely functional language capable of measuring time as well as space usage. Our profiler is implemented in a production-quality optimising compiler for Haskell, has low overheads, and can successfully profile large applications.A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centres. This specification enables us to discuss our design decisions in a precise framework. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementation-oriented operational semantics, and prove it equivalent to the specification.},
  booktitle = {Proceedings of the 22Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '95},
  publisher = {{ACM}},
  urldate = {2014-07-10},
  date = {1995},
  pages = {355--366},
  author = {Sansom, Patrick M. and Peyton Jones, Simon L.}
}

@incollection{Harrison2006selfverification,
  langid = {english},
  title = {Towards Self-verification of HOL Light},
  isbn = {978-3-540-37187-8 978-3-540-37188-5},
  url = {http://link.springer.com/chapter/10.1007/11814771_17},
  abstract = {The HOL Light prover is based on a logical kernel consisting of about 400 lines of mostly functional OCaml, whose complete formal verification seems to be quite feasible. We would like to formally verify (i) that the abstract HOL logic is indeed correct, and (ii) that the OCaml code does correctly implement this logic. We have performed a full verification of an imperfect but quite detailed model of the basic HOL Light core, without definitional mechanisms, and this verification is entirely conducted with respect to a set-theoretic semantics within HOL Light itself. We will duly explain why the obvious logical and pragmatic difficulties do not vitiate this approach, even though it looks impossible or useless at first sight. Extension to include definitional mechanisms seems straightforward enough, and the results so far allay most of our practical worries.},
  number = {4130},
  booktitle = {Automated Reasoning},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-15},
  date = {2006-01-01},
  pages = {177-191},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Harrison, John},
  editor = {Furbach, Ulrich and Shankar, Natarajan},
  file = {/Users/pgiarrusso/Zotero/storage/JCKKBZK4/Harrison - 2006 - Towards Self-verification of HOL Light.pdf;/Users/pgiarrusso/Zotero/storage/JDAGHD29/11814771_17.html}
}
% == BibLateX quality report for Harrison2006selfverification:
% 'isbn': not a valid ISBN

@thesis{Norell2007practical,
  location = {{SE-412 96 Göteborg, Sweden}},
  title = {Towards a practical programming language based on dependent type theory},
  abstract = {Dependent type theories have a long history of being used for theorem proving. One aspect of type theory which makes it very powerful as a proof language is that it mixes deduction with computation. This also makes type theory a good candidate for programming---the strength of the type system allows properties of programs to be stated and established, and the computational properties provide semantics for the programs.

This thesis is concerned with bridging the gap between the theoretical presentations of type theory and the requirements on a practical programming language. Although there are many challenging research problems left to solve before we have an industrial scale programming language based on type theory, this thesis takes us a good step along the way.

In functional programming languages pattern matching provides a concise notation for defining functions. In dependent type theory, pattern matching becomes even more powerful, in that inspecting the value of a particular term can reveal information about the types and values of other terms. In this thesis we give a type checking algorithm for definitions by pattern matching in type theory, supporting overlapping patterns, and pattern matching on intermediate results using the with rule.

Traditional presentations of type theory suffers from rather verbose notation, cluttering programs and proofs with, for instance, explicit type information. One solution to this problem is to allow terms that can be inferred automatically to be omitted. This is usually implemented by inserting metavariables in place of the omitted terms and using unification to solve these metavariables during type checking. We present a type checking algorithm for a theory with metavariables and prove its soundness independent of whether the metavariables are solved or not.

In any programming language it is important to be able to structure large programs into separate units or modules and limit the interaction between these modules. In this thesis we present a simple, but powerful module system for a dependently typed language. The main focus of the module system is to manage the name space of a program, and an important characteristic is a clear separation between the module system and the type checker, making it largely independent of the underlying language.

As a side track, not directly related to the use of type theory for programming, we present a connection between type theory and a first-order logic theorem prover. This connection saves the user the burden of proving simple, but tedious first-order theorems by leaving them for the prover. We use a transparent translation to first-order logic which makes the proofs constructed by the theorem prover human readable. The soundness of the connection is established by a general metatheorem.

Finally we put our work into practise in the implementation of a programming language, Agda, based on type theory. As an illustrating example we show how to program a simple certfied prover for equations in a commutative monoid, which can be used internally in Agda. Much more impressive examples have been done by others, showing that the ideas developed in this thesis are viable in practise.},
  institution = {{Department of Computer Science and Engineering, Chalmers University of Technology}},
  date = {2007-09},
  author = {Norell, Ulf},
  file = {/Users/pgiarrusso/Zotero/storage/MIAHFR44/Norell - 2007 - Towards a practical programming language based on dependent type theory.pdf}
}
% == BibLateX quality report for Norell2007practical:
% Missing required field 'type'

@article{Harper1992constructing,
  title = {Constructing type systems over an operational semantics},
  volume = {14},
  issn = {0747-7171},
  url = {http://www.sciencedirect.com/science/article/pii/074771719290026Z},
  doi = {10.1016/0747-7171(92)90026-Z},
  abstract = {Type theories in the sense of Martin-Löf and the NuPRL system are based on taking as primitive a type-free programming language given by an operational semantics, and defining types as partial equivalence relations on the set of closed terms. The construction of a type system is based on a general form of inductive definition that may either be taken as acceptable in its own right, or further explicated in terms of other patterns of induction. One such account, based on a general theory of inductively-defined relations, was given by Allen. An alternative account, based on an essentially set-theoretic argument, is presented.},
  number = {1},
  journaltitle = {Journal of Symbolic Computation},
  shortjournal = {Journal of Symbolic Computation},
  urldate = {2014-07-16},
  date = {1992-07},
  pages = {71-84},
  author = {Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/SSFAJZBH/Harper - 1992 - Constructing type systems over an operational semantics.pdf;/Users/pgiarrusso/Zotero/storage/7SEG9ZDU/074771719290026Z.html}
}

@article{Hinze2006generics,
  title = {Generics for the masses},
  volume = {16},
  doi = {10.1017/S0956796806006022},
  abstract = {A generic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of generic functions are the functions that can be derived in Haskell, such as show, read, and ‘==’. The recent years have seen a number of proposals that support the definition of generic functions. Some of the proposals define new languages, some define extensions to existing languages. As a common characteristic none of the proposals can be made to work within Haskell 98: they all require something extra, either a more sophisticated type system or an additional language construct. The purpose of this paper is to show that one can, in fact, program generically within Haskell 98 obviating to some extent the need for fancy type systems or separate tools. Haskell's type classes are at the heart of this approach: they ensure that generic functions can be defined succinctly and, in particular, that they can be used painlessly. We detail three different implementations of generics both from a practical and from a theoretical perspective.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  date = {2006},
  pages = {451-483},
  author = {Hinze, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/EQ2PJQ9T/Hinze - 2006 - Generics for the masses.pdf;/Users/pgiarrusso/Zotero/storage/DNKUW3FX/displayAbstract.html}
}

@inproceedings{Devriese2013typed,
  location = {{New York, NY, USA}},
  title = {Typed Syntactic Meta-programming},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500575},
  doi = {10.1145/2500365.2500575},
  abstract = {We present a novel set of meta-programming primitives for use in a dependently-typed functional language. The types of our meta-programs provide strong and precise guarantees about their termination, correctness and completeness. Our system supports type-safe construction and analysis of terms, types and typing contexts. Unlike alternative approaches, they are written in the same style as normal programs and use the language's standard functional computational model. We formalise the new meta-programming primitives, implement them as an extension of Agda, and provide evidence of usefulness by means of two compelling applications in the fields of datatype-generic programming and proof tactics.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2014-07-24},
  date = {2013},
  pages = {73--86},
  keywords = {Datatype-generic programming,dependent types,tactics,meta-programming},
  author = {Devriese, Dominique and Piessens, Frank},
  file = {/Users/pgiarrusso/Zotero/storage/SP2N2TQX/Devriese and Piessens - 2013 - Typed Syntactic Meta-programming.pdf}
}

@incollection{OConnor2005essential,
  langid = {english},
  title = {Essential Incompleteness of Arithmetic Verified by Coq},
  isbn = {978-3-540-28372-0 978-3-540-31820-0},
  url = {http://link.springer.com/chapter/10.1007/11541868_16},
  abstract = {A constructive proof of the Gödel-Rosser incompleteness theorem [9] has been completed using Coq proof assistant. Some theory of classical first-order logic over an arbitrary language is formalized. A development of primitive recursive functions is given, and all primitive recursive functions are proved to be representable in a weak axiom system. Formulas and proofs are encoded as natural numbers, and functions operating on these codes are proved to be primitive recursive. The weak axiom system is proved to be essentially incomplete. In particular, Peano arithmetic is proved to be consistent in Coq’s type theory and therefore is incomplete.},
  number = {3603},
  booktitle = {Theorem Proving in Higher Order Logics},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-24},
  date = {2005-01-01},
  pages = {245-260},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Logic Design},
  author = {O’Connor, Russell},
  editor = {Hurd, Joe and Melham, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/R5ZVJQRC/10.html}
}
% == BibLateX quality report for OConnor2005essential:
% 'isbn': not a valid ISBN

@inproceedings{Schmidt1998data,
  location = {{New York, NY, USA}},
  title = {Data Flow Analysis is Model Checking of Abstract Interpretations},
  isbn = {0-89791-979-3},
  url = {http://doi.acm.org/10.1145/268946.268950},
  doi = {10.1145/268946.268950},
  abstract = {This expository paper simplifies and clarifies Steifen’s depiction of data flow analysis (d.f.a.) as model checking: By employing abstract interpretation (a.i.) to generate program traces and by utilizing Kozen’s modal mu-calculus to express trace properties, we express in simplest possible terms that a d.f.a. is a model check of a program’s a.i. trace. In particular, the classic flow equations for bit-vector-based d.f.a.s reformat trivially into modal mu-calculus formulas. A surprising consequence is that two of the classical d.f.a.s are exposed as unsound; this problem is analyzed and simply repaired. In the process of making the above discoveries, we cIarify the relationship between a.i. and d.f.a. in terms of the often-misunderstood notion of collecting semantics and we highlight how the research areas of flow analysis, abstract interpretation, and model checking have grown together.},
  booktitle = {Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '98},
  publisher = {{ACM}},
  urldate = {2014-07-26},
  date = {1998},
  pages = {38--48},
  author = {Schmidt, David A.}
}

@inproceedings{Reynolds2000intuitionistic,
  title = {Intuitionistic Reasoning about Shared Mutable Data Structure},
  abstract = {Drawing upon early work by Burstall, we extend Hoare's approach  to proving the correctness of imperative programs, to deal with programs  that perform destructive updates to data structures containing  more than one pointer to the same location. The key concept is an  "independent conjunction" P \& Q that holds only when P and Q  are both true and depend upon distinct areas of storage. To make  this concept precise we use an intuitionistic logic of assertions, with a  Kripke semantics whose possible worlds are heaps (mapping locations  into tuples of values).},
  booktitle = {Millennial Perspectives in Computer Science},
  publisher = {{Palgrave}},
  date = {2000},
  pages = {303--321},
  author = {Reynolds, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/HAJG9GFX/Reynolds - 2000 - Intuitionistic Reasoning about Shared Mutable Data.pdf;/Users/pgiarrusso/Zotero/storage/7KMN6KSI/summary.html}
}
% == BibLateX quality report for Reynolds2000intuitionistic:
% ? Unsure about the formatting of the booktitle

@inproceedings{Ishtiaq2001bi,
  location = {{New York, NY, USA}},
  title = {BI As an Assertion Language for Mutable Data Structures},
  isbn = {1-58113-336-7},
  url = {http://doi.acm.org/10.1145/360204.375719},
  doi = {10.1145/360204.375719},
  abstract = {Reynolds has developed a logic for reasoning about mutable data structures in which the pre- and postconditions are written in an intuitionistic logic enriched with a spatial form of conjunction. We investigate the approach from the point of view of the logic BI of bunched implications of O'Hearnand Pym. We begin by giving a model in which the law of the excluded middleholds, thus showing that the approach is compatible with classical logic. The relationship between the intuitionistic and classical versions of the system is established by a translation, analogous to a translation from intuitionistic logic into the modal logic S4. We also consider the question of completeness of the axioms. BI's spatial implication is used to express weakest preconditions for object-component assignments, and an axiom for allocating a cons cell is shown to be complete under an interpretation of triplesthat allows a command to be applied to states with dangling pointers. We make this latter a feature, by incorporating an operation, and axiom, for disposing of memory. Finally, we describe a local character enjoyed by specifications in the logic, and show how this enables a class of frame axioms, which say what parts of the heap don't change, to be inferred automatically.},
  booktitle = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '01},
  publisher = {{ACM}},
  urldate = {2014-07-30},
  date = {2001},
  pages = {14--26},
  author = {Ishtiaq, Samin S. and O'Hearn, Peter W.},
  file = {/Users/pgiarrusso/Zotero/storage/EPN2HUX3/Ishtiaq and O'Hearn - 2001 - BI As an Assertion Language for Mutable Data Struc.pdf}
}

@incollection{OHearn2001local,
  langid = {english},
  title = {Local Reasoning about Programs that Alter Data Structures},
  isbn = {978-3-540-42554-0 978-3-540-44802-0},
  url = {http://link.springer.com/chapter/10.1007/3-540-44802-0_1},
  abstract = {We describe an extension of Hoare’s logic for reasoning about programs that alter data structures. We consider a low-level storage model based on a heap with associated lookup, update, allocation and deallocation operations, and unrestricted address arithmetic. The assertion language is based on a possible worlds model of the logic of bunched implications, and includes spatial conjunction and implication connectives alongside those of classical logic. Heap operations are axiomatized using what we call the “small axioms”, each of which mentions only those cells accessed by a particular command. Through these and a number of examples we show that the formalism supports local reasoning: A specification and proof can concentrate on only those cells in memory that a program accesses. This paper builds on earlier work by Burstall, Reynolds, Ishtiaq and O’Hearn on reasoning about data structures.},
  number = {2142},
  booktitle = {Computer Science Logic},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-30},
  date = {2001-01-01},
  pages = {1-19},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {O’Hearn, Peter and Reynolds, John and Yang, Hongseok},
  editor = {Fribourg, Laurent},
  file = {/Users/pgiarrusso/Zotero/storage/ZM5ECMQ2/O’Hearn et al - 2001 - Local Reasoning about Programs that Alter Data Structures.pdf;/Users/pgiarrusso/Zotero/storage/65JFHVX2/3-540-44802-0_1.html}
}
% == BibLateX quality report for OHearn2001local:
% 'isbn': not a valid ISBN

@inproceedings{Lerner2002composing,
  location = {{New York, NY, USA}},
  title = {Composing Dataflow Analyses and Transformations},
  isbn = {1-58113-450-9},
  url = {http://doi.acm.org/10.1145/503272.503298},
  doi = {10.1145/503272.503298},
  abstract = {Dataflow analyses can have mutually beneficial interactions. Previous efforts to exploit these interactions have either (1) iteratively performed each individual analysis until no further improvements are discovered or (2) developed "super-analyses" that manually combine conceptually separate analyses. We have devised a new approach that allows analyses to be defined independently while still enabling them to be combined automatically and profitably. Our approach avoids the loss of precision associated with iterating individual analyses and the implementation difficulties of manually writing a super-analysis. The key to our approach is a novel method of implicit communication between the individual components of a super-analysis based on graph transformations. In this paper, we precisely define our approach; we demonstrate that it is sound and it terminates; finally we give experimental results showing that in practice (1) our framework produces results at least as precise as iterating the individual analyses while compiling at least 5 times faster, and (2) our framework achieves the same precision as a manually written super-analysis while incurring a compile-time overhead of less than 20\%.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '02},
  publisher = {{ACM}},
  urldate = {2014-07-31},
  date = {2002},
  pages = {270--282},
  author = {Lerner, Sorin and Grove, David and Chambers, Craig},
  file = {/Users/pgiarrusso/Zotero/storage/DB9U2IST/Lerner et al - 2002 - Composing Dataflow Analyses and Transformations.pdf}
}

@inproceedings{Ramsey2010hoopl,
  location = {{New York, NY, USA}},
  title = {Hoopl: A Modular, Reusable Library for Dataflow Analysis and Transformation},
  isbn = {978-1-4503-0252-4},
  url = {http://doi.acm.org/10.1145/1863523.1863539},
  doi = {10.1145/1863523.1863539},
  shorttitle = {Hoopl},
  abstract = {Dataflow analysis and transformation of control-flow graphs is pervasive in optimizing compilers, but it is typically entangled with the details of a particular compiler. We describe Hoopl, a reusable library that makes it unusually easy to define new analyses and transformations for any compiler written in Haskell. Hoopl's interface is modular and polymorphic, and it offers unusually strong static guarantees. The implementation encapsulates state-of-the-art algorithms (interleaved analysis and rewriting, dynamic error isolation), and it cleanly separates their tricky elements so that they can be understood independently.},
  booktitle = {Proceedings of the Third ACM Haskell Symposium on Haskell},
  series = {Haskell '10},
  publisher = {{ACM}},
  urldate = {2014-07-31},
  date = {2010},
  pages = {121--134},
  keywords = {dataflow},
  author = {Ramsey, Norman and Dias, João and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/CH3TI7VT/Ramsey et al - 2010 - Hoopl - A Modular, Reusable Library for Dataflow Analysis and Transformation.pdf}
}

@inproceedings{Farmer2012hermit,
  location = {{New York, NY, USA}},
  title = {The HERMIT in the Machine: A Plugin for the Interactive Transformation of GHC Core Language Programs},
  isbn = {978-1-4503-1574-6},
  url = {http://doi.acm.org/10.1145/2364506.2364508},
  doi = {10.1145/2364506.2364508},
  shorttitle = {The HERMIT in the Machine},
  abstract = {The importance of reasoning about and refactoring programs is a central tenet of functional programming. Yet our compilers and development toolchains only provide rudimentary support for these tasks. This paper introduces a programmatic and compiler-centric interface that facilitates refactoring and equational reasoning. To develop our ideas, we have implemented HERMIT, a toolkit enabling informal but systematic transformation of Haskell programs from inside the Glasgow Haskell Compiler's optimization pipeline. With HERMIT, users can experiment with optimizations and equational reasoning, while the tedious heavy lifting of performing the actual transformations is done for them. HERMIT provides a transformation API that can be used to build higher-level rewrite tools. One use-case is prototyping new optimizations as clients of this API before being committed to the GHC toolchain. We describe a HERMIT application - a read-eval-print shell for performing transformations using HERMIT. We also demonstrate using this shell to prototype an optimization on a specific example, and report our initial experiences and remaining challenges.},
  booktitle = {Proceedings of the 2012 Haskell Symposium},
  series = {Haskell '12},
  publisher = {{ACM}},
  urldate = {2014-07-31},
  date = {2012},
  pages = {1--12},
  keywords = {equational reasoning,optimization,ghc,dsls,strategic programming},
  author = {Farmer, Andrew and Gill, Andy and Komp, Ed and Sculthorpe, Neil},
  file = {/Users/pgiarrusso/Zotero/storage/58DCMIQB/Farmer et al - 2012 - The HERMIT in the Machine - A Plugin for the Interactive Transformation of GHC Core Language Programs.pdf}
}

@incollection{Sculthorpe2013hermit,
  langid = {english},
  title = {The HERMIT in the Tree},
  isbn = {978-3-642-41581-4 978-3-642-41582-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-41582-1_6},
  abstract = {This paper describes our experience using the HERMIT toolkit to apply well-known transformations to the internal core language of the Glasgow Haskell Compiler. HERMIT provides several mechanisms to support writing general-purpose transformations: a domain-specific language for strategic programming specialized to GHC’s core language, a library of primitive rewrites, and a shell-style–based scripting language for interactive and batch usage. There are many program transformation techniques that have been described in the literature but have not been mechanized and made available inside GHC — either because they are too specialized to include in a general-purpose compiler, or because the developers’ interest is in theory rather than implementation. The mechanization process can often reveal pragmatic obstacles that are glossed over in pen-and-paper proofs; understanding and removing these obstacles is our concern. Using HERMIT, we implement eleven examples of three program transformations, report on our experience, and describe improvements made in the process.},
  booktitle = {Implementation and Application of Functional Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-31},
  date = {2013-01-01},
  pages = {86-103},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,ghc,Information Systems Applications (incl. Internet),Mechanization,Transformation,Worker/wrapper},
  author = {Sculthorpe, Neil and Farmer, Andrew and Gill, Andy},
  editor = {Hinze, Ralf},
  file = {/Users/pgiarrusso/Zotero/storage/6Z3DTI5A/Sculthorpe et al - 2013 - The HERMIT in the Tree.pdf;/Users/pgiarrusso/Zotero/storage/QH4QPAZT/10.html}
}
% == BibLateX quality report for Sculthorpe2013hermit:
% 'isbn': not a valid ISBN

@article{Sculthorpe2014kansas,
  title = {The Kansas University Rewrite Engine: A Haskell-Embedded Strategic Programming Language with Custom Closed Universes},
  url = {http://www.ittc.ku.edu/csdl/fpg/files/Sculthorpe-14-KURE.pdf},
  journaltitle = {Journal of Functional Programming},
  date = {2014},
  author = {Sculthorpe, Neil and Frisby, Nicolas and Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/GPMB4J35/Sculthorpe et al - 2014 - The Kansas University Rewrite Engine - A Haskell-Embedded Strategic Programming Language with Custom Closed Universes.pdf},
  xurl = {http://www.ittc.ku.edu/csdl/fpg/software/kure.html}
}
% == BibLateX quality report for Sculthorpe2014kansas:
% Unexpected field 'xurl'

@inproceedings{Adams2014optimizing,
  location = {{New York, NY, USA}},
  title = {Optimizing SYB is Easy!},
  isbn = {978-1-4503-2619-3},
  url = {http://doi.acm.org/10.1145/2543728.2543730},
  doi = {10.1145/2543728.2543730},
  abstract = {The most widely used generic-programming system in the Haskell community, Scrap Your Boilerplate (SYB), also happens to be one of the slowest. Generic traversals in SYB are often an order of magnitude slower than equivalent handwritten, non-generic traversals. Thus while SYB allows the concise expression of many traversals, its use incurs a significant runtime cost. Existing techniques for optimizing other generic-programming systems are not able to eliminate this overhead. This paper presents an optimization that completely eliminates this cost. Essentially, it is a partial evaluation that takes advantage of domain-specific knowledge about the structure of SYB. It optimizes SYB-style traversals to be as fast as handwritten, non-generic code, and benchmarks show that this optimization improves the speed of SYB-style code by an order of magnitude or more.},
  booktitle = {Proceedings of the ACM SIGPLAN 2014 Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM '14},
  publisher = {{ACM}},
  urldate = {2014-07-31},
  date = {2014},
  pages = {71--82},
  keywords = {Haskell,Datatype-generic programming,performance,partial evaluation,keywords: optimization,scrap your boilerplate (syb)},
  author = {Adams, Michael D. and Farmer, Andrew and Magalhães, José Pedro},
  file = {/Users/pgiarrusso/Zotero/storage/D8MEIQ7C/Adams et al - 2014 - Optimizing SYB is Easy!.pdf}
}

@inproceedings{Reynolds1978syntactic,
  location = {{New York, NY, USA}},
  title = {Syntactic Control of Interference},
  url = {http://doi.acm.org/10.1145/512760.512766},
  doi = {10.1145/512760.512766},
  abstract = {In programming languages which permit both assignment and procedures, distinct identifiers can represent data structures which share storage or procedures with interfering side effects. In addition to being a direct source of programming errors, this phenomenon, which we call interference can impact type structure and parallelism. We show how to eliminate these difficulties by imposing syntactic restrictions, without prohibiting the kind of constructive interference which occurs with higher-order procedures or SIMULA classes. The basic idea is to prohibit interference between identifiers, but to permit interference among components of collections named by single identifiers.},
  booktitle = {Proceedings of the 5th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '78},
  publisher = {{ACM}},
  urldate = {2014-08-02},
  date = {1978},
  pages = {39--46},
  author = {Reynolds, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/KJZPDZFG/Reynolds - 1978 - Syntactic Control of Interference.pdf}
}

@inproceedings{Meyer1986type,
  location = {{New York, NY, USA}},
  title = {"Type" is Not a Type},
  url = {http://doi.acm.org/10.1145/512644.512671},
  doi = {10.1145/512644.512671},
  abstract = {A function has a \emph{dependent type} when the type of its result depends upon the value of its argument. Dependent types originated in the type theory of intuitionistic mathematics and have reappeared independently in programming languages such as CLU, Pebble, and Russell. Some of these languages make the assumption that there exists a \emph{type-of-all-types} which is its own type as well as the type of all other types. Girard proved that this approach is inconsistent from the perspective of intuitionistic logic. We apply Girard's techniques to establish that the type-of-all-types assumption creates serious pathologies from a programming perspective: a system using this assumption is inherently not normalizing, term equality is undecidable, and the resulting theory fails to be a conservative extension of the theory of the underlying base types. The failure of conservative extension means that classical reasoning about programs in such a system is not sound.},
  booktitle = {Proceedings of the 13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '86},
  publisher = {{ACM}},
  urldate = {2014-08-02},
  date = {1986},
  pages = {287--295},
  author = {Meyer, Albert R. and Reinhold, Mark B.},
  file = {/Users/pgiarrusso/Zotero/storage/BR77QGIG/Meyer-Reinhold - 1986 - Type is Not a Type.pdf}
}

@incollection{Reynolds1989syntactic,
  langid = {english},
  title = {Syntactic control of interference Part 2},
  isbn = {978-3-540-51371-1 978-3-540-46201-9},
  url = {http://link.springer.com/chapter/10.1007/BFb0035793},
  abstract = {In 1978, we proposed that Algol-like languages should be constrained so that aliasing between variables and, more generally, interference between commands or procedures would be syntactically detectable in a fail-safe manner. In particular, we proposed syntactic restrictions that prohibited interference between distinct identifiers, while permitting interference between qualifications of the same identifier. However, these restrictions had the unfortunate property that syntactic correctness was not preserved by beta reduction. In the present paper, we show how this difficulty can be avoided by the use of a variant of conjunctive types. We also give an algorithm for typechecking explicitly typed programs.},
  number = {372},
  booktitle = {Automata, Languages and Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-02},
  date = {1989-01-01},
  pages = {704-722},
  keywords = {Logics and Meanings of Programs,Processor Architectures,Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Mathematical Logic and Formal Languages,Combinatorics},
  author = {Reynolds, John C.},
  editor = {Ausiello, Giorgio and Dezani-Ciancaglini, Mariangiola and Rocca, Simonetta Ronchi Della},
  file = {/Users/pgiarrusso/Zotero/storage/EJVCJ72N/BFb0035793.html}
}
% == BibLateX quality report for Reynolds1989syntactic:
% 'isbn': not a valid ISBN

@article{OHearn1999syntactic,
  title = {Syntactic control of interference revisited},
  volume = {228},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397598003594},
  doi = {10.1016/S0304-3975(98)00359-4},
  abstract = {In “syntactic control of interference” (POPL, 1978), J.C. Reynolds proposes three design principles intended to constrain the scope of imperative state effects in Algol-like languages. The resulting linguistic framework seems to be a very satisfactory way of combining functional and imperative concepts, having the desirable attributes of both purely functional languages (such as PCF) and simple imperative languages (such as the language of while programs). However, Reynolds points out that the “obvious” syntax for interference control has the unfortunate property that β-reductions do not always preserve typings. Reynolds has subsequently presented a solution to this problem (ICALP, 1989), but it is fairly complicated and requires intersection types in the type system. Here, we present a much simpler solution which does not require intersection types. We first describe a new type system inspired in part by linear logic and verify that reductions preserve typings. We then define a class of “bireflective” models, which provide a categorical analysis of structure underlying the new typing rules; a companion paper “Bireflectivity”, in this volume, exposes wider ramifications of this structure. Finally, we describe a concrete model for an illustrative programming language based on the new type system; this improves on earlier such efforts in that states are not assumed to be structured using locations.},
  number = {1–2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2014-08-02},
  date = {1999-10-28},
  pages = {211-252},
  keywords = {type systems,Denotational semantics,Aliasing,Interference},
  author = {O’Hearn, P. W. and Power, A. J. and Takeyama, M. and Tennent, R. D.},
  file = {/Users/pgiarrusso/Zotero/storage/3AEX5ZQB/O’Hearn et al - 1999 - Syntactic control of interference revisited.pdf;/Users/pgiarrusso/Zotero/storage/XBX77ZHE/S0304397598003594.html}
}

@article{Barthe1999typechecking,
  title = {Type-checking injective pure type systems},
  volume = {9},
  abstract = {Injective pure type systems form a large class of pure type systems for which one can compute by purely syntactic means two sorts elmt(Γ|M) and sort(Γ|M), where Γ is a pseudo-context and M is a pseudo-term, and such that for every sort s,
Γ⊢M:A ∧ Γ⊢A:s ⇒ elmt(Γ|M)=s
Γ ⊢ M:s ⇒ sort(Γ|M)=s.
By eliminating the problematic clause in the (abstraction) rule in favor of constraints over elmt(.|.) and sort(.|.), we provide a sound and complete type-checking algorithm for injective pure type systems. In addition, we prove expansion postponement for a variant of injective pure type systems where the problematic clause in the (abstraction) rule is replaced in favor of constraints over elmt(.|.) and sort(.|.).},
  number = {06},
  journaltitle = {Journal of Functional Programming},
  date = {1999},
  pages = {675-698},
  author = {Barthe, Gilles},
  file = {/Users/pgiarrusso/Zotero/storage/5X7BZSMG/Barthe - 1999 - Type-checking injective pure type systems.pdf;/Users/pgiarrusso/Zotero/storage/7A95DK46/displayFulltext.html}
}

@article{Adams2006pure,
  title = {Pure type systems with judgemental equality},
  volume = {16},
  doi = {10.1017/S0956796805005770},
  number = {02},
  journaltitle = {Journal of Functional Programming},
  date = {2006},
  pages = {219-246},
  author = {Adams, Robin},
  file = {/Users/pgiarrusso/Zotero/storage/8UVN787R/displayAbstract.html}
}

@incollection{McKinna1993pure,
  langid = {english},
  title = {Pure type systems formalized},
  isbn = {978-3-540-56517-8 978-3-540-47586-6},
  url = {http://link.springer.com/chapter/10.1007/BFb0037113},
  abstract = {In doing this work of formalizing a well known body of mathematics, we spent a large amount of time solving mathematical problems, e.g. the Thinning Lemma. Another big problem was maintaining and organizing the formal knowledge, e.g. allowing two people to extend different parts of the data base at the same time, and finding the right lemma in the mass of checked material. We feel that better understanding of mathematical issues of formalization (e.g. names/namefree, intentional/extentional), and organization of formal development are the most useful areas to work on now for the long-term goal of formal mathematics. Finally, it is not so easy to understand the relationship between some informal mathematics and a claimed formalization of it. Are you satisfied with our definition of reduction? It might be more satisfying if we also defined de Bruijn terms and their reduction, and proved a correspondence between the two representations, but this only changes the degree of the problem, not its nature. What about the choice between the typing rules Lda and Lda'? There may be no “right” answer, as we may have different ideas in mind informally. There is no such thing as certain truth, and formalization does not change this state of affairs.},
  number = {664},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-03},
  date = {1993-01-01},
  pages = {289-305},
  keywords = {Programming Techniques,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  author = {McKinna, James and Pollack, Robert},
  editor = {Bezem, Marc and Groote, Jan Friso},
  file = {/Users/pgiarrusso/Zotero/storage/3BV2ISXZ/BFb0037113.html}
}
% == BibLateX quality report for McKinna1993pure:
% 'isbn': not a valid ISBN

@inproceedings{Beguet2014accelerating,
  location = {{New York, NY, USA}},
  title = {Accelerating Parser Combinators with Macros},
  isbn = {978-1-4503-2868-5},
  url = {http://doi.acm.org/10.1145/2637647.2637653},
  doi = {10.1145/2637647.2637653},
  abstract = {Parser combinators provide an elegant way of writing parsers: parser implementations closely follow the structure of the underlying grammar, while accommodating interleaved host language code for data processing. However, the host language features used for composition introduce substantial overhead, which leads to poor performance. In this paper, we present a technique to systematically eliminate this overhead. We use Scala macros to analyse the grammar specification at compile-time and remove composition, leaving behind an efficient top-down, recursive-descent parser. We compare our macro-based approach to a staging-based approach using the LMS framework, and provide an experience report in which we discuss the advantages and drawbacks of both methods. Our library outperforms Scala's standard parser combinators on a set of benchmarks by an order of magnitude, and is 2x faster than code generated by LMS.},
  booktitle = {Proceedings of the Fifth Annual Scala Workshop},
  series = {SCALA '14},
  publisher = {{ACM}},
  urldate = {2014-08-03},
  date = {2014},
  pages = {7--17},
  keywords = {optimization,Scala,parser combinators,macros},
  author = {Béguet, Eric and Jonnalagedda, Manohar},
  file = {/Users/pgiarrusso/Zotero/storage/M77C24G6/Béguet_Jonnalagedda - 2014 - Accelerating Parser Combinators with Macros.pdf}
}

@incollection{Ernst2001family,
  langid = {english},
  title = {Family Polymorphism},
  isbn = {978-3-540-42206-8 978-3-540-45337-6},
  url = {http://link.springer.com/chapter/10.1007/3-540-45337-7_17},
  abstract = {This paper takes polymorphism to the multi-object level. Traditional inheritance, polymorphism, and late binding interact nicely to provide both flexibility and safety — when a method is invoked on an object via a polymorphic reference, late binding ensures that we get the appropriate implementation of that method for the actual object. We are granted the flexibility of using different kinds of objects and different method implementations, and we are guaranteed the safety of the combination. Nested classes, polymorphism, and late binding of nested classes interact similarly to provide both safety and flexibility at the level of multi-object systems. We are granted the flexibility of using different families of kinds of objects, and we are guaranteed the safety of the combination. This paper highlights the inability of traditional polymorphism to handle multiple objects, and presents family polymorphism as a way to overcome this problem. Family polymorphism has been implemented in the programming language gbeta, a generalized version of Beta, and the source code of this implementation is available under GPL.1},
  number = {2072},
  booktitle = {ECOOP 2001 — Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-04},
  date = {2001-01-01},
  pages = {303-326},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Business Information Systems,Computer Communication Networks},
  author = {Ernst, Erik},
  editor = {Knudsen, Jørgen Lindskov},
  file = {/Users/pgiarrusso/Zotero/storage/F3VEXSBV/Ernst - 2001 - Family Polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/2HF5475G/3-540-45337-7_17.html}
}
% == BibLateX quality report for Ernst2001family:
% 'isbn': not a valid ISBN

@inproceedings{Ungar1987self,
  location = {{New York, NY, USA}},
  title = {Self: The Power of Simplicity},
  isbn = {0-89791-247-0},
  url = {http://doi.acm.org/10.1145/38765.38828},
  doi = {10.1145/38765.38828},
  shorttitle = {Self},
  abstract = {Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages and Applications},
  series = {OOPSLA '87},
  publisher = {{ACM}},
  urldate = {2014-08-04},
  date = {1987},
  pages = {227--242},
  author = {Ungar, David and Smith, Randall B.}
}

@incollection{Oliveira2009modular,
  langid = {english},
  title = {Modular Visitor Components},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-03013-0_13},
  abstract = {The expression families problem can be defined as the problem of achieving reusability and composability across the components involved in a family of related datatypes and corresponding operations over those datatypes. Like the traditional expression problem, adding new components (either variants or operations) should be possible while preserving modular and static type-safety. Moreover, different combinations of components should have different type identities and the subtyping relationships between components should be preserved. By generalizing previous work exploring the connection between type-theoretic encodings of datatypes and visitors, we propose two solutions for this problem in Scala using modular visitor components. These components can be grouped into features that can be easily composed in a feature-oriented programming style to obtain customized datatypes and operations.},
  number = {5653},
  booktitle = {ECOOP 2009 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-06},
  date = {2009-01-01},
  pages = {269-293},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Computer Communication Networks,Management of Computing and Information Systems},
  author = {Oliveira, Bruno C. d S.},
  editor = {Drossopoulou, Sophia},
  file = {/Users/pgiarrusso/Zotero/storage/UIU6V26P/Oliveira - 2009 - Modular Visitor Components.pdf;/Users/pgiarrusso/Zotero/storage/F3QHIJFW/978-3-642-03013-0_13.html}
}
% == BibLateX quality report for Oliveira2009modular:
% 'isbn': not a valid ISBN

@article{Ungar1991organizing,
  langid = {english},
  title = {Organizing programs without classes},
  volume = {4},
  issn = {0892-4635, 1573-0557},
  url = {http://link.springer.com/article/10.1007/BF01806107},
  doi = {10.1007/BF01806107},
  abstract = {All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model—dividing types into prototypes and traits—supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless languages can even express “class-based” encapsulation. These stylized uses of object inheritance become instantly recognizable idioms, and extend the repertory of organizing principles to cover a wider range of programs.},
  number = {3},
  journaltitle = {LISP and Symbolic Computation},
  shortjournal = {Lisp and Symbolic Computation},
  urldate = {2014-08-06},
  date = {1991-07-01},
  pages = {223-242},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Software Engineering/Programming and Operating Systems,Numeric Computing},
  author = {Ungar, David and Chambers, Craig and Chang, Bay-Wei and Hölzle, Urs},
  file = {/Users/pgiarrusso/Zotero/storage/XQI2XD8Q/Ungar et al. - 1991 - Organizing programs without classes.pdf;/Users/pgiarrusso/Zotero/storage/7688KAAT/BF01806107.html}
}
% == BibLateX quality report for Ungar1991organizing:
% 'issn': not a valid ISSN

@inproceedings{Gabriel2012structure,
  location = {{New York, NY, USA}},
  title = {The Structure of a Programming Language Revolution},
  isbn = {978-1-4503-1562-3},
  url = {http://doi.acm.org/10.1145/2384592.2384611},
  doi = {10.1145/2384592.2384611},
  abstract = {Engineering often precedes science. Incommensurability is real.},
  booktitle = {Proceedings of the ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  series = {Onward! '12},
  publisher = {{ACM}},
  urldate = {2014-05-04},
  date = {2012},
  pages = {195--214},
  keywords = {engineering,incommensurability,paradigms,science},
  author = {Gabriel, Richard P.}
}

@inproceedings{Cardelli1997program,
  location = {{New York, NY, USA}},
  title = {Program Fragments, Linking, and Modularization},
  isbn = {0-89791-853-3},
  url = {http://doi.acm.org/10.1145/263699.263735},
  doi = {10.1145/263699.263735},
  booktitle = {Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '97},
  publisher = {{ACM}},
  urldate = {2014-12-15},
  date = {1997},
  pages = {266--277},
  author = {Cardelli, Luca},
  file = {/Users/pgiarrusso/Zotero/storage/8G9ASSRD/Cardelli - 1997 - Program Fragments, Linking, and Modularization.pdf}
}

@inproceedings{Cardelli1984Compiling,
  location = {{New York, NY, USA}},
  title = {Compiling a Functional Language},
  isbn = {978-0-89791-142-9},
  url = {http://doi.acm.org/10.1145/800055.802037},
  doi = {10.1145/800055.802037},
  abstract = {This paper summarizes my experience in implementing a compiler for a functional language. The language is ML(1) [Milner 84] and the compiler was first implemented in 1980 as a personal project when I was a postgraduate student at the University of Edinburgh(2). At the time, I was familiar with programming language semantics but knew very little about compiler technology; interpreters had been my main programming concern. Major influences in the design of this compiler have been [Steele 77] [Steele 78] and the implementation folklore for statically and dynamically scoped dialects of Lisp [Allen 78]. As a result, the internal structure of the compiler is fairly unorthodox, if compared for example with [Aho 78]. Anyway, a compiler for a language like ML has to be different. ML is interactive, statically scoped, strongly typed, polymorphic, and has first class higher-order functions, type inference and dynamic allocation. These features preclude many well-known implementation styles, particularly the ones used for Lisp (because of static scoping), the Algol family (because of functional values) and C (because of nested scoping and strong typing). The interaction of these features is what gives ML its “character”, and makes compilation challenging. The compiler has been recently partially converted to the new ML standard. The major points of interest which are discussed in this paper are: (a) the interactive interpreter-like usage; (b) the polymorphic type inference algorithm; (c) the compilation of pattern matching; (d) the optimization of the representation of user defined data types; (e) the compilation of functional closures, function application and variable access; (f) the intermediate abstract machine and its formal operational semantics; (g) modules and type-safe separate compilation.},
  booktitle = {Proceedings of the 1984 ACM Symposium on LISP and Functional Programming},
  series = {LFP '84},
  publisher = {{ACM}},
  urldate = {2016-08-03},
  date = {1984},
  pages = {208--217},
  author = {Cardelli, Luca}
}

@article{Berardi2008typed,
  title = {A typed lambda calculus with intersection types},
  volume = {398},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397508000625},
  doi = {10.1016/j.tcs.2008.01.046},
  shorttitle = {Calculi, Types and Applications},
  abstract = {Intersection types are well known to type theorists mainly for two reasons. Firstly, they type all and only the strongly normalizable lambda terms. Secondly, the intersection type operator is a meta-level operator, that is, there is no direct logical counterpart in the Curry–Howard isomorphism sense. In particular, its meta-level nature implies that it does not correspond to the intuitionistic conjunction. The intersection type system is naturally a type inference system (system à la Curry), but the meta-level nature of the intersection operator does not allow to easily design an equivalent typed system (system à la Church). There are many proposals in the literature to design such systems, but none of them gives an entirely satisfactory answer to the problem. In this paper, we will review the main results in the literature both on the logical interpretation of intersection types and on proposed typed lambda calculi. The core of this paper is a new proposal for a true intersection typed lambda calculus, without any meta-level notion. Namely, any typable term (in the intersection type inference) has a corresponding typed term (which is the same as the untyped term by erasing the type decorations and the typed term constructors) with the same type, and vice versa. The main idea is to introduce a relevant parallel term constructor which corresponds to the intersection type constructor, in such a way that terms in parallel share the same resources, that is, the same context of free typed variables. Three rules allow us to generate all typed terms. The first two rules, Application and Lambda-abstraction, are performed on all the components of a parallel term in a synchronized way. Finally, via the third rule of Local Renaming, once a free typed variable is bounded by lambda-abstraction, each of the terms in parallel can do its local renaming, with type refinement, of that particular resource.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2016-07-15},
  date = {2008-05-28},
  pages = {95-113},
  keywords = {intersection types,lambda calculus,type inference,Church style,Curry style,Parallelism,Shared resources},
  author = {Berardi, S. and de’ Liguoro, U. and Bono, Viviana and Venneri, Betti and Bettini, Lorenzo},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/7RSAGMDQ/Berardi et al - 2008 - A typed lambda calculus with intersection types.pdf;/Users/pgiarrusso/Zotero/storage/BK6ZV364/S0304397508000625.html}
}

@incollection{Wegner1988Inheritance,
  langid = {english},
  title = {Inheritance as an Incremental Modification Mechanism or What Like Is and Isn’t Like},
  isbn = {978-3-540-50053-7 978-3-540-45910-1},
  url = {http://link.springer.com/chapter/10.1007/3-540-45910-3_4},
  abstract = {Incremental modification is a fundamental mechanism not only in software systems, but also in physical and mathematical systems. Inheritance owes its importance in large measure to its flexibility as a discrete incremental modification mechanism. Four increasingly permissive properties of incremental modification realizable by inheritance are examined: behavior compatibility, signature compatibility. name compatibility, and cancellation. Inheritance for entities with finite sets of attributes is defined and characterized as incremental modification with deferred binding of self-reference. Types defined as predicates for type checking are contrasted with classes defined as templates for object generation. Mathematical, operational, and conceptual models of inheritance are then examined in detail, leading to a discussion of algebraic models of behavioral compatibility. horizontal and vertical signature modification, algorithmically defined name modification, additive and subtractive exceptions, abstract inheritance networks, and parametric polymorphism. Liketypes are defined as a symmetrical general form of incremental modification that provide a framework for modeling similarity. The combination of safe behaviorally compatible changes and less safe radical incremental changes in a single programming language is considered.},
  number = {322},
  booktitle = {ECOOP ’88 European Conference on Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-07-15},
  date = {1988-08-15},
  pages = {55-77},
  keywords = {Programming Techniques,Artificial Intelligence (incl. Robotics)},
  author = {Wegner, Peter and Zdonik, Stanley B.},
  editor = {Gjessing, Stein and Nygaard, Kristen},
  file = {/Users/pgiarrusso/Zotero/storage/KHQ6GGX4/Wegner_Zdonik - 1988 - Inheritance as an Incremental Modification Mechanism or What Like Is and Isn’t Like.pdf;/Users/pgiarrusso/Zotero/storage/QCH9QDZH/3-540-45910-3_4.html},
  doi = {10.1007/3-540-45910-3_4}
}
% == BibLateX quality report for Wegner1988Inheritance:
% 'isbn': not a valid ISBN

@inproceedings{Okasaki1995Purely,
  location = {{New York, NY, USA}},
  title = {Purely Functional Random-access Lists},
  isbn = {978-0-89791-719-3},
  url = {http://doi.acm.org/10.1145/224164.224187},
  doi = {10.1145/224164.224187},
  booktitle = {Proceedings of the Seventh International Conference on Functional Programming Languages and Computer Architecture},
  series = {FPCA '95},
  publisher = {{ACM}},
  urldate = {2016-07-13},
  date = {1995},
  pages = {86--95},
  author = {Okasaki, Chris},
  file = {/Users/pgiarrusso/Zotero/storage/4TK7XX8F/Okasaki - 1995 - Purely Functional Random-access Lists.pdf}
}

@article{Brusilovsky1997Minilanguages,
  langid = {english},
  title = {Mini-languages: a way to learn programming principles},
  volume = {2},
  issn = {1360-2357, 1573-7608},
  url = {http://link.springer.com/article/10.1023/A%3A1018636507883},
  doi = {10.1023/A:1018636507883},
  shorttitle = {Mini-languages},
  abstract = {Mini-languages are a visually intuitive, simple and powerful way to introduce students to programming. They are a good foundation for general computer science instruction, provide insight into programming for the general population, and teach algorithmic thinking. The goal of the paper is to provide an extensive review of the mini-language approach to teaching programming. For different audiences and in different countries, the authors have extensive experience in design and application of mini-languages. We outline the problems that motivate the application of this approach, present a brief history, review several existing mini-languages, and provide discussion of lessons learned. In particular, we discuss how to choose a mini-language for a particular group of students and list some requirements for a successful application of a mini- language. We conclude with a discussion of possible future directions of the mini-language approach development},
  number = {1},
  journaltitle = {Education and Information Technologies},
  shortjournal = {Education and Information Technologies},
  urldate = {2016-07-12},
  date = {1997-03},
  pages = {65-83},
  keywords = {Data Structures; Cryptology and Information Theory,Computer Science; general,programming,Education (general),highereducation,informatics,languages,logo,secondaryeducation,Technology Education},
  author = {Brusilovsky, Peter and Calabrese, Eduardo and Hvorecky, Jozef and Kouchnirenko, Anatoly and Miller, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/5DP2BNP6/Brusilovsky et al - 1997 - Mini-languages - a way to learn programming principles.pdf;/Users/pgiarrusso/Zotero/storage/MR34KCQD/10.html}
}
% == BibLateX quality report for Brusilovsky1997Minilanguages:
% 'issn': not a valid ISSN

@inproceedings{Stolarek2015Injective,
  location = {{New York, NY, USA}},
  title = {Injective Type Families for Haskell},
  isbn = {978-1-4503-3808-0},
  url = {http://doi.acm.org/10.1145/2804302.2804314},
  doi = {10.1145/2804302.2804314},
  abstract = {Haskell, as implemented by the Glasgow Haskell Compiler (GHC), allows expressive type-level programming. The most popular type-level programming extension is TypeFamilies, which allows users to write functions on types. Yet, using type functions can cripple type inference in certain situations. In particular, lack of injectivity in type functions means that GHC can never infer an instantiation of a type variable appearing only under type functions. In this paper, we describe a small modification to GHC that allows type functions to be annotated as injective. GHC naturally must check validity of the injectivity annotations. The algorithm to do so is surprisingly subtle. We prove soundness for a simplification of our algorithm, and state and prove a completeness property, though the algorithm is not fully complete. As much of our reasoning surrounds functions defined by a simple pattern-matching structure, we believe our results extend beyond just Haskell. We have implemented our solution on a branch of GHC and plan to make it available to regular users with the next stable release of the compiler.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '15},
  publisher = {{ACM}},
  urldate = {2016-07-11},
  date = {2015},
  pages = {118--128},
  keywords = {Haskell,type-level programming,type families,functional dependencies,injectivity},
  author = {Stolarek, Jan and Peyton Jones, Simon and Eisenberg, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/6IA8KTF9/Stolarek et al - 2015 - Injective Type Families for Haskell.pdf}
}

@inproceedings{Vytiniotis2010Let,
  location = {{New York, NY, USA}},
  title = {Let Should Not Be Generalized},
  isbn = {978-1-60558-891-9},
  url = {http://doi.acm.org/10.1145/1708016.1708023},
  doi = {10.1145/1708016.1708023},
  abstract = {From the dawn of time, all derivatives of the classic Hindley-Milner type system have supported implicit generalisation of local let-bindings. Yet, as we will show, for more sophisticated type systems implicit let-generalisation imposes a disproportionate complexity burden. Moreover, it turns out that the feature is very seldom used, so we propose to eliminate it. The payoff is a substantial simplification, both of the specification of the type system, and of its implementation.},
  booktitle = {Proceedings of the 5th ACM SIGPLAN Workshop on Types in Language Design and Implementation},
  series = {TLDI '10},
  publisher = {{ACM}},
  urldate = {2016-07-11},
  date = {2010},
  pages = {39--50},
  keywords = {Generalized Algebraic Data Types,Haskell,type families,type inference,type classes},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Schrijvers, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/I8ANUXCR/Vytiniotis et al - 2010 - Let Should Not Be Generalized.pdf}
}

@inproceedings{Schrijvers2009Complete,
  location = {{New York, NY, USA}},
  title = {Complete and Decidable Type Inference for GADTs},
  isbn = {978-1-60558-332-7},
  url = {http://doi.acm.org/10.1145/1596550.1596599},
  doi = {10.1145/1596550.1596599},
  abstract = {GADTs have proven to be an invaluable language extension, for ensuring data invariants and program correctness among others. Unfortunately, they pose a tough problem for type inference: we lose the principal-type property, which is necessary for modular type inference. We present a novel and simplified type inference approach for local type assumptions from GADT pattern matches. Our approach is complete and decidable, while more liberal than previous such approaches.},
  booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '09},
  publisher = {{ACM}},
  urldate = {2016-07-08},
  date = {2009},
  pages = {341--352},
  keywords = {Haskell,type inference,_tablet,GADTs},
  author = {Schrijvers, Tom and Peyton Jones, Simon and Sulzmann, Martin and Vytiniotis, Dimitrios},
  file = {/Users/pgiarrusso/Zotero/storage/9NCKQDHH/Schrijvers-Peyton Jones-Sulzmann-Vytiniotis - 2009 - Complete and Decidable Type Inference for GADTs.pdf}
}

@article{Cazzola2016Language,
  title = {Language components for modular DSLs using traits},
  volume = {45},
  issn = {1477-8424},
  url = {http://www.sciencedirect.com/science/article/pii/S1477842415300208},
  doi = {10.1016/j.cl.2015.12.001},
  abstract = {Recent advances in tooling and modern programming languages have progressively brought back the practice of developing domain-specific languages as a means to improve software development. Consequently, the problem of making composition between languages easier by emphasizing code reuse and componentized programming is a topic of increasing interest in research. In fact, it is not uncommon for different languages to share common features, and, because in the same project different DSLs may coexist to model concepts from different problem areas, it is interesting to study ways to develop modular, extensible languages. Earlier work has shown that traits can be used to modularize the semantics of a language implementation; a lot of attention is often spent on embedded DSLs; even when external DSLs are discussed, the main focus is on modularizing the semantics. In this paper we will show a complete trait-based approach to modularize not only the semantics but also the syntax of external DSLs, thereby simplifying extension and therefore evolution of a language implementation. We show the benefits of implementing these techniques using the Scala programming language.},
  journaltitle = {Computer Languages, Systems \& Structures},
  shortjournal = {Computer Languages, Systems \& Structures},
  urldate = {2016-07-06},
  date = {2016-04},
  pages = {16-34},
  author = {Cazzola, Walter and Vacchi, Edoardo},
  file = {/Users/pgiarrusso/Zotero/storage/IZS94SRE/Cazzola_Vacchi - 2016 - Language components for modular DSLs using traits.pdf;/Users/pgiarrusso/Zotero/storage/CBB6P9XZ/S1477842415300208.html}
}

@inproceedings{Cartwright1985Types,
  location = {{New York, NY, USA}},
  title = {Types As Intervals},
  isbn = {978-0-89791-147-4},
  url = {http://doi.acm.org/10.1145/318593.318604},
  doi = {10.1145/318593.318604},
  abstract = {To accommodate polymorphic data types and operations, several computer scientists--most notably MacQueen, Plotkin, and Sethi--have proposed formalizing types as ideals. Although this approach is intuitively appealing, the resulting type system is both complex and restrictive because the type constructor that creates function types is not monotonic, and hence not computable. As a result, types cannot be treated as data values, precluding the formalization of type constructors and polymorphic program modules (where types are values) as higher order computable functions. Moreover, recursive definitions of new types do not necessarily have solutions.
This paper proposes a new formulation of types--called intervals--that subsumes the theory of types as ideals, yet avoids the pathologies caused by non-monotonic type constructors. In particular, the set of interval types contains the set of ideal types as a proper subset and all of the primitive type operations on intervals are extensions of the corresponding operations on ideals. Nevertheless, all of the primitive interval type constructors including the function type constructor and type quantifiers are computable operations. Consequently, types are higher order data values that can be freely manipulated within programs.
The key idea underlying the formalization of types as intervals is that negative information should be included in the description of a type. Negative information identifies the finite elements that do not belong to a type, just as conventional, positive information identifies the elements that do. Unless the negative information in a type description is the exact complement of the positive information, the description is partial in the sense that it approximates many different types--an interval of ideals between the positive information and the complement of the negative information. Although programmers typically deal with total (maximal) types, partial types appear to be an essential feature of a comprehensive polymorphic type system that accommodates types as data, just as partial functions are essential in any universal programming language.},
  booktitle = {Proceedings of the 12th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '85},
  publisher = {{ACM}},
  urldate = {2016-07-01},
  date = {1985},
  pages = {22--36},
  author = {Cartwright, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/CFMMHKQI/Cartwright - 1985 - Types As Intervals.pdf}
}

@inproceedings{Ofenbeck2013Spiral,
  location = {{New York, NY, USA}},
  title = {Spiral in Scala: Towards the Systematic Construction of Generators for Performance Libraries},
  isbn = {978-1-4503-2373-4},
  url = {http://doi.acm.org/10.1145/2517208.2517228},
  doi = {10.1145/2517208.2517228},
  shorttitle = {Spiral in Scala},
  abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
  booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts \& Experiences},
  series = {GPCE '13},
  publisher = {{ACM}},
  urldate = {2016-06-30},
  date = {2013},
  pages = {125--134},
  keywords = {abstraction over staging,data representation,scalar replacement,selective precomputation,synthesis},
  author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and Püschel, Markus}
}

@article{Leinster2012rethinking,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.6543},
  primaryClass = {math},
  title = {Rethinking set theory},
  url = {http://arxiv.org/abs/1212.6543},
  abstract = {Mathematicians manipulate sets with confidence almost every day, rarely making mistakes. Few of us, however, could accurately quote what are often referred to as "the" axioms of set theory. This suggests that we all carry around with us, perhaps subconsciously, a reliable body of operating principles for manipulating sets. What if we were to take some of those principles and adopt them as our axioms instead? The message of this article is that this can be done, in a simple, practical way (due to Lawvere). The resulting axioms are ten thoroughly mundane statements about sets. This is an expository article for a general mathematical readership.},
  urldate = {2015-05-14},
  date = {2012-12-28},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  author = {Leinster, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/4WAEN44X/Leinster - 2012 - Rethinking set theory.pdf;/Users/pgiarrusso/Zotero/storage/2TU9A69K/1212.html}
}
% == BibLateX quality report for Leinster2012rethinking:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Hudak2003Arrows,
  langid = {english},
  title = {Arrows, Robots, and Functional Reactive Programming},
  isbn = {978-3-540-40132-2 978-3-540-44833-4},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-44833-4_6},
  abstract = {Functional reactive programming},
  number = {2638},
  booktitle = {Advanced Functional Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-06-07},
  date = {2003},
  pages = {159-187},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs},
  author = {Hudak, Paul and Courtney, Antony and Nilsson, Henrik and Peterson, John},
  editor = {Jeuring, Johan and Jones, Simon L. Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/KAI5JUQP/Hudak et al - 2003 - Arrows, Robots, and Functional Reactive Programming.pdf;/Users/pgiarrusso/Zotero/storage/FX2EP6JZ/Hudak et al. - 2003 - Arrows, Robots, and Functional Reactive Programmin.html},
  doi = {10.1007/978-3-540-44833-4_6}
}
% == BibLateX quality report for Hudak2003Arrows:
% 'isbn': not a valid ISBN

@online{BeringerCandidates,
  title = {Candidates for Substitution},
  url = {http://www.lfcs.inf.ed.ac.uk/reports/97/ECS-LFCS-97-358/},
  urldate = {2016-05-27},
  author = {Beringer, Lennart},
  file = {/Users/pgiarrusso/Zotero/storage/IIGWTX7T/Beringer - Candidates for Substitution.pdf;/Users/pgiarrusso/Zotero/storage/FBQZM5W2/ECS-LFCS-97-358.html}
}
% == BibLateX quality report for BeringerCandidates:
% Exactly one of 'date' / 'year' must be present

@book{Mcbride2005Typepreserving,
  title = {Type-preserving renaming and substitution},
  abstract = {I present a substitution algorithm for the simply-typed λ-calculus, represented in the style of Altenkirch and Reus (1999) which is statically guaranteed to respect scope and type. Moreover, I use a single traversal function, instantiated first to renaming, then to substitution. The program is written in Epigram (McBride \& McKinna, 2004). 1},
  date = {2005},
  author = {Mcbride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/UIHKSP62/Mcbride - 2005 - Type-preserving renaming and substitution.pdf;/Users/pgiarrusso/Zotero/storage/WGXMGU2Z/summary.html}
}

@inproceedings{Vouillon2004Semantic,
  location = {{New York, NY, USA}},
  title = {Semantic Types: A Fresh Look at the Ideal Model for Types},
  isbn = {978-1-58113-729-3},
  url = {http://doi.acm.org/10.1145/964001.964006},
  doi = {10.1145/964001.964006},
  shorttitle = {Semantic Types},
  abstract = {We present a generalization of the ideal model for recursive polymorphic types. Types are defined as sets of terms instead of sets of elements of a semantic domain. Our proof of the existence of types (computed by fixpoint of a typing operator) does not rely on metric properties, but on the fact that the identity is the limit of a sequence of projection terms. This establishes a connection with the work of Pitts on relational properties of domains. This also suggests that ideals are better understood as closed sets of terms defined by orthogonality with respect to a set of contexts.},
  booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '04},
  publisher = {{ACM}},
  urldate = {2016-05-27},
  date = {2004},
  pages = {52--63},
  keywords = {polymorphism,recursive types,subtyping,realizability,ideal model,inductive/coinductive principle},
  author = {Vouillon, Jerome and Melliès, Paul-André},
  file = {/Users/pgiarrusso/Zotero/storage/BSS5UGVB/Vouillon_Melliès - 2004 - Semantic Types - A Fresh Look at the Ideal Model for Types.pdf}
}

@article{Pitts2000Parametric,
  title = {Parametric polymorphism and operational equivalence},
  volume = {10},
  issn = {1469-8072},
  url = {http://journals.cambridge.org/article_S0960129500003066},
  doi = {null},
  abstract = {Studies of the mathematical properties of impredicative polymorphic types have for the most part focused on the polymorphic lambda calculus of Girard–Reynolds, which is a calculus of total polymorphic functions. This paper considers polymorphic types from a functional programming perspective, where the partialness arising from the presence of fixpoint recursion complicates the nature of potentially infinite (‘lazy’) data types. An approach to Reynolds' notion of relational parametricity is developed that works directly on the syntax of a programming language, using a novel closure operator to relate operational behaviour to parametricity properties of types. Working with an extension of Plotkin's PCF with ∀-types, lazy lists and existential types, we show by example how the resulting logical relation can be used to prove properties of polymorphic types up to operational equivalence.},
  number = {03},
  journaltitle = {Mathematical Structures in Computer Science},
  urldate = {2016-05-27},
  date = {2000-06},
  pages = {321--359},
  author = {Pitts, Andrew M.},
  file = {/Users/pgiarrusso/Zotero/storage/32MBXA4G/Pitts - 2000 - Parametric polymorphism and operational equivalence.pdf;/Users/pgiarrusso/Zotero/storage/8BJ3IJF7/displayAbstract.html}
}

@inproceedings{Crafa2015Chemical,
  location = {{New York, NY, USA}},
  title = {The Chemical Approach to Typestate-oriented Programming},
  isbn = {978-1-4503-3689-5},
  url = {http://doi.acm.org/10.1145/2814270.2814287},
  doi = {10.1145/2814270.2814287},
  abstract = {We study a novel approach to typestate-oriented programming based on the chemical metaphor: state and operations on objects are molecules of messages and state transformations are chemical reactions. This approach allows us to investigate typestate in an inherently concurrent setting, whereby objects can be accessed and modified concurrently by several processes, each potentially changing only part of their state. We introduce a simple behavioral type theory to express in a uniform way both the private and the public interfaces of objects, to describe and enforce structured object protocols consisting of possibilities, prohibitions, and obligations, and to control object sharing.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2015},
  publisher = {{ACM}},
  urldate = {2016-05-26},
  date = {2015},
  pages = {917--934},
  keywords = {concurrency,behavioral types,join calculus,Typestate},
  author = {Crafa, Silvia and Padovani, Luca},
  file = {/Users/pgiarrusso/Zotero/storage/TSJT6KXZ/Crafa_Padovani - 2015 - The Chemical Approach to Typestate-oriented Programming.pdf}
}

@inproceedings{Najd2016Everything,
  location = {{New York, NY, USA}},
  title = {Everything Old is New Again: Quoted Domain-specific Languages},
  isbn = {978-1-4503-4097-7},
  url = {http://doi.acm.org/10.1145/2847538.2847541},
  doi = {10.1145/2847538.2847541},
  shorttitle = {Everything Old is New Again},
  abstract = {We describe a new approach to implementing Domain-Specific Languages(DSLs), called Quoted DSLs (QDSLs), that is inspired by two old ideas:quasi-quotation, from McCarthy's Lisp of 1960, and the subformula principle of normal proofs, from Gentzen's natural deduction of 1935. QDSLs reuse facilities provided for the host language, since host and quoted terms share the same syntax, type system, and normalisation rules. QDSL terms are normalised to a canonical form, inspired by the subformula principle, which guarantees that one can use higher-order types in the source while guaranteeing first-order types in the target, and enables using types to guide fusion. We test our ideas by re-implementing Feldspar, which was originally implemented as an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants. The two variants produce identical code.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM 2016},
  publisher = {{ACM}},
  urldate = {2016-05-23},
  date = {2016},
  pages = {25--36},
  keywords = {DSL,domain-specific language,EDSL,embedded language,normalisation,QDSL,quotation,subformula principle},
  author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/W86Q3AIX/Najd et al - 2016 - Everything Old is New Again - Quoted Domain-specific Languages.pdf}
}

@incollection{Rompf2016Essence,
  langid = {english},
  title = {The Essence of Multi-stage Evaluation in LMS},
  isbn = {978-3-319-30935-4 978-3-319-30936-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-30936-1_17},
  abstract = {Embedded domain-specific languages (DSLs) are the subject of wide-spread interest, and a variety of implementation techniques exist. Some of them have been invented, and some of them discovered. Many are based on a form of generative or multi-stage programming, where the host language program builds up DSL terms during its evaluation. In this paper, we examine the execution model of LMS (Lightweight Modular Staging), a framework for embedded DSLs in Scala, and link it to evaluation in a two-stage lambda calculus. This clarifies the semantics of certain ad-hoc implementation choices, and provides guidance for implementing similar multi-stage evaluation facilities in other languages.},
  number = {9600},
  booktitle = {A List of Successes That Can Change the World},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2016-05-20},
  date = {2016},
  pages = {318-335},
  keywords = {domain-specific languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,multi-stage programming,Scala,partial evaluation,LMS (Lightweight Modular Staging)},
  author = {Rompf, Tiark},
  editor = {Lindley, Sam and McBride, Conor and Trinder, Phil and Sannella, Don},
  file = {/Users/pgiarrusso/Zotero/storage/SGN2N3DC/Rompf - 2016 - The Essence of Multi-stage Evaluation in LMS.pdf;/Users/pgiarrusso/Zotero/storage/RK9U6RXP/978-3-319-30936-1_17.html},
  doi = {10.1007/978-3-319-30936-1_17}
}
% == BibLateX quality report for Rompf2016Essence:
% 'isbn': not a valid ISBN

@inproceedings{Ahmed2008Typed,
  location = {{New York, NY, USA}},
  title = {Typed Closure Conversion Preserves Observational Equivalence},
  isbn = {978-1-59593-919-7},
  url = {http://doi.acm.org/10.1145/1411204.1411227},
  doi = {10.1145/1411204.1411227},
  abstract = {Language-based security relies on the assumption that all potential attacks are bound by the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence. We investigate the problem of fully abstract compilation, i.e., compilation that both preserves and reflects observational equivalence. In particular, we prove that typed closure conversion for the polymorphic »-calculus with existential and recursive types is fully abstract. Our proof uses operational techniques in the form of a step-indexed logical relation and construction of certain wrapper terms that "back-translate" from target values to source values. Although typed closure conversion has been assumed to be fully abstract, we are not aware of any previous result that actually proves this.},
  booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '08},
  publisher = {{ACM}},
  urldate = {2016-05-19},
  date = {2008},
  pages = {157--168},
  keywords = {equivalence-preserving compilation,full abstraction,step-indexed logical relations,typed closure conversion},
  author = {Ahmed, Amal and Blume, Matthias},
  file = {/Users/pgiarrusso/Zotero/storage/JAHBL3CV/Ahmed-Blume - 2008 - Typed Closure Conversion Preserves Observational Equivalence.pdf}
}

@article{Abel2014Normalization,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2059},
  title = {Normalization by Evaluation in the Delay Monad: A Case Study for Coinduction via Copatterns and Sized Types},
  volume = {153},
  issn = {2075-2180},
  url = {http://arxiv.org/abs/1406.2059},
  doi = {10.4204/EPTCS.153.4},
  shorttitle = {Normalization by Evaluation in the Delay Monad},
  abstract = {In this paper, we present an Agda formalization of a normalizer for simply-typed lambda terms. The normalizer consists of two coinductively defined functions in the delay monad: One is a standard evaluator of lambda terms to closures, the other a type-directed reifier from values to eta-long beta-normal forms. Their composition, normalization-by-evaluation, is shown to be a total function a posteriori, using a standard logical-relations argument. The successful formalization serves as a proof-of-concept for coinductive programming and reasoning using sized types and copatterns, a new and presently experimental feature of Agda.},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  urldate = {2016-05-09},
  date = {2014-06-05},
  pages = {51-67},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,F.4.1,D.3.3,F.3.3,F.3.2},
  author = {Abel, Andreas and Chapman, James},
  file = {/Users/pgiarrusso/Zotero/storage/9TSXGDGF/Abel_Chapman - 2014 - Normalization by Evaluation in the Delay Monad - A Case Study for Coinduction via Copatterns and Sized Types.pdf;/Users/pgiarrusso/Zotero/storage/UANREUGV/1406.html}
}
% == BibLateX quality report for Abel2014Normalization:
% Unexpected field 'archivePrefix'

@article{Betts2015Design,
  title = {The Design and Implementation of a Verification Technique for GPU Kernels},
  volume = {37},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/2743017},
  doi = {10.1145/2743017},
  abstract = {We present a technique for the formal verification of GPU kernels, addressing two classes of correctness properties: data races and barrier divergence. Our approach is founded on a novel formal operational semantics for GPU kernels termed \emph{synchronous, delayed visibility (SDV)} semantics, which captures the execution of a GPU kernel by multiple groups of threads. The SDV semantics provides operational definitions for barrier divergence and for both inter- and intra-group data races. We build on the semantics to develop a method for reducing the task of verifying a massively parallel GPU kernel to that of verifying a sequential program. This completely avoids the need to reason about thread interleavings, and allows existing techniques for sequential program verification to be leveraged. We describe an efficient encoding of data race detection and propose a method for automatically inferring the loop invariants that are required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, that can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 162 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels.},
  number = {3},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2016-05-06},
  date = {2015-05},
  pages = {10:1--10:49},
  keywords = {concurrency,barrier synchronization,data races,GPUs,Verification},
  author = {Betts, Adam and Chong, Nathan and Donaldson, Alastair F. and Ketema, Jeroen and Qadeer, Shaz and Thomson, Paul and Wickerson, John},
  file = {/Users/pgiarrusso/Zotero/storage/QIT7ZPUB/Betts et al - 2015 - The Design and Implementation of a Verification Technique for GPU Kernels.pdf}
}
% == BibLateX quality report for Betts2015Design:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@article{Farmer2008seven,
  title = {The seven virtues of simple type theory},
  volume = {6},
  issn = {1570-8683},
  url = {http://www.sciencedirect.com/science/article/pii/S157086830700081X},
  doi = {10.1016/j.jal.2007.11.001},
  abstract = {Simple type theory, also known as higher-order logic, is a natural extension of first-order logic which is simple, elegant, highly expressive, and practical. This paper surveys the virtues of simple type theory and attempts to show that simple type theory is an attractive alternative to first-order logic for practical-minded scientists, engineers, and mathematicians. It recommends that simple type theory be incorporated into introductory logic courses offered by mathematics departments and into the undergraduate curricula for computer science and software engineering students.},
  number = {3},
  journaltitle = {Journal of Applied Logic},
  shortjournal = {Journal of Applied Logic},
  urldate = {2016-05-02},
  date = {2008-09},
  pages = {267-286},
  keywords = {type theory,Complete ordered field,higher-order logic,Nonstandard models,Peano arithmetic,Practical logics},
  author = {Farmer, William M.},
  file = {/Users/pgiarrusso/Zotero/storage/2SFH2SQC/Farmer - 2008 - The seven virtues of simple type theory.pdf;/Users/pgiarrusso/Zotero/storage/WDCPZXAP/S157086830700081X.html}
}

@article{Church1940Formulation,
  eprinttype = {jstor},
  eprint = {2266170},
  title = {A Formulation of the Simple Theory of Types},
  volume = {5},
  issn = {0022-4812},
  doi = {10.2307/2266170},
  number = {2},
  journaltitle = {The Journal of Symbolic Logic},
  shortjournal = {The Journal of Symbolic Logic},
  date = {1940},
  pages = {56-68},
  author = {Church, Alonzo},
  file = {/Users/pgiarrusso/Zotero/storage/8GFIUMKA/Church - 1940 - A Formulation of the Simple Theory of Types.pdf}
}

@article{K2016LivenessBased,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.05841},
  primaryClass = {cs},
  title = {Liveness-Based Garbage Collection for Lazy Languages},
  url = {http://arxiv.org/abs/1604.05841},
  abstract = {We consider the problem of reducing the memory required to run lazy first-order functional programs. Our approach is to analyze programs for liveness of heap-allocated data. The result of the analysis is used to preserve only live data---a subset of reachable data---during garbage collection. The result is an increase in the garbage reclaimed and a reduction in the peak memory requirement of programs. While this technique has already been shown to yield benefits for eager first-order languages, the lack of a statically determinable execution order and the presence of closures pose new challenges for lazy languages. These require changes both in the liveness analysis itself and in the design of the garbage collector. To show the effectiveness of our method, we implemented a copying collector that uses the results of the liveness analysis to preserve live objects, both evaluated (i.e., in WHNF) and closures. Our experiments confirm that for programs running with a liveness-based garbage collector, there is a significant decrease in peak memory requirements. In addition, a sizable reduction in the number of collections ensures that in spite of using a more complex garbage collector, the execution times of programs running with liveness and reachability-based collectors remain comparable.},
  urldate = {2016-04-21},
  date = {2016-04-20},
  keywords = {Computer Science - Programming Languages},
  author = {K, Prasanna Kumar and Sanyal, Amitabha and Karkare, Amey},
  file = {/Users/pgiarrusso/Zotero/storage/9HNWSXG9/K et al - 2016 - Liveness-Based Garbage Collection for Lazy Languages.pdf;/Users/pgiarrusso/Zotero/storage/6QAW2I2X/1604.html}
}
% == BibLateX quality report for K2016LivenessBased:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Mitchell2009Losing,
  location = {{New York, NY, USA}},
  title = {Losing Functions Without Gaining Data: Another Look at Defunctionalisation},
  isbn = {978-1-60558-508-6},
  url = {http://doi.acm.org/10.1145/1596638.1596641},
  doi = {10.1145/1596638.1596641},
  shorttitle = {Losing Functions Without Gaining Data},
  abstract = {We describe a transformation which takes a higher-order program, and produces an equivalent first-order program. Unlike Reynolds-style defunctionalisation, it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. We can use our method to improve the results of existing analysis operations, including strictness analysis, pattern-match safety and termination checking. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method cannot always succeed in removing all functional values, but in practice is remarkably successful.},
  booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '09},
  publisher = {{ACM}},
  urldate = {2016-04-21},
  date = {2009},
  pages = {13--24},
  keywords = {Haskell,defunctionalisation,firstification},
  author = {Mitchell, Neil and Runciman, Colin},
  file = {/Users/pgiarrusso/Zotero/storage/5ET7RUJ8/Mitchell_Runciman - 2009 - Losing Functions Without Gaining Data - Another Look at Defunctionalisation.pdf}
}

@article{Najd2016Embedding,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.05197},
  primaryClass = {cs},
  title = {Embedding by Normalisation},
  url = {http://arxiv.org/abs/1603.05197},
  abstract = {This paper presents the insight that practical embedding techniques, commonly used for implementing Domain-Specific Languages, correspond to theoretical Normalisation-By-Evaluation (NBE) techniques, commonly used for deriving canonical form of terms with respect to an equational theory. NBE constitutes of four components: a syntactic domain, a semantic domain, and a pair of translations between the two. Embedding also often constitutes of four components: an object language, a host language, encoding of object terms in the host, and extraction of object code from the host. The correspondence is deep in that all four components in embedding and NBE correspond to each other. Based on this correspondence, this paper introduces Embedding-By-Normalisation (EBN) as a principled approach to study and structure embedding. The correspondence is useful in that solutions from NBE can be borrowed to solve problems in embedding. In particular, based on NBE techniques, such as Type-Directed Partial Evaluation, this paper presents a solution to the problem of extracting object code from embedded programs involving sum types, such as conditional expressions, and primitives, such as literals and operations on them.},
  urldate = {2016-04-19},
  date = {2016-03-16},
  keywords = {Computer Science - Programming Languages},
  author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/YWW5LKVD/Najd et al. - 2016 - Embedding by Normalisation.pdf;/Users/pgiarrusso/Zotero/storage/VKV278G9/1603.html}
}
% == BibLateX quality report for Najd2016Embedding:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Kalibera2013Rigorous,
  location = {{New York, NY, USA}},
  title = {Rigorous Benchmarking in Reasonable Time},
  isbn = {978-1-4503-2100-6},
  url = {http://doi.acm.org/10.1145/2464157.2464160},
  doi = {10.1145/2464157.2464160},
  abstract = {Experimental evaluation is key to systems research. Because modern systems are complex and non-deterministic, good experimental methodology demands that researchers account for uncertainty. To obtain valid results, they are expected to run many iterations of benchmarks, invoke virtual machines (VMs) several times, or even rebuild VM or benchmark binaries more than once. All this repetition costs time to complete experiments. Currently, many evaluations give up on sufficient repetition or rigorous statistical methods, or even run benchmarks only in training sizes. The results reported often lack proper variation estimates and, when a small difference between two systems is reported, some are simply unreliable. In contrast, we provide a statistically rigorous methodology for repetition and summarising results that makes efficient use of experimentation time. Time efficiency comes from two key observations. First, a given benchmark on a given platform is typically prone to much less non-determinism than the common worst-case of published corner-case studies. Second, repetition is most needed where most uncertainty arises (whether between builds, between executions or between iterations). We capture experimentation cost with a novel mathematical model, which we use to identify the number of repetitions at each level of an experiment necessary and sufficient to obtain a given level of precision. We present our methodology as a cookbook that guides researchers on the number of repetitions they should run to obtain reliable results. We also show how to present results with an effect size confidence interval. As an example, we show how to use our methodology to conduct throughput experiments with the DaCapo and SPEC CPU benchmarks on three recent platforms.},
  booktitle = {Proceedings of the 2013 International Symposium on Memory Management},
  series = {ISMM '13},
  publisher = {{ACM}},
  urldate = {2016-03-16},
  date = {2013},
  pages = {63--74},
  keywords = {benchmarking methodology,dacapo,spec cpu,statistical methods},
  author = {Kalibera, Tomas and Jones, Richard},
  file = {/Users/pgiarrusso/Zotero/storage/ZFTK4V6I/Kalibera_Jones - 2013 - Rigorous Benchmarking in Reasonable Time.pdf}
}

@incollection{Leather2009PullUps,
  langid = {english},
  title = {Pull-Ups, Push-Downs, and Passing It Around},
  isbn = {978-3-642-16477-4 978-3-642-16478-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-16478-1_10},
  abstract = {Programs in languages such as Haskell are often datatype-centric and make extensive use of folds on that datatype. Incrementalization of such a program can significantly improve its performance by transforming monolithic atomic folds into incremental computations. Functional incrementalization separates the recursion from the application of the algebra in order to reduce redundant computations and reuse intermediate results. In this paper, we motivate incrementalization with a simple example and present a library for transforming programs using upwards, downwards, and circular incrementalization. Our benchmarks show that incrementalized computations using the library are nearly as fast as handwritten atomic functions.},
  number = {6041},
  booktitle = {Implementation and Application of Functional Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-03-09},
  date = {2009-09-23},
  pages = {159-178},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering/Programming and Operating Systems},
  author = {Leather, Sean and Löh, Andres and Jeuring, Johan},
  editor = {Morazán, Marco T. and Scholz, Sven-Bodo},
  file = {/Users/pgiarrusso/Zotero/storage/5E9765Z4/Leather et al - 2009 - Pull-Ups, Push-Downs, and Passing It Around.pdf;/Users/pgiarrusso/Zotero/storage/UZR8KX2E/978-3-642-16478-1_10.html},
  doi = {10.1007/978-3-642-16478-1_10}
}
% == BibLateX quality report for Leather2009PullUps:
% 'isbn': not a valid ISBN

@inproceedings{Lempsink2009Typesafe,
  location = {{New York, NY, USA}},
  title = {Type-safe Diff for Families of Datatypes},
  isbn = {978-1-60558-510-9},
  url = {http://doi.acm.org/10.1145/1596614.1596624},
  doi = {10.1145/1596614.1596624},
  abstract = {The UNIX diff program finds the difference between two text files using a classic algorithm for determining the longest common subsequence; however, when working with structured input (e.g. program code), we often want to find the difference between tree-like data (e.g. the abstract syntax tree). In a functional programming language such as Haskell, we can represent this data with a family of (mutually recursive) datatypes. In this paper, we describe a functional, datatype-generic implementation of diff (and the associated program patch). Our approach requires advanced type system features to preserve type safety; therefore, we present the code in Agda, a dependently-typed language well-suited to datatype-generic programming. In order to establish the usefulness of our work, we show that its efficiency can be improved with memoization and that it can also be defined in Haskell.},
  booktitle = {Proceedings of the 2009 ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '09},
  publisher = {{ACM}},
  urldate = {2016-03-09},
  date = {2009},
  pages = {61--72},
  keywords = {Datatype-generic programming,dependent types,edit distance},
  author = {Lempsink, Eelco and Leather, Sean and Löh, Andres},
  file = {/Users/pgiarrusso/Zotero/storage/85CGCP4M/Lempsink et al - 2009 - Type-safe Diff for Families of Datatypes.pdf}
}

@inproceedings{Ohori2007Lightweight,
  location = {{New York, NY, USA}},
  title = {Lightweight Fusion by Fixed Point Promotion},
  isbn = {1-59593-575-4},
  url = {http://doi.acm.org/10.1145/1190216.1190241},
  doi = {10.1145/1190216.1190241},
  abstract = {This paper proposes a lightweight fusion method for general recursive function definitions. Compared with existing proposals, our method has several significant practical features: it works for general recursive functions on general algebraic data types; it does not produce extra runtime overhea (except for possible code size increase due to the success of fusion); and it is readily incorporated in standard inlining optimization. This is achieved by extending the ordinary inlining process with a new fusion law that transforms a term of the form f o (fixgλx.E) to a new fixed point term fixhλx.E′ by promoting the function f through the fixed point operator. This is a sound syntactic transformation rule that is not sensitive to the types of f and g. This property makes our method applicable to wide range of functions including those with multi-parameters in both curried and uncurried forms. Although this method does not guarantee any form of completeness, it fuses typical examples discussed in the literature and others that involve accumulating parameters, either in the tt foldl-like specific forms or in general recursive forms, without any additional machinery. In order to substantiate our claim, we have implemented our method in a compiler. Although it is preliminary, it demonstrates practical feasibility of this method.},
  booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '07},
  publisher = {{ACM}},
  urldate = {2016-03-07},
  date = {2007},
  pages = {143--154},
  keywords = {fusion,fixed point,inlining},
  author = {Ohori, Atsushi and Sasano, Isao},
  file = {/Users/pgiarrusso/Zotero/storage/7N9ZVSH7/Ohori_Sasano - 2007 - Lightweight Fusion by Fixed Point Promotion.pdf}
}

@inproceedings{Sergey2014Modular,
  location = {{New York, NY, USA}},
  title = {Modular, Higher-order Cardinality Analysis in Theory and Practice},
  isbn = {978-1-4503-2544-8},
  url = {http://doi.acm.org/10.1145/2535838.2535861},
  doi = {10.1145/2535838.2535861},
  abstract = {Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '14},
  publisher = {{ACM}},
  urldate = {2016-03-01},
  date = {2014},
  pages = {335--347},
  keywords = {Lazy Evaluation,Haskell,functional programming languages,operational semantics,Compilers,Program optimisation,thunks,static analysis,cardinality analysis,types and effects},
  author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/W5U3IS6V/Sergey et al - 2014 - Modular, Higher-order Cardinality Analysis in Theory and Practice.pdf}
}

@report{Jeuring1993Theories,
  title = {Theories for Algorithm Calculation},
  date = {1993},
  author = {Jeuring, Johan Theodoor},
  file = {/Users/pgiarrusso/Zotero/storage/QNZJISXX/Jeuring - 1993 - Theories for Algorithm Calculation.pdf;/Users/pgiarrusso/Zotero/storage/I7N78GMK/summary.html}
}
% == BibLateX quality report for Jeuring1993Theories:
% Missing required field 'type'
% Missing required field 'institution'

@inproceedings{Jeuring1991Incremental,
  title = {Incremental Algorithms on Lists},
  abstract = {Incremental computations can improve the performance of interactive programs such as spreadsheet programs, program development environments, text editors, etc. Incremental algorithms describe how to compute a required value depending on the input, after the input has been edited. By considering the possible different edit actions on the data type lists, the basic data type used in spreadsheet programs and text editors, we define incremental algorithms on lists. Some theory for the construction of incremental algorithms is developed, and we give an incremental algorithm for a more involved example: formatting a text.  CR categories and descriptors: D11 [Software]: Programming Techniques --- Applicative Programming, D43 [Software]: Programming Languages --- Language constructs, I22 [Artificial Intelligence]: Automatic Programming --- Program transformation. General terms: algorithm, design, theory. Additional keywords and phrases: Bird-Meertens calculus for program construction, incremen...},
  booktitle = {Proceedings SION Computing Science in the Netherlands},
  date = {1991},
  pages = {315--335},
  author = {Jeuring, Johan},
  file = {/Users/pgiarrusso/Zotero/storage/8CSF4SJ9/Jeuring - 1991 - Incremental Algorithms on Lists.pdf;/Users/pgiarrusso/Zotero/storage/BQ8TKTI6/summary.html}
}

@article{Leroy2009Coinductive,
  title = {Coinductive big-step operational semantics},
  volume = {207},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540108001296},
  doi = {10.1016/j.ic.2007.12.004},
  abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  series = {Special issue on Structural Operational Semantics (SOS)},
  urldate = {2016-02-29},
  date = {2009-02},
  pages = {284-304},
  keywords = {coinduction,operational semantics,The Coq proof assistant,Big-step semantics,Compiler correctness,Mechanized proofs,Natural semantics,Reduction semantics,Small-step semantics,Type soundness},
  author = {Leroy, Xavier and Grall, Hervé},
  file = {/Users/pgiarrusso/Zotero/storage/X2DAUB5T/Leroy_Grall - 2009 - Coinductive big-step operational semantics.pdf;/Users/pgiarrusso/Zotero/storage/6HCU5DGA/S0890540108001296.html}
}

@article{Marlow2006Making,
  title = {Making a fast curry: push/enter vs. eval/apply for higher-order languages},
  volume = {16},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796806005995},
  doi = {10.1017/S0956796806005995},
  shorttitle = {Making a fast curry},
  abstract = {Higher-order languages that encourage currying are typically implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other. Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell. Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  urldate = {2016-02-29},
  date = {2006},
  pages = {415--449},
  author = {Marlow, Simon and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/X5KCUUWV/Marlow_Jones - 2006 - Making a fast curry - push-enter vs. eval-apply for higher-order languages.pdf;/Users/pgiarrusso/Zotero/storage/I82EQG3N/displayAbstract.html}
}

@inproceedings{Marlow2004Making,
  location = {{New York, NY, USA}},
  title = {Making a Fast Curry: Push/Enter vs. Eval/Apply for Higher-order Languages},
  isbn = {1-58113-905-5},
  url = {http://doi.acm.org/10.1145/1016850.1016856},
  doi = {10.1145/1016850.1016856},
  shorttitle = {Making a Fast Curry},
  abstract = {Higher-order languages that encourage currying are implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other.Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell.Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  booktitle = {Proceedings of the Ninth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '04},
  publisher = {{ACM}},
  urldate = {2016-02-29},
  date = {2004},
  pages = {4--15},
  author = {Marlow, Simon and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/EU7GU4S9/Marlow_Jones - 2004 - Making a Fast Curry - Push-Enter vs. Eval-Apply for Higher-order Languages.pdf}
}

@inproceedings{Bolingbroke2009types,
  location = {{New York, NY, USA}},
  title = {Types Are Calling Conventions},
  isbn = {978-1-60558-508-6},
  url = {http://doi.acm.org/10.1145/1596638.1596640},
  doi = {10.1145/1596638.1596640},
  abstract = {It is common for compilers to derive the calling convention of a function from its type. Doing so is simple and modular but misses many optimisation opportunities, particularly in lazy, higher-order functional languages with extensive use of currying. We restore the lost opportunities by defining Strict Core, a new intermediate language whose type system makes the missing distinctions: laziness is explicit, and functions take multiple arguments and return multiple results.},
  booktitle = {Proceedings of the 2nd ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '09},
  publisher = {{ACM}},
  urldate = {2016-02-08},
  date = {2009},
  pages = {1--12},
  keywords = {arity,calling conventions,intermediate language,strictness,unboxing,uncurrying},
  author = {Bolingbroke, Maximilian C. and Peyton Jones, Simon L.},
  file = {/Users/pgiarrusso/Zotero/storage/7JN4CHH4/Bolingbroke_Peyton Jones - 2009 - Types Are Calling Conventions.pdf}
}

@incollection{Morihata2016incremental,
  langid = {english},
  title = {Incremental Computing with Abstract Data Structures},
  isbn = {978-3-319-29603-6 978-3-319-29604-3},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-29604-3_14},
  abstract = {Incremental computing is a method of keeping consistency between an input and an output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for abstract data structures such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an incremental computing method, which can deal with complex modifications and therefore is suitable for abstract data structures. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring’s incremental computing method, which can deal with algebraic data structures, so as to deal with abstract data structures. Our method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  number = {9613},
  booktitle = {Functional and Logic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2016-02-29},
  date = {2016-03-04},
  pages = {215-231},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Morihata, Akimasa},
  editor = {Kiselyov, Oleg and King, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/76H2XX8X/Morihata - 2016 - Incremental Computing with Abstract Data Structures.pdf;/Users/pgiarrusso/Zotero/storage/792TJXCE/978-3-319-29604-3_14.html},
  doi = {10.1007/978-3-319-29604-3_14}
}
% == BibLateX quality report for Morihata2016incremental:
% 'isbn': not a valid ISBN

@inproceedings{Chen2016principal,
  location = {{New York, NY, USA}},
  title = {Principal Type Inference for GADTs},
  isbn = {978-1-4503-3549-2},
  url = {http://doi.acm.org/10.1145/2837614.2837665},
  doi = {10.1145/2837614.2837665},
  abstract = {We present a new method for GADT type inference that improves the precision of previous approaches. In particular, our approach accepts more type-correct programs than previous approaches when they do not employ type annotations. A side benefit of our approach is that it can detect a wide range of runtime errors that are missed by previous approaches. Our method is based on the idea to represent type refinements in pattern-matching branches by choice types, which facilitate a separation of the typing and reconciliation phases and thus support case expressions. This idea is formalized in a type system, which is both sound and a conservative extension of the classical Hindley-Milner system. We present the results of an empirical evaluation that compares our algorithm with previous approaches.},
  booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL 2016},
  publisher = {{ACM}},
  urldate = {2016-01-12},
  date = {2016},
  pages = {416--428},
  keywords = {type inference,_tablet,Choice Type,GADT,Type Reconciliation,Variational Unification},
  author = {Chen, Sheng and Erwig, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/N3R3KKRJ/Chen-Erwig - 2016 - Principal Type Inference for GADTs.pdf}
}

@inproceedings{Suzuki2016finally,
  location = {{New York, NY, USA}},
  title = {Finally, Safely-extensible and Efficient Language-integrated Query},
  isbn = {978-1-4503-4097-7},
  url = {http://doi.acm.org/10.1145/2847538.2847542},
  doi = {10.1145/2847538.2847542},
  abstract = {Language-integrated query is an embedding of database queries into a host language to code queries at a higher level than the all-to-common concatenation of strings of SQL fragments. The eventually produced SQL is ensured to be well-formed and well-typed, and hence free from the embarrassing (security) problems. Language-integrated query takes advantage of the host language's functional and modular abstractions to compose and reuse queries and build query libraries. Furthermore, language-integrated query systems like T-LINQ generate efficient SQL, by applying a number of program transformations to the embedded query. Alas, the set of transformation rules is not designed to be extensible. We demonstrate a new technique of integrating database queries into a typed functional programming language, so to write well-typed, composable queries and execute them efficiently on any SQL back-end as well as on an in-memory noSQL store. A distinct feature of our framework is that both the query language as well as the transformation rules needed to generate efficient SQL are safely user-extensible, to account for many variations in the SQL back-ends, as well for domain-specific knowledge. The transformation rules are guaranteed to be type-preserving and hygienic by their very construction. They can be built from separately developed and reusable parts and arbitrarily composed into optimization pipelines. With this technique we have embedded into OCaml a relational query language that supports a very large subset of SQL including grouping and aggregation. Its types cover the complete set of intricate SQL behaviors.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM 2016},
  publisher = {{ACM}},
  urldate = {2016-02-28},
  date = {2016},
  pages = {37--48},
  keywords = {sql,language-integrated query,EDSL,LINQ,tagless-final},
  author = {Suzuki, Kenichi and Kiselyov, Oleg and Kameyama, Yukiyoshi},
  file = {/Users/pgiarrusso/Zotero/storage/6IZDIMKZ/Suzuki et al - 2016 - Finally, Safely-extensible and Efficient Language-integrated Query.pdf}
}

@inproceedings{Xi2003guarded,
  location = {{New York, NY, USA}},
  title = {Guarded Recursive Datatype Constructors},
  isbn = {1-58113-628-5},
  url = {http://doi.acm.org/10.1145/604131.604150},
  doi = {10.1145/604131.604150},
  abstract = {We introduce a notion of guarded recursive (g.r.) datatype constructors, generalizing the notion of recursive datatypes in functional programming languages such as ML and Haskell. We address both theoretical and practical issues resulted from this generalization. On one hand, we design a type system to formalize the notion of g.r. datatype constructors and then prove the soundness of the type system. On the other hand, we present some significant applications (e.g., implementing objects, implementing staged computation, etc.) of g.r. datatype constructors, arguing that g.r. datatype constructors can have far-reaching consequences in programming. The main contribution of the paper lies in the recognition and then the formalization of a programming notion that is of both theoretical interest and practical use.},
  booktitle = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '03},
  publisher = {{ACM}},
  urldate = {2016-02-26},
  date = {2003},
  pages = {224--235},
  keywords = {datatype,constructors,guarded,recursive},
  author = {Xi, Hongwei and Chen, Chiyan and Chen, Gang},
  file = {/Users/pgiarrusso/Zotero/storage/KEXWXH6R/Xi et al - 2003 - Guarded Recursive Datatype Constructors.pdf}
}

@article{Mcbride2003firstorder,
  title = {First-order unification by structural recursion},
  volume = {13},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796803004957},
  doi = {10.1017/S0956796803004957},
  abstract = {First-order unification algorithms (Robinson, 1965) are traditionally implemented via general recursion, with separate proofs for partial correctness and termination. The latter tends to involve counting the number of unsolved variables and showing that this total decreases each time a substitution enlarges the terms. There are many such proofs in the literature (Manna \& Waldinger, 1981; Paulson, 1985; Coen, 1992; Rouyer, 1992; Jaume, 1997; Bove, 1999). This paper shows how a dependent type can relate terms to the set of variables over which they are constructed. As a consequence, first-order unification becomes a structurally recursive program, and a termination proof is no longer required. Both the program and its correctness proof have been checked using the proof assistant LEGO (Luo \& Pollack, 1992; McBride, 1999).},
  number = {06},
  journaltitle = {Journal of Functional Programming},
  urldate = {2016-02-25},
  date = {2003},
  pages = {1061--1075},
  author = {Mcbride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/4ENW3GSQ/Mcbride - 2003 - First-order unification by structural recursion.pdf;/Users/pgiarrusso/Zotero/storage/E4PW8FX2/displayAbstract.html}
}

@inproceedings{Ernst2006virtual,
  location = {{New York, NY, USA}},
  title = {A Virtual Class Calculus},
  isbn = {1-59593-027-2},
  url = {http://doi.acm.org/10.1145/1111037.1111062},
  doi = {10.1145/1111037.1111062},
  abstract = {Virtual classes are class-valued attributes of objects. Like virtual methods, virtual classes are defined in an object's class and may be redefined within subclasses. They resemble inner classes, which are also defined within a class, but virtual classes are accessed through object instances, not as static components of a class. When used as types, virtual classes depend upon object identity -- each object instance introduces a new family of virtual class types. Virtual classes support large-scale program composition techniques, including higher-order hierarchies and family polymorphism. The original definition of virtual classes in BETA left open the question of static type safety, since some type errors were not caught until runtime. Later the languages Caesar and gbeta have used a more strict static analysis in order to ensure static type safety. However, the existence of a sound, statically typed model for virtual classes has been a long-standing open question. This paper presents a virtual class calculus, VC, that captures the essence of virtual classes in these full-fledged programming languages. The key contributions of the paper are a formalization of the dynamic and static semantics of VC and a proof of the soundness of VC.},
  booktitle = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '06},
  publisher = {{ACM}},
  urldate = {2016-02-22},
  date = {2006},
  pages = {270--282},
  keywords = {soundness,virtual classes},
  author = {Ernst, Erik and Ostermann, Klaus and Cook, William R.},
  file = {/Users/pgiarrusso/Zotero/storage/V896UNP2/Ernst et al - 2006 - A Virtual Class Calculus.pdf}
}
% == BibLateX quality report for Ernst2006virtual:
% ? Unsure about the formatting of the booktitle

@inproceedings{Gramoli2014why,
  location = {{New York, NY, USA}},
  title = {Why Inheritance Anomaly is Not Worth Solving},
  isbn = {978-1-4503-2914-9},
  url = {http://doi.acm.org/10.1145/2633301.2633307},
  doi = {10.1145/2633301.2633307},
  abstract = {Modern computers improve their predecessors with additional parallelism but require concurrent software to exploit it. Object-orientation is instrumental in simplifying sequential programming, however, in a concurrent setting, programmers adding new methods in a subclass typically have to modify the code of the superclass, which inhibits reuse, a problem known as inheritance anomaly. There have been much efforts by researchers in the last two decades to solve the problem by deriving anomaly-free languages. Yet, these proposals have not ended up as practical solutions, thus one may ask why. In this article, we investigate from a theoretical perspective if a solution of the problem would introduce extra code complexity. We model object behavior as a regular language, and show that freedom from inheritance anomaly necessitates a language where ensuring Liskov-Wing substitutability becomes a language containment problem, which in our modeling is PSPACE hard. This indicates that we cannot expect programmers to manually ensure that subtyping holds in an anomaly-free language. Anomaly freedom thus predictably leads to software bugs and we doubt the value of providing it. From the practical perspective, the problem is already solved. Inheritance anomaly is part of the general fragile base class problem of object-oriented programming, that arises due to code coupling in implementation inheritance. In modern software practice, the fragile base class problem is circumvented by interface abstraction to avoid implementation inheritance, and opting for composition as means for reuse. We discuss concurrent programming issues with composition for reuse.},
  booktitle = {Proceedings of the 9th International Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems PLE},
  series = {ICOOOLPS '14},
  publisher = {{ACM}},
  urldate = {2016-02-19},
  date = {2014},
  pages = {6:1--6:12},
  keywords = {object-oriented programming,concurrent programming,inheritance anomaly},
  author = {Gramoli, Vincent and Santosa, Andrew E.},
  file = {/Users/pgiarrusso/Zotero/storage/3HZKH4UP/Gramoli_Santosa - 2014 - Why Inheritance Anomaly is Not Worth Solving.pdf}
}

@article{Hyland2007combining,
  title = {Combining algebraic effects with continuations},
  volume = {375},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397506009157},
  doi = {10.1016/j.tcs.2006.12.026},
  abstract = {We consider the natural combinations of algebraic computational effects such as side-effects, exceptions, interactive input/output, and nondeterminism with continuations. Continuations are not an algebraic effect, but previously developed combinations of algebraic effects given by sum and tensor extend, with effort, to include commonly used combinations of the various algebraic effects with continuations. Continuations also give rise to a third sort of combination, that given by applying the continuations monad transformer to an algebraic effect. We investigate the extent to which sum and tensor extend from algebraic effects to arbitrary monads, and the extent to which Felleisen et~al.’s C operator extends from continuations to its combination with algebraic effects. To do all this, we use Dubuc’s characterisation of strong monads in terms of enriched large Lawvere theories.},
  number = {1–3},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Festschrift for John C. Reynolds’s 70th birthday},
  urldate = {2016-02-18},
  date = {2007-05-01},
  pages = {20-40},
  keywords = {Monad,modularity,Computational effect,Lawvere theory},
  author = {Hyland, Martin and Levy, Paul Blain and Plotkin, Gordon and Power, John},
  file = {/Users/pgiarrusso/Zotero/storage/B8NZ2MVM/Hyland et al - 2007 - Combining algebraic effects with continuations.pdf;/Users/pgiarrusso/Zotero/storage/JM64AIXM/S0304397506009157.html}
}

@inproceedings{Danielsson2006fast,
  location = {{New York, NY, USA}},
  title = {Fast and Loose Reasoning is Morally Correct},
  isbn = {1-59593-027-2},
  url = {http://doi.acm.org/10.1145/1111037.1111056},
  doi = {10.1145/1111037.1111056},
  abstract = {Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.},
  booktitle = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '06},
  publisher = {{ACM}},
  urldate = {2016-02-17},
  date = {2006},
  pages = {206--217},
  keywords = {equational reasoning,inductive and coinductive types,lifted types,non-strict and strict languages,partial and infinite values,partial and total languages},
  author = {Danielsson, Nils Anders and Hughes, John and Jansson, Patrik and Gibbons, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/T6WQ7DR9/Danielsson et al - 2006 - Fast and Loose Reasoning is Morally Correct.pdf}
}
% == BibLateX quality report for Danielsson2006fast:
% ? Unsure about the formatting of the booktitle

@article{Klin2004adding,
  title = {Adding recursive constructs to bialgebraic semantics},
  volume = {60–61},
  issn = {1567-8326},
  url = {http://www.sciencedirect.com/science/article/pii/S156783260400030X},
  doi = {10.1016/j.jlap.2004.03.005},
  abstract = {This paper aims at fitting a general class of recursive equations into the framework of `well-behaved' structural operational semantics, formalized as bialgebraic semantics by Turi and Plotkin. Rather than interpreting recursive constructs by means of operational rules, separate recursive equations are added to semantic descriptions of languages. The equations, together with the remaining rules, are then interpreted in a suitable category and merged by means of certain fixpoint constructions. For a class of recursive equations called regular unfolding rules, this construction yields distributive laws as analyzed by Turi and Plotkin. This means that regular unfolding rules can be merged seamlessly with abstract operational rules.},
  journaltitle = {The Journal of Logic and Algebraic Programming},
  shortjournal = {The Journal of Logic and Algebraic Programming},
  series = {Structural Operational Semantics},
  urldate = {2016-02-17},
  date = {2004-07},
  pages = {259-286},
  author = {Klin, Bartek},
  file = {/Users/pgiarrusso/Zotero/storage/FV3G9BK4/Klin - 2004 - Adding recursive constructs to bialgebraic semantics.pdf;/Users/pgiarrusso/Zotero/storage/MWENX5FH/S156783260400030X.html}
}

@article{Jaskelioff2011modularity,
  title = {Modularity and Implementation of Mathematical Operational Semantics},
  volume = {229},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066111000545},
  doi = {10.1016/j.entcs.2011.02.017},
  abstract = {Structural operational semantics is a popular technique for specifying the meaning of programs by means of inductive clauses. One seeks syntactic restrictions on those clauses so that the resulting operational semantics is well-behaved. This approach is simple and concrete but it has some drawbacks. Turi pioneered a more abstract categorical treatment based upon the idea that operational semantics is essentially a distribution of syntax over behaviour. In this article we take Turiʼs approach in two new directions. Firstly, we show how to write operational semantics as modular components and how to combine such components to specify complete languages. Secondly, we show how the categorical nature of Turiʼs operational semantics makes it ideal for implementation in a functional programming language such as Haskell.},
  number = {5},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the Second Workshop on Mathematically Structured Functional Programming (MSFP 2008)},
  urldate = {2016-02-14},
  date = {2011-03-08},
  pages = {75-95},
  keywords = {Haskell,modularity,category theory,operational semantics},
  author = {Jaskelioff, Mauro and Ghani, Neil and Hutton, Graham},
  file = {/Users/pgiarrusso/Zotero/storage/XN2RK993/Jaskelioff et al - 2011 - Modularity and Implementation of Mathematical Operational Semantics.pdf;/Users/pgiarrusso/Zotero/storage/VGA73BHA/S1571066111000545.html}
}

@inproceedings{Duggan1996mixin,
  location = {{New York, NY, USA}},
  title = {Mixin Modules},
  isbn = {0-89791-770-7},
  url = {http://doi.acm.org/10.1145/232627.232654},
  doi = {10.1145/232627.232654},
  abstract = {Mixin modules are proposed as a new construct for module languages, allowing recursive definitions to span module boundaries. Mixin modules are proposed specifically for the Standard ML language. Several applications are described, including the resolution of cycles in module import dependency graphs, as well as functionality related to Haskell type classes and CLOS generic functions, though without any complications to the core language semantics. Mixin modules require no changes to the core ML type system, and only a very minor change to its run-time semantics. A type system and reduction semantics are provided, and the former is verified to be sound relative to the latter.},
  booktitle = {Proceedings of the First ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '96},
  publisher = {{ACM}},
  urldate = {2016-02-12},
  date = {1996},
  pages = {262--273},
  author = {Duggan, Dominic and Sourelis, Constantinos},
  file = {/Users/pgiarrusso/Zotero/storage/THIGSAFU/Duggan_Sourelis - 1996 - Mixin Modules.pdf}
}

@article{Rompf2015f,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.05216},
  primaryClass = {cs},
  title = {From F to DOT: Type Soundness Proofs with Definitional Interpreters},
  url = {http://arxiv.org/abs/1510.05216},
  shorttitle = {From F to DOT},
  abstract = {Scala's type system unifies ML modules, object-oriented, and functional programming. The Dependent Object Types (DOT) family of calculi has been proposed as a new foundation for Scala and similar languages. Unfortunately, it is not clear how DOT relates to any well-known type systems, and type soundness has only been established for very restricted subsets. In fact, important Scala features are known to break at least one key metatheoretic property such as environment narrowing or subtyping transitivity, which are usually required for a type soundness proof. First, and, perhaps surprisingly, we show how rich DOT calculi can still be proved sound. The key insight is that narrowing and subtyping transitivity only need to hold for runtime objects, but not for code that is never executed. Alas, the dominant method of proving type soundness, Wright and Felleisen's syntactic approach, is based on term rewriting, which does not a priori make a distinction between runtime and type assignment time. Second, we demonstrate how type soundness can be proved for advanced, polymorphic, type systems with respect to high-level, definitional interpreters, implemented in Coq. We present the first mechanized soundness proof in this style for System F$<$: and several extensions, including mutable references. Our proofs use only simple induction: another surprising result, as the combination of big-step semantics, mutable references, and polymorphism is commonly believed to require co-inductive proof techniques. Third, we show how DOT-like calculi emerge as generalizations of F$<$:, exposing a rich design space of calculi with path-dependent types which we collectively call System D. Armed with insights from the definitional interpreter semantics, we also show how equivalent small-step semantics and soundness proofs in Wright-Felleisen-style can be derived for these systems.},
  urldate = {2016-02-09},
  date = {2015-10-18},
  keywords = {Computer Science - Programming Languages,_tablet},
  author = {Rompf, Tiark and Amin, Nada},
  file = {/Users/pgiarrusso/Zotero/storage/HC2XP57Z/Rompf-Amin - 2015 - From F to DOT - Type Soundness Proofs with Definitional Interpreters.pdf;/Users/pgiarrusso/Zotero/storage/I7SKJXB7/Rompf-Amin - 2015 - From F to DOT - Type Soundness Proofs with Definitional Interpreters.pdf;/Users/pgiarrusso/Zotero/storage/MZJIMPND/1510.html;/Users/pgiarrusso/Zotero/storage/UVVVBW64/1510.html}
}
% == BibLateX quality report for Rompf2015f:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Polakow2015embedding,
  location = {{New York, NY, USA}},
  title = {Embedding a Full Linear Lambda Calculus in Haskell},
  isbn = {978-1-4503-3808-0},
  url = {http://doi.acm.org/10.1145/2804302.2804309},
  doi = {10.1145/2804302.2804309},
  abstract = {We present an encoding of full linear lambda calculus in Haskell using higher order abstract syntax. By making use of promoted data kinds, multi-parameter type classes and functional dependencies, the encoding allows Haskell to do both linear type checking and linear type inference.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '15},
  publisher = {{ACM}},
  urldate = {2016-02-12},
  date = {2015},
  pages = {177--188},
  keywords = {higher-order abstract syntax,domain specific language,linear lambda calculus},
  author = {Polakow, Jeff},
  file = {/Users/pgiarrusso/Zotero/storage/6RAPGJE4/Polakow - 2015 - Embedding a Full Linear Lambda Calculus in Haskell.pdf}
}

@inproceedings{Castagna2015polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Functions with Set-Theoretic Types: Part 2: Local Type Inference and Type Reconstruction},
  isbn = {978-1-4503-3300-9},
  url = {http://doi.acm.org/10.1145/2676726.2676991},
  doi = {10.1145/2676726.2676991},
  shorttitle = {Polymorphic Functions with Set-Theoretic Types},
  abstract = {This article is the second part of a two articles series about the definition of higher order polymorphic functions in a type system with recursive types and set-theoretic type connectives (unions, intersections, and negations). In the first part, presented in a companion paper, we defined and studied the syntax, semantics, and evaluation of the explicitly-typed version of a calculus, in which type instantiation is driven by explicit instantiation annotations. In this second part we present a local type inference system that allows the programmer to omit explicit instantiation annotations for function applications, and a type reconstruction system that allows the programmer to omit explicit type annotations for function definitions. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages with union and intersection types and/or for semi-structured data processing.},
  booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '15},
  publisher = {{ACM}},
  urldate = {2016-02-12},
  date = {2015},
  pages = {289--302},
  keywords = {intersection types,polymorphism,XML,types,semantic subtyping,type constraints},
  author = {Castagna, Giuseppe and Nguyen, Kim and Xu, Zhiwu and Abate, Pietro},
  file = {/Users/pgiarrusso/Zotero/storage/QMMDW3VH/Castagna et al - 2015 - Polymorphic Functions with Set-Theoretic Types - Part 2 - Local Type Inference and Type Reconstruction.pdf}
}

@inproceedings{Castagna2014polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Functions with Set-theoretic Types: Part 1: Syntax, Semantics, and Evaluation},
  isbn = {978-1-4503-2544-8},
  url = {http://doi.acm.org/10.1145/2535838.2535840},
  doi = {10.1145/2535838.2535840},
  shorttitle = {Polymorphic Functions with Set-theoretic Types},
  abstract = {This article is the first part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed lambda-calculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data.},
  booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '14},
  publisher = {{ACM}},
  urldate = {2016-02-12},
  date = {2014},
  pages = {5--17},
  keywords = {intersection types,polymorphism,XML,types},
  author = {Castagna, Giuseppe and Nguyen, Kim and Xu, Zhiwu and Im, Hyeonseung and Lenglet, Sergueï and Padovani, Luca},
  file = {/Users/pgiarrusso/Zotero/storage/6EAD4G94/Castagna et al - 2014 - Polymorphic Functions with Set-theoretic Types - Part 1 - Syntax, Semantics, and Evaluation.pdf}
}

@incollection{Watkins2003concurrent,
  langid = {english},
  title = {A Concurrent Logical Framework: The Propositional Fragment},
  isbn = {978-3-540-22164-7 978-3-540-24849-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-24849-1_23},
  shorttitle = {A Concurrent Logical Framework},
  abstract = {We present the propositional fragment CLF0 of the Concurrent Logical Framework (CLF). CLF extends the Linear Logical Framework to allow the natural representation of concurrent computations in an object language. The underlying type theory uses monadic types to segregate values from computations. This separation leads to a tractable notion of definitional equality that identifies computations differing only in the order of execution of independent steps. From a logical point of view our type theory can be seen as a novel combination of lax logic and dual intuitionistic linear logic. An encoding of a small Petri net exemplifies the representation methodology, which can be summarized as “concurrent computations as monadic expressions”.},
  number = {3085},
  booktitle = {Types for Proofs and Programs},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-02-11},
  date = {2003-04-30},
  pages = {355-377},
  keywords = {Programming Languages; Compilers; Interpreters,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Watkins, Kevin and Cervesato, Iliano and Pfenning, Frank and Walker, David},
  editor = {Berardi, Stefano and Coppo, Mario and Damiani, Ferruccio},
  file = {/Users/pgiarrusso/Zotero/storage/TDP4Q3WM/Watkins et al - 2003 - A Concurrent Logical Framework - The Propositional Fragment.pdf;/Users/pgiarrusso/Zotero/storage/R3CHS7HT/978-3-540-24849-1_23.html},
  doi = {10.1007/978-3-540-24849-1_23}
}
% == BibLateX quality report for Watkins2003concurrent:
% 'isbn': not a valid ISBN

@inproceedings{Ploeg2014reflection,
  location = {{New York, NY, USA}},
  title = {Reflection Without Remorse: Revealing a Hidden Sequence to Speed Up Monadic Reflection},
  isbn = {978-1-4503-3041-1},
  url = {http://doi.acm.org/10.1145/2633357.2633360},
  doi = {10.1145/2633357.2633360},
  shorttitle = {Reflection Without Remorse},
  abstract = {A series of list appends or monadic binds for many monads performs algorithmically worse when left-associated. Continuation-passing style (CPS) is well-known to cure this severe dependence of performance on the association pattern. The advantage of CPS dwindles or disappears if we have to examine or modify the intermediate result of a series of appends or binds, before continuing the series. Such examination is frequently needed, for example, to control search in non-determinism monads. We present an alternative approach that is just as general as CPS but more robust: it makes series of binds and other such operations efficient regardless of the association pattern-- and also provides efficient access to intermediate results. The key is to represent such a conceptual sequence as an efficient sequence data structure. Efficient sequence data structures from the literature are homogeneous and cannot be applied as they are in a type-safe way to series of monadic binds. We generalize them to type aligned sequences and show how to construct their (assuredly order-preserving) implementations. We demonstrate that our solution solves previously undocumented, severe performance problems in iteratees, LogicT transformers, free monads and extensible effects.},
  booktitle = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '14},
  publisher = {{ACM}},
  urldate = {2016-02-10},
  date = {2014},
  pages = {133--144},
  keywords = {Data Structures,Monads,performance,reflection},
  author = {van der Ploeg, Atze and Kiselyov, Oleg},
  file = {/Users/pgiarrusso/Zotero/storage/TZK3PHD4/Ploeg_Kiselyov - 2014 - Reflection Without Remorse - Revealing a Hidden Sequence to Speed Up Monadic Reflection.pdf}
}

@incollection{Wu2015fusion,
  langid = {english},
  title = {Fusion for Free},
  isbn = {978-3-319-19796-8 978-3-319-19797-5},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-19797-5_15},
  abstract = {Algebraic effect handlers are a recently popular approach for modelling side-effects that separates the syntax and semantics of effectful operations. The shape of syntax is captured by functors, and free monads over these functors denote syntax trees. The semantics is captured by algebras, and effect handlers pass these over the syntax trees to interpret them into a semantic domain. This approach is inherently modular: different functors can be composed to make trees with richer structure. Such trees are interpreted by applying several handlers in sequence, each removing the syntactic constructs it recognizes. Unfortunately, the construction and traversal of intermediate trees is painfully inefficient and has hindered the adoption of the handler approach. This paper explains how a sequence of handlers can be fused into one, so that multiple tree traversals can be reduced to a single one and no intermediate trees need to be allocated. At the heart of this optimization is keeping the notion of a free monad abstract, thus enabling a change of representation that opens up the possibility of fusion. We demonstrate how the ensuing code can be inlined at compile time to produce efficient handlers.},
  number = {9129},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2016-02-10},
  date = {2015-06-29},
  pages = {302-322},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Discrete Mathematics in Computer Science,Math Applications in Computer Science},
  author = {Wu, Nicolas and Schrijvers, Tom},
  editor = {Hinze, Ralf and Voigtländer, Janis},
  file = {/Users/pgiarrusso/Zotero/storage/ZP86CP26/Wu_Schrijvers - 2015 - Fusion for Free.pdf;/Users/pgiarrusso/Zotero/storage/DE2BVPTN/978-3-319-19797-5_15.html},
  doi = {10.1007/978-3-319-19797-5_15}
}
% == BibLateX quality report for Wu2015fusion:
% 'isbn': not a valid ISBN

@inproceedings{Blanchette2015foundational,
  location = {{New York, NY, USA}},
  title = {Foundational Extensible Corecursion: A Proof Assistant Perspective},
  isbn = {978-1-4503-3669-7},
  url = {http://doi.acm.org/10.1145/2784731.2784732},
  doi = {10.1145/2784731.2784732},
  shorttitle = {Foundational Extensible Corecursion},
  abstract = {This paper presents a formalized framework for defining corecursive functions safely in a total setting, based on corecursion up-to and relational parametricity. The end product is a general corecursor that allows corecursive (and even recursive) calls under "friendly" operations, including constructors. Friendly corecursive functions can be registered as such, thereby increasing the corecursor's expressiveness. The metatheory is formalized in the Isabelle proof assistant and forms the core of a prototype tool. The corecursor is derived from first principles, without requiring new axioms or extensions of the logic.},
  booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2015},
  publisher = {{ACM}},
  urldate = {2015-09-09},
  date = {2015},
  pages = {192--204},
  keywords = {parametricity,proof assistants,higher-order logic,(Co)recursion,Isabelle},
  author = {Blanchette, Jasmin Christian and Popescu, Andrei and Traytel, Dmitriy},
  file = {/Users/pgiarrusso/Zotero/storage/4JHP47Q8/Blanchette et al - 2015 - Foundational Extensible Corecursion - A Proof Assistant Perspective.pdf}
}

@inproceedings{Hinze2010reason,
  location = {{New York, NY, USA}},
  title = {Reason Isomorphically!},
  isbn = {978-1-4503-0251-7},
  url = {http://doi.acm.org/10.1145/1863495.1863507},
  doi = {10.1145/1863495.1863507},
  abstract = {When are two types the same? In this paper we argue that isomorphism is a more useful notion than equality. We explain a succinct and elegant approach to establishing isomorphisms, with our focus on showing their existence over deriving the witnesses. We use category theory as a framework, but rather than chasing diagrams or arguing with arrows, we present our proofs in a calculational style. In particular, we hope to showcase to the reader why the Yoneda lemma and adjunctions should be in their reasoning toolbox.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '10},
  publisher = {{ACM}},
  urldate = {2016-02-08},
  date = {2010},
  pages = {85--96},
  keywords = {category theory,adjunctions,isomorphism,yoneda lemma},
  author = {Hinze, Ralf and James, Daniel W.H.},
  file = {/Users/pgiarrusso/Zotero/storage/7SI6TCMG/Hinze_James - 2010 - Reason Isomorphically!.pdf}
}

@article{Leroy1990zinc,
  title = {The ZINC experiment: an economical implementation of the ML language},
  abstract = {This report details the design and implementation of the ZINC system. This is an experimental implementation of the ML language, which has later evolved in the Caml Light system. This system is strongly oriented toward separate compilation and the production of small, standalone programs; type safety is ensured by a Modula-2-like module system. ZINC uses simple, portable techniques, such as bytecode interpretation; a sophisticated execution model helps counterbalance the interpretation overhead.},
  number = {117},
  date = {1990},
  author = {Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/56VVES3R/Leroy - 1990 - The ZINC experiment - an economical implementation of the ML language.pdf},
  urllocal = {http://gallium.inria.fr/ xleroy/publi/ZINC.pdf},
  xtopic = {caml}
}
% == BibLateX quality report for Leroy1990zinc:
% Unexpected field 'urllocal'
% Unexpected field 'xtopic'
% Missing required field 'journaltitle'

@article{Barrett2016virtual,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.00602},
  primaryClass = {cs},
  title = {Virtual Machine Warmup Blows Hot and Cold},
  url = {http://arxiv.org/abs/1602.00602},
  abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: first the warmup phase determines which parts of a program would most benefit from dynamic compilation; after compilation has occurred the program is said to be at peak performance. When measuring the performance of JIT compiling VMs, data collected during the warmup phase is generally discarded, placing the focus on peak performance. In this paper we run a number of small, deterministic benchmarks on a variety of well known VMs. In our experiment, less than one quarter of the benchmark/VM pairs conform to the traditional notion of warmup, and none of the VMs we tested consistently warms up in the traditional notion. This raises a number of questions about VM benchmarking, which are of interest to both VM authors and end users.},
  urldate = {2016-02-02},
  date = {2016-02-01},
  keywords = {Computer Science - Programming Languages,D.3},
  author = {Barrett, Edd and Bolz, Carl Friedrich and Killick, Rebecca and Knight, Vincent and Mount, Sarah and Tratt, Laurence},
  file = {/Users/pgiarrusso/Zotero/storage/MWTMJIFN/Barrett et al - 2016 - Virtual Machine Warmup Blows Hot and Cold.pdf;/Users/pgiarrusso/Zotero/storage/3DHPCRCQ/1602.html}
}
% == BibLateX quality report for Barrett2016virtual:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Dybjer1994inductive,
  langid = {english},
  title = {Inductive families},
  volume = {6},
  issn = {0934-5043, 1433-299X},
  url = {http://link.springer.com/article/10.1007/BF01211308},
  doi = {10.1007/BF01211308},
  abstract = {A general formulation of inductive and recursive definitions in Martin-Löf's type theory is presented. It extends Backhouse's ‘Do-It-Yourself Type Theory’ to include inductive definitions of families of sets and definitions of functions by recursion on the way elements of such sets are generated. The formulation is in natural deduction and is intended to be a natural generalisation to type theory of Martin-Löf's theory of iterated inductive definitions in predicate logic. Formal criteria are given for correct formation and introduction rules of a new set former capturing definition by strictly positive, iterated, generalised induction. Moreover, there is an inversion principle for deriving elimination and equality rules from the formation and introduction rules. Finally, there is an alternative schematic presentation of definition by recursion. The resulting theory is a flexible and powerful language for programming and constructive mathematics. We hint at the wealth of possible applications by showing several basic examples: predicate logic, generalised induction, and a formalisation of the untyped lambda calculus.},
  number = {4},
  journaltitle = {Formal Aspects of Computing},
  shortjournal = {Formal Aspects of Computing},
  urldate = {2016-01-27},
  date = {1994-07},
  pages = {440-465},
  keywords = {Computational Mathematics and Numerical Analysis,Theory of Computation,Math Applications in Computer Science,natural deduction,Inductive definitions,Intuitionistic type theory},
  author = {Dybjer, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/QQAU89K5/Dybjer - 1994 - Inductive families.pdf;/Users/pgiarrusso/Zotero/storage/XXV4ZDAG/BF01211308.html}
}
% == BibLateX quality report for Dybjer1994inductive:
% 'issn': not a valid ISSN

@article{Bruijn1991telescopic,
  title = {Telescopic mappings in typed lambda calculus},
  volume = {91},
  abstract = {The paper develops notation for strings of abstracters in typed lambda calculus, and shows how to treat them more or less as single abstracters. 0 1991 Academic Press. Inc. 1.},
  journaltitle = {Information and Computation},
  date = {1991},
  pages = {189--204},
  author = {Bruijn, N. G. De},
  file = {/Users/pgiarrusso/Zotero/storage/CKXXVRSX/summary.html}
}

@article{Amin2016essence,
  title = {The Essence of Dependent Object Types},
  url = {http://infoscience.epfl.ch/record/215280},
  journaltitle = {WadlerFest 2016},
  urldate = {2016-01-26},
  date = {2016},
  keywords = {_tablet},
  author = {Amin, Nada and Grütter, Karl Samuel and Odersky, Martin and Rompf, Tiark and Stucki, Sandro},
  file = {/Users/pgiarrusso/Zotero/storage/FIX4UP4Q/Amin-Grütter-Odersky-Rompf-Stucki - 2016 - The Essence of Dependent Object Types.pdf;/Users/pgiarrusso/Zotero/storage/RZNHW76D/215280.html}
}

@inproceedings{Clement1986simple,
  location = {{New York, NY, USA}},
  title = {A Simple Applicative Language: Mini-ML},
  isbn = {0-89791-200-4},
  url = {http://doi.acm.org/10.1145/319838.319847},
  doi = {10.1145/319838.319847},
  shorttitle = {A Simple Applicative Language},
  booktitle = {Proceedings of the 1986 ACM Conference on LISP and Functional Programming},
  series = {LFP '86},
  publisher = {{ACM}},
  urldate = {2016-01-19},
  date = {1986},
  pages = {13--27},
  author = {Clément, Dominique and Despeyroux, Thierry and Kahn, Gilles and Despeyroux, Joëlle},
  file = {/Users/pgiarrusso/Zotero/storage/DJ3P6BCP/Clément et al - 1986 - A Simple Applicative Language - Mini-ML.pdf}
}

@article{Fegaras2015incremental,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.07846},
  primaryClass = {cs},
  title = {Incremental Query Processing on Big Data Streams},
  url = {http://arxiv.org/abs/1511.07846},
  abstract = {This paper addresses online processing for large-scale, incremental computations on a distributed stream processing engine (DSPE). Our goal is to convert any distributed batch query to an incremental DSPE program automatically. In contrast to other approaches, we derive incremental programs that return accurate results, not approximate answers, by retaining a minimal state during the query evaluation lifetime and by using incremental evaluation techniques to return an accurate snapshot answer at each time interval that depends on the current state and the latest batches of data. Our methods can handle many forms of queries, including iterative and nested queries, group-by with aggregation, and joins on one-to-many relationships. Finally, we report on a prototype implementation of our framework using MRQL running on top of Spark and we experimentally validate the effectiveness of our methods.},
  urldate = {2016-01-18},
  date = {2015-11-24},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Databases},
  author = {Fegaras, Leonidas},
  file = {/Users/pgiarrusso/Zotero/storage/8B2XQ9EH/Fegaras - 2015 - Incremental Query Processing on Big Data Streams.pdf;/Users/pgiarrusso/Zotero/storage/RF932NKH/1511.html}
}
% == BibLateX quality report for Fegaras2015incremental:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Kiselyov2015freer,
  location = {{New York, NY, USA}},
  title = {Freer Monads, More Extensible Effects},
  isbn = {978-1-4503-3808-0},
  url = {http://doi.acm.org/10.1145/2804302.2804319},
  doi = {10.1145/2804302.2804319},
  abstract = {We present a rational reconstruction of extensible effects, the recently proposed alternative to monad transformers, as the confluence of efforts to make effectful computations compose. Free monads and then extensible effects emerge from the straightforward term representation of an effectful computation, as more and more boilerplate is abstracted away. The generalization process further leads to freer monads, constructed without the Functor constraint. The continuation exposed in freer monads can then be represented as an efficient type-aligned data structure. The end result is the algorithmically efficient extensible effects library, which is not only more comprehensible but also faster than earlier implementations. As an illustration of the new library, we show three surprisingly simple applications: non-determinism with committed choice (LogicT), catching IO exceptions in the presence of other effects, and the semi-automatic management of file handles and other resources through monadic regions. We extensively use and promote the new sort of `laziness', which underlies the left Kan extension: instead of performing an operation, keep its operands and pretend it is done.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell},
  series = {Haskell '15},
  publisher = {{ACM}},
  urldate = {2016-01-18},
  date = {2015},
  pages = {94--105},
  keywords = {free monad,Kan extension,coroutine,effect handler,effect interaction,open union,type and effect system},
  author = {Kiselyov, Oleg and Ishii, Hiromi},
  file = {/Users/pgiarrusso/Zotero/storage/79J88N37/Kiselyov_Ishii - 2015 - Freer Monads, More Extensible Effects.pdf}
}

@inproceedings{Ramsey1996relocating,
  location = {{New York, NY, USA}},
  title = {Relocating Machine Instructions by Currying},
  isbn = {0-89791-795-2},
  url = {http://doi.acm.org/10.1145/231379.231429},
  doi = {10.1145/231379.231429},
  abstract = {Relocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machine-dependent object-file format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machine-dependence.The New Jersey Machine-Code Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instruction-set specification, the toolkit generates the bit-manipulating code. Instructions are described by constructors, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as "relocatable," meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machine-independent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machine-independent method of relocation.Currying rewrites a \&lambda;-term into two nested \&lambda;-terms. The standard implementation has the first \&lambda; allocate a closure and store therein its operands and a pointer to the second \&lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner \&lambda;-terms---one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second \&lambda; into the first makes many of the second \&lambda;s identical---a handful are enough for a whole instruction set. This optimization reduces the size of machine-dependent assembly and linking code by 15--20\% for the MIPS, SPARC, and PowerPC, and by about 30\% for the Pentium. It also makes the second \&lambda;s equivalent to relocating transformations named in standard object-file formats.},
  booktitle = {Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation},
  series = {PLDI '96},
  publisher = {{ACM}},
  urldate = {2016-01-18},
  date = {1996},
  pages = {226--236},
  author = {Ramsey, Norman},
  file = {/Users/pgiarrusso/Zotero/storage/F4GP7BHT/Ramsey - 1996 - Relocating Machine Instructions by Currying.pdf}
}

@inproceedings{DellaToffola2015performance,
  location = {{New York, NY, USA}},
  title = {Performance Problems You Can Fix: A Dynamic Analysis of Memoization Opportunities},
  isbn = {978-1-4503-3689-5},
  url = {http://doi.acm.org/10.1145/2814270.2814290},
  doi = {10.1145/2814270.2814290},
  shorttitle = {Performance Problems You Can Fix},
  abstract = {Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2015},
  publisher = {{ACM}},
  urldate = {2016-01-15},
  date = {2015},
  pages = {607--622},
  keywords = {memoization,caching,performance bugs,profiling},
  author = {Della Toffola, Luca and Pradel, Michael and Gross, Thomas R.},
  file = {/Users/pgiarrusso/Zotero/storage/VCPIJXDA/Della Toffola et al - 2015 - Performance Problems You Can Fix - A Dynamic Analysis of Memoization Opportunities.pdf}
}

@article{Bille2005survey,
  title = {A survey on tree edit distance and related problems},
  volume = {337},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397505000174},
  doi = {10.1016/j.tcs.2004.12.030},
  abstract = {We survey the problem of comparing labeled trees based on simple local operations of deleting, inserting, and relabeling nodes. These operations lead to the tree edit distance, alignment distance, and inclusion problem. For each problem we review the results available and present, in detail, one or more of the central algorithms for solving the problem.},
  number = {1–3},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2016-01-13},
  date = {2005-06-09},
  pages = {217-239},
  keywords = {Tree alignment,Tree edit distance,Tree inclusion,Tree matching},
  author = {Bille, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/54NZB3X3/Bille - 2005 - A survey on tree edit distance and related problems.pdf;/Users/pgiarrusso/Zotero/storage/AVC8A2P2/S0304397505000174.html}
}

@incollection{Buffenbarger1995syntactic,
  langid = {english},
  title = {Syntactic software merging},
  isbn = {978-3-540-60578-2 978-3-540-47768-6},
  url = {http://link.springer.com/chapter/10.1007/3-540-60578-9_14},
  abstract = {Software merging is the process of combining multiple existing versions of a source file, to produce a new version. Typically, the goal is for the new version to implement some kind of union of the features implemented by the existing versions. A variety of merge tools are available, but software merging is still a tedious process, and mistakes are easy to make. This paper describes the fundamentals of merging, surveys the known methods of software merging, including a method based on programming-language syntax, and discusses a set of tools that perform syntactic merging.},
  number = {1005},
  booktitle = {Software Configuration Management},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-01-13},
  date = {1995},
  pages = {153-172},
  keywords = {Software Engineering,Software Engineering/Programming and Operating Systems,IT in Business},
  author = {Buffenbarger, Jim},
  editor = {Estublier, Jacky},
  file = {/Users/pgiarrusso/Zotero/storage/BJAJEHXT/3-540-60578-9_14.html},
  doi = {10.1007/3-540-60578-9_14}
}
% == BibLateX quality report for Buffenbarger1995syntactic:
% 'isbn': not a valid ISBN

@inproceedings{Apel2011semistructured,
  location = {{New York, NY, USA}},
  title = {Semistructured Merge: Rethinking Merge in Revision Control Systems},
  isbn = {978-1-4503-0443-6},
  url = {http://doi.acm.org/10.1145/2025113.2025141},
  doi = {10.1145/2025113.2025141},
  shorttitle = {Semistructured Merge},
  abstract = {An ongoing problem in revision control systems is how to resolve conflicts in a merge of independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems that inherit the strengths of both: the generality of unstructured systems and the expressiveness of structured systems. The idea is to provide structural information of the underlying software artifacts --- declaratively, in the form of annotated grammars. This way, a wide variety of languages can be supported and the information provided can assist in the automatic resolution of two classes of conflicts: ordering conflicts and semantic conflicts. The former can be resolved independently of the language and the latter using specific conflict handlers. We have been developing a tool that supports semistructured merge and conducted an empirical study on 24 software projects developed in Java, C\#, and Python comprising 180 merge scenarios. We found that semistructured merge reduces the number of conflicts in 60\% of the sample merge scenarios by, on average, 34\%, compared to unstructured merge. We found also that renaming is challenging in that it can increase the number of conflicts during semistructured merge, and that a combination of unstructured and semistructured merge is a pragmatic way to go.},
  booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
  series = {ESEC/FSE '11},
  publisher = {{ACM}},
  urldate = {2016-01-13},
  date = {2011},
  pages = {190--200},
  keywords = {featurehouse,fstmerge,revision control,semistructured merge,software merging,version control},
  author = {Apel, Sven and Liebig, Jörg and Brandl, Benjamin and Lengauer, Christian and Kästner, Christian},
  file = {/Users/pgiarrusso/Zotero/storage/GGG36NTW/Apel et al - 2011 - Semistructured Merge - Rethinking Merge in Revision Control Systems.pdf}
}

@article{Mens2002stateoftheart,
  title = {A state-of-the-art survey on software merging},
  volume = {28},
  issn = {0098-5589},
  doi = {10.1109/TSE.2002.1000449},
  abstract = {Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions},
  number = {5},
  journaltitle = {IEEE Transactions on Software Engineering},
  date = {2002-05},
  pages = {449-462},
  keywords = {software merging,configuration management,conflict detection,conflict resolution,large-scale software systems,merge approaches,merging,software life-cycle,software maintenance,textual merging},
  author = {Mens, T.},
  file = {/Users/pgiarrusso/Zotero/storage/666RSWXE/Mens - 2002 - A state-of-the-art survey on software merging.pdf;/Users/pgiarrusso/Zotero/storage/5X25MIUA/login.html}
}

@inproceedings{Jackson1994semantic,
  title = {Semantic Diff: a tool for summarizing the effects of modifications},
  doi = {10.1109/ICSM.1994.336770},
  shorttitle = {Semantic Diff},
  abstract = {Describes a tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them. Unlike existing tools based on comparison of program dependence graphs, our tool expresses its results in terms of the observable input-output behaviour of the procedure, rather than its syntactic structure. And because the analysis is truly semantic, it requires no prior matching of syntactic components, and generates fewer spurious differences, so that meaning-preserving transformations (such as renaming local variables) are correctly determined to have no visible effect. A preliminary experiment on modifications applied to the code of a large real-time system suggests that the approach is practical},
  eventtitle = {, International Conference on Software Maintenance, 1994. Proceedings},
  booktitle = {, International Conference on Software Maintenance, 1994. Proceedings},
  date = {1994-09},
  pages = {243-252},
  keywords = {configuration management,input-output programs,local variable renaming,meaning-preserving transformations,observable input-output behaviour,procedure versions,program dependence graphs,program diagnostics,real-time system,real-time systems,report generation,Semantic Diff,semantic differences,Software fault diagnosis,software modification effects summarization,software tools,spurious differences,syntactic components},
  author = {Jackson, D. and Ladd, D.A.},
  file = {/Users/pgiarrusso/Zotero/storage/2CVS9B2S/Jackson_Ladd - 1994 - Semantic Diff - a tool for summarizing the effects of modifications.pdf;/Users/pgiarrusso/Zotero/storage/239ZDEZP/login.html}
}
% == BibLateX quality report for Jackson1994semantic:
% ? Unsure about the formatting of the booktitle

@incollection{Lahiri2012symdiff,
  langid = {english},
  title = {SYMDIFF: A Language-Agnostic Semantic Diff Tool for Imperative Programs},
  isbn = {978-3-642-31423-0 978-3-642-31424-7},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-31424-7_54},
  shorttitle = {SYMDIFF},
  abstract = {In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C\# and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs.},
  number = {7358},
  booktitle = {Computer Aided Verification},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-01-13},
  date = {2012-07-07},
  pages = {712-717},
  keywords = {Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Computer Hardware,Special Purpose and Application-Based Systems},
  author = {Lahiri, Shuvendu K. and Hawblitzel, Chris and Kawaguchi, Ming and Rebêlo, Henrique},
  editor = {Madhusudan, P. and Seshia, Sanjit A.},
  file = {/Users/pgiarrusso/Zotero/storage/EBRW8D76/Lahiri et al - 2012 - SYMDIFF - A Language-Agnostic Semantic Diff Tool for Imperative Programs.pdf;/Users/pgiarrusso/Zotero/storage/PZJX4GAK/978-3-642-31424-7_54.html},
  doi = {10.1007/978-3-642-31424-7_54}
}
% == BibLateX quality report for Lahiri2012symdiff:
% 'isbn': not a valid ISBN

@inproceedings{Frandsen2006singular,
  location = {{New York, NY, USA}},
  title = {A Singular Choice for Multiple Choice},
  isbn = {1-59593-603-3},
  url = {http://doi.acm.org/10.1145/1189215.1189164},
  doi = {10.1145/1189215.1189164},
  abstract = {How should multiple choice tests be scored and graded, in particular when students are allowed to check several boxes to convey partial knowledge? Many strategies may seem reasonable, but we demonstrate that five self-evident axioms are sufficient to determine completely the correct strategy. We also discuss how to measure robustness of the obtained grades. Our results have practical advantages and also suggest criteria for designing multiple choice questions.},
  booktitle = {Working Group Reports on ITiCSE on Innovation and Technology in Computer Science Education},
  series = {ITiCSE-WGR '06},
  publisher = {{ACM}},
  urldate = {2016-01-12},
  date = {2006},
  pages = {34--38},
  keywords = {grading scales,multiple choice,scoring strategies,theory},
  author = {Frandsen, Gudmund S. and Schwartzbach, Michael I.},
  file = {/Users/pgiarrusso/Zotero/storage/3PNS3QWC/Frandsen_Schwartzbach - 2006 - A Singular Choice for Multiple Choice.pdf}
}
% == BibLateX quality report for Frandsen2006singular:
% ? Unsure about the formatting of the booktitle

@inproceedings{Hackett2012fast,
  location = {{New York, NY, USA}},
  title = {Fast and Precise Hybrid Type Inference for JavaScript},
  isbn = {978-1-4503-1205-9},
  url = {http://doi.acm.org/10.1145/2254064.2254094},
  doi = {10.1145/2254064.2254094},
  abstract = {JavaScript performance is often bound by its dynamically typed nature. Compilers do not have access to static type information, making generation of efficient, type-specialized machine code difficult. We seek to solve this problem by inferring types. In this paper we present a hybrid type inference algorithm for JavaScript based on points-to analysis. Our algorithm is fast, in that it pays for itself in the optimizations it enables. Our algorithm is also precise, generating information that closely reflects the program's actual behavior even when analyzing polymorphic code, by augmenting static analysis with run-time type barriers. We showcase an implementation for Mozilla Firefox's JavaScript engine, demonstrating both performance gains and viability. Through integration with the just-in-time (JIT) compiler in Firefox, we have improved performance on major benchmarks and JavaScript-heavy websites by up to 50\%. Inference-enabled compilation is the default compilation mode as of Firefox 9.},
  booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '12},
  publisher = {{ACM}},
  urldate = {2015-12-22},
  date = {2012},
  pages = {239--250},
  keywords = {type inference,hybrid,just-in-time compilation},
  author = {Hackett, Brian and Guo, Shu-yu}
}

@article{Avigad2004forcing,
  title = {Forcing in Proof Theory},
  volume = {10},
  issn = {1943-5894},
  url = {http://journals.cambridge.org/article_S1079898600003772},
  doi = {10.2178/bsl/1102022660},
  abstract = {Paul Cohen's method of forcing, together with Saul Kripke's related semantics for modal and intuitionistic logic, has had profound effects on a number of branches of mathematical logic, from set theory and model theory to constructive and categorical logic. Here, I argue that forcing also has a place in traditional Hilbert-style proof theory, where the goal is to formalize portions of ordinary mathematics in restricted axiomatic theories, and study those theories in constructive or syntactic terms. I will discuss the aspects of forcing that are useful in this respect, and some sample applications. The latter include ways of obtaining conservation results for classical and intuitionistic theories, interpreting classical theories in constructive ones, and constructivizing model-theoretic arguments.},
  number = {03},
  journaltitle = {Bulletin of Symbolic Logic},
  urldate = {2015-12-16},
  date = {2004-09},
  pages = {305--333},
  author = {Avigad, Jeremy},
  file = {/Users/pgiarrusso/Zotero/storage/JE666CRD/Avigad - 2004 - Forcing in Proof Theory.pdf;/Users/pgiarrusso/Zotero/storage/BBPHAWEJ/displayAbstract.html}
}

@inproceedings{Ramsey2001algebraic,
  location = {{New York, NY, USA}},
  title = {An Algebraic Approach to File Synchronization},
  isbn = {1-58113-390-1},
  url = {http://doi.acm.org/10.1145/503209.503233},
  doi = {10.1145/503209.503233},
  abstract = {A file synchronizer restores consistency after multiple replicas of a filesystem have been changed independently. We present an algebra for reasoning about operations on filesystems and show that it is sound and complete with respect to a simple model. The algebra enables us to specify a file-synchronization algorithm that can be combined with several different conflict-resolution policies. By contrast, previous work builds the conflict-resolution policy into the specification, or worse, does not specify the synchronizer's behavior precisely. We classify synchronizers by asking whether conflicts can be resolved at a single disconnected replica and whether all replicas are identical after synchronization. We also discuss timestamps and argue that there is no good way to propagate timestamps when there is severe clock skew between replicas.},
  booktitle = {Proceedings of the 8th European Software Engineering Conference Held Jointly with 9th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  series = {ESEC/FSE-9},
  publisher = {{ACM}},
  urldate = {2015-12-15},
  date = {2001},
  pages = {175--185},
  author = {Ramsey, Norman and Csirmaz, El\$\^”\$od},
  file = {/Users/pgiarrusso/Zotero/storage/MW42U4MZ/Ramsey_Csirmaz - 2001 - An Algebraic Approach to File Synchronization.pdf}
}

@incollection{St-Amour2012typing,
  langid = {english},
  title = {Typing the Numeric Tower},
  isbn = {978-3-642-27693-4 978-3-642-27694-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-27694-1_21},
  abstract = {In the past, the creators of numerical programs had to choose between simple expression of mathematical formulas and static type checking. While the Lisp family and its dynamically typed relatives support the straightforward expression via a rich numeric tower, existing statically typed languages force programmers to pollute textbook formulas with explicit coercions or unwieldy notation. In this paper, we demonstrate how the type system of Typed Racket accommodates both a textbook programming style and expressive static checking. The type system provides a hierarchy of numeric types that can be freely mixed as well as precise specifications of sign, representation, and range information—all while supporting generic operations. In addition, the type system provides information to the compiler so that it can perform standard numeric optimizations.},
  number = {7149},
  booktitle = {Practical Aspects of Declarative Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2015-12-11},
  date = {2012-01-23},
  pages = {289-303},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {St-Amour, Vincent and Tobin-Hochstadt, Sam and Flatt, Matthew and Felleisen, Matthias},
  editor = {Russo, Claudio and Zhou, Neng-Fa},
  file = {/Users/pgiarrusso/Zotero/storage/3M9SWP6V/St-Amour et al - 2012 - Typing the Numeric Tower.pdf;/Users/pgiarrusso/Zotero/storage/IID54ZHI/978-3-642-27694-1_21.html},
  doi = {10.1007/978-3-642-27694-1_21}
}
% == BibLateX quality report for St-Amour2012typing:
% 'isbn': not a valid ISBN

@article{Kent2015occurrence,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.07033},
  primaryClass = {cs},
  title = {Occurrence Typing Modulo Theories},
  url = {http://arxiv.org/abs/1511.07033},
  abstract = {We present a new type system combining occurrence typing, previously used to type check programs in dynamically-typed languages such as Racket, JavaScript, and Ruby, with dependent refinement types. We demonstrate that the addition of refinement types allows the integration of arbitrary solver-backed reasoning about logical propositions from external theories. By building on occurrence typing, we can add our enriched type system as an extension of Typed Racket---adding dependency and refinement reuses the existing formalism while increasing its expressiveness. Dependent refinement types allow Typed Racket programmers to express rich type relationships, ranging from data structure invariants such as red-black tree balance to preconditions such as vector bounds. Refinements allow programmers to embed the propositions that occurrence typing in Typed Racket already reasons about into their types. Further, extending occurrence typing to refinements allows us to make the underlying formalism simpler and more powerful. In addition to presenting the design of our system, we present a formal model of the system, show how to integrate it with theories over both linear arithmetic and bitvectors, and evaluate the system in the context of the full Typed Racket implementation. Specifically, we take safe vector access as a case study, and examine all vector accesses in a 56,000 line corpus of Typed Racket programs. Our system is able to prove that 50\% of these are safe with no new annotation, and with a few annotations and modifications, we can capture close to 80\%.},
  urldate = {2015-12-10},
  date = {2015-11-22},
  keywords = {Computer Science - Programming Languages},
  author = {Kent, Andrew M. and Kempe, David and Tobin-Hochstadt, Sam},
  file = {/Users/pgiarrusso/Zotero/storage/GEGFJJU5/Kent et al - 2015 - Occurrence Typing Modulo Theories.pdf;/Users/pgiarrusso/Zotero/storage/7DMAFX4G/1511.html}
}
% == BibLateX quality report for Kent2015occurrence:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Ramsey2002stochastic,
  location = {{New York, NY, USA}},
  title = {Stochastic Lambda Calculus and Monads of Probability Distributions},
  isbn = {1-58113-450-9},
  url = {http://doi.acm.org/10.1145/503272.503288},
  doi = {10.1145/503272.503288},
  abstract = {Probability distributions are useful for expressing the meanings of probabilistic languages, which support formal modeling of and reasoning about uncertainty. Probability distributions form a monad, and the monadic definition leads to a simple, natural semantics for a stochastic lambda calculus, as well as simple, clean implementations of common queries. But the monadic implementation of the expectation query can be much less efficient than current best practices in probabilistic modeling. We therefore present a language of measure terms, which can not only denote discrete probability distributions but can also support the best known modeling techniques. We give a translation of stochastic lambda calculus into measure terms. Whether one translates into the probability monad or into measure terms, the results of the translations denote the same probability distribution.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '02},
  publisher = {{ACM}},
  urldate = {2015-12-09},
  date = {2002},
  pages = {154--165},
  author = {Ramsey, Norman and Pfeffer, Avi},
  file = {/Users/pgiarrusso/Zotero/storage/TJ7EEI3I/Ramsey_Pfeffer - 2002 - Stochastic Lambda Calculus and Monads of Probability Distributions.pdf}
}

@inproceedings{Pottier2004polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Typed Defunctionalization},
  isbn = {1-58113-729-X},
  url = {http://doi.acm.org/10.1145/964001.964009},
  doi = {10.1145/964001.964009},
  abstract = {Defunctionalization is a program transformation that aims to turn a higher-order functional program into a first-order one, that is, to eliminate the use of functions as first-class values. Its purpose is thus identical to that of closure conversion. It differs from closure conversion, however, by storing a tag, instead of a code pointer, within every closure. Defunctionalization has been used both as a reasoning tool and as a compilation technique.Defunctionalization is commonly defined and studied in the setting of a simply-typed λ-calculus, where it is shown that semantics and well-typedness are preserved. It has been observed that, in the setting of a polymorphic type system, such as ML or System F, defunctionalization is not type-preserving. In this paper, we show that extending System F with guarded algebraic data types allows recovering type preservation. This result allows adding defunctionalization to the toolbox of type-preserving compiler writers.},
  booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '04},
  publisher = {{ACM}},
  urldate = {2015-11-26},
  date = {2004},
  pages = {89--98},
  keywords = {polymorphism,closure conversion,defunctionalization,type preservation},
  author = {Pottier, François and Gauthier, Nadji},
  file = {/Users/pgiarrusso/Zotero/storage/5WH6QFAB/Pottier_Gauthier - 2004 - Polymorphic Typed Defunctionalization.pdf}
}

@article{Shulman2008set,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0810.1279},
  primaryClass = {math},
  title = {Set theory for category theory},
  url = {http://arxiv.org/abs/0810.1279},
  abstract = {Questions of set-theoretic size play an essential role in category theory, especially the distinction between sets and proper classes (or small sets and large sets). There are many different ways to formalize this, and which choice is made can have noticeable effects on what categorical constructions are permissible. In this expository paper we summarize and compare a number of such "set-theoretic foundations for category theory," and describe their implications for the everyday use of category theory. We assume the reader has some basic knowledge of category theory, but little or no prior experience with formal logic or set theory.},
  urldate = {2015-12-08},
  date = {2008-10-07},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  author = {Shulman, Michael A.},
  file = {/Users/pgiarrusso/Zotero/storage/APVXZJQJ/Shulman - 2008 - Set theory for category theory.pdf;/Users/pgiarrusso/Zotero/storage/WMBMHU3H/0810.html}
}
% == BibLateX quality report for Shulman2008set:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Liu2015demanddriven,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.04583},
  primaryClass = {cs},
  title = {Demand-Driven Incremental Object Queries},
  url = {http://arxiv.org/abs/1511.04583},
  abstract = {Object queries are significantly easier to write, understand, and maintain than efficient low-level programs. However, a query may involve any number and combination of objects and sets, which can be arbitrarily nested and aliased. The objects and sets involved, starting from the given demand---the given parameter values of interest---can change arbitrarily. How to generate efficient implementations automatically, and furthermore to provide complexity guarantees? This paper describes such an automatic method. The method allows the queries to be written completely declaratively. It transforms demand into relations, based on the same basic idea for transforming objects and sets into relations in a prior work. Most importantly, it defines and incrementally maintains invariants for not only the query results, but also all auxiliary values about the objects and sets involved, starting from the demand. Implementation and experiments with problems from a variety of application areas, including distributed algorithms, confirm the analyzed complexities, trade-offs, and significant improvements over prior works.},
  urldate = {2015-12-01},
  date = {2015-11-14},
  keywords = {Computer Science - Programming Languages,Computer Science - Databases},
  author = {Liu, Yanhong A. and Brandvein, Jon and Stoller, Scott D. and Lin, Bo},
  file = {/Users/pgiarrusso/Zotero/storage/E5KXZ7PP/Liu-Brandvein-Stoller-Lin - 2015 - Demand-Driven Incremental Object Queries.pdf;/Users/pgiarrusso/Zotero/storage/IIBA9DHJ/1511.html}
}
% == BibLateX quality report for Liu2015demanddriven:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Oliveira2012functional,
  location = {{New York, NY, USA}},
  title = {Functional Programming with Structured Graphs},
  isbn = {978-1-4503-1054-3},
  url = {http://doi.acm.org/10.1145/2364527.2364541},
  doi = {10.1145/2364527.2364541},
  abstract = {This paper presents a new functional programming model for graph structures called structured graphs. Structured graphs extend conventional algebraic datatypes with explicit definition and manipulation of cycles and/or sharing, and offer a practical and convenient way to program graphs in functional programming languages like Haskell. The representation of sharing and cycles (edges) employs recursive binders and uses an encoding inspired by parametric higher-order abstract syntax. Unlike traditional approaches based on mutable references or node/edge lists, well-formedness of the graph structure is ensured statically and reasoning can be done with standard functional programming techniques. Since the binding structure is generic, we can define many useful generic combinators for manipulating structured graphs. We give applications and show how to reason about structured graphs.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '12},
  publisher = {{ACM}},
  urldate = {2015-11-25},
  date = {2012},
  pages = {77--88},
  keywords = {Haskell,graphs,parametric hoas},
  author = {Oliveira, Bruno C.d.S. and Cook, William R.},
  file = {/Users/pgiarrusso/Zotero/storage/NJVJXGJ8/Oliveira_Cook - 2012 - Functional Programming with Structured Graphs.pdf}
}

@inproceedings{Petri2015cooking,
  location = {{Dagstuhl, Germany}},
  title = {Cooking the Books: Formalizing JMM Implementation Recipes},
  volume = {37},
  isbn = {978-3-939897-86-6},
  url = {http://drops.dagstuhl.de/opus/volltexte/2015/5233},
  doi = {http://dx.doi.org/10.4230/LIPIcs.ECOOP.2015.445},
  booktitle = {29th European Conference on Object-Oriented Programming (ECOOP 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  date = {2015},
  pages = {445-469},
  author = {Petri, Gustavo and Vitek, Jan and Jagannathan, Suresh},
  editor = {Boyland, John Tang},
  file = {/Users/pgiarrusso/Zotero/storage/5J5XDRMP/Petri et al - 2015 - Cooking the Books - Formalizing JMM Implementation Recipes.pdf},
  urn = {urn:nbn:de:0030-drops-52334}
}
% == BibLateX quality report for Petri2015cooking:
% Unexpected field 'urn'
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Rossberg2014fing,
  title = {F-ing modules},
  volume = {24},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S0956796814000264},
  doi = {10.1017/S0956796814000264},
  abstract = {ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being “complex” and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we present a novel formalization of ML modules, which defines their semantics directly by a compositional “elaboration” translation into plain System Fω (the higher-order polymorphic λ-calculus). To demonstrate the scalability of our “F-ing” semantics, we use it to define a representative, higher-order ML-style module language, encompassing all the major features of existing ML module dialects (except for recursive modules). We thereby show that ML modules are merely a particular mode of use of System Fω. To streamline the exposition, we present the semantics of our module language in stages. We begin by defining a subset of the language supporting a Standard ML-like language with second-class modules and generative functors. We then extend this sublanguage with the ability to package modules as first-class values (a very simple extension, as it turns out) and OCaml-style applicative functors (somewhat harder). Unlike previous work combining both generative and applicative functors, we do not require two distinct forms of functor or signature sealing. Instead, whether a functor is applicative or not depends only on the computational purity of its body. In fact, we argue that applicative/generative is rather incidental terminology for pure versus impure functors. This approach results in a semantics that we feel is simpler and more natural than previous accounts, and moreover prohibits breaches of abstraction safety that were possible under them.},
  number = {05},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-11-06},
  date = {2014},
  pages = {529--607},
  author = {Rossberg, Andreas and Russo, Claudio and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/78KK5S5J/displayAbstract.html}
}

@article{Soule2015river,
  langid = {english},
  title = {River: an intermediate language for stream processing},
  issn = {1097-024X},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.2338/abstract},
  doi = {10.1002/spe.2338},
  shorttitle = {River},
  abstract = {This paper presents both a calculus for stream processing, named Brooklet, and its realization as an intermediate language, named River. Because River is based on Brooklet, it has a formal semantics that enables reasoning about the correctness of source translations and optimizations. River builds on Brooklet by addressing the real-world details that the calculus elides. We evaluated our system by implementing front-ends for three streaming languages, and three important optimizations, and a back-end for the System S distributed streaming runtime. Overall, we significantly lower the barrier to entry for new stream-processing languages and thus grow the ecosystem of this crucial style of programming. Copyright © 2015 John Wiley \& Sons, Ltd.},
  journaltitle = {Software: Practice and Experience},
  shortjournal = {Softw. Pract. Exper.},
  urldate = {2015-11-03},
  date = {2015-06-01},
  pages = {n/a-n/a},
  keywords = {optimizations,stream processing},
  author = {Soulé, Robert and Hirzel, Martin and Gedik, Buğra and Grimm, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/5HH7AARR/Soulé et al - 2015 - River - an intermediate language for stream processing.pdf;/Users/pgiarrusso/Zotero/storage/S82ENQNE/full.html}
}

@inproceedings{Scherr2015almost,
  location = {{New York, NY, USA}},
  title = {Almost First-class Language Embedding: Taming Staged Embedded DSLs},
  isbn = {978-1-4503-3687-1},
  url = {http://doi.acm.org/10.1145/2814204.2814217},
  doi = {10.1145/2814204.2814217},
  shorttitle = {Almost First-class Language Embedding},
  abstract = {Embedded domain-specific languages (EDSLs), inheriting a general-purpose language's features as well as look-and-feel, have traditionally been second-class or rather non-citizens in terms of host-language design. This makes sense when one regards them to be on the same level as traditional, non-EDSL library interfaces. However, this equivalence only applies to the simplest of EDSLs. In this paper we illustrate why this is detrimental when moving on to EDSLs that employ staging, i.e. program reification, by example of various issues that affect authors and users alike. We believe that if EDSLs are to be considered a reliable, language-like interface abstraction, they require exceptional attention and design scrutiny. Instead of unenforceable conventions, we advocate the acceptance of EDSLs as proper, i.e. almost first-class, citizens while retaining most advantages of pure embeddings. As a small step towards this goal, we present a pragmatic framework prototype for Java. It is based on annotations that explicate and document membership to explicit EDSL entities. In a nutshell, our framework identifies (annotated) method calls and field accesses as EDSL terms and dynamically constructs an abstract-syntax representation, which is eventually passed to a semantics-defining back end implemented by the EDSL author.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
  series = {GPCE 2015},
  publisher = {{ACM}},
  urldate = {2015-10-28},
  date = {2015},
  pages = {21--30},
  keywords = {program transformation,staging,java,programming languages,metaprogramming,design,embedded DSLs,implementation},
  author = {Scherr, Maximilian and Chiba, Shigeru},
  file = {/Users/pgiarrusso/Zotero/storage/H2QUMMNP/Scherr_Chiba - 2015 - Almost First-class Language Embedding - Taming Staged Embedded DSLs.pdf}
}

@inproceedings{Kuci2015incremental,
  location = {{New York, NY, USA}},
  title = {Toward Incremental Type Checking for Java},
  isbn = {978-1-4503-3722-9},
  url = {http://doi.acm.org/10.1145/2814189.2817272},
  doi = {10.1145/2814189.2817272},
  abstract = {A type system is a set of type rules and with respect to these type rules a type checker has an important role to ensure that programs exhibit a desired behavior. We consider Java type rules and extend the co-contextual formulation of type rules introduced in [1] to enable it for Java. Regarding the extension type rules result is a type, a set of context requirements and a set of class requirements. Since context and class requirements are propagated bottom-up and while traversing the syntax tree bottom-up and are merged from independent subexpression, this enables the type system to be incremental therefore the performance is increased.},
  booktitle = {Companion Proceedings of the 2015 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
  series = {SPLASH Companion 2015},
  publisher = {{ACM}},
  urldate = {2015-10-28},
  date = {2015},
  pages = {46--47},
  keywords = {type checking,java,constraints,class table,co-contextual,incremental},
  author = {Kuci, Edlira and Erdweg, Sebastian and Mezini, Mira},
  file = {/Users/pgiarrusso/Zotero/storage/6HDQUDVE/Kuci et al - 2015 - Toward Incremental Type Checking for Java.pdf}
}

@inproceedings{Afroozeh2015one,
  location = {{New York, NY, USA}},
  title = {One Parser to Rule Them All},
  isbn = {978-1-4503-3688-8},
  url = {http://doi.acm.org/10.1145/2814228.2814242},
  doi = {10.1145/2814228.2814242},
  abstract = {Despite the long history of research in parsing, constructing parsers for real programming languages remains a difficult and painful task. In the last decades, different parser generators emerged to allow the construction of parsers from a BNF-like specification. However, still today, many parsers are handwritten, or are only partly generated, and include various hacks to deal with different peculiarities in programming languages. The main problem is that current declarative syntax definition techniques are based on pure context-free grammars, while many constructs found in programming languages require context information. In this paper we propose a parsing framework that embraces context information in its core. Our framework is based on data-dependent grammars, which extend context-free grammars with arbitrary computation, variable binding and constraints. We present an implementation of our framework on top of the Generalized LL (GLL) parsing algorithm, and show how common idioms in syntax of programming languages such as (1) lexical disambiguation filters, (2) operator precedence, (3) indentation-sensitive rules, and (4) conditional preprocessor directives can be mapped to data-dependent grammars. We demonstrate the initial experience with our framework, by parsing more than 20000 Java, C\#, Haskell, and OCaml source files.},
  booktitle = {2015 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward!)},
  series = {Onward! 2015},
  publisher = {{ACM}},
  urldate = {2015-10-28},
  date = {2015},
  pages = {151--170},
  keywords = {parsing,context-aware scanning,data-dependent grammars,disambiguation,GLL,offside rule,operator precedence,preprocessor directives,scannerless parsing},
  author = {Afroozeh, Ali and Izmaylova, Anastasia},
  file = {/Users/pgiarrusso/Zotero/storage/ZCNDKQ3I/Afroozeh_Izmaylova - 2015 - One Parser to Rule Them All.pdf}
}
% == BibLateX quality report for Afroozeh2015one:
% ? Unsure about the formatting of the booktitle

@inproceedings{Amin2014foundations,
  location = {{New York, NY, USA}},
  title = {Foundations of Path-dependent Types},
  isbn = {978-1-4503-2585-1},
  url = {http://doi.acm.org/10.1145/2660193.2660216},
  doi = {10.1145/2660193.2660216},
  abstract = {A scalable programming language is one in which the same concepts can describe small as well as large parts. Towards this goal, Scala unifies concepts from object and module systems. An essential ingredient of this unification is the concept of objects with type members, which can be referenced through path-dependent types. Unfortunately, path-dependent types are not well-understood, and have been a roadblock in grounding the Scala type system on firm theory. We study several calculi for path-dependent types. We present DOT which captures the essence - DOT stands for Dependent Object Types. We explore the design space bottom-up, teasing apart inherent from accidental complexities, while fully mechanizing our models at each step. Even in this simple setting, many interesting patterns arise from the interaction of structural and nominal features. Whereas our simple calculus enjoys many desirable and intuitive properties, we demonstrate that the theory gets much more complicated once we add another Scala feature, type refinement, or extend the subtyping relation to a lattice. We discuss possible remedies and trade-offs in modeling type systems for Scala-like languages.},
  booktitle = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications},
  series = {OOPSLA '14},
  publisher = {{ACM}},
  urldate = {2015-10-27},
  date = {2014},
  pages = {233--249},
  keywords = {dependent types,Calculus,Objects},
  author = {Amin, Nada and Rompf, Tiark and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/4Q24J27D/Amin et al - 2014 - Foundations of Path-dependent Types.pdf}
}

@article{Brady2013idris,
  title = {Idris, a general-purpose dependently typed programming language: Design and implementation},
  volume = {23},
  issn = {1469-7653},
  url = {http://journals.cambridge.org/article_S095679681300018X},
  doi = {10.1017/S095679681300018X},
  shorttitle = {Idris, a general-purpose dependently typed programming language},
  abstract = {Many components of a dependently typed programming language are by now well understood, for example, the underlying type theory, type checking, unification and evaluation. How to combine these components into a realistic and usable high-level language is, however, folklore, discovered anew by successive language implementors. In this paper, I describe the implementation of Idris, a new dependently typed functional programming language. Idris is intended to be a general-purpose programming language and as such provides high-level concepts such as implicit syntax, type classes and do notation. I describe the high-level language and the underlying type theory, and present a tactic-based method for elaborating concrete high-level syntax with implicit arguments and type classes into a fully explicit type theory. Furthermore, I show how this method facilitates the implementation of new high-level language constructs.},
  number = {05},
  journaltitle = {Journal of Functional Programming},
  urldate = {2015-10-24},
  date = {2013},
  pages = {552--593},
  author = {Brady, Edwin},
  file = {/Users/pgiarrusso/Zotero/storage/7NTQESQV/Brady - 2013 - Idris, a general-purpose dependently typed programming language - Design and implementation.pdf;/Users/pgiarrusso/Zotero/storage/X6GT7FSA/displayAbstract.html}
}

@article{Blelloch2015cache,
  title = {Cache Efficient Functional Algorithms},
  volume = {58},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/2776825},
  doi = {10.1145/2776825},
  abstract = {The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size fast memory (cache) of size M, and an unbounded slow memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard Random Access Machine (RAM) model. The models, however, require specifying algorithms at a very low level, requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation. We present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bounds imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an o(n/BlogM/Bn/B) cost sorting algorithm, which is optimal in the ideal cache and I/O models.},
  number = {7},
  journaltitle = {Commun. ACM},
  urldate = {2015-10-04},
  date = {2015-06},
  pages = {101--108},
  author = {Blelloch, Guy E. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/JT5HHM65/Blelloch_Harper - 2015 - Cache Efficient Functional Algorithms.pdf}
}
% == BibLateX quality report for Blelloch2015cache:
% ? Possibly abbreviated journal title Commun. ACM

@inproceedings{Blelloch2013cache,
  location = {{New York, NY, USA}},
  title = {Cache and I/O Efficent Functional Algorithms},
  isbn = {978-1-4503-1832-7},
  url = {http://doi.acm.org/10.1145/2429069.2429077},
  doi = {10.1145/2429069.2429077},
  abstract = {The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size primary memory(cache) of size M, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard RAM model. The models, however, require specifying algorithms at a very low level requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation. In this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an O(n\_B logM/Bn\_B)cost sorting algorithm, which is optimal in the ideal cache and I/O models.},
  booktitle = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '13},
  publisher = {{ACM}},
  urldate = {2015-10-04},
  date = {2013},
  pages = {39--50},
  keywords = {cost semantics,I/O efficient algorithms},
  author = {Blelloch, Guy E. and Harper, Robert},
  file = {/Users/pgiarrusso/Zotero/storage/KMSGKDZJ/Blelloch_Harper - 2013 - Cache and I-O Efficent Functional Algorithms.pdf}
}

@inproceedings{Crary1998intensional,
  location = {{New York, NY, USA}},
  title = {Intensional Polymorphism in Type-erasure Semantics},
  isbn = {1-58113-024-4},
  url = {http://doi.acm.org/10.1145/289423.289459},
  doi = {10.1145/289423.289459},
  abstract = {Intensional polymorphism, the ability to dispatch to different routines based on types at run time, enables a variety of advanced implementation techniques for polymorphic languages, including tag-free garbage collection, unboxed function arguments, polymorphic marshalling, and flattened data structures. To date, languages that support intensional polymorphism have required a type-passing (as opposed to type-erasure) interpretation where types are constructed and passed to polymorphic functions at run time. Unfortunately, type-passing suffers from a number of drawbacks: it requires duplication of constructs at the term and type levels, it prevents abstraction, and it severely complicates polymorphic closure conversion.We present a type-theoretic framework that supports intensional polymorphism, but avoids many of the disadvantages of type passing. In our approach, run-time type information is represented by ordinary terms. This avoids the duplication problem, allows us to recover abstraction, and avoids complications with closure conversion. In addition, our type system provides another improvement in expressiveness; it allows unknown types to be refined in place thereby avoiding certain beta-expansions required by other frameworks.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2015-09-05},
  date = {1998},
  pages = {301--312},
  author = {Crary, Karl and Weirich, Stephanie and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/D2NIRBQB/Crary et al - 1998 - Intensional Polymorphism in Type-erasure Semantics.pdf}
}

@article{Jaffe1993theoretical,
  title = {“Theoretical mathematics”: toward a cultural synthesis of mathematics and theoretical physics},
  volume = {29},
  issn = {0273-0979, 1088-9485},
  url = {http://www.ams.org/bull/1993-29-01/S0273-0979-1993-00413-0/},
  doi = {10.1090/S0273-0979-1993-00413-0},
  shorttitle = {“Theoretical mathematics”},
  abstract = {Is speculative mathematics dangerous? Recent interactions between physics and mathematics pose the question with some force: traditional mathematical norms discourage speculation, but it is the fabric of theoretical physics. In practice there can be benefits, but there can also be unpleasant and destructive consequences. Serious caution is required, and the issue should be considered before, rather than after, obvious damage occurs. With the hazards carefully in mind, we propose a framework that should allow a healthy and positive role for speculation.},
  number = {1},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2017-06-29},
  date = {1993},
  pages = {1-13},
  author = {Jaffe, Arthur and Quinn, Frank},
  file = {/Users/pgiarrusso/Zotero/storage/AHS5D5IF/Jaffe_Quinn - 1993 - “Theoretical mathematics” - toward a cultural synthesis of mathematics and theoretical physics.pdf;/Users/pgiarrusso/Zotero/storage/529QA4Q2/S0273-0979-1993-00413-0.html}
}
% == BibLateX quality report for Jaffe1993theoretical:
% 'issn': not a valid ISSN

@article{Vytiniotis2011outsideinx,
  title = {OutsideIn(X): Modular type inference with local assumptions},
  volume = {21},
  url = {https://www.microsoft.com/en-us/research/publication/outsideinx-modular-type-inference-with-local-assumptions/},
  shorttitle = {OutsideIn(X)},
  abstract = {Advanced type system features, such as GADTs, type classes and type families, have proven to be invaluable language extensions for ensuring data invariants and program correctness. Unfortunately, they pose a tough problem for type inference when they are used as local type assumptions. Local type assumptions often result in the lack of principal types and …},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-08-19},
  date = {2011-09-01},
  keywords = {_tablet},
  author = {Vytiniotis, Dimitrios and Jones, Simon Peyton and Schrijvers, Tom and Sulzmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/XAFMFN49/Vytiniotis-Jones-Schrijvers-Sulzmann - 2011 - OutsideIn(X) - Modular type inference with local assumptions.pdf;/Users/pgiarrusso/Zotero/storage/5C6XTSZP/outsideinx-modular-type-inference-with-local-assumptions.html}
}

@article{Schmidt-Schauss2015Observational,
  title = {Observational program calculi and the correctness of translations},
  volume = {577},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397515001577},
  doi = {10.1016/j.tcs.2015.02.027},
  abstract = {For the issue of translations between programming languages with observational semantics, this paper clarifies the notions, the relevant questions, and the methods; it constructs a general framework, and provides several tools for proving various correctness properties of translations like adequacy and full abstractness, with a special emphasis on observational correctness. We will demonstrate that a wide range of programming languages and programming calculi and their translations can make advantageous use of our framework for focusing the analysis of their correctness.},
  issue = {Supplement C},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  date = {2015-04-27},
  pages = {98-124},
  keywords = {semantics,contextual equivalence,Correctness,Translations},
  author = {Schmidt-Schauß, Manfred and Sabel, David and Niehren, Joachim and Schwinghammer, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/4Q654AU6/Schmidt-Schauß et al - 2015 - Observational program calculi and the correctness of translations.pdf;/Users/pgiarrusso/Zotero/storage/EXD4XXGC/S0304397515001577.html},
  note = {00000}
}

@article{Grechanik2013Supercompilation,
  title = {Supercompilation by hypergraph transformation},
  url = {http://library.keldysh.ru/preprint.asp?id=2013-26&lg=e},
  abstract = {This paper presents a reformulation of the notion of multi-result supercompilation in terms of graph transformations. For this purpose we use a hypergraph-based representation of the program being transformed. The presented approach bridges the gap between supercompilation and equality saturation. We also show how higher-level supercompilation naturally arises in this setting.},
  urldate = {2017-10-16},
  date = {2013},
  author = {Grechanik, Sergei},
  file = {/Users/pgiarrusso/Zotero/storage/BZ53CEXV/Grechanik - 2013 - Supercompilation by hypergraph transformation.pdf;/Users/pgiarrusso/Zotero/storage/ZV2U6BPH/preprint.html},
  note = {00000}
}
% == BibLateX quality report for Grechanik2013Supercompilation:
% Missing required field 'journaltitle'

@inproceedings{Aydemir2008Engineering,
  location = {{New York, NY, USA}},
  title = {Engineering Formal Metatheory},
  isbn = {978-1-59593-689-9},
  url = {http://doi.acm.org/10.1145/1328438.1328443},
  doi = {10.1145/1328438.1328443},
  abstract = {Machine-checked proofs of properties of programming languages have become acritical need, both for increased confidence in large and complex designsand as a foundation for technologies such as proof-carrying code. However, constructing these proofs remains a black art, involving many choices in the formulation of definitions and theorems that make a huge cumulative difference in the difficulty of carrying out large formal developments. There presentation and manipulation of terms with variable binding is a key issue. We propose a novel style for formalizing metatheory, combining locally nameless representation of terms and cofinite quantification of free variable names in inductivedefinitions of relations on terms (typing, reduction, ...). The key technical insight is that our use of cofinite quantification obviates the need for reasoning about equivariance (the fact that free names can be renamed in derivations); in particular, the structural induction principles of relations defined using cofinite quantification are strong enough for metatheoretic reasoning, and need not be explicitly strengthened. Strong inversion principles follow (automatically, in Coq) from the induction principles. Although many of the underlying ingredients of our technique have been used before, their combination here yields a significant improvement over other methodologies using first-order representations, leading to developments that are faithful to informal practice, yet require noexternal tool support and little infrastructure within the proof assistant. We have carried out several large developments in this style using the Coq proof assistant and have made them publicly available. Our developments include type soundness for System F sub; and core ML (with references, exceptions, datatypes, recursion, and patterns) and subject reduction for the Calculus of Constructions. Not only do these developments demonstrate the comprehensiveness of our approach; they have also been optimized for clarity and robustness, making them good templates for future extension.},
  booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '08},
  publisher = {{ACM}},
  date = {2008},
  pages = {3--15},
  keywords = {coq,binding,locally nameless},
  author = {Aydemir, Brian and Charguéraud, Arthur and Pierce, Benjamin C. and Pollack, Randy and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/N7726TV3/Aydemir et al - 2008 - Engineering Formal Metatheory.pdf},
  note = {00000}
}

@inproceedings{Grechanik2014Inductive,
  langid = {english},
  title = {Inductive Prover Based on Equality Saturation for a Lazy Functional Language},
  isbn = {978-3-662-46822-7 978-3-662-46823-4},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-46823-4_11},
  doi = {10.1007/978-3-662-46823-4_11},
  abstract = {The present paper shows how the idea of equality saturation can be used to build an inductive prover for a non-total first-order lazy functional language. We adapt equality saturation approach to a functional language by using transformations borrowed from supercompilation. A special transformation called merging by bisimilarity is used to perform proof by induction of equivalence between nodes of the E-graph. Equalities proved this way are just added to the E-graph. We also experimentally compare our prover with HOSC and HipSpec.},
  eventtitle = {International Andrei Ershov Memorial Conference on Perspectives of System Informatics},
  booktitle = {Perspectives of System Informatics},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-10-16},
  date = {2014-06-24},
  pages = {127-141},
  author = {Grechanik, Sergei},
  file = {/Users/pgiarrusso/Zotero/storage/IKD7HKB3/Grechanik - 2014 - Inductive Prover Based on Equality Saturation for a Lazy Functional Language.pdf;/Users/pgiarrusso/Zotero/storage/XDCGH6TE/978-3-662-46823-4_11.html}
}
% == BibLateX quality report for Grechanik2014Inductive:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Ilik2013Continuationpassing,
  title = {Continuation-passing style models complete for intuitionistic logic},
  volume = {164},
  issn = {0168-0072},
  url = {http://www.sciencedirect.com/science/article/pii/S0168007212000796},
  doi = {10.1016/j.apal.2012.05.003},
  abstract = {A class of models is presented, in the form of continuation monads polymorphic for first-order individuals, that is sound and complete for minimal intuitionistic predicate logic (including disjunction and the existential quantifier). The proofs of soundness and completeness are constructive and the computational content of their composition is, in particular, a β-normalisation-by-evaluation program for simply typed lambda calculus with sum types. Although the inspiration comes from Danvyʼs type-directed partial evaluator for the same lambda calculus, the use of delimited control operators (i.e. computational effects) is avoided. The role of polymorphism is crucial – dropping it allows one to obtain a notion of model complete for classical predicate logic.},
  number = {6},
  journaltitle = {Annals of Pure and Applied Logic},
  shortjournal = {Annals of Pure and Applied Logic},
  series = {Classical Logic and Computation 2010(CLAC 2010)},
  date = {2013-06-01},
  pages = {651-662},
  keywords = {Completeness,Double-negation shift,Intuitionistic logic,Kripke models,Normalisation by evaluation},
  author = {Ilik, Danko},
  file = {/Users/pgiarrusso/Zotero/storage/6QAT6UGE/Ilik - 2013 - Continuation-passing style models complete for intuitionistic logic.pdf;/Users/pgiarrusso/Zotero/storage/BKDHTCIJ/S0168007212000796.html},
  note = {00000}
}

@book{Dockins2012Constructing,
  title = {Constructing Hereditary Worlds Within Worlds},
  abstract = {Abstract. Although they appear unrelated, the type system of the polymorphic λ-calculus with references and the assertions of concurrent separation logic with first-class locks share a critical feature: an unsound contravariant circularity in their naïve semantic model. We developed indirection theory to automatically construct, and cleanly axiomatize, step-indexed approximations to these naïve models, as well as a large number of others [HDA10b]. Unfortunately, the previous axiomatization had a flaw. One is usually only interested in using hereditary predicates: those which are closed under the action of approximation. As previously presented, indirection theory allows nonhereditary predicates to exist in certain parts of the construction. Although not fatal, this flaw requires workarounds that are not entirely obvious to the uninitiated. We correct this flaw by presenting a new axiomatization of indirection theory that only permits heredity predicates and show that the new interface is sound by constructing a model. The new axiomatization is somewhat more subtle than the previous one, but it retains the same flavor, cleanliness, and metatheoretic properties. In contrast, the new construction is markedly more complex, especially in a mechanized context. Indeed, our Coq mechanization is one of our key contributions, and accordingly we present it in considerable detail. 1},
  date = {2012},
  keywords = {_tablet},
  author = {Dockins, Robert and Hobor, Aquinas},
  file = {/Users/pgiarrusso/Zotero/storage/V9JM56GR/Dockins-Hobor - 2012 - Constructing Hereditary Worlds Within Worlds.pdf;/Users/pgiarrusso/Zotero/storage/UQIVFZMK/summary.html},
  note = {00000}
}

@inproceedings{Nieto2017Algorithmic,
  location = {{New York, NY, USA}},
  title = {Towards Algorithmic Typing for DOT (Short Paper)},
  isbn = {978-1-4503-5529-2},
  url = {http://doi.acm.org/10.1145/3136000.3136003},
  doi = {10.1145/3136000.3136003},
  abstract = {The Dependent Object Types (DOT) calculus formalizes key features of Scala. The D$<$: calculus is the core of DOT. To date, presentations of D$<$: have used declarative, as opposed to algorithmic, typing and subtyping rules. Unfortunately, algorithmic typing for full D$<$: is known to be an undecidable problem. We explore the design space for a restricted version of D$<$: that has decidable typechecking. Even in this simplified D$<$:, algorithmic typing and subtyping are tricky, due to the âbad boundsâ problem. The Scala compiler bypasses bad bounds at the cost of a loss in expressiveness in its type system. Based on the approach taken in the Scala compiler, we present the Step Typing and Step Subtyping relations for D$<$:. These relations are sound and decidable. They are not complete with respect to the original D$<$: typing rules.},
  booktitle = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala},
  series = {SCALA 2017},
  publisher = {{ACM}},
  date = {2017},
  pages = {2--7},
  keywords = {Scala,algorithmic typing,dependent object types,DOT calculus},
  author = {Nieto, Abel},
  file = {/Users/pgiarrusso/Zotero/storage/ZCSEN7RD/Nieto - 2017 - Towards Algorithmic Typing for DOT (Short Paper).pdf},
  note = {00000}
}

@inproceedings{Armbrust2015Spark,
  location = {{New York, NY, USA}},
  title = {Spark SQL: Relational Data Processing in Spark},
  isbn = {978-1-4503-2758-9},
  url = {http://doi.acm.org/10.1145/2723372.2742797},
  doi = {10.1145/2723372.2742797},
  shorttitle = {Spark SQL},
  abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '15},
  publisher = {{ACM}},
  date = {2015},
  pages = {1383--1394},
  keywords = {data warehouse,databases,hadoop,machine learning,spark},
  author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
  file = {/Users/pgiarrusso/Zotero/storage/XEXN4ZKM/Armbrust et al. - 2015 - Spark SQL Relational Data Processing in Spark.pdf},
  note = {00000}
}

@inproceedings{Castagna2011Settheoretic,
  location = {{New York, NY, USA}},
  title = {Set-theoretic Foundation of Parametric Polymorphism and Subtyping},
  isbn = {978-1-4503-0865-6},
  url = {http://doi.acm.org/10.1145/2034773.2034788},
  doi = {10.1145/2034773.2034788},
  abstract = {We define and study parametric polymorphism for a type system with recursive, product, union, intersection, negation, and function types. We first recall why the definition of such a system was considered hard "when not impossible" and then present the main ideas at the basis of our solution. In particular, we introduce the notion of "convexity" on which our solution is built up and discuss its connections with parametricity as defined by Reynolds to whose study our work sheds new light.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '11},
  publisher = {{ACM}},
  date = {2011},
  pages = {94--106},
  keywords = {polymorphism,XML,subtyping,types,parametricity},
  author = {Castagna, Giuseppe and Xu, Zhiwu},
  file = {/Users/pgiarrusso/Zotero/storage/VCZAB6ZA/Castagna_Xu - 2011 - Set-theoretic Foundation of Parametric Polymorphism and Subtyping.pdf}
}

@inproceedings{Scherer2017Search,
  location = {{Dagstuhl, Germany}},
  title = {Search for Program Structure},
  volume = {71},
  isbn = {978-3-95977-032-3},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7134},
  doi = {10.4230/LIPIcs.SNAPL.2017.15},
  booktitle = {2nd Summit on Advances in Programming Languages (SNAPL 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {15:1--15:14},
  keywords = {canonicity,focusing,programs,proofs},
  author = {Scherer, Gabriel},
  editor = {Lerner, Benjamin S. and Bodík, Rastislav and Krishnamurthi, Shriram},
  file = {/Users/pgiarrusso/Zotero/storage/IQDDQX6A/Scherer - 2017 - Search for Program Structure.pdf;/Users/pgiarrusso/Zotero/storage/TWCDTKM4/frontdoor.html}
}
% == BibLateX quality report for Scherer2017Search:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Tobin-Hochstadt2017Migratory,
  location = {{Dagstuhl, Germany}},
  title = {Migratory Typing: Ten Years Later},
  volume = {71},
  isbn = {978-3-95977-032-3},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7120},
  doi = {10.4230/LIPIcs.SNAPL.2017.17},
  shorttitle = {Migratory Typing},
  booktitle = {2nd Summit on Advances in Programming Languages (SNAPL 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {17:1--17:17},
  keywords = {gradual typing,type systems,design principles},
  author = {Tobin-Hochstadt, Sam and Felleisen, Matthias and Findler, Robert and Flatt, Matthew and Greenman, Ben and Kent, Andrew M. and St-Amour, Vincent and Strickland, T. Stephen and Takikawa, Asumu},
  editor = {Lerner, Benjamin S. and Bodík, Rastislav and Krishnamurthi, Shriram},
  file = {/Users/pgiarrusso/Zotero/storage/7IMEED9W/Tobin-Hochstadt et al - 2017 - Migratory Typing - Ten Years Later.pdf;/Users/pgiarrusso/Zotero/storage/F5VZR2CC/frontdoor.html}
}
% == BibLateX quality report for Tobin-Hochstadt2017Migratory:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Orton2017Models,
  location = {{Dagstuhl, Germany}},
  title = {Models of Type Theory Based on Moore Paths},
  volume = {84},
  isbn = {978-3-95977-047-7},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7714},
  doi = {10.4230/LIPIcs.FSCD.2017.28},
  booktitle = {2nd International Conference on Formal Structures for Computation and Deduction (FSCD 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {28:1--28:16},
  keywords = {Dependent type theory,homotopy theory,Moore path,topos},
  author = {Orton, Ian and Pitts, Andrew M.},
  editor = {Miller, Dale},
  file = {/Users/pgiarrusso/Zotero/storage/V47BFCXK/Orton_Pitts - 2017 - Models of Type Theory Based on Moore Paths.pdf;/Users/pgiarrusso/Zotero/storage/2N75VASV/frontdoor.html}
}
% == BibLateX quality report for Orton2017Models:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Pistone2017Dinaturality,
  location = {{Dagstuhl, Germany}},
  title = {On Dinaturality, Typability and beta-eta-Stable Models},
  volume = {84},
  isbn = {978-3-95977-047-7},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7729},
  doi = {10.4230/LIPIcs.FSCD.2017.29},
  booktitle = {2nd International Conference on Formal Structures for Computation and Deduction (FSCD 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {29:1--29:17},
  keywords = {simply-typed lambda-calculus,Completeness,beta-eta-stable semantics,Dinaturality},
  author = {Pistone, Paolo},
  editor = {Miller, Dale},
  file = {/Users/pgiarrusso/Zotero/storage/AQICKZGH/Pistone - 2017 - On Dinaturality, Typability and beta-eta-Stable Models.pdf;/Users/pgiarrusso/Zotero/storage/T3PZZ6ZB/frontdoor.html}
}
% == BibLateX quality report for Pistone2017Dinaturality:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Intrigila2017Refutation,
  location = {{Dagstuhl, Germany}},
  title = {Refutation of Sallé's Longstanding Conjecture},
  volume = {84},
  isbn = {978-3-95977-047-7},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7723},
  doi = {10.4230/LIPIcs.FSCD.2017.20},
  booktitle = {2nd International Conference on Formal Structures for Computation and Deduction (FSCD 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {20:1--20:18},
  keywords = {lambda calculus,Observational equivalence,Böhm trees,omega-rule},
  author = {Intrigila, Benedetto and Manzonetto, Giulio and Polonsky, Andrew},
  editor = {Miller, Dale},
  file = {/Users/pgiarrusso/Zotero/storage/B7HNP9PZ/Intrigila et al - 2017 - Refutation of Sallé's Longstanding Conjecture.pdf;/Users/pgiarrusso/Zotero/storage/Z8MSPXXH/frontdoor.html}
}
% == BibLateX quality report for Intrigila2017Refutation:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Kuci2017Cocontextual,
  location = {{Dagstuhl, Germany}},
  title = {A Co-contextual Type Checker for Featherweight Java},
  volume = {74},
  isbn = {978-3-95977-035-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7262},
  doi = {10.4230/LIPIcs.ECOOP.2017.18},
  booktitle = {31st European Conference on Object-Oriented Programming (ECOOP 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {18:1--18:26},
  keywords = {type checking,constraints,class table,co-contextual,Featherweight Java},
  author = {Kuci, Edlira and Erdweg, Sebastian and Bracevac, Oliver and Bejleri, Andi and Mezini, Mira},
  editor = {Müller, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/I7P826F5/Kuci et al - 2017 - A Co-contextual Type Checker for Featherweight Java.pdf;/Users/pgiarrusso/Zotero/storage/Q26JREKB/frontdoor.html}
}
% == BibLateX quality report for Kuci2017Cocontextual:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Wang2017Strong,
  location = {{Dagstuhl, Germany}},
  title = {Towards Strong Normalization for Dependent Object Types (DOT)},
  volume = {74},
  isbn = {978-3-95977-035-4},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/7276},
  doi = {10.4230/LIPIcs.ECOOP.2017.27},
  booktitle = {31st European Conference on Object-Oriented Programming (ECOOP 2017)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  date = {2017},
  pages = {27:1--27:25},
  keywords = {recursive types,Strong Normalization,logical relations,_tablet,Scala,DOT},
  author = {Wang, Fei and Rompf, Tiark},
  editor = {Müller, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/TQ28KV6T/Wang-Rompf - 2017 - Towards Strong Normalization for Dependent Object Types (DOT).pdf;/Users/pgiarrusso/Zotero/storage/JF5STNRB/7276.html}
}
% == BibLateX quality report for Wang2017Strong:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Parreaux2017Quoted,
  title = {Quoted Staged Rewriting: A Practical Approach to Library-Defined Optimizations},
  url = {https://infoscience.epfl.ch/record/231076},
  doi = {10.1145/3136040.3136043},
  shorttitle = {Quoted Staged Rewriting},
  journaltitle = {Proceedings of 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences (GPCE’17)},
  urldate = {2017-09-19},
  date = {2017},
  author = {Parreaux, Lionel Emile Vincent and Shaikhha, Amir and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/CVSJA8HA/Parreaux et al - 2017 - Quoted Staged Rewriting - A Practical Approach to Library-Defined Optimizations.pdf;/Users/pgiarrusso/Zotero/storage/KBTR2XCP/231076.html}
}

@article{Rabe2013Logical,
  title = {Logical Relations for a Logical Framework},
  volume = {14},
  issn = {1529-3785},
  url = {http://doi.acm.org/10.1145/2536740.2536741},
  doi = {10.1145/2536740.2536741},
  abstract = {Logical relations are a central concept used to study various higher-order type theories and occur frequently in the proofs of a wide variety of meta-theorems. Besides extending the logical relation principle to more general languages, an important research question has been how to represent and thus verify logical relation arguments in logical frameworks. We formulate a theory of logical relations for Dependent Type Theory (DTT) with β η-equality which guarantees that any valid logical relation satisfies the Basic Lemma. Our definition is syntactic and reflective in the sense that a relation at a type is represented as a DTT type family but also permits expressing certain semantic definitions. We use the Edinburgh Logical Framework (LF) incarnation of DTT and implement our notion of logical relations in the type-checker Twelf. This enables us to formalize and mechanically decide the validity of logical relation arguments. Furthermore, our implementation includes a module system so that logical relations can be built modularly. We validate our approach by formalizing and verifying several syntactic and semantic meta-theorems in Twelf. Moreover, we show how object languages encoded in DTT can inherit a notion of logical relation from the logical framework.},
  number = {4},
  journaltitle = {ACM Trans. Comput. Logic},
  date = {2013-11},
  pages = {32:1--32:34},
  keywords = {twelf,Dependent type theory,parametricity,logical relation,logical framework,LF,module system},
  author = {Rabe, Florian and Sojakova, Kristina},
  file = {/Users/pgiarrusso/Zotero/storage/2AJK2TCJ/Rabe_Sojakova - 2013 - Logical Relations for a Logical Framework.pdf}
}
% == BibLateX quality report for Rabe2013Logical:
% ? Possibly abbreviated journal title ACM Trans. Comput. Logic

@inproceedings{Abel2014Formalized,
  langid = {english},
  title = {A Formalized Proof of Strong Normalization for Guarded Recursive Types},
  isbn = {978-3-319-12735-4 978-3-319-12736-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-12736-1_8},
  doi = {10.1007/978-3-319-12736-1_8},
  abstract = {We consider a simplified version of Nakano’s guarded fixed-point types in a representation by infinite type expressions, defined coinductively. Smallstep reduction is parametrized by a natural number “depth” that expresses under how many guards we may step during evaluation. We prove that reduction is strongly normalizing for any depth. The proof involves a typed inductive notion of strong normalization and a Kripke model of types in two dimensions: depth and typing context. Our results have been formalized in Agda and serve as a case study of reasoning about a language with coinductive type expressions.},
  eventtitle = {Asian Symposium on Programming Languages and Systems},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2017-09-08},
  date = {2014-11-17},
  pages = {140-158},
  author = {Abel, Andreas and Vezzosi, Andrea},
  file = {/Users/pgiarrusso/Zotero/storage/8PZ9E8GQ/Abel_Vezzosi - 2014 - A Formalized Proof of Strong Normalization for Guarded Recursive Types.pdf;/Users/pgiarrusso/Zotero/storage/UH6MSGM6/978-3-319-12736-1_8.html}
}
% == BibLateX quality report for Abel2014Formalized:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Mellies1995Typed,
  langid = {english},
  title = {Typed λ-calculi with explicit substitutions may not terminate},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  url = {https://link.springer.com/chapter/10.1007/BFb0014062},
  doi = {10.1007/BFb0014062},
  abstract = {We present a simply typed λ-term whose computation in the λσ-calculus does not always terminate.},
  eventtitle = {International Conference on Typed Lambda Calculi and Applications},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-09-08},
  date = {1995-04-10},
  pages = {328-334},
  author = {Mellies, Paul-André},
  file = {/Users/pgiarrusso/Zotero/storage/C4TEEZAT/Mellies - 1995 - Typed λ-calculi with explicit substitutions may not terminate.pdf;/Users/pgiarrusso/Zotero/storage/75XXPZJ7/10.html}
}
% == BibLateX quality report for Mellies1995Typed:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Shaikhha2017Destinationpassing,
  location = {{New York, NY, USA}},
  title = {Destination-passing Style for Efficient Memory Management},
  isbn = {978-1-4503-5181-2},
  url = {http://doi.acm.org/10.1145/3122948.3122949},
  doi = {10.1145/3122948.3122949},
  abstract = {We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination-passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN International Workshop on Functional High-Performance Computing},
  series = {FHPC 2017},
  publisher = {{ACM}},
  date = {2017},
  pages = {12--23},
  keywords = {Array Programming,Destination-Passing Style},
  author = {Shaikhha, Amir and Fitzgibbon, Andrew and Peyton Jones, Simon and Vytiniotis, Dimitrios}
}

@article{Coquand2002Formalised,
  langid = {english},
  title = {A Formalised Proof of the Soundness and Completeness of a Simply Typed Lambda-Calculus with Explicit Substitutions},
  volume = {15},
  issn = {1388-3690, 1573-0557},
  url = {https://link.springer.com/article/10.1023/A:1019964114625},
  doi = {10.1023/A:1019964114625},
  abstract = {We present a simply-typed λ-calculus with explicit substitutions and we give a fully formalised proof of its soundness and completeness with respect to Kripke models. We further give conversion rules for the calculus and show also for them that they are sound and complete with respect to extensional equality in the Kripke model. A decision algorithm for conversion is given and proven correct. We use the technique “normalisation by evaluation” in order to prove these results. An important aspect of this work is that it is not a formalisation of an existing proof, instead the proof has been done in interaction with the proof system, ALF.},
  number = {1},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order and Symbolic Computation},
  urldate = {2017-09-08},
  date = {2002-03-01},
  pages = {57-90},
  author = {Coquand, Catarina},
  file = {/Users/pgiarrusso/Zotero/storage/IEG3MD35/Coquand - 2002 - A Formalised Proof of the Soundness and Completeness of a Simply Typed Lambda-Calculus with Explicit Substitutions.pdf;/Users/pgiarrusso/Zotero/storage/GID4DJAI/10.html}
}
% == BibLateX quality report for Coquand2002Formalised:
% 'issn': not a valid ISSN

@article{Hammer2016Refinement,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00097},
  primaryClass = {cs},
  title = {Refinement types for precisely named cache locations},
  url = {http://arxiv.org/abs/1610.00097v4},
  abstract = {State-of-the-art programming language techniques for incremental computation employ programmer-specified names, whose role is critical for the asymptotic efficiency of many online algorithms. These names identify "cache locations" for dynamic data and sub-computations---the program's dynamic dependencies. To work well, or at all, these names must be precise, meaning that for an evaluation derivation in question, each name identifies at most one value or subcomputation; we call all other names imprecise, or ambiguous. The precise name problem consists of statically verifying that a program allocates names precisely, for all inputs. Past theoretical work ignores this problem by not permitting programs to use names directly, and past implementations, which necessarily permit explicit names, employ ad hoc workarounds for imprecise incremental programs. In this work, we give the first static verification approach to the precise naming problem. Specifically, we define a refinement type system that gives name term and index term sub-languages for approximating programmer-specified names of dynamic data and sub-computations. We prove that our type system enforces that these names are precise. We demonstrate the practical value of our proposed type system by verifying examples of incremental sequences and sets from a recent collections library, including both library client and implementation code. Drawing closer to an implementation of our type system, we derive a bidirectional version, and prove that it corresponds to our declarative type system. A key challenge in implementing the bidirectional system is handling constraints over names, name terms and name sets; toward this goal, we give decidable, syntactic rules to guide these checks.},
  urldate = {2017-09-07},
  date = {2016-10-01},
  keywords = {Computer Science - Programming Languages},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  file = {/Users/pgiarrusso/Zotero/storage/53WWBBKJ/Hammer-Dunfield-Economou-Narasimhamurthy - 2016 - Refinement types for precisely named cache locations.pdf;/Users/pgiarrusso/Zotero/storage/WREVWN5B/1610.html}
}
% == BibLateX quality report for Hammer2016Refinement:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Anand2017Revisiting,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.01163},
  primaryClass = {cs},
  title = {Revisiting Parametricity: Inductives and Uniformity of Propositions},
  url = {http://arxiv.org/abs/1705.01163},
  shorttitle = {Revisiting Parametricity},
  abstract = {Reynold's parametricity theory captures the property that parametrically polymorphic functions behave uniformly: they produce related results on related instantiations. In dependently-typed programming languages, such relations and uniformity proofs can be expressed internally, and generated as a program translation. We present a new parametricity translation for a significant fragment of Coq. Previous translations of parametrically polymorphic propositions allowed non-uniformity. For example, on related instantiations, a function may return propositions that are logically inequivalent (e.g. True and False). We show that uniformity of polymorphic propositions is not achievable in general. Nevertheless, our translation produces proofs that the two propositions are logically equivalent and also that any two proofs of those propositions are related. This is achieved at the cost of potentially requiring more assumptions on the instantiations, requiring them to be isomorphic in the worst case. Our translation augments the previous one for Coq by carrying and compositionally building extra proofs about parametricity relations. It is made easier by a new method for translating inductive types and pattern matching. The new method builds upon and generalizes previous such translations for dependently-typed programming languages. Using reification and reflection, we have implemented our translation as Coq programs. We obtain several stronger free theorems applicable to an ongoing compiler-correctness project. Previously, proofs of some of these theorems took several hours to finish.},
  date = {2017-05-02},
  keywords = {Computer Science - Logic in Computer Science},
  author = {Anand, Abhishek and Morrisett, Greg},
  file = {/Users/pgiarrusso/Zotero/storage/FWCMEHR5/Anand_Morrisett - 2017 - Revisiting Parametricity - Inductives and Uniformity of Propositions.pdf;/Users/pgiarrusso/Zotero/storage/TMQZMGHQ/1705.html}
}
% == BibLateX quality report for Anand2017Revisiting:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Crary1999Simple,
  location = {{New York, NY, USA}},
  title = {A Simple Proof Technique for Certain Parametricity Results},
  isbn = {978-1-58113-111-6},
  url = {http://doi.acm.org/10.1145/317636.317787},
  doi = {10.1145/317636.317787},
  abstract = {Many properties of parametric, polymorphic functions can be determined simply by inspection of their types. Such results are usually proven using Reynolds's parametricity theorem. However, Reynolds's theorem can be difficult to show in some settings, particularly ones involving computational effects. I present an alternative technique for proving some parametricity results. This technique is considerably simpler and easily generalizes to effectful settings. It works by instantiating polymorphic functions with singleton types that fully specify the behavior of the functions. Using this technique, I show that callers' stacks are protected from corruption during function calls in Typed Assembly Language programs.},
  booktitle = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '99},
  publisher = {{ACM}},
  date = {1999},
  pages = {82--89},
  author = {Crary, Karl},
  file = {/Users/pgiarrusso/Zotero/storage/UHCCREME/Crary - 1999 - A Simple Proof Technique for Certain Parametricity Results.pdf}
}

@inproceedings{Hobor2010Theory,
  location = {{New York, NY, USA}},
  title = {A Theory of Indirection via Approximation},
  isbn = {978-1-60558-479-9},
  url = {http://doi.acm.org/10.1145/1706299.1706322},
  doi = {10.1145/1706299.1706322},
  abstract = {Building semantic models that account for various kinds of indirect reference has traditionally been a difficult problem. Indirect reference can appear in many guises, such as heap pointers, higher-order functions, object references, and shared-memory mutexes. We give a general method to construct models containing indirect reference by presenting a "theory of indirection". Our method can be applied in a wide variety of settings and uses only simple, elementary mathematics. In addition to various forms of indirect reference, the resulting models support powerful features such as impredicative quantification and equirecursion; moreover they are compatible with the kind of powerful substructural accounting required to model (higher-order) separation logic. In contrast to previous work, our model is easy to apply to new settings and has a simple axiomatization, which is complete in the sense that all models of it are isomorphic. Our proofs are machine-checked in Coq.},
  booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '10},
  publisher = {{ACM}},
  date = {2010},
  pages = {171--184},
  keywords = {_tablet,indirection theory,step-indexed models},
  author = {Hobor, Aquinas and Dockins, Robert and Appel, Andrew W.},
  file = {/Users/pgiarrusso/Zotero/storage/GHXGDRQE/Hobor-Dockins-Appel - 2010 - A Theory of Indirection via Approximation.pdf}
}

@inproceedings{Devriese2016Reasoning,
  title = {Reasoning about Object Capabilities with Logical Relations and Effect Parametricity},
  doi = {10.1109/EuroSP.2016.22},
  abstract = {Object capabilities are a technique for fine-grained privilege separation in programming languages and systems, with important applications in security. However, current formal characterisations do not fully capture capability-safety of a programming language and are not sufficient for verifying typical applications. Using state-of-the-art techniques from programming languages research, we define a logical relation for a core calculus of JavaScript that better characterises capability-safety. The relation is powerful enough to reason about typical capability patterns and supports evolvable invariants on shared data structures, capabilities with restricted authority over them and isolated components with restricted communication channels. We use a novel notion of effect parametricity for deriving properties about effects. Our results imply memory access bounds that have previously been used to characterise capability-safety.},
  eventtitle = {2016 IEEE European Symposium on Security and Privacy (EuroS P)},
  booktitle = {2016 IEEE European Symposium on Security and Privacy (EuroS P)},
  date = {2016-03},
  pages = {147-162},
  keywords = {Data Structures,java,javascript,logical relations,Calculus,Computer languages,Logic programming,programming languages,capability-safety,Cognition,effect parametricity,Electronic mail,fine-grained privilege separation,high level languages,inference mechanisms,object capabilities,reasoning,security,security of data,Syntactics},
  author = {Devriese, D. and Birkedal, L. and Piessens, F.},
  file = {/Users/pgiarrusso/Zotero/storage/9ZXSUQHT/7467352.html}
}
% == BibLateX quality report for Devriese2016Reasoning:
% ? Unsure about the formatting of the booktitle

@inproceedings{Svendsen2016Transfinite,
  langid = {english},
  title = {Transfinite Step-Indexing: Decoupling Concrete and Logical Steps},
  isbn = {978-3-662-49497-4 978-3-662-49498-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-49498-1_28},
  doi = {10.1007/978-3-662-49498-1_28},
  shorttitle = {Transfinite Step-Indexing},
  abstract = {Step-indexing has proven to be a powerful technique for defining logical relations for languages with advanced type systems and models of expressive program logics. In both cases, the model is stratified using natural numbers to solve a recursive equation that has no naive solutions. As a result of this stratification, current models require that each unfolding of the recursive equation – each logical step – must coincide with a concrete reduction step. This tight coupling is problematic for applications where the number of logical steps cannot be statically bounded.In this paper we demonstrate that this tight coupling between logical and concrete steps is artificial and show how to loosen it using transfinite step-indexing. We present a logical relation that supports an arbitrary but finite number of logical steps for each concrete step.},
  eventtitle = {European Symposium on Programming Languages and Systems},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-09-05},
  date = {2016-04-03},
  pages = {727-751},
  keywords = {_tablet},
  author = {Svendsen, Kasper and Sieczkowski, Filip and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/R3AHEVTZ/Svendsen-Sieczkowski-Birkedal - 2016 - Transfinite Step-Indexing - Decoupling Concrete and Logical Steps.pdf;/Users/pgiarrusso/Zotero/storage/GRUQB7HX/978-3-662-49498-1_28.html}
}
% == BibLateX quality report for Svendsen2016Transfinite:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Hoshino2012Step,
  location = {{Washington, DC, USA}},
  title = {Step Indexed Realizability Semantics for a Call-by-Value Language Based on Basic Combinatorial Objects},
  isbn = {978-0-7695-4769-5},
  url = {http://dx.doi.org/10.1109/LICS.2012.74},
  doi = {10.1109/LICS.2012.74},
  abstract = {We propose a mathematical framework for step indexed realizability semantics of a call-by-value polymorphic lambda calculus with recursion, existential types and recursive types. Our framework subsumes step indexed realizability semantics by untyped call-by-value lambda calculi as well as categorical abstract machines. Starting from an extension of Hofstra's basic combinatorial objects, we construct a step indexed categorical realizability semantics. Our main result is soundness and adequacy of our step indexed realizability semantics. As an application, we show that a small step operational semantics captures the big step operational semantics of the call-by-value polymorphic lambda calculus. We also give a safe implementation of the call-by-value polymorphic lambda calculus into a categorical abstract machine.},
  booktitle = {Proceedings of the 2012 27th Annual IEEE/ACM Symposium on Logic in Computer Science},
  series = {LICS '12},
  publisher = {{IEEE Computer Society}},
  date = {2012},
  pages = {385--394},
  keywords = {Denotational semantics,operational semantics,Semantics of Programming Languages},
  author = {Hoshino, Naohiko},
  file = {/Users/pgiarrusso/Zotero/storage/EVQERQ7D/Hoshino - 2012 - Step Indexed Realizability Semantics for a Call-by-Value Language Based on Basic Combinatorial Objects.pdf}
}

@article{Altenkirch2009Bigstep,
  title = {Big-step normalisation},
  volume = {19},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/big-step-normalisation/749A35BCD063A03AE16897256E08DD09},
  doi = {10.1017/S0956796809007278},
  abstract = {AbstractTraditionally, decidability of conversion for typed λ-calculi is established by showing that small-step reduction is confluent and strongly normalising. Here we investigate an alternative approach employing a recursively defined normalisation function which we show to be terminating and which reflects and preserves conversion. We apply our approach to the simply typed λ-calculus with explicit substitutions and βη-equality, a system which is not strongly normalising. We also show how the construction can be extended to system T with the usual β-rules for the recursion combinator. Our approach is practical, since it does verify an actual implementation of normalisation which, unlike normalisation by evaluation, is first order. An important feature of our approach is that we are using logical relations to establish equational soundness (identity of normal forms reflects the equational theory), instead of the usual syntactic reasoning using the Church–Rosser property of a term rewriting system.},
  number = {3-4},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-09-04},
  date = {2009-07},
  pages = {311-333},
  author = {Altenkirch, Thorsten and Chapman, James},
  file = {/Users/pgiarrusso/Zotero/storage/DRDVIGGI/Altenkirch_Chapman - 2009 - Big-step normalisation.pdf;/Users/pgiarrusso/Zotero/storage/4SPN7R2M/749A35BCD063A03AE16897256E08DD09.html}
}
% == BibLateX quality report for Altenkirch2009Bigstep:
% 'issn': not a valid ISSN

@inproceedings{Bruce1998statically,
  langid = {english},
  title = {A statically safe alternative to virtual types},
  isbn = {978-3-540-64737-9 978-3-540-69064-1},
  url = {https://link.springer.com/chapter/10.1007/BFb0054106},
  doi = {10.1007/BFb0054106},
  abstract = {Parametric types and virtual types have recently been proposed as extensions to Java to support genericity. In this paper we investigate the strengths and weaknesses of each. We suggest a variant of virtual types which has similar expressiveness, but supports safe static type checking. This results in a language in which both parametric types and virtual types are well-integrated, and which is statically type-safe.},
  eventtitle = {European Conference on Object-Oriented Programming},
  booktitle = {ECOOP’98 — Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-09-03},
  date = {1998-07-20},
  pages = {523-549},
  author = {Bruce, Kim B. and Odersky, Martin and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/PZC3ZCW6/Bruce et al - 1998 - A statically safe alternative to virtual types.pdf;/Users/pgiarrusso/Zotero/storage/USNSJZWB/BFb0054106.html}
}
% == BibLateX quality report for Bruce1998statically:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Barthe2002CPS,
  location = {{New York, NY, USA}},
  title = {CPS Translating Inductive and Coinductive Types},
  isbn = {978-1-58113-455-1},
  url = {http://doi.acm.org/10.1145/503032.503043},
  doi = {10.1145/503032.503043},
  abstract = {We investigate CPS translatability of typed λ-calculi with inductive and coinductive types. We show that tenable Plotkin-style call-by-name CPS translations exist for simply typed λ-calculi with a natural number type and stream types and, more generally, with arbitrary positive inductive and coinductive types. These translations also work in the presence of control operators and generalize for dependently typed calculi where case-like eliminations are only allowed in non-dependent forms. No translation is possible along the same lines for small Σ-types and sum types with dependent case.},
  booktitle = {Proceedings of the 2002 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-based Program Manipulation},
  series = {PEPM '02},
  publisher = {{ACM}},
  date = {2002},
  pages = {131--142},
  keywords = {dependent types,inductive and coinductive types,classical logic and control,CPS translations,typed $\\lambda$-calculi},
  author = {Barthe, Gilles and Uustalu, Tarmo},
  file = {/Users/pgiarrusso/Zotero/storage/KQQ5GERN/Barthe_Uustalu - 2002 - CPS Translating Inductive and Coinductive Types.pdf}
}

@article{Elliott2017Compiling,
  title = {Compiling to Categories},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110271},
  doi = {10.1145/3110271},
  abstract = {It is well-known that the simply typed lambda-calculus is modeled by any cartesian closed category (CCC). This correspondence suggests giving typed functional programs a variety of interpretations, each corresponding to a different category. A convenient way to realize this idea is as a collection of meaning-preserving transformations added to an existing compiler, such as GHC for Haskell. This paper describes such an implementation and demonstrates its use for a variety of interpretations including hardware circuits, automatic differentiation, incremental computation, and interval analysis. Each such interpretation is a category easily defined in Haskell (outside of the compiler). The general technique appears to provide a compelling alternative to deeply embedded domain-specific languages.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {27:1--27:27},
  keywords = {domain-specific languages,category theory,compile-time optimization},
  author = {Elliott, Conal},
  file = {/Users/pgiarrusso/Zotero/storage/K8Q2GZI9/Elliott - 2017 - Compiling to Categories.pdf}
}
% == BibLateX quality report for Elliott2017Compiling:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Pombrio2017Inferring,
  title = {Inferring Scope Through Syntactic Sugar},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110288},
  doi = {10.1145/3110288},
  abstract = {Many languages use syntactic sugar to define parts of their surface language in terms of a smaller core. Thus some properties of the surface language, like its scoping rules, are not immediately evident. Nevertheless, IDEs, refactorers, and other tools that traffic in source code depend on these rules to present information to users and to soundly perform their operations. In this paper, we show how to lift scoping rules defined on a core language to rules on the surface, a process of scope inference. In the process we introduce a new representation of binding structure---scope as a preorder---and present a theoretical advance: proving that a desugaring system preserves Î±-equivalence even though scoping rules have been provided only for the core language. We have also implemented the system presented in this paper.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {44:1--44:28},
  keywords = {syntactic sugar,binding,Scope},
  author = {Pombrio, Justin and Krishnamurthi, Shriram and Wand, Mitchell},
  file = {/Users/pgiarrusso/Zotero/storage/4SIFV2S3/Pombrio et al - 2017 - Inferring Scope Through Syntactic Sugar.pdf}
}
% == BibLateX quality report for Pombrio2017Inferring:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Morris2017Constrained,
  title = {Constrained Type Families},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110286},
  doi = {10.1145/3110286},
  abstract = {We present an approach to support partiality in type-level computation without compromising expressiveness or type safety. Existing frameworks for type-level computation either require totality or implicitly assume it. For example, type families in Haskell provide a powerful, modular means of defining type-level computation. However, their current design implicitly assumes that type families are total, introducing nonsensical types and significantly complicating the metatheory of type families and their extensions. We propose an alternative design, using qualified types to pair type-level computations with predicates that capture their domains. Our approach naturally captures the intuitive partiality of type families, simplifying their metatheory. As evidence, we present the first complete proof of consistency for a language with closed type families.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {42:1--42:28},
  keywords = {Haskell,type families,type-level computation,type classes},
  author = {Morris, J. Garrett and Eisenberg, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/GB93HVN7/Morris_Eisenberg - 2017 - Constrained Type Families.pdf}
}
% == BibLateX quality report for Morris2017Constrained:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Weirich2017Specification,
  title = {A Specification for Dependent Types in Haskell},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110275},
  doi = {10.1145/3110275},
  abstract = {We propose a core semantics for Dependent Haskell, an extension of Haskell with full-spectrum dependent types. Our semantics consists of two related languages. The first is a Curry-style dependently-typed language with nontermination, irrelevant arguments, and equality abstraction. The second, inspired by the Glasgow Haskell Compiler's core language FC, is its explicitly-typed analogue, suitable for implementation in GHC. All of our results---chiefly, type safety, along with theorems that relate these two languages---have been formalized using the Coq proof assistant. Because our work is backwards compatible with Haskell, our type safety proof holds in the presence of nonterminating computation. However, unlike other full-spectrum dependently-typed languages, such as Coq, Agda or Idris, because of this nontermination, Haskell's term language does not correspond to a consistent logic.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {31:1--31:29},
  keywords = {Haskell,dependent types},
  author = {Weirich, Stephanie and Voizard, Antoine and de Amorim, Pedro Henrique Avezedo and Eisenberg, Richard A.},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/FJSJ5PCG/Weirich et al - 2017 - A Specification for Dependent Types in Haskell.pdf}
}
% == BibLateX quality report for Weirich2017Specification:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Ahmed2017Theorems,
  title = {Theorems for Free for Free: Parametricity, with and Without Types},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110283},
  doi = {10.1145/3110283},
  shorttitle = {Theorems for Free for Free},
  abstract = {The polymorphic blame calculus integrates static typing, including universal types, with dynamic typing. The primary challenge with this integration is preserving parametricity: even dynamically-typed code should satisfy it once it has been cast to a universal type. Ahmed et al. (2011) employ runtime type generation in the polymorphic blame calculus to preserve parametricity, but a proof that it does so has been elusive. Matthews and Ahmed (2008) gave a proof of parametricity for a closely related system that combines ML and Scheme, but later found a flaw in their proof. In this paper we present an improved version of the polymorphic blame calculus and we prove that it satisfies relational parametricity. The proof relies on a step-indexed Kripke logical relation. The step-indexing is required to make the logical relation well-defined in the case for the dynamic type. The possible worlds include the mapping of generated type names to their types and the mapping of type names to relations. We prove the Fundamental Property of this logical relation and that it is sound with respect to contextual equivalence. To demonstrate the utility of parametricity in the polymorphic blame calculus, we derive two free theorems.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {39:1--39:28},
  keywords = {dynamic typing,gradual typing,parametricity,logical relation},
  author = {Ahmed, Amal and Jamner, Dustin and Siek, Jeremy G. and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/DHM3JF77/Ahmed et al - 2017 - Theorems for Free for Free - Parametricity, with and Without Types.pdf}
}
% == BibLateX quality report for Ahmed2017Theorems:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Nuyts2017Parametric,
  title = {Parametric Quantifiers for Dependent Type Theory},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110276},
  doi = {10.1145/3110276},
  abstract = {Polymorphic type systems such as System F enjoy the parametricity property: polymorphic functions cannot inspect their type argument and will therefore apply the same algorithm to any type they are instantiated on. This idea is formalized mathematically in Reynoldsâs theory of relational parametricity, which allows the metatheoretical derivation of parametricity theorems about all values of a given type. Although predicative System F embeds into dependent type systems such as Martin-LÃ¶f Type Theory (MLTT), parametricity does not carry over as easily. The identity extension lemma, which is crucial if we want to prove theorems involving equality, has only been shown to hold for small types, excluding the universe. We attribute this to the fact that MLTT uses a single type former Î to generalize both the parametric quantifier â and the type former â which is non-parametric in the sense that its elements may use their argument as a value. We equip MLTT with parametric quantifiers â and â alongside the existing Î and Î£, and provide relation type formers for proving parametricity theorems internally. We show internally the existence of initial algebras and final co-algebras of indexed functors both by Church encoding and, for a large class of functors, by using sized types. We prove soundness of our type system by enhancing existing iterated reflexive graph (cubical set) models of dependently typed parametricity by distinguishing between edges that express relatedness of objects (bridges) and edges that express equality (paths). The parametric functions are those that map bridges to paths. We implement an extension to the Agda proof assistant that type-checks proofs in our type system.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {32:1--32:29},
  keywords = {Agda,parametricity,cubical type theory,presheaf semantics,sized types},
  author = {Nuyts, Andreas and Vezzosi, Andrea and Devriese, Dominique},
  file = {/Users/pgiarrusso/Zotero/storage/RRD3P7DN/Nuyts et al - 2017 - Parametric Quantifiers for Dependent Type Theory.pdf}
}
% == BibLateX quality report for Nuyts2017Parametric:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Davis2017Nobrainer,
  title = {No-brainer CPS Conversion (Functional Pearl)},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110267},
  doi = {10.1145/3110267},
  abstract = {Algorithms that convert direct-style Î»-calculus terms to their equivalent terms in continuation-passing style (CPS) typically introduce so-called âadministrative redexes:â useless artifacts of the conversion that must be cleaned up by a subsequent pass over the result to reduce them away. We present a simple, linear-time algorithm for CPS conversion that introduces no administrative redexes. In fact, the output term is a normal form in a reduction system that generalizes the notion of âadministrative redexesâ to what we call âno-brainer redexes,â that is, redexes whose reduction shrinks the size of the term. We state the theorems which establish the algorithmâs desireable properties, along with sketches of the full proofs.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {23:1--23:25},
  keywords = {lambda calculus,CPS,Continuation-passing Style,compiler,continuation,functional language},
  author = {Davis, Milo and Meehan, William and Shivers, Olin},
  file = {/Users/pgiarrusso/Zotero/storage/5NIDZ8MR/Davis et al - 2017 - No-brainer CPS Conversion (Functional Pearl).pdf}
}
% == BibLateX quality report for Davis2017Nobrainer:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Abel2017Normalization,
  title = {Normalization by Evaluation for Sized Dependent Types},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110277},
  doi = {10.1145/3110277},
  abstract = {Sized types have been developed to make termination checking more perspicuous, more powerful, and more modular by integrating termination into type checking. In dependently-typed proof assistants where proofs by induction are just recursive functional programs, the termination checker is an integral component of the trusted core, as validity of proofs depend on termination. However, a rigorous integration of full-fledged sized types into dependent type theory is lacking so far. Such an integration is non-trivial, as explicit sizes in proof terms might get in the way of equality checking, making terms appear distinct that should have the same semantics. In this article, we integrate dependent types and sized types with higher-rank size polymorphism, which is essential for generic programming and abstraction. We introduce a size quantifier â which lets us ignore sizes in terms for equality checking, alongside with a second quantifier Î for abstracting over sizes that do affect the semantics of types and terms. Judgmental equality is decided by an adaptation of normalization-by-evaluation for our new type theory, which features type shape-directed reflection and reification. It follows that subtyping and type checking of normal forms are decidable as well, the latter by a bidirectional algorithm.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {33:1--33:30},
  keywords = {universes,dependent types,subtyping,sized types,eta-equality,normalization-by-evaluation,proof irrelevance},
  author = {Abel, Andreas and Vezzosi, Andrea and Winterhalter, Theo},
  file = {/Users/pgiarrusso/Zotero/storage/E5NA3QNB/Abel et al - 2017 - Normalization by Evaluation for Sized Dependent Types.pdf}
}
% == BibLateX quality report for Abel2017Normalization:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Aguirre2017Relational,
  title = {A Relational Logic for Higher-order Programs},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110265},
  doi = {10.1145/3110265},
  abstract = {Relational program verification is a variant of program verification where one can reason about two programs and as a special case about two executions of a single program on different inputs. Relational program verification can be used for reasoning about a broad range of properties, including equivalence and refinement, and specialized notions such as continuity, information flow security or relative cost. In a higher-order setting, relational program verification can be achieved using relational refinement type systems, a form of refinement types where assertions have a relational interpretation. Relational refinement type systems excel at relating structurally equivalent terms but provide limited support for relating terms with very different structures. We present a logic, called Relational Higher Order Logic (RHOL), for proving relational properties of a simply typed Î»-calculus with inductive types and recursive definitions. RHOL retains the type-directed flavour of relational refinement type systems but achieves greater expressivity through rules which simultaneously reason about the two terms as well as rules which only contemplate one of the two terms. We show that RHOL has strong foundations, by proving an equivalence with higher-order logic (HOL), and leverage this equivalence to derive key meta-theoretical properties: subject reduction, admissibility of a transitivity rule and set-theoretical soundness. Moreover, we define sound embeddings for several existing relational type systems such as relational refinement types and type systems for dependency analysis and relative cost, and we verify examples that were out of reach of prior work.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {21:1--21:29},
  keywords = {refinement types,Formal Verification,Relational Logic},
  author = {Aguirre, Alejandro and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Strub, Pierre-Yves},
  file = {/Users/pgiarrusso/Zotero/storage/NJE56Z38/Aguirre et al - 2017 - A Relational Logic for Higher-order Programs.pdf}
}
% == BibLateX quality report for Aguirre2017Relational:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Forster2017Expressive,
  title = {On the Expressive Power of User-defined Effects: Effect Handlers, Monadic Reflection, Delimited Control},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110257},
  doi = {10.1145/3110257},
  shorttitle = {On the Expressive Power of User-defined Effects},
  abstract = {We compare the expressive power of three programming abstractions for user-defined computational effects: Plotkin and Pretnar's effect handlers, Filinski's monadic reflection, and delimited control without answer-type-modification. This comparison allows a precise discussion about the relative expressiveness of each programming abstraction. It also demonstrates the sensitivity of the relative expressiveness of user-defined effects to seemingly orthogonal language features.   We present three calculi, one per abstraction, extending Levy's call-by-push-value. For each calculus, we present syntax, operational semantics, a natural type-and-effect system, and, for effect handlers and monadic reflection, a set-theoretic denotational semantics. We establish their basic metatheoretic properties: safety, termination, and, where applicable, soundness and adequacy. Using Felleisen's notion of a macro translation, we show that these abstractions can macro-express each other, and show which translations preserve typeability. We use the adequate finitary set-theoretic denotational semantics for the monadic calculus to show that effect handlers cannot be macro-expressed while preserving typeability either by monadic reflection or by delimited control. Our argument fails with simple changes to the type system such as polymorphism and inductive types. We supplement our development with a mechanised Abella formalisation.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {13:1--13:29},
  keywords = {Monads,lambda calculus,computational effects,Denotational semantics,call-by-push-value,language extension,type-and-effect systems,algebraic effects and handlers,delimited control,macro expressiveness,monadic reflection,reify and reflect,shift and reset},
  author = {Forster, Yannick and Kammar, Ohad and Lindley, Sam and Pretnar, Matija},
  file = {/Users/pgiarrusso/Zotero/storage/ZG8ZCXIN/Forster et al - 2017 - On the Expressive Power of User-defined Effects - Effect Handlers, Monadic Reflection, Delimited Control.pdf}
}
% == BibLateX quality report for Forster2017Expressive:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Balabonski2017Foundations,
  title = {Foundations of Strong Call by Need},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3110264},
  doi = {10.1145/3110264},
  abstract = {We present a call-by-need strategy for computing strong normal forms of open terms (reduction is admitted inside the body of abstractions and substitutions, and the terms may contain free variables), which guarantees that arguments are only evaluated when needed and at most once. The strategy is shown to be complete with respect to Î²-reduction to strong normal form. The proof of completeness relies on two key tools: (1) the definition of a strong call-by-need calculus where reduction may be performed inside any context, and (2) the use of non-idempotent intersection types. More precisely, terms admitting a Î²-normal form in pure lambda calculus are typable, typability implies (weak) normalisation in the strong call-by-need calculus, and weak normalisation in the strong call-by-need calculus implies normalisation in the strong call-by-need strategy. Our (strong) call-by-need strategy is also shown to be conservative over the standard (weak) call-by-need.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  date = {2017-08},
  pages = {20:1--20:29},
  keywords = {Call-by-need,Completeness,Evaluation Strategies},
  author = {Balabonski, Thibaut and Barenbaum, Pablo and Bonelli, Eduardo and Kesner, Delia},
  file = {/Users/pgiarrusso/Zotero/storage/F82FA7RH/Balabonski et al - 2017 - Foundations of Strong Call by Need.pdf}
}
% == BibLateX quality report for Balabonski2017Foundations:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@online{SubtypingRelation,
  title = {On Subtyping-Relation Completeness, with an Application to Iso-Recursive Types},
  url = {https://www.researchgate.net/publication/314306031_On_Subtyping-Relation_Completeness_with_an_Application_to_Iso-Recursive_Types},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  journaltitle = {ResearchGate},
  urldate = {2017-08-22},
  file = {/Users/pgiarrusso/Zotero/storage/3S6BDM5A/314306031_On_Subtyping-Relation_Completeness_with_an_Application_to_Iso-Recursive_Types.html}
}
% == BibLateX quality report for SubtypingRelation:
% Unexpected field 'journaltitle'
% Exactly one of 'date' / 'year' must be present

@article{Fluet2006Phantom,
  title = {Phantom types and subtyping},
  volume = {16},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/phantom-types-and-subtyping/08E1C18BA8C61F0EDF70EFD4051604E5},
  doi = {10.1017/S0956796806006046},
  abstract = {We investigate a technique from the literature, called the phantom-types technique, that uses parametric polymorphism, type constraints, and unification of polymorphic types to model a subtyping hierarchy. Hindley-Milner type systems, such as the one found in Standard ML, can be used to enforce the subtyping relation, at least for first-order values. We show that this technique can be used to encode any finite subtyping hierarchy (including hierarchies arising from multiple interface inheritance). We formally demonstrate the suitability of the phantom-types technique for capturing first-order subtyping by exhibiting a type-preserving translation from a simple calculus with bounded polymorphism to a calculus embodying the type system of SML.},
  number = {6},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-08-20},
  date = {2006-11},
  pages = {751-791},
  author = {Fluet, Matthew and Pucella, Riccardo},
  file = {/Users/pgiarrusso/Zotero/storage/KGF3CWBJ/Fluet_Pucella - 2006 - Phantom types and subtyping.pdf;/Users/pgiarrusso/Zotero/storage/2JKF87C9/08E1C18BA8C61F0EDF70EFD4051604E5.html}
}
% == BibLateX quality report for Fluet2006Phantom:
% 'issn': not a valid ISSN

@article{Kiselyov2007Lightweight,
  title = {Lightweight Static Capabilities},
  volume = {174},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S157106610700254X},
  doi = {10.1016/j.entcs.2006.10.039},
  abstract = {We describe a modular programming style that harnesses modern type systems to verify safety conditions in practical systems. This style has three ingredients:(i)A compact kernel of trust that is specific to the problem domain.(ii)Unique names (capabilities) that confer rights and certify properties, so as to extend the trust from the kernel to the rest of the application.(iii)Static (type) proxies for dynamic values. We illustrate our approach using examples from the dependent-type literature, but our programs are written in Haskell and OCaml today, so our techniques are compatible with imperative code, native mutable arrays, and general recursion. The three ingredients of this programming style call for (1) an expressive core language, (2) higher-rank polymorphism, and (3) phantom types.},
  number = {7},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the Programming Languages meets Program Verification (PLPV 2006)},
  urldate = {2017-08-20},
  date = {2007-06-04},
  pages = {79-104},
  keywords = {Verification,Modular programming,safety property,static types},
  author = {Kiselyov, Oleg and Shan, Chung-chieh},
  file = {/Users/pgiarrusso/Zotero/storage/4I5MCD64/Kiselyov_Shan - 2007 - Lightweight Static Capabilities.pdf;/Users/pgiarrusso/Zotero/storage/5T2WEZBC/S157106610700254X.html}
}

@article{Odersky1999Type,
  title = {Type Inference with Constrained Types},
  volume = {5},
  issn = {1074-3227},
  url = {http://dx.doi.org/10.1002/(SICI)1096-9942(199901/03)5:1<35::AID-TAPO4>3.0.CO;2-4},
  doi = {10.1002/(SICI)1096-9942(199901/03)5:1<35::AID-TAPO4>3.0.CO;2-4},
  number = {1},
  journaltitle = {Theor. Pract. Object Syst.},
  urldate = {2017-08-19},
  date = {1999-01},
  pages = {35--55},
  author = {Odersky, Martin and Sulzmann, Martin and Wehr, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/KFMJBRX3/Odersky et al - 1999 - Type Inference with Constrained Types.pdf}
}
% == BibLateX quality report for Odersky1999Type:
% ? Possibly abbreviated journal title Theor. Pract. Object Syst.

@inproceedings{Jones1992theory,
  langid = {english},
  title = {A theory of qualified types},
  isbn = {10.1007/3-540-55253-7\_17},
  url = {https://link.springer.com/chapter/10.1007/3-540-55253-7_17},
  doi = {10.1007/3-540-55253-7_17},
  abstract = {No Abstract available for this paper.},
  eventtitle = {European Symposium on Programming},
  booktitle = {ESOP '92},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-08-19},
  date = {1992-02-26},
  pages = {287-306},
  author = {Jones, Mark P.},
  file = {/Users/pgiarrusso/Zotero/storage/P76TTT28/Jones - 1992 - A theory of qualified types.pdf;/Users/pgiarrusso/Zotero/storage/5UX22VXW/10.html}
}
% == BibLateX quality report for Jones1992theory:
% 'isbn': not a valid ISBN

@inproceedings{Gonthier2011how,
  location = {{New York, NY, USA}},
  title = {How to Make Ad Hoc Proof Automation Less Ad Hoc},
  isbn = {978-1-4503-0865-6},
  url = {http://doi.acm.org/10.1145/2034773.2034798},
  doi = {10.1145/2034773.2034798},
  abstract = {Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself. We propose a novel approach to proof automation in Coq that allows the user to specify the behavior of custom automated routines in terms of Coq's own type system. Our approach involves a sophisticated application of Coq's canonical structures, which generalize Haskell type classes and facilitate a flexible style of dependently-typed logic programming. Specifically, just as Haskell type classes are used to infer the canonical implementation of an overloaded term at a given type, canonical structures can be used to infer the canonical proof of an overloaded lemma for a given instantiation of its parameters. We present a series of design patterns for canonical structure programming that enable one to carefully and predictably coax Coq's type inference engine into triggering the execution of user-supplied algorithms during unification, and we illustrate these patterns through several realistic examples drawn from Hoare Type Theory. We assume no prior knowledge of Coq and describe the relevant aspects of Coq type inference from first principles.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '11},
  publisher = {{ACM}},
  urldate = {2014-06-14},
  date = {2011},
  pages = {163--175},
  keywords = {coq,interactive theorem proving,type classes,custom proof automation,tactics,canonical structures,hoare type theory},
  author = {Gonthier, Georges and Ziliani, Beta and Nanevski, Aleksandar and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/XI224RCC/Gonthier et al - 2011 - How to Make Ad Hoc Proof Automation Less Ad Hoc.pdf}
}

@inproceedings{Lieberman1986Using,
  location = {{New York, NY, USA}},
  title = {Using Prototypical Objects to Implement Shared Behavior in Object-oriented Systems},
  isbn = {0-89791-204-7},
  url = {http://doi.acm.org/10.1145/28697.28718},
  doi = {10.1145/28697.28718},
  abstract = {A traditional philosophical controversy between representing general concepts as abstract sets or classes and representing concepts as concrete prototypes is reflected in a controversy between two mechanisms for sharing behavior between objects in object oriented programming languages. Inheritance splits the object world into classes, which encode behavior shared among a group of instances, which represent individual members of these sets. The class/instance distinction is not needed if the alternative of using prototypes is adopted. A prototype represents the default behavior for a concept, and new objects can re-use part of the knowledge stored in the prototype by saying how the new object differs from the prototype. The prototype approach seems to hold some advantages for representing default knowledge, and incrementally and dynamically modifying concepts. Delegation is the mechanism for implementing this in object oriented languages. After checking its idiosyncratic behavior, an object can forward a message to prototypes to invoke more general knowledge. Because class objects must be created before their instances can be used, and behavior can only be associated with classes, inheritance fixes the communication patterns between objects at instance creation time. Because any object can be used as a prototype, and any messages can be forwarded at any time, delegation is the more flexible and general of the two techniques.},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages and Applications},
  series = {OOPSLA '86},
  publisher = {{ACM}},
  urldate = {2017-08-12},
  date = {1986},
  pages = {214--223},
  keywords = {_tablet},
  author = {Lieberman, Henry},
  file = {/Users/pgiarrusso/Zotero/storage/H25DAJWQ/Lieberman - 1986 - Using Prototypical Objects to Implement Shared Behavior in Object-oriented Systems.pdf}
}

@inproceedings{Ostermann2001Objectoriented,
  location = {{New York, NY, USA}},
  title = {Object-oriented Composition Untangled},
  isbn = {1-58113-335-9},
  url = {http://doi.acm.org/10.1145/504282.504303},
  doi = {10.1145/504282.504303},
  abstract = {Object-oriented languages come with pre-defined composition mechansims, such as inheritance, object composition, or delegation, each characterized by a certain set of composition properties, which do not themselves individually exist as abstractions at the language level. However, often non-standard composition semantics is needed, with a mixture of composition mechanisms. Such non-standard semantics are simulated by complicated architectures that are sensitive to requirement changes and cannot easily be adapted without invalidating existing clients. In this paper, we propose compound references, a new abstraction for object references, that allows us to provide explicit linguistic means for expressing and combining individual composition properties on-demand. The model is statically typed and allows the programmer to express a seamless spectrum of composition semantics in the interval between object composition and inheritance. The resulting programs are better understandable, due to explicity expressed design decisions, and less sensitive to requirement changes.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA '01},
  publisher = {{ACM}},
  urldate = {2017-08-12},
  date = {2001},
  pages = {283--299},
  keywords = {_tablet},
  author = {Ostermann, Klaus and Mezini, Mira},
  file = {/Users/pgiarrusso/Zotero/storage/IX5IXU57/Ostermann-Mezini - 2001 - Object-oriented Composition Untangled.pdf}
}

@inproceedings{Stein1987Delegation,
  location = {{New York, NY, USA}},
  title = {Delegation is Inheritance},
  isbn = {0-89791-247-0},
  url = {http://doi.acm.org/10.1145/38765.38820},
  doi = {10.1145/38765.38820},
  abstract = {Inheritance and delegation are alternate methods for incremental definition and sharing. It has commonly been believed that delegation provides a more powerful model. This paper demonstrates that there is a “natural” model of inheritance which captures all of the properties of delegation. Independently, certain constraints on the ability of delegation to capture inheritance are demonstrated. Finally, a new framework which fully captures both delegation and inheritance is outlined, and some of the ramifications of this hybrid model are explored.},
  booktitle = {Conference Proceedings on Object-oriented Programming Systems, Languages and Applications},
  series = {OOPSLA '87},
  publisher = {{ACM}},
  urldate = {2017-08-12},
  date = {1987},
  pages = {138--146},
  keywords = {_tablet},
  author = {Stein, Lynn Andrea},
  file = {/Users/pgiarrusso/Zotero/storage/5IBV5745/Stein - 1987 - Delegation is Inheritance.pdf}
}

@inproceedings{Kniesel1999TypeSafe,
  langid = {english},
  title = {Type-Safe Delegation for Run-Time Component Adaptation},
  isbn = {10.1007/3-540-48743-3\_16},
  url = {https://link.springer.com/chapter/10.1007/3-540-48743-3_16},
  doi = {10.1007/3-540-48743-3_16},
  abstract = {The aim of component technology is the replacement of large monolithic applications with sets of smaller software components, whose particular functionality and interoperation can be adapted to users’ needs. However, the adaptation mechanisms of component software are still limited. Most proposals concentrate on adaptations that can be achieved either at compile time or at link time. Current support for dynamic component adaptation, i.e. unanticipated, incremental modifications of a component system at run-time, is not sufficient.This paper proposes object-based inheritance (also known as delegation) as a complement to purely forwarding-based object composition. It presents a typesafe integration of delegation into a class-based object model and shows how it overcomes the problems faced by forwarding-based component interaction, how it supports independent extensibility of components and unanticipated, dynamic component adaptation.},
  eventtitle = {European Conference on Object-Oriented Programming},
  booktitle = {ECOOP’ 99 — Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-08-12},
  date = {1999-06-14},
  pages = {351-366},
  keywords = {_tablet},
  author = {Kniesel, Günter},
  file = {/Users/pgiarrusso/Zotero/storage/IPBVWW4V/Kniesel - 1999 - Type-Safe Delegation for Run-Time Component Adaptation.pdf;/Users/pgiarrusso/Zotero/storage/PE5GNBMJ/3-540-48743-3_16.html}
}
% == BibLateX quality report for Kniesel1999TypeSafe:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Leduc2017Revisiting,
  langid = {english},
  title = {Revisiting Visitors for Modular Extension of Executable DSMLs},
  url = {https://hal.inria.fr/hal-01568169/document},
  abstract = {Executable Domain-Specific Modeling Languages (xDSMLs) are typically defined by metamodels that specify their abstract syntax, and model interpreters or compilers that define their execution semantics. To face the proliferation of xDSMLs in many domains, it is important to provide language engineering facilities for opportunistic reuse, extension, and customization of existing xDSMLs to ease the definition of new ones. Current approaches to language reuse either require to anticipate reuse, make use of advanced features that are not widely available in programming languages, or are not directly applicable to metamodel-based xDSMLs. In this paper, we propose a new language implementation pattern, named REVISITOR, that enables independent extensibility of the syntax and semantics of metamodel-based xDSMLs with incremental compilation and without anticipation. We seamlessly implement our approach alongside the compilation chain of the Eclipse Modeling Framework, thereby demonstrating that it is directly and broadly applicable in various modeling environments. We show how it can be employed to incrementally extend both the syntax and semantics of the fUML language without requiring anticipation or re-compilation of existing code, and with acceptable performance penalty compared to classical handmade visitors.},
  eventtitle = {ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems},
  urldate = {2017-08-12},
  date = {2017-09-17},
  author = {Leduc, Manuel and Degueule, Thomas and Combemale, Benoit and Storm, Tijs Van Der and Barais, Olivier},
  file = {/Users/pgiarrusso/Zotero/storage/G9SIR57J/Leduc et al - 2017 - Revisiting Visitors for Modular Extension of Executable DSMLs.pdf;/Users/pgiarrusso/Zotero/storage/DTPS8A89/hal-01568169.html}
}
% == BibLateX quality report for Leduc2017Revisiting:
% Missing required field 'booktitle'

@inproceedings{Cai2016System,
  title = {System F-omega with Equirecursive Types for Datatype-generic Programming},
  volume = {2016},
  url = {http://dl.acm.org/citation.cfm?id=2837660},
  booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2016},
  keywords = {Datatype-generic programming,functors,equirecursive types},
  author = {Cai, Yufei and Giarrusso, Paolo G. and Ostermann, Klaus},
  file = {/Users/pgiarrusso/Zotero/storage/U7VFZDQP/Cai et al - 2016 - System F-omega with Equirecursive Types for Datatype-generic Programming.pdf;/Users/pgiarrusso/Zotero/storage/XEHQ35DZ/citation.html}
}

@article{Knuth1981Breaking,
  langid = {english},
  title = {Breaking paragraphs into lines},
  volume = {11},
  issn = {1097-024X},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380111102/abstract},
  doi = {10.1002/spe.4380111102},
  abstract = {This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called ‘boxes’, ‘glue’, and ‘penalties’ provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.},
  number = {11},
  journaltitle = {Software: Practice and Experience},
  shortjournal = {Softw: Pract. Exper.},
  urldate = {2017-08-11},
  date = {1981-11-01},
  pages = {1119-1184},
  keywords = {Dynamic programming,Box/glue/penalty algebra,Composition,History of printing,Justification,Layout,Line breaking,Shortest paths,Spacing,TEX (Tau Epsilon Chi),Typesetting,Word processing},
  author = {Knuth, Donald E. and Plass, Michael F.},
  file = {/Users/pgiarrusso/Zotero/storage/SPWD33IS/Knuth and Plass - 1981 - Breaking paragraphs into lines.html}
}

@inproceedings{Grust2009FERRY,
  location = {{New York, NY, USA}},
  title = {FERRY: Database-supported Program Execution},
  isbn = {978-1-60558-551-2},
  url = {http://doi.acm.org/10.1145/1559845.1559982},
  doi = {10.1145/1559845.1559982},
  shorttitle = {FERRY},
  abstract = {We demonstrate the language Ferry and its editing, compilation, and execution environment FerryDeck. Ferry's type system and operations match those of scripting or programming languages; its compiler has been designed to emit (bundles of) compliant and efficient SQL:1999 statements. Ferry acts as glue that permits a programming style in which developers access database tables using their programming language's own syntax and idioms -- the Ferry-expressible fragments of such programs may be executed by a relational database back-end, i.e., close to the data. The demonstrator FerryDeck implements compile-and-execute-as-you-type interactivity for Ferry and offers a variety of (graphical) hooks to explore and inspect this approach to database-supported program execution.},
  booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '09},
  publisher = {{ACM}},
  urldate = {2017-07-28},
  date = {2009},
  pages = {1063--1066},
  keywords = {LINQ,ferry,pathfinder,sql:1999},
  author = {Grust, Torsten and Mayr, Manuel and Rittinger, Jan and Schreiber, Tom},
  file = {/Users/pgiarrusso/Zotero/storage/P4M26MK6/Grust et al - 2009 - FERRY - Database-supported Program Execution.pdf}
}

@inproceedings{Acar2017Brief,
  location = {{New York, NY, USA}},
  title = {Brief Announcement: Parallel Dynamic Tree Contraction via Self-Adjusting Computation},
  isbn = {978-1-4503-4593-4},
  url = {http://doi.acm.org/10.1145/3087556.3087595},
  doi = {10.1145/3087556.3087595},
  shorttitle = {Brief Announcement},
  abstract = {Dynamic algorithms are used to compute a property of some data while the data undergoes changes over time. Many dynamic algorithms have been proposed but nearly all are sequential. In this paper, we present our ongoing work on designing a parallel algorithm for the dynamic trees problem, which requires computing a property of a forest as the forest undergoes changes. Our algorithm allows insertion and/or deletion of both vertices and edges anywhere in the input and performs updates in parallel. We obtain our algorithm by applying a dynamization technique called self-adjusting computation to the classic algorithm of Miller and Reif for tree contraction.},
  booktitle = {Proceedings of the 29th ACM Symposium on Parallelism in Algorithms and Architectures},
  series = {SPAA '17},
  publisher = {{ACM}},
  urldate = {2017-07-28},
  date = {2017},
  pages = {275--277},
  keywords = {change propagation,self-adjusting computation,dynamic,parallel,tree contraction},
  author = {Acar, Umut A. and Aksenov, Vitaly and Westrick, Sam},
  file = {/Users/pgiarrusso/Zotero/storage/C8EF9ABT/Acar et al - 2017 - Brief Announcement - Parallel Dynamic Tree Contraction via Self-Adjusting Computation.pdf}
}

@inproceedings{Cheney2013Practical,
  location = {{New York, NY, USA}},
  title = {A Practical Theory of Language-integrated Query},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500586},
  doi = {10.1145/2500365.2500586},
  abstract = {Language-integrated query is receiving renewed attention, in part because of its support through Microsoft's LINQ framework. We present a practical theory of language-integrated query based on quotation and normalisation of quoted terms. Our technique supports join queries, abstraction over values and predicates, composition of queries, dynamic generation of queries, and queries with nested intermediate data. Higher-order features prove useful even for constructing first-order queries. We prove a theorem characterising when a host query is guaranteed to generate a single SQL query. We present experimental results confirming our technique works, even in situations where Microsoft's LINQ framework either fails to produce an SQL query or, in one case, produces an avalanche of SQL queries.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2017-07-28},
  date = {2013},
  pages = {403--416},
  keywords = {sql,lambda calculus,quotation,LINQ,antiquotation,f\#},
  author = {Cheney, James and Lindley, Sam and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/BRG3AF8D/Cheney et al - 2013 - A Practical Theory of Language-integrated Query.pdf}
}

@article{Delaware2013Feature,
  langid = {american},
  title = {Feature modularity in mechanized reasoning},
  url = {https://repositories.lib.utexas.edu/handle/2152/22867},
  abstract = {Complex systems are naturally understood as combinations of their
distinguishing characteristics or $\backslash$definit\{features\}. Distinct features
differentiate between variations of configurable systems and also
identify the novelties of extensions. The implementation of a
conceptual feature is often scattered throughout an artifact, forcing
designers to understand the entire artifact in order to reason about
the behavior of a single feature. It is particularly challenging to
independently develop novel extensions to complex systems as a
result.

This dissertation shows how to modularly reason about the
implementation of conceptual features in both the formalizations of
programming languages and object-oriented software product lines. In
both domains, modular verification of features can be leveraged to
reason about the behavior of artifacts in which they are included:
fully mechanized metatheory proofs for programming languages can be
synthesized from independently developed proofs, and programs built
from well-formed feature modules are guaranteed to be well-formed
without needing to be typechecked. Modular reasoning about individual
features can furthermore be used to efficiently reason about families
of languages and programs which share a common set of features.},
  urldate = {2017-07-27},
  date = {2013-12},
  author = {Delaware, Benjamin James},
  file = {/Users/pgiarrusso/Zotero/storage/GKRHJUHU/Delaware - 2013 - Feature modularity in mechanized reasoning.pdf;/Users/pgiarrusso/Zotero/storage/RTBI6VG8/22867.html}
}
% == BibLateX quality report for Delaware2013Feature:
% Missing required field 'journaltitle'

@article{Grust2013FirstClass,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.0158},
  primaryClass = {cs},
  title = {First-Class Functions for First-Order Database Engines},
  url = {http://arxiv.org/abs/1308.0158},
  abstract = {We describe Query Defunctionalization which enables off-the-shelf first-order database engines to process queries over first-class functions. Support for first-class functions is characterized by the ability to treat functions like regular data items that can be constructed at query runtime, passed to or returned from other (higher-order) functions, assigned to variables, and stored in persistent data structures. Query defunctionalization is a non-invasive approach that transforms such function-centric queries into the data-centric operations implemented by common query processors. Experiments with XQuery and PL/SQL database systems demonstrate that first-order database engines can faithfully and efficiently support the expressive "functions as data" paradigm.},
  urldate = {2017-07-25},
  date = {2013-08-01},
  keywords = {Computer Science - Programming Languages,Computer Science - Databases,D.3.2,H.2.3},
  author = {Grust, Torsten and Ulrich, Alexander},
  file = {/Users/pgiarrusso/Zotero/storage/FR43VM4J/Grust_Ulrich - 2013 - First-Class Functions for First-Order Database Engines.pdf;/Users/pgiarrusso/Zotero/storage/7ZS86EKV/1308.html}
}
% == BibLateX quality report for Grust2013FirstClass:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Grust2013Functions,
  title = {Functions Are Data Too: Defunctionalization for PL/SQL},
  volume = {6},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/2536274.2536279},
  doi = {10.14778/2536274.2536279},
  shorttitle = {Functions Are Data Too},
  abstract = {We demonstrate a full-fledged implementation of first-class functions for the widely used PL/SQL database programming language. Functions are treated as regular data items that may be (1) constructed at query runtime, (2) stored in and retrieved from tables, (3) assigned to variables, and (4) passed to and from other (higher-order) functions. The resulting PL/SQL dialect concisely and elegantly expresses a wide range of new query idioms which would be cumbersome to formulate if functions remained second-class citizens. We include a diverse set of application scenarios that make these advantages tangible. First-class PL/SQL functions require featherweight syntactic extensions only and come with a non-invasive implementation-- the defunctionalization transformation--that can entirely be built on top of existing relational DBMS infrastructure. An interactive demonstrator helps users to experiment with the "function as data" paradigm and to earn a solid intuition of its inner workings.},
  number = {12},
  journaltitle = {Proc. VLDB Endow.},
  urldate = {2017-07-25},
  date = {2013-08},
  pages = {1214--1217},
  author = {Grust, Torsten and Schweinsberg, Nils and Ulrich, Alexander},
  file = {/Users/pgiarrusso/Zotero/storage/GP743KJM/Grust et al - 2013 - Functions Are Data Too - Defunctionalization for PL-SQL.pdf}
}
% == BibLateX quality report for Grust2013Functions:
% ? Possibly abbreviated journal title Proc. VLDB Endow.

@book{Barendregt1984Lambda,
  title = {The Lambda Calculus: Its Syntax and Semantics},
  shorttitle = {The Lambda Calculus},
  publisher = {{Elsevier}},
  date = {1984},
  author = {Barendregt, H. P.}
}

@article{Schwinghammer2009Stepindexed,
  langid = {english},
  title = {A Step-indexed Semantics of Imperative Objects},
  volume = {Volume 5, Issue 4},
  url = {https://lmcs.episciences.org/744/pdf},
  abstract = {Step-indexed semantic interpretations of types were proposed as an alternative to purely syntactic proofs of type safety using subject reduction. The types are interpreted as sets of values indexed by the number of computation steps for which these values are guaranteed to behave like proper elements of the type. Building on work by Ahmed, Appel and others, we introduce a step-indexed semantics for the imperative object calculus of Abadi and Cardelli. Providing a semantic account of this calculus using more `traditional', domain-theoretic approaches has proved challenging due to the combination of dynamically allocated objects, higher-order store, and an expressive type system. Here we show that, using step-indexing, one can interpret a rich type discipline with object types, subtyping, recursive and bounded quantified types in the presence of state.},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2017-07-18},
  date = {2009-12-18},
  keywords = {_tablet},
  author = {Schwinghammer, Jan and Hritcu, Catalin},
  file = {/Users/pgiarrusso/Zotero/storage/MBKT7DT6/Schwinghammer-Hritcu - 2009 - A Step-indexed Semantics of Imperative Objects.pdf;/Users/pgiarrusso/Zotero/storage/JNVJF5RH/744.html}
}

@article{Odersky2017Simplicitly,
  title = {Simplicitly},
  url = {https://infoscience.epfl.ch/record/229878},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over. This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler. To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  urldate = {2017-07-16},
  date = {2017},
  author = {Odersky, Martin and Biboudis, Aggelos and Liu, Fengyun and Blanvillain, Olivier and Miller, Heather},
  file = {/Users/pgiarrusso/Zotero/storage/Q9R8CIEI/Odersky et al - 2017 - Simplicitly.pdf;/Users/pgiarrusso/Zotero/storage/74KWN3H4/229878.html}
}
% == BibLateX quality report for Odersky2017Simplicitly:
% Missing required field 'journaltitle'

@article{Lupei2017SLeNDer,
  title = {SLeNDer: Query Compilation for Agile Collection Processing},
  url = {https://infoscience.epfl.ch/record/229427/},
  shorttitle = {SLeNDer},
  urldate = {2017-07-11},
  date = {2017},
  author = {Lupei, Daniel and Nikolic, Milos and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/CVSW95UW/Lupei et al. - 2017 - SLeNDer Query Compilation for Agile Collection Pr.html}
}
% == BibLateX quality report for Lupei2017SLeNDer:
% Missing required field 'journaltitle'

@inproceedings{Chen2014Functional,
  location = {{New York, NY, USA}},
  title = {Functional Programming for Dynamic and Large Data with Self-adjusting Computation},
  isbn = {978-1-4503-2873-9},
  url = {http://doi.acm.org/10.1145/2628136.2628150},
  doi = {10.1145/2628136.2628150},
  abstract = {Combining type theory, language design, and empirical work, we present techniques for computing with large and dynamically changing datasets. Based on lambda calculus, our techniques are suitable for expressing a diverse set of algorithms on large datasets and, via self-adjusting computation, enable computations to respond automatically to changes in their data. To improve the scalability of self-adjusting computation, we present a type system for precise dependency tracking that minimizes the time and space for storing dependency metadata. The type system eliminates an important assumption of prior work that can lead to recording spurious dependencies. We present a type-directed translation algorithm that generates correct self-adjusting programs without relying on this assumption. We then show a probabilistic-chunking technique to further decrease space usage by controlling the fundamental space-time tradeoff in self-adjusting computation. We implement and evaluate these techniques, showing promising results on challenging benchmarks involving large graphs.},
  booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '14},
  publisher = {{ACM}},
  date = {2014},
  pages = {227--240},
  keywords = {performance,self-adjusting computation,granularity control,incremental graph algorithms,information-flow type system},
  author = {Chen, Yan and Acar, Umut A. and Tangwongsan, Kanat},
  file = {/Users/pgiarrusso/Zotero/storage/PZTW3T3K/Chen-Acar-Tangwongsan - 2014 - Functional Programming for Dynamic and Large Data with Self-adjusting Computation.pdf}
}

@inproceedings{Hammer2015Incremental,
  location = {{New York, NY, USA}},
  title = {Incremental Computation with Names},
  isbn = {978-1-4503-3689-5},
  url = {http://doi.acm.org/10.1145/2814270.2814305},
  doi = {10.1145/2814270.2814305},
  abstract = {Over the past thirty years, there has been significant progress in developing general-purpose, language-based approaches to incremental computation, which aims to efficiently update the result of a computation when an input is changed. A key design challenge in such approaches is how to provide efficient incremental support for a broad range of programs. In this paper, we argue that first-class names are a critical linguistic feature for efficient incremental computation. Names identify computations to be reused across differing runs of a program, and making them first class gives programmers a high level of control over reuse. We demonstrate the benefits of names by presenting Nominal Adapton, an ML-like language for incremental computation with names. We describe how to use Nominal Adapton to efficiently incrementalize several standard programming patterns---including maps, folds, and unfolds---and show how to build efficient, incremental probabilistic trees and tries. Since Nominal Adapton's implementation is subtle, we formalize it as a core calculus and prove it is from-scratch consistent, meaning it always produces the same answer as simply re-running the computation. Finally, we demonstrate that Nominal Adapton can provide large speedups over both from-scratch computation and Adapton, a previous state-of-the-art incremental computation system.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2015},
  publisher = {{ACM}},
  urldate = {2017-07-07},
  date = {2015},
  pages = {748--766},
  keywords = {memoization,self-adjusting computation,laziness,call-by-push-value (CBPV),thunks,demanded computation graph (DCG),incremental compu- tation,nominal matching,structural matching},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Labich, Nicholas and Foster, Jeffrey S. and Hicks, Michael and Van Horn, David},
  file = {/Users/pgiarrusso/Zotero/storage/6QTC8SN4/Hammer et al - 2015 - Incremental Computation with Names.pdf}
}

@article{Hammer2016Typed,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00097},
  primaryClass = {cs},
  title = {Typed Adapton: Refinement types for incremental computations with precise names},
  url = {http://arxiv.org/abs/1610.00097},
  shorttitle = {Typed Adapton},
  abstract = {Over the past decade, programming language techniques for incremental computation have demonstrated that incremental programs with precise, carefully chosen dynamic names for data and sub-computations can dramatically outperform non-incremental programs, as well as those using traditional memoization (without such names). However, prior work lacks a verification mechanism to solve the ambiguous name problem, the problem of statically enforcing precise names. We say that an allocated pointer name is precise for an evaluation derivation when it identifies at most one value or subcomputation, and ambiguous otherwise. In this work, we define a refinement type system that gives practical static approximations to enforce precise, deterministic allocation names in otherwise functional programs. We show that this type system permits expressing familiar functional programs, and generic, composable library components. We prove that our type system enforces that well-typed programs name their values and sub-computations precisely, without ambiguity. Drawing closer to an implementation, we derive a bidirectional version of the type system, and prove that it corresponds to our declarative type system. A key challenge in implementing the bidirectional system is handling constraints over names, name terms and name sets; toward this goal, we give decidable, syntactic rules to guide these checks.},
  date = {2016-10-01},
  keywords = {Computer Science - Programming Languages},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  file = {/Users/pgiarrusso/Zotero/storage/PG36VUVG/Hammer et al - 2016 - Typed Adapton - Refinement types for incremental computations with precise names.pdf;/Users/pgiarrusso/Zotero/storage/JA8NMP26/1610.html}
}
% == BibLateX quality report for Hammer2016Typed:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Rummelhoff2004Polynat,
  title = {Polynat in PER models},
  volume = {316},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397504000842},
  doi = {10.1016/j.tcs.2004.01.031},
  abstract = {The polymorphic lambda-calculus can be modelled using PERs on a partial combinatory algebra. We say that the type of natural numbers (polynat) is polymorphically standard in such a model if the interpretation of the type only contains (the interpretations of) the Church numerals. We show that this is not always the case by constructing an explicit counterexample. On the other hand, when the PCA has either (strong) equality or weak equality plus a form of continuity, we show polynat is standard.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Recent Developments in Domain Theory: A collection of papers in honour of Dana S. Scott},
  urldate = {2017-07-06},
  date = {2004-05-28},
  pages = {215-224},
  keywords = {polymorphism,type theory,semantics,PER},
  author = {Rummelhoff, Ivar},
  file = {/Users/pgiarrusso/Zotero/storage/754C35PM/Rummelhoff - 2004 - Polynat in PER models.pdf;/Users/pgiarrusso/Zotero/storage/F6JQBE2V/S0304397504000842.html}
}

@article{Wadler2003Girard,
  title = {The Girard–Reynolds isomorphism},
  volume = {186},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S089054010300141X},
  doi = {10.1016/S0890-5401(03)00141-X},
  abstract = {The second-order polymorphic lambda calculus, F2, was independently discovered by Girard and Reynolds. Girard additionally proved a Representation Theorem: every function on natural numbers that can be proved total in second-order intuitionistic predicate logic, P2, can be represented in F2. Reynolds additionally proved an Abstraction Theorem: for a suitable notion of logical relation, every term in F2 takes related arguments into related results. We observe that the essence of Girard’s result is a projection from P2 into F2, and that the essence of Reynolds’s result is an embedding of F2 into P2, and that the Reynolds embedding followed by the Girard projection is the identity. The Girard projection discards all first-order quantifiers, so it seems unreasonable to expect that the Girard projection followed by the Reynolds embedding should also be the identity. However, we show that in the presence of Reynolds’s parametricity property that this is indeed the case, for propositions corresponding to inductive definitions of naturals or other algebraic types.},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  series = {Theoretical Aspects of Computer Software (TACS 2001)},
  urldate = {2017-07-05},
  date = {2003-11-01},
  pages = {260-284},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/6BBBAXAQ/S089054010300141X.html}
}

@inproceedings{Mellies2005Recursive,
  title = {Recursive polymorphic types and parametricity in an operational framework},
  doi = {10.1109/LICS.2005.42},
  abstract = {We construct a realizability model of recursive polymorphic types, starting from an untyped language of terms and contexts. An orthogonality relation e⊥π indicates when a term e and a context π may be safely combined in the language. Types are interpreted as sets of terms closed by biorthogonality. Our main result states that recursive types are approximated by converging sequences of interval types. Our proof is based on a "type-directed" approximation technique, which departs from the "language-directed" approximation technique developed by MacQueen, Plotkin and Sethi in the ideal model. We thus keep the language elementary (a call-by-name λ-calculus) and unstratified (no typecase, no reduction labels). We also include a short account of parametricity, based on an orthogonality relation between quadruples of terms and contexts.},
  eventtitle = {20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05)},
  booktitle = {20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05)},
  date = {2005-06},
  pages = {82-91},
  keywords = {type theory,lambda calculus,Computer science,Logic,recursive functions,Layout,call-by-name lambda-calculus,Context modeling,Equations,H infinity control,Lattices,orthogonality relation,pi calculus,realizability model,recursive polymorphic types,Terminology,type-directed approximation,untyped language},
  author = {Mellies, P. A. and Vouillon, J.},
  file = {/Users/pgiarrusso/Zotero/storage/WHDPJWIH/Mellies_Vouillon - 2005 - Recursive polymorphic types and parametricity in an operational framework.pdf;/Users/pgiarrusso/Zotero/storage/NBE3M84I/1509212.html}
}
% == BibLateX quality report for Mellies2005Recursive:
% ? Unsure about the formatting of the booktitle

@incollection{vanBenthemJutting1994checking,
  langid = {english},
  title = {Checking algorithms for Pure Type Systems},
  isbn = {978-3-540-58085-0 978-3-540-48440-0},
  url = {http://link.springer.com/chapter/10.1007/3-540-58085-9_71},
  abstract = {We have presented efficient syntax directed presentations of two subclasses of PTS: the semi-full systems, via the ⊢ sdsf relation the functional systems, via the ⊢f relation The only remaining defect in these presentations lies in the possible failure of tests for conversion in the application rule. Thus for normalizing functional and semi-full systems, everything has been said. For non-functional systems the situation is less clear. We know of no a priori bound on the amount of reduction necessary to correctly type λ-abstractions, so we must be content with the collective completeness of the family of syntax directed systems ⊢sd−n. We have made little impact on the Expansion Postponement problem, which we leave as future work. We can however bask in the relative peace of mind gained from the machine-checked presentation of most (i.e. those not concerning schematic judgments) of the above results.},
  number = {806},
  booktitle = {Types for Proofs and Programs},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-07-29},
  date = {1994-01-01},
  pages = {19-61},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {van Benthem Jutting, L. S. and McKinna, J. and Pollack, R.},
  editor = {Barendregt, Henk and Nipkow, Tobias},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/XVNUFWRM/3-540-58085-9_71.html}
}
% == BibLateX quality report for vanBenthemJutting1994checking:
% 'isbn': not a valid ISBN

@article{Abramsky2000Handbook,
  title = {The Handbook of Logic in Computer Science},
  url = {https://web.comlab.ox.ac.uk/publications/publication1808-abstract.html},
  urldate = {2017-07-02},
  date = {2000},
  author = {Abramsky, Samson}
}
% == BibLateX quality report for Abramsky2000Handbook:
% Missing required field 'journaltitle'

@article{Blaszczyk2013Ten,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1202.4153},
  title = {Ten Misconceptions from the History of Analysis and Their Debunking},
  volume = {18},
  issn = {1233-1821, 1572-8471},
  url = {http://arxiv.org/abs/1202.4153},
  doi = {10.1007/s10699-012-9285-8},
  abstract = {The widespread idea that infinitesimals were "eliminated" by the "great triumvirate" of Cantor, Dedekind, and Weierstrass is refuted by an uninterrupted chain of work on infinitesimal-enriched number systems. The elimination claim is an oversimplification created by triumvirate followers, who tend to view the history of analysis as a pre-ordained march toward the radiant future of Weierstrassian epsilontics. In the present text, we document distortions of the history of analysis stemming from the triumvirate ideology of ontological minimalism, which identified the continuum with a single number system. Such anachronistic distortions characterize the received interpretation of Stevin, Leibniz, d'Alembert, Cauchy, and others.},
  number = {1},
  journaltitle = {Foundations of Science},
  urldate = {2017-06-29},
  date = {2013-03},
  pages = {43-74},
  keywords = {Mathematics - Logic,Mathematics - History and Overview,01A85; 26E35; 03A05; 97A20; 97C30,Mathematics - Classical Analysis and ODEs},
  author = {Blaszczyk, Piotr and Katz, Mikhail G. and Sherry, David},
  file = {/Users/pgiarrusso/Zotero/storage/IZE84CAZ/Blaszczyk et al - 2013 - Ten Misconceptions from the History of Analysis and Their Debunking.pdf;/Users/pgiarrusso/Zotero/storage/I7TPAH5F/1202.html}
}
% == BibLateX quality report for Blaszczyk2013Ten:
% Unexpected field 'archivePrefix'
% 'issn': not a valid ISSN

@article{Thurston1994proof,
  title = {On proof and progress in mathematics},
  volume = {30},
  issn = {0273-0979, 1088-9485},
  url = {http://www.ams.org/bull/1994-30-02/S0273-0979-1994-00502-6/},
  doi = {10.1090/S0273-0979-1994-00502-6},
  number = {2},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2017-06-29},
  date = {1994},
  pages = {161-177},
  author = {Thurston, William P.},
  file = {/Users/pgiarrusso/Zotero/storage/9THEURHA/Thurston - 1994 - On proof and progress in mathematics.pdf;/Users/pgiarrusso/Zotero/storage/KQD5ZDJU/S0273-0979-1994-00502-6.html}
}
% == BibLateX quality report for Thurston1994proof:
% 'issn': not a valid ISSN

@article{Atiyah1994Responses,
  title = {Responses to: A. Jaffe and F. Quinn, “Theoretical mathematics: toward a cultural synthesis of mathematics and theoretical physics” [Bull. Amer. Math. Soc. (N.S.) 29 (1993), no. 1, 1–13; MR1202292 (94h:00007)]},
  volume = {30},
  issn = {0273-0979, 1088-9485},
  url = {http://www.ams.org/bull/1994-30-02/S0273-0979-1994-00503-8/},
  doi = {10.1090/S0273-0979-1994-00503-8},
  shorttitle = {Responses to},
  number = {2},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2017-06-29},
  date = {1994},
  pages = {178-207},
  author = {Atiyah, Michael and Borel, Armand and Chaitin, G. J. and Friedan, Daniel and Glimm, James and Gray, Jeremy J. and Hirsch, Morris W. and Mac Lane, Saunders and Mandelbrot, Benoit B. and Ruelle, David and Schwarz, Albert and Uhlenbeck, Karen and Thom, René and Witten, Edward and Zeeman, Sir Christopher},
  file = {/Users/pgiarrusso/Zotero/storage/MIB4CSDN/Atiyah et al - 1994 - Responses to - A. Jaffe and F. Quinn, “Theoretical mathematics - toward a cultural synthesis of mathematics and theoretical physics” [Bull. Amer. Math. Soc..pdf;/Users/pgiarrusso/Zotero/storage/MNJ6CRAC/S0273-0979-1994-00503-8.html}
}
% == BibLateX quality report for Atiyah1994Responses:
% 'issn': not a valid ISSN

@article{Foster2007Combinators,
  title = {Combinators for Bidirectional Tree Transformations: A Linguistic Approach to the View-update Problem},
  volume = {29},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/1232420.1232424},
  doi = {10.1145/1232420.1232424},
  shorttitle = {Combinators for Bidirectional Tree Transformations},
  abstract = {We propose a novel approach to the view-update problem for tree-structured data: a domain-specific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations---dubbed lenses---map a concrete tree into a simplified abstract view; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses. We begin by identifying a natural space of well-behaved bidirectional transformations over arbitrary structures, studying definedness and continuity in this setting. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe bidirectional transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional list-processing transformations as derived forms. An extended example shows how our combinators can be used to define a lens that translates between a native HTML representation of browser bookmarks and a generic abstract bookmark format.},
  number = {3},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2017-06-29},
  date = {2007-05},
  keywords = {harmony,lenses,view update problem,XML,Bidirectional programming},
  author = {Foster, J. Nathan and Greenwald, Michael B. and Moore, Jonathan T. and Pierce, Benjamin C. and Schmitt, Alan},
  file = {/Users/pgiarrusso/Zotero/storage/KX5EQATB/Foster et al - 2007 - Combinators for Bidirectional Tree Transformations - A Linguistic Approach to the View-update Problem.pdf}
}
% == BibLateX quality report for Foster2007Combinators:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@article{Mason1991Equivalence,
  title = {Equivalence in functional languages with effects},
  volume = {1},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/equivalence-in-functional-languages-with-effects/AC91F676E376BD48AA9DB0423E179249},
  doi = {10.1017/S0956796800000125},
  abstract = {AbstractTraditionally the view has been that direct expression of control and store mechanisms and clear mathematical semantics are incompatible requirements. This paper shows that adding objects with memory to the call-by-value lambda calculus results in a language with a rich equational theory, satisfying many of the usual laws. Combined with other recent work, this provides evidence that expressive, mathematically clean programming languages are indeed possible.},
  number = {3},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-06-26},
  date = {1991-07},
  pages = {287-327},
  author = {Mason, Ian and Talcott, Carolyn},
  file = {/Users/pgiarrusso/Zotero/storage/MHZ3VUAT/Mason_Talcott - 1991 - Equivalence in functional languages with effects.pdf;/Users/pgiarrusso/Zotero/storage/A7KAVP5P/AC91F676E376BD48AA9DB0423E179249.html}
}
% == BibLateX quality report for Mason1991Equivalence:
% 'issn': not a valid ISSN

@article{Gallier1989Girard,
  title = {On Girard's "Candidats de Reductibilité"},
  url = {http://repository.upenn.edu/cis_reports/703},
  journaltitle = {Technical Reports (CIS)},
  date = {1989-11-01},
  author = {Gallier, Jean},
  file = {/Users/pgiarrusso/Zotero/storage/ITMFGUNF/Gallier - 1989 - On Girard's Candidats de Reductibilité.pdf;/Users/pgiarrusso/Zotero/storage/CE87X4PK/703.html}
}

@article{Odersky2017Foundations,
  title = {Foundations of Implicit Function Types},
  url = {https://infoscience.epfl.ch/record/229203},
  urldate = {2017-06-25},
  date = {2017},
  author = {Odersky, Martin and Biboudis, Aggelos and Liu, Fengyun and Blanvillain, Olivier},
  file = {/Users/pgiarrusso/Zotero/storage/RMQADMG6/Odersky et al - 2017 - Foundations of Implicit Function Types.pdf;/Users/pgiarrusso/Zotero/storage/ZEXBQJJD/229203.html}
}
% == BibLateX quality report for Odersky2017Foundations:
% Missing required field 'journaltitle'

@inproceedings{Schafer2015Autosubst,
  langid = {english},
  title = {Autosubst: Reasoning with de Bruijn Terms and Parallel Substitutions},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-22102-1_24},
  doi = {10.1007/978-3-319-22102-1_24},
  shorttitle = {Autosubst},
  abstract = {Reasoning about syntax with binders plays an essential role in the formalization of the metatheory of programming languages. While the intricacies of binders can be ignored in paper proofs, formalizations involving binders tend to be heavyweight. We present a discipline for syntax with binders based on de Bruijn terms and parallel substitutions, with a decision procedure covering all assumption-free equational substitution lemmas. The approach is implemented in the Coq library Autosubst, which additionally derives substitution operations and proofs of substitution lemmas for custom term types. We demonstrate the effectiveness of the approach with several case studies, including part A of the POPLmark challenge.},
  eventtitle = {International Conference on Interactive Theorem Proving},
  booktitle = {Interactive Theorem Proving},
  publisher = {{Springer, Cham}},
  urldate = {2017-06-24},
  date = {2015-08-24},
  pages = {359-374},
  author = {Schäfer, Steven and Tebbi, Tobias and Smolka, Gert},
  file = {/Users/pgiarrusso/Zotero/storage/S2S349GE/Schäfer et al - 2015 - Autosubst - Reasoning with de Bruijn Terms and Parallel Substitutions.pdf;/Users/pgiarrusso/Zotero/storage/2HMNVSJT/978-3-319-22102-1_24.html}
}
% == BibLateX quality report for Schafer2015Autosubst:
% ? Unsure about the formatting of the booktitle

@inproceedings{Kaiser2017Equivalence,
  location = {{New York, NY, USA}},
  title = {Equivalence of System F and λ2 in Coq Based on Context Morphism Lemmas},
  isbn = {978-1-4503-4705-1},
  url = {http://doi.acm.org/10.1145/3018610.3018618},
  doi = {10.1145/3018610.3018618},
  abstract = {We give a machine-checked proof of the equivalence of the usual, two-sorted presentation of System F and its single-sorted pure type system variant Î»2. This is established by reducing the typability problem of F to Î»2 and vice versa. The difficulty lies in aligning different binding-structures and different contexts (dependent vs. non-dependent). The use of deÂ Bruijn syntax, parallel substitutions, and context morphism lemmas leads to an elegant proof. We use the Coq proof assistant and the substitution library Autosubst.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs},
  series = {CPP 2017},
  publisher = {{ACM}},
  date = {2017},
  pages = {222--234},
  keywords = {system f,pure type systems,Context Morphism Lemmas,de Bruijn Substitutions},
  author = {Kaiser, Jonas and Tebbi, Tobias and Smolka, Gert},
  file = {/Users/pgiarrusso/Zotero/storage/AE5KEK85/Kaiser et al - 2017 - Equivalence of System F and λ2 in Coq Based on Context Morphism Lemmas.pdf}
}

@online{Typeandscope,
  title = {Type-and-scope safe programs and their proofs},
  url = {http://dl.acm.org/citation.cfm?id=3018613&CFID=611533572&CFTOKEN=37240912},
  urldate = {2017-06-24},
  file = {/Users/pgiarrusso/Zotero/storage/WR6JD6DM/citation.html}
}
% == BibLateX quality report for Typeandscope:
% Exactly one of 'date' / 'year' must be present

@incollection{Allais2017Typeandscope,
  langid = {english},
  location = {{New York}},
  title = {Type-and-scope safe programs and their proofs},
  isbn = {978-1-4503-4705-1},
  url = {http://dx.doi.org/10.1145/3018610.3018613},
  abstract = {We abstract the common type-and-scope safe structure fromcomputations on lambda-terms that deliver, e.g., renaming, substitution, evaluation, CPS-transformation, and printing witha name supply. By exposing this structure, we can prove generic simulation and fusion lemmas relating operations built this way. This work has been fully formalised in Agda.},
  booktitle = {CPP 2017},
  publisher = {{ACM, New York, NY}},
  urldate = {2017-06-24},
  date = {2017-01-16},
  keywords = {_tablet},
  author = {Allais, Guillaume and Chapman, James and McBride, Conor and McKinna, James},
  editor = {Bertot, Yves and Vafeiadis, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/2N5WCWKR/Allais-Chapman-McBride-McKinna - 2017 - Type-and-scope safe programs and their proofs.pdf;/Users/pgiarrusso/Zotero/storage/H2GF769P/59447.html}
}

@article{Bakouny2017Coqbased,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.05271},
  primaryClass = {cs},
  title = {A Coq-based synthesis of Scala programs which are correct-by-construction},
  url = {http://arxiv.org/abs/1706.05271},
  doi = {10.1145/3103111.3104041},
  abstract = {The present paper introduces Scala-of-Coq, a new compiler that allows a Coq-based synthesis of Scala programs which are "correct-by-construction". A typical workflow features a user implementing a Coq functional program, proving this program's correctness with regards to its specification and making use of Scala-of-Coq to synthesize a Scala program that can seamlessly be integrated into an existing industrial Scala or Java application.},
  urldate = {2017-06-23},
  date = {2017-06-16},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  author = {Bakouny, Youssef El and Crolard, Tristan and Mezher, Dani},
  file = {/Users/pgiarrusso/Zotero/storage/TRQ3VKTK/Bakouny et al - 2017 - A Coq-based synthesis of Scala programs which are correct-by-construction.pdf;/Users/pgiarrusso/Zotero/storage/FAASZD3F/1706.html}
}
% == BibLateX quality report for Bakouny2017Coqbased:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Narasimhan2017Interactive,
  location = {{New York, NY, USA}},
  title = {Interactive Data Representation Migration: Exploiting Program Dependence to Aid Program Transformation},
  isbn = {978-1-4503-4721-1},
  url = {http://doi.acm.org/10.1145/3018882.3018890},
  doi = {10.1145/3018882.3018890},
  shorttitle = {Interactive Data Representation Migration},
  abstract = {Data representation migration is a program transformation that involves changing the type of a particular data structure, and then updating all of the operations that somehow depend on that data structure according to the new type. Changing the data representation can provide benefits such as improving efficiency and improving the quality of the computed results. Performing such a transformation is challenging, because it requires applying data-type specific changes to code fragments that may be widely scattered throughout the source code, connected by dataflow dependencies. Refactoring systems are typically sensitive to dataflow dependencies, but are not programmable with respect to the features of particular data types. Existing program transformation languages provide the needed flexibility, but do not concisely support reasoning about dataflow dependencies. To address the needs of data representation migration, we propose a new approach to program transformation that relies on a notion of semantic dependency: every transformation step propagates the transformation process onward to code that somehow depends on the transformed code. Our approach provides a declarative transformation-specification language, for expressing typespecific transformation rules. We further provide scoped rules, a mechanism for guiding rule application, and tags, a device for simple program analysis within our framework, to enable more powerful program transformations. We have implemented a prototype transformation system based on these ideas for C and C++ code and evaluate it against three example specifications, including vectorization, transformation of integers to big integers, and transformation of array-of-structures data types to structure-of-arrays format. Our evaluation shows that our approach can improve program performance and the precision of the computed results, and that it scales to programs of up to 3700 lines.},
  booktitle = {Proceedings of the 2017 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM 2017},
  publisher = {{ACM}},
  urldate = {2017-06-21},
  date = {2017},
  pages = {47--58},
  keywords = {program transformation,DSL,static analysis},
  author = {Narasimhan, Krishna and Reichenbach, Christoph and Lawall, Julia},
  file = {/Users/pgiarrusso/Zotero/storage/KHRPE3ND/Narasimhan et al - 2017 - Interactive Data Representation Migration - Exploiting Program Dependence to Aid Program Transformation.pdf}
}

@article{Tait1967Intensional,
  eprinttype = {jstor},
  eprint = {2271658},
  title = {Intensional interpretations of functionals of finite type I},
  volume = {32},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2271658},
  abstract = {T0 will denote Gödel's theory T[3] of functionals of finite type (f.t.) with intuitionistic quantification over each f.t. added. T1 will denote T0 together with definition by bar recursion of type o, the axiom schema of bar induction, and the schemaof choice. Precise descriptions of these systems are given below in §4. The main results of this paper are interpretations of T0 in intuitionistic arithmetic U0 and of T1 in intuitionistic analysis is U1. U1 is U0 with quantification over functionals of type (0,0) and the axiom schemata AC00 and of bar induction.},
  number = {2},
  journaltitle = {The Journal of Symbolic Logic},
  date = {1967-08},
  pages = {198-212},
  author = {Tait, W. W.},
  file = {/Users/pgiarrusso/Zotero/storage/MDR9BC9C/Tait - 1967 - Intensional Interpretations of Functionals of Finite Type I.pdf;/Users/pgiarrusso/Zotero/storage/EXJDBWHA/9F30EA199783BD797DF6FA44525F114E.html}
}
% == BibLateX quality report for Tait1967Intensional:
% 'issn': not a valid ISSN

@inproceedings{Abadi1990model,
  title = {A PER model of polymorphism and recursive types},
  doi = {10.1109/LICS.1990.113761},
  abstract = {A model of Reynold's polymorphic lambda calculus is provided, which also allows the recursive definition of elements and types. The techniques uses a good class of partial equivalence relations (PERs) over a certain CPO. This allows the combination of inverse-limits for recursion and intersection for polymorphism},
  eventtitle = {[1990] Proceedings. Fifth Annual IEEE Symposium on Logic in Computer Science},
  booktitle = {[1990] Proceedings. Fifth Annual IEEE Symposium on Logic in Computer Science},
  date = {1990-06},
  pages = {355-365},
  keywords = {polymorphism,recursive types,recursion,Polymorphic lambda calculus,Buildings,Computer science,intersection,PER model,Logic,formal languages,equivalence classes,recursive functions,Equations,CPO,inverse-limits,O-category,partial equivalence relations,partial orders,recursive definition},
  author = {Abadi, M. and Plotkin, G. D.},
  file = {/Users/pgiarrusso/Zotero/storage/WMZ7SFBM/Abadi_Plotkin - 1990 - A PER model of polymorphism and recursive types.pdf;/Users/pgiarrusso/Zotero/storage/CC29P48K/113761.html}
}
% == BibLateX quality report for Abadi1990model:
% ? Unsure about the formatting of the booktitle

@article{Johnson-Freyd2016First,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.06378},
  title = {First Class Call Stacks: Exploring Head Reduction},
  volume = {212},
  issn = {2075-2180},
  url = {http://arxiv.org/abs/1606.06378},
  doi = {10.4204/EPTCS.212.2},
  shorttitle = {First Class Call Stacks},
  abstract = {Weak-head normalization is inconsistent with functional extensionality in the call-by-name \$$\backslash$lambda\$-calculus. We explore this problem from a new angle via the conflict between extensionality and effects. Leveraging ideas from work on the \$$\backslash$lambda\$-calculus with control, we derive and justify alternative operational semantics and a sequence of abstract machines for performing head reduction. Head reduction avoids the problems with weak-head reduction and extensionality, while our operational semantics and associated abstract machines show us how to retain weak-head reduction's ease of implementation.},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  urldate = {2017-06-14},
  date = {2016-06-19},
  pages = {18-35},
  keywords = {Computer Science - Programming Languages,F.3.3,F.3.2},
  author = {Johnson-Freyd, Philip and Downen, Paul and Ariola, Zena M.},
  file = {/Users/pgiarrusso/Zotero/storage/G448M5UE/Johnson-Freyd et al - 2016 - First Class Call Stacks - Exploring Head Reduction.pdf;/Users/pgiarrusso/Zotero/storage/K7DIAQPE/1606.html}
}
% == BibLateX quality report for Johnson-Freyd2016First:
% Unexpected field 'archivePrefix'

@article{Abadi1991Explicit,
  title = {Explicit substitutions},
  volume = {1},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/explicit-substitutions/C1B1AFAE8F34C953C1B2DF3C2D4C2125},
  doi = {10.1017/S0956796800000186},
  abstract = {AbstractThe λσ-calculus is a refinement of the λ-calculus where substitutions are manipulated explicitly. The λσ-calculus provides a setting for studying the theory of substitutions, with pleasant mathematical properties. It is also a useful bridge between the classical λ-calculus and concrete implementations.},
  number = {4},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-06-14},
  date = {1991-10},
  pages = {375-416},
  author = {Abadi, M. and Cardelli, L. and Curien, P.-L. and Lévy, J.-J.},
  file = {/Users/pgiarrusso/Zotero/storage/JZDX6UNA/Abadi et al - 1991 - Explicit substitutions.pdf;/Users/pgiarrusso/Zotero/storage/78USSSGA/C1B1AFAE8F34C953C1B2DF3C2D4C2125.html}
}
% == BibLateX quality report for Abadi1991Explicit:
% 'issn': not a valid ISSN

@inproceedings{Kang2016Lightweight,
  location = {{New York, NY, USA}},
  title = {Lightweight Verification of Separate Compilation},
  isbn = {978-1-4503-3549-2},
  url = {http://doi.acm.org/10.1145/2837614.2837642},
  doi = {10.1145/2837614.2837642},
  abstract = {Major compiler verification efforts, such as the CompCert project, have traditionally simplified the verification problem by restricting attention to the correctness of whole-program compilation, leaving open the question of how to verify the correctness of separate compilation. Recently, a number of sophisticated techniques have been proposed for proving more flexible, compositional notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the simple "closed simulations" used in verifying whole-program compilation. Applying such techniques to a compiler like CompCert, as Stewart et al. have done, involves major changes and extensions to its original verification. In this paper, we show that if we aim somewhat lower---to prove correctness of separate compilation, but only for a *single* compiler---we can drastically simplify the proof effort. Toward this end, we develop several lightweight techniques that recast the compositional verification problem in terms of whole-program compilation, thereby enabling us to largely reuse the closed-simulation proofs from existing compiler verifications. We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting its verification of whole-program compilation into a verification of separate compilation in less than two person-months. This conversion only required a small number of changes to the original proofs, and uncovered two compiler bugs along the way. The result is SepCompCert, the first verification of separate compilation for the full CompCert compiler.},
  booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '16},
  publisher = {{ACM}},
  urldate = {2017-06-13},
  date = {2016},
  pages = {178--190},
  keywords = {Compositional compiler verification,CompCert,separate compilation},
  author = {Kang, Jeehoon and Kim, Yoonseung and Hur, Chung-Kil and Dreyer, Derek and Vafeiadis, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/F3NANNGM/Kang et al - 2016 - Lightweight Verification of Separate Compilation.pdf}
}

@inproceedings{Mitchell1986Typeinference,
  location = {{New York, NY, USA}},
  title = {A Type-inference Approach to Reduction Properties and Semantics of Polymorphic Expressions (Summary)},
  isbn = {0-89791-200-4},
  url = {http://doi.acm.org/10.1145/319838.319872},
  doi = {10.1145/319838.319872},
  booktitle = {Proceedings of the 1986 ACM Conference on LISP and Functional Programming},
  series = {LFP '86},
  publisher = {{ACM}},
  urldate = {2017-06-13},
  date = {1986},
  pages = {308--319},
  author = {Mitchell, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/4UHDFTZ6/Mitchell - 1986 - A Type-inference Approach to Reduction Properties and Semantics of Polymorphic Expressions (Summary).pdf}
}

@article{Breazu-Tannen1988Extensional,
  title = {Extensional models for polymorphism},
  volume = {59},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/0304397588900977},
  doi = {10.1016/0304-3975(88)90097-7},
  abstract = {We present a general method for constructing extensional models for the Girard-Reynolds polymorphic lambda calculus—the polymorphic extensional collapse. The method yields models that satisfy additional, computationally motivated constraints like having only two polymorphic booleans and having only the numerals as polymorphic integers. Moreover, the method can be used to show that any simply typed lambda model can be fully and faithfully embedded into a model of the polymorphic lambda calculus.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2017-06-13},
  date = {1988-07-01},
  pages = {85-114},
  author = {Breazu-Tannen, Val and Coquand, Thierry},
  file = {/Users/pgiarrusso/Zotero/storage/KS6U2MZQ/Breazu-Tannen_Coquand - 1988 - Extensional models for polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/TMU2TV7J/0304397588900977.html}
}

@incollection{Bernardy2011realizabilitya,
  langid = {english},
  title = {Realizability and Parametricity in Pure Type Systems},
  isbn = {978-3-642-19804-5 978-3-642-19805-2},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-19805-2_8},
  abstract = {We describe a systematic method to build a logic from any programming language described as a Pure Type System (PTS). The formulas of this logic express properties about programs. We define a parametricity theory about programs and a realizability theory for the logic. The logic is expressive enough to internalize both theories. Thanks to the PTS setting, we abstract most idiosyncrasies specific to particular type theories. This confers generality to the results, and reveals parallels between parametricity and realizability.},
  number = {6604},
  booktitle = {Foundations of Software Science and Computational Structures},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-08-11},
  date = {2011-01-01},
  pages = {108-122},
  keywords = {Programming Languages; Compilers; Interpreters,Software Engineering,Logics and Meanings of Programs,Computation by Abstract Devices,Mathematical Logic and Formal Languages},
  author = {Bernardy, Jean-Philippe and Lasson, Marc},
  editor = {Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/EIDA78PT/Bernardy_Lasson - 2011 - Realizability and Parametricity in Pure Type Systems.pdf;/Users/pgiarrusso/Zotero/storage/ZSQK4428/10.html}
}
% == BibLateX quality report for Bernardy2011realizabilitya:
% 'isbn': not a valid ISBN

@inproceedings{Bernardy2011realizability,
  location = {{Berlin, Heidelberg}},
  title = {Realizability and Parametricity in Pure Type Systems},
  isbn = {978-3-642-19804-5},
  url = {http://dl.acm.org/citation.cfm?id=1987171.1987181},
  abstract = {We describe a systematic method to build a logic from any programming language described as a Pure Type System (PTS). The formulas of this logic express properties about programs. We define a parametricity theory about programs and a realizability theory for the logic. The logic is expressive enough to internalize both theories. Thanks to the PTS setting, we abstract most idiosyncrasies specific to particular type theories. This confers generality to the results, and reveals parallels between parametricity and realizability.},
  booktitle = {Proceedings of the 14th International Conference on Foundations of Software Science and Computational Structures: Part of the Joint European Conferences on Theory and Practice of Software},
  series = {FOSSACS'11/ETAPS'11},
  publisher = {{Springer-Verlag}},
  urldate = {2014-08-11},
  date = {2011},
  pages = {108--122},
  author = {Bernardy, Jean-Philippe and Lasson, Marc}
}
% == BibLateX quality report for Bernardy2011realizability:
% 'isbn': not a valid ISBN

@inproceedings{Marsik2016Introducing,
  langid = {english},
  title = {Introducing a Calculus of Effects and Handlers for Natural Language Semantics},
  volume = {9804},
  url = {https://hal.inria.fr/hal-01332762/document},
  doi = {10.1007/978-3-662-53042-9},
  abstract = {In compositional model-theoretic semantics, researchers assemble truth-conditions or other kinds of denotations using the lambda calculus. It was previously observed that the lambda terms and/or the denotations studied tend to follow the same pattern: they are instances of a monad. In this paper, we present an extension of the simply-typed lambda calculus that exploits this uniformity using the recently discovered technique of effect handlers. We prove that our calculus exhibits some of the key formal properties of the lambda calculus and we use it to construct a modular semantics for a small fragment that involves multiple distinct semantic phenomena.},
  eventtitle = {Formal Grammar 2016},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2017-06-07},
  date = {2016-08-20},
  pages = {257--272},
  author = {Maršík, Jirka and Amblard, Maxime},
  file = {/Users/pgiarrusso/Zotero/storage/3IJZSPAN/Maršík_Amblard - 2016 - Introducing a Calculus of Effects and Handlers for Natural Language Semantics.pdf;/Users/pgiarrusso/Zotero/storage/AQAKFZ7B/hal-01332762.html}
}
% == BibLateX quality report for Marsik2016Introducing:
% Missing required field 'booktitle'

@inproceedings{Henglein2010Optimizing,
  location = {{New York, NY, USA}},
  title = {Optimizing Relational Algebra Operations Using Generic Equivalence Discriminators and Lazy Products},
  isbn = {978-1-60558-727-1},
  url = {http://doi.acm.org/10.1145/1706356.1706372},
  doi = {10.1145/1706356.1706372},
  abstract = {We show how to efficiently evaluate generic map-filter-product queries, generalizations of select-project-join (SPJ) queries in relational algebra, based on a combination of two novel techniques: generic discrimination-based joins and lazy (formal) products. Discrimination-based joins are based on the notion of (equivalence) discriminator. A discriminator partitions a list of values according to a user-specified equivalence relation on keys the values are associated with. Equivalence relations can be specified in an expressive embedded language for denoting equivalence relations. We show that discriminators can be constructed generically (by structural recursion on equivalence expressions), purely functionally, and efficiently (worst-case linear time). The array-based basic multiset discrimination algorithm of Cai and Paige (1995) provides a base discriminator that is both asymptotically and practically efficient. In contrast to hashing, discrimination is fully abstract (only depends on which equivalences hold on its inputs), and in contrast to comparison-based sorting, it does not require an ordering relation on its inputs. In particular, it is applicable to references (pointers). Furthermore, it has better asymptotic computational complexity than both sorting and hashing. We represent cross-products and unions lazily (symbolically) as formal products of the argument sets (relations). This allows the selection operation to recognize on the fly whenever it is applied to a cross-product and invoke an efficient equijoin implementation. In particular, queries can still be formulated naively, using filter, map and product without an explicit join operation, yet garner the advantages of efficient join-algorithms during evaluation. The techniques subsume many of the optimization techniques based on relational algebra equalities, without need for a query preprocessing phase. They require no indexes and behave purely functionally. They can be considered a form of symbolic execution of set expressions that automate and encapsulate dynamic program transformation of such expressions and lead to asymptotic performance improvements over naive execution in many cases.},
  booktitle = {Proceedings of the 2010 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
  series = {PEPM '10},
  publisher = {{ACM}},
  urldate = {2017-06-04},
  date = {2010},
  pages = {73--82},
  keywords = {optimization,Query,Algebra,relational,cross-product,discrimination,disriminator,equivalence,evaluation,formal,generic,join,lazy,optimize,optimizing,product,symbolic},
  author = {Henglein, Fritz},
  file = {/Users/pgiarrusso/Zotero/storage/BA6RWNRE/Henglein - 2010 - Optimizing Relational Algebra Operations Using Generic Equivalence Discriminators and Lazy Products.pdf}
}

@inproceedings{Henglein2010Generic,
  location = {{New York, NY, USA}},
  title = {Generic Multiset Programming for Language-integrated Querying},
  isbn = {978-1-4503-0251-7},
  url = {http://doi.acm.org/10.1145/1863495.1863503},
  doi = {10.1145/1863495.1863503},
  abstract = {This paper demonstrates how relational algebraic programming based on efficient symbolic representations of multisets and operations on them can be applied to the query sublanguage of SQL in a type-safe fashion. In essence, it provides a library for naïve programming with multisets in a generalized SQL-style fashion, but avoids many cases of asymptotically inefficient nested iteration through cross-products.},
  booktitle = {Proceedings of the 6th ACM SIGPLAN Workshop on Generic Programming},
  series = {WGP '10},
  publisher = {{ACM}},
  urldate = {2017-06-04},
  date = {2010},
  pages = {49--60},
  keywords = {Haskell,sql,programming,Query,Algebra,relational,GADT,LINQ,discrimination,equivalence,generic,join,lazy,symbolic,bag,filter,map,multiset,ordering,project,querying,select},
  author = {Henglein, Fritz and Larsen, Ken Friis},
  file = {/Users/pgiarrusso/Zotero/storage/RVKG3FG7/Henglein_Larsen - 2010 - Generic Multiset Programming for Language-integrated Querying.pdf}
}

@inproceedings{Tate2011Taming,
  location = {{New York, NY, USA}},
  title = {Taming Wildcards in Java's Type System},
  isbn = {978-1-4503-0663-8},
  url = {http://doi.acm.org/10.1145/1993498.1993570},
  doi = {10.1145/1993498.1993570},
  abstract = {Wildcards have become an important part of Java's type system since their introduction 7 years ago. Yet there are still many open problems with Java's wildcards. For example, there are no known sound and complete algorithms for subtyping (and consequently type checking) Java wildcards, and in fact subtyping is suspected to be undecidable because wildcards are a form of bounded existential types. Furthermore, some Java types with wildcards have no joins, making inference of type arguments for generic methods particularly difficult. Although there has been progress on these fronts, we have identified significant shortcomings of the current state of the art, along with new problems that have not been addressed. In this paper, we illustrate how these shortcomings reflect the subtle complexity of the problem domain, and then present major improvements to the current algorithms for wildcards by making slight restrictions on the usage of wildcards. Our survey of existing Java programs suggests that realistic code should already satisfy our restrictions without any modifications. We present a simple algorithm for subtyping which is both sound and complete with our restrictions, an algorithm for lazily joining types with wildcards which addresses some of the shortcomings of prior work, and techniques for improving the Java type system as a whole. Lastly, we describe various extensions to wildcards that would be compatible with our algorithms.},
  booktitle = {Proceedings of the 32Nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '11},
  publisher = {{ACM}},
  urldate = {2017-06-04},
  date = {2011},
  pages = {614--627},
  keywords = {existential types,type inference,subtyping,joins,parametric types,single-instantiation inheritance,wildcards},
  author = {Tate, Ross and Leung, Alan and Lerner, Sorin},
  file = {/Users/pgiarrusso/Zotero/storage/PIXCMSUI/Tate et al - 2011 - Taming Wildcards in Java's Type System.pdf}
}

@article{Torgersen2004Adding,
  langid = {english},
  title = {Adding Wildcards to the Java Programming Language.},
  volume = {3},
  issn = {1660-1769},
  url = {http://www.jot.fm/contents/issue_2004_12/article5.html},
  doi = {10.5381/jot.2004.3.11.a5},
  number = {11},
  journaltitle = {The Journal of Object Technology},
  urldate = {2017-06-04},
  date = {2004},
  pages = {97},
  author = {Torgersen, Mads and Ernst, Erik and Hansen, Christian Plesner and von der Ahe, Peter and Bracha, Gilad and Gafter, Neal},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/UAXWIK6E/Torgersen et al - 2004 - Adding Wildcards to the Java Programming Language.pdf}
}

@inproceedings{Petrashko2017Miniphases,
  title = {Miniphases: Compilation using Modular and Efficient Tree Transformations},
  url = {https://infoscience.epfl.ch/record/228518},
  shorttitle = {Miniphases},
  eventtitle = {PLDI},
  urldate = {2017-06-04},
  date = {2017},
  keywords = {cache locality,compiler performance,tree traversal fusion},
  author = {Petrashko, Dmytro and Lhoták, Ondrej and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/3BV92V7U/Petrashko-Lhoták-Odersky - 2017 - Miniphases - Compilation using Modular and Efficient Tree Transformations.pdf;/Users/pgiarrusso/Zotero/storage/X6BREFP8/228518.html}
}
% == BibLateX quality report for Petrashko2017Miniphases:
% Missing required field 'booktitle'

@phdthesis{Huesca2015Incrementality,
  langid = {english},
  title = {Incrementality and effect simulation in the simply typed lambda calculus},
  url = {https://hal.inria.fr/tel-01248531/document},
  abstract = {Certified programming is a framework in which any program is correct by construction. Proof assistants and dependently typed programming languages are the representatives of this paradigm where the proof and implementation of a program are done at the same time. However, it has some limitations: a program in Type Theory is built only with pure and total functions. Our objective is to write efficient and certified programs. The contributions of this work are the formalization, in the Simply Typed Lambda Calculus, of two mechanisms to achieve efficiency: to validate impure computations and to optimize computations by incrementality. An impure computation, that is a program with effects, and its validation in a functional and total language is done through a posteriori simulation. The simulation is performed afterwards on a monadic procedure and is guided by a prophecy. An efficient oracle is responsible for producing prophecies which is actually, the monadic procedure itself translated into an effectful programming language. The second contribution is an optimization to perform incremental computations. Incrementality as a way to propagate an input change into a corresponding output change is guided by formal change descriptions over terms and dynamic differentiation of functions. Displaceable types represent data-changes while an extension of the simply typed lambda calculus with differentials and partial derivatives offers a language to reason about incrementality.},
  institution = {{Universite Paris Diderot-Paris VII}},
  urldate = {2017-06-04},
  date = {2015-11-27},
  author = {Huesca, Lourdes del Carmen Gonzalez},
  file = {/Users/pgiarrusso/Zotero/storage/9J23MDV8/Huesca - 2015 - Incrementality and effect simulation in the simply typed lambda calculus.pdf;/Users/pgiarrusso/Zotero/storage/FR8MMS3M/Huesca - 2015 - Incrementality and effect simulation in the simply typed lambda calculus.pdf;/Users/pgiarrusso/Zotero/storage/B9QZV6JR/tel-01248531v1.html;/Users/pgiarrusso/Zotero/storage/XKP8BXNQ/tel-01248531.html}
}
% == BibLateX quality report for Huesca2015Incrementality:
% I don't know how to quality-check phdthesis references

@inproceedings{Sumii2005Bisimulation,
  location = {{New York, NY, USA}},
  title = {A Bisimulation for Type Abstraction and Recursion},
  isbn = {1-58113-830-X},
  url = {http://doi.acm.org/10.1145/1040305.1040311},
  doi = {10.1145/1040305.1040311},
  abstract = {We present a sound, complete, and elementary proof method, based on bisimulation, for contextual equivalence in a λ-calculus with full universal, existential, and recursive types. Unlike logical relations (either semantic or syntactic), our development is elementary, using only sets and relations and avoiding advanced machinery such as domain theory, admissibility, and ΤΤ-closure. Unlike other bisimulations, ours is complete even for existential types. The key idea is to consider sets of relations---instead of just relations---as bisimulations.},
  booktitle = {Proceedings of the 32Nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '05},
  publisher = {{ACM}},
  urldate = {2017-05-29},
  date = {2005},
  pages = {63--74},
  keywords = {recursive types,existential types,logical relations,Lambda-calculus,contextual equivalence,bisimulations},
  author = {Sumii, Eijiro and Pierce, Benjamin C.},
  file = {/Users/pgiarrusso/Zotero/storage/S5A8P7P5/Sumii_Pierce - 2005 - A Bisimulation for Type Abstraction and Recursion.pdf}
}

@inproceedings{Matthews2008Parametric,
  langid = {english},
  title = {Parametric Polymorphism through Run-Time Sealing or, Theorems for Low, Low Prices!},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-78739-6_2},
  doi = {10.1007/978-3-540-78739-6_2},
  abstract = {We show how to extend System F’s parametricity guarantee to a Matthews-Findler-style multi-language system that combines System F with an untyped language by use of dynamic sealing. While the use of sealing for this purpose has been suggested before, it has never been proven to preserve parametricity. In this paper we prove that it does using step-indexed logical relations. Using this result we show a scheme for implementing parametric higher-order contracts in an untyped setting which corresponds to a translation given by Sumii and Pierce. These contracts satisfy rich enough guarantees that we can extract analogues to Wadler’s free theorems that rely on run-time enforcement of dynamic seals.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-05-28},
  date = {2008-03-29},
  pages = {16-31},
  author = {Matthews, Jacob and Ahmed, Amal},
  file = {/Users/pgiarrusso/Zotero/storage/6AAXITF7/Matthews_Ahmed - 2008 - Parametric Polymorphism through Run-Time Sealing or, Theorems for Low, Low Prices!.pdf;/Users/pgiarrusso/Zotero/storage/EBVX2W3E/978-3-540-78739-6_2.html}
}
% == BibLateX quality report for Matthews2008Parametric:
% ? Unsure about the formatting of the booktitle

@article{Nikolic2014LINVIEW,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.6968},
  primaryClass = {cs},
  title = {LINVIEW: Incremental View Maintenance for Complex Analytical Queries},
  url = {http://arxiv.org/abs/1403.6968},
  shorttitle = {LINVIEW},
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  urldate = {2014-04-25},
  date = {2014-03-27},
  keywords = {Computer Science - Databases,Computer Science - Numerical Analysis},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/RPE654Z6/Nikolic et al - 2014 - LINVIEW - Incremental View Maintenance for Complex Analytical Queries.pdf;/Users/pgiarrusso/Zotero/storage/TDBJMBTQ/1403.html}
}
% == BibLateX quality report for Nikolic2014LINVIEW:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Nikolic2014linview,
  location = {{New York, NY, USA}},
  title = {LINVIEW: Incremental View Maintenance for Complex Analytical Queries},
  isbn = {978-1-4503-2376-5},
  url = {http://doi.acm.org/10.1145/2588555.2610519},
  doi = {10.1145/2588555.2610519},
  shorttitle = {LINVIEW},
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  series = {SIGMOD '14},
  publisher = {{ACM}},
  urldate = {2017-05-25},
  date = {2014},
  pages = {253--264},
  keywords = {Compilation,Incremental view maintenance,machine learning,spark,linear algebra},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/6D9KEZSD/Nikolic et al - 2014 - LINVIEW - Incremental View Maintenance for Complex Analytical Queries.pdf}
}

@inproceedings{Flanagan1993Essence,
  location = {{New York, NY, USA}},
  title = {The Essence of Compiling with Continuations},
  isbn = {0-89791-598-4},
  url = {http://doi.acm.org/10.1145/155090.155113},
  doi = {10.1145/155090.155113},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the “continuation”). Since the nai¨ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.
A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the nai¨ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
  series = {PLDI '93},
  publisher = {{ACM}},
  urldate = {2017-05-25},
  date = {1993},
  pages = {237--247},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  file = {/Users/pgiarrusso/Zotero/storage/RQGWXXKZ/Flanagan et al - 1993 - The Essence of Compiling with Continuations.pdf}
}

@article{Shulman2017Homotopy,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.03007},
  primaryClass = {math},
  title = {Homotopy type theory: the logic of space},
  url = {http://arxiv.org/abs/1703.03007},
  shorttitle = {Homotopy type theory},
  abstract = {This is an introduction to type theory, synthetic topology, and homotopy type theory from a category-theoretic and topological point of view, written as a chapter for the book "New Spaces for Mathematics and Physics" (ed. Gabriel Catren and Mathieu Anel).},
  urldate = {2017-05-25},
  date = {2017-03-08},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  author = {Shulman, Michael},
  file = {/Users/pgiarrusso/Zotero/storage/J86NHM6W/Shulman - 2017 - Homotopy type theory - the logic of space.pdf;/Users/pgiarrusso/Zotero/storage/FKVQKIG2/1703.html}
}
% == BibLateX quality report for Shulman2017Homotopy:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Amin2016Dependent,
  title = {Dependent Object Types},
  url = {https://infoscience.epfl.ch/record/223518},
  doi = {10.5075/epfl-thesis-7156, urn:nbn:ch:bel-epfl-thesis7156-6},
  urldate = {2017-05-23},
  date = {2016},
  keywords = {_tablet},
  author = {Amin, Nada},
  file = {/Users/pgiarrusso/Zotero/storage/AIZWEN7P/Amin - 2016 - Dependent Object Types.pdf;/Users/pgiarrusso/Zotero/storage/5VPDJHUS/223518.html}
}
% == BibLateX quality report for Amin2016Dependent:
% Missing required field 'journaltitle'

@inproceedings{Flatt2002Composable,
  location = {{New York, NY, USA}},
  title = {Composable and Compilable Macros:: You Want It when?},
  isbn = {978-1-58113-487-2},
  url = {http://doi.acm.org/10.1145/581478.581486},
  doi = {10.1145/581478.581486},
  shorttitle = {Composable and Compilable Macros},
  abstract = {Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.},
  booktitle = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '02},
  publisher = {{ACM}},
  urldate = {2017-05-18},
  date = {2002},
  pages = {72--83},
  keywords = {modules,macros,language tower},
  author = {Flatt, Matthew},
  file = {/Users/pgiarrusso/Zotero/storage/3SXUMVZ6/Flatt - 2002 - Composable and Compilable Macros - - You Want It when.pdf}
}

@article{Cheney2012Formalizing,
  langid = {english},
  title = {Formalizing Adequacy: A Case Study for Higher-order Abstract Syntax},
  volume = {49},
  issn = {0168-7433, 1573-0670},
  url = {https://link.springer.com/article/10.1007/s10817-011-9221-6},
  doi = {10.1007/s10817-011-9221-6},
  shorttitle = {Formalizing Adequacy},
  abstract = {Adequacy is an important criterion for judging whether a formalization is suitable for reasoning about the actual object of study. The issue is particularly subtle in the expansive case of approaches to languages with name-binding. In prior work, adequacy has been formalized only with respect to specific representation techniques. In this article, we give a general formal definition based on model-theoretic isomorphisms or interpretations. We investigate and formalize an adequate interpretation of untyped lambda-calculus within a higher-order metalanguage in Isabelle/HOL using the Nominal Datatype Package. Formalization elucidates some subtle issues that have been neglected in informal arguments concerning adequacy.},
  number = {2},
  journaltitle = {Journal of Automated Reasoning},
  shortjournal = {J Autom Reasoning},
  urldate = {2017-05-12},
  date = {2012-08-01},
  pages = {209-239},
  author = {Cheney, James and Norrish, Michael and Vestergaard, René},
  file = {/Users/pgiarrusso/Zotero/storage/DDFD8H7F/Cheney et al - 2012 - Formalizing Adequacy - A Case Study for Higher-order Abstract Syntax.pdf;/Users/pgiarrusso/Zotero/storage/9EDJMH5J/10.html}
}
% == BibLateX quality report for Cheney2012Formalizing:
% 'issn': not a valid ISSN

@article{Lamport2016How,
  title = {How to Write a 21st Century Proof},
  url = {https://www.microsoft.com/en-us/research/publication/write-21st-century-proof/},
  abstract = {I was invited to give a talk at a celebration of the 80th birthday of Richard Palais. It was at a celebration of his 60th birthday that I first gave a talk about how to write a proof–a talk that led to [101]. So, I thought it would be fun to give the same talk, …},
  journaltitle = {Journal of Fixed Point Theory and Applications},
  urldate = {2017-05-06},
  date = {2016-12-20},
  author = {Lamport, Leslie},
  file = {/Users/pgiarrusso/Zotero/storage/B8Q5F4KP/Lamport - 2016 - How to Write a 21st Century Proof.pdf;/Users/pgiarrusso/Zotero/storage/XJBEATBZ/write-21st-century-proof.html}
}

@article{Lamport2016Howa,
  title = {How to Write a Proof},
  volume = {102/7},
  url = {https://www.microsoft.com/en-us/research/publication/how-to-write-a-proof/},
  abstract = {TLA gave me, for the first time, a formalism in which it was possible to write completely formal proofs without first having to add an additional layer of formal semantics. I began writing proofs the way I and all mathematicians and computer scientists had learned to write them, using a sequence of lemmas whose proofs …},
  journaltitle = {Microsoft Research},
  urldate = {2017-05-06},
  date = {2016-12-20},
  author = {Lamport, Leslie},
  file = {/Users/pgiarrusso/Zotero/storage/GW8R5PPF/Lamport - 2016 - How to Write a Proof.pdf;/Users/pgiarrusso/Zotero/storage/ZTRU5PNK/how-to-write-a-proof.html}
}

@inproceedings{Ahmed2009Statedependent,
  location = {{New York, NY, USA}},
  title = {State-dependent Representation Independence},
  isbn = {978-1-60558-379-2},
  url = {http://doi.acm.org/10.1145/1480881.1480925},
  doi = {10.1145/1480881.1480925},
  abstract = {Mitchell's notion of representation independence is a particularly useful application of Reynolds' relational parametricity -- two different implementations of an abstract data type can be shown contextually equivalent so long as there exists a relation between their type representations that is preserved by their operations. There have been a number of methods proposed for proving representation independence in various pure extensions of System F (where data abstraction is achieved through existential typing), as well as in Algol- or Java-like languages (where data abstraction is achieved through the use of local mutable state). However, none of these approaches addresses the interaction of existential type abstraction and local state. In particular, none allows one to prove representation independence results for generative ADTs -- i.e. ADTs that both maintain some local state and define abstract types whose internal representations are dependent on that local state. In this paper, we present a syntactic, logical-relations-based method for proving representation independence of generative ADTs in a language supporting polymorphic types, existential types, general recursive types, and unrestricted ML-style mutable references. We demonstrate the effectiveness of our method by using it to prove several interesting contextual equivalences that involve a close interaction between existential typing and local state, as well as some well-known equivalences from the literature (such as Pitts and Stark's "awkward" example) that have caused trouble for previous logical-relations-based methods. The success of our method relies on two key technical innovations. First, in order to handle generative ADTs, we develop a possible-worlds model in which relational interpretations of types are allowed to grow over time in a manner that is tightly coupled with changes to some local state. Second, we employ a step-indexed stratification of possible worlds, which facilitates a simplified account of mutable references of higher type.},
  booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '09},
  publisher = {{ACM}},
  urldate = {2017-05-04},
  date = {2009},
  pages = {340--353},
  keywords = {abstract data types,existential types,_tablet,local state,step-indexed logical relations,representation independence},
  author = {Ahmed, Amal and Dreyer, Derek and Rossberg, Andreas},
  file = {/Users/pgiarrusso/Zotero/storage/FEP9Z6FC/Ahmed-Dreyer-Rossberg - 2009 - State-dependent Representation Independence.pdf}
}

@article{Dreyer2012impact,
  title = {The impact of higher-order state and control effects on local relational reasoning},
  volume = {22},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/impact-of-higherorder-state-and-control-effects-on-local-relational-reasoning/DA921680527426F49ED79644C8C6A565},
  doi = {10.1017/S095679681200024X},
  abstract = {AbstractReasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages—languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higher-order state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the “semantic cube”: fully abstract game-semantic characterizations of various axes in the design space of ML-like languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences.In this paper, we marry the aspirations of the semantic cube to the powerful proof method of step-indexed Kripke logical relations. Building on recent work of Ahmed et al. (2009), we define the first fully abstract logical relation for an ML-like language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power of our language—namely, the restriction to first-order state and/or the removal of call/cc—we can enhance the proving power of our possible-worlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  urldate = {2017-05-04},
  date = {2012-09},
  pages = {477-528},
  keywords = {_tablet},
  author = {Dreyer, Derek and Neis, Georg and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/N4W36H5E/Dreyer-Neis-Birkedal - 2012 - The impact of higher-order state and control effects on local relational reasoning.pdf;/Users/pgiarrusso/Zotero/storage/GK3QTW4E/DA921680527426F49ED79644C8C6A565.html}
}
% == BibLateX quality report for Dreyer2012impact:
% 'issn': not a valid ISSN

@inproceedings{Dreyer2010Impact,
  location = {{New York, NY, USA}},
  title = {The Impact of Higher-order State and Control Effects on Local Relational Reasoning},
  isbn = {978-1-60558-794-3},
  url = {http://doi.acm.org/10.1145/1863543.1863566},
  doi = {10.1145/1863543.1863566},
  abstract = {Reasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages - languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higher-order state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the "semantic cube": fully abstract game-semantic characterizations of various axes in the design space of ML-like languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences. In this paper, we marry the aspirations of the semantic cube to the powerful proof method of step-indexed Kripke logical relations. Building on recent work of Ahmed, Dreyer, and Rossberg, we define the first fully abstract logical relation for an ML-like language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power our language - namely, the restriction to first-order state and/or the removal of call/cc - we can enhance the proving power of our possible-worlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '10},
  publisher = {{ACM}},
  urldate = {2017-05-04},
  date = {2010},
  pages = {143--156},
  keywords = {higher-order state,_tablet,local state,Observational equivalence,biorthogonality,exceptions,first-class continuations,state transition systems,step-indexed kripke logical relations},
  author = {Dreyer, Derek and Neis, Georg and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/HIKEDCFJ/Dreyer-Neis-Birkedal - 2010 - The Impact of Higher-order State and Control Effects on Local Relational Reasoning.pdf}
}

@inproceedings{Benton2009Biorthogonality,
  location = {{New York, NY, USA}},
  title = {Biorthogonality, Step-indexing and Compiler Correctness},
  isbn = {978-1-60558-332-7},
  url = {http://doi.acm.org/10.1145/1596550.1596567},
  doi = {10.1145/1596550.1596567},
  abstract = {We define logical relations between the denotational semantics of a simply typed functional language with recursion and the operational behaviour of low-level programs in a variant SECD machine. The relations, which are defined using biorthogonality and stepindexing, capture what it means for a piece of low-level code to implement a mathematical, domain-theoretic function and are used to prove correctness of a simple compiler. The results have been formalized in the Coq proof assistant.},
  booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '09},
  publisher = {{ACM}},
  urldate = {2017-05-04},
  date = {2009},
  pages = {97--108},
  keywords = {Denotational semantics,proof assistants,biorthogonality,compiler verification,step-indexing},
  author = {Benton, Nick and Hur, Chung-Kil},
  file = {/Users/pgiarrusso/Zotero/storage/QPVT6G5W/Benton_Hur - 2009 - Biorthogonality, Step-indexing and Compiler Correctness.pdf}
}

@inproceedings{Hur2012Marriage,
  location = {{New York, NY, USA}},
  title = {The Marriage of Bisimulations and Kripke Logical Relations},
  isbn = {978-1-4503-1083-3},
  url = {http://doi.acm.org/10.1145/2103656.2103666},
  doi = {10.1145/2103656.2103666},
  abstract = {There has been great progress in recent years on developing effective techniques for reasoning about program equivalence in ML-like languages---that is, languages that combine features like higher-order functions, recursive types, abstract types, and general mutable references. Two of the most prominent types of techniques to have emerged are *bisimulations* and *Kripke logical relations (KLRs)*. While both approaches are powerful, their complementary advantages have led us and other researchers to wonder whether there is an essential tradeoff between them. Furthermore, both approaches seem to suffer from fundamental limitations if one is interested in scaling them to inter-language reasoning. In this paper, we propose *relation transition systems (RTSs)*, which marry together some of the most appealing aspects of KLRs and bisimulations. In particular, RTSs show how bisimulations' support for reasoning about recursive features via *coinduction* can be synthesized with KLRs' support for reasoning about local state via *state transition systems*. Moreover, we have designed RTSs to avoid the limitations of KLRs and bisimulations that preclude their generalization to inter-language reasoning. Notably, unlike KLRs, RTSs are transitively composable.},
  booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '12},
  publisher = {{ACM}},
  urldate = {2017-05-04},
  date = {2012},
  pages = {59--72},
  keywords = {Kripke Logical Relations,abstract types,higher-order state,recursive types,transitivity,contextual equivalence,bisimulations,global vs. local knowledge,relation transition systems},
  author = {Hur, Chung-Kil and Dreyer, Derek and Neis, Georg and Vafeiadis, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/4KHIBTAK/Hur et al - 2012 - The Marriage of Bisimulations and Kripke Logical Relations.pdf}
}

@inproceedings{Appel2007Very,
  location = {{New York, NY, USA}},
  title = {A Very Modal Model of a Modern, Major, General Type System},
  isbn = {978-1-59593-575-5},
  url = {http://doi.acm.org/10.1145/1190216.1190235},
  doi = {10.1145/1190216.1190235},
  abstract = {We present a model of recursive and impredicatively quantified types with mutable references. We interpret in this model all of the type constructors needed for typed intermediate languages and typed assembly languages used for object-oriented and functional languages. We establish in this purely semantic fashion a soundness proof of the typing systems underlying these TILs and TALs---ensuring that every well-typed program is safe. The technique is generic, and applies to any small-step semantics including λ-calculus, labeled transition systems, and von Neumann machines. It is also simple, and reduces mainly to defining a Kripke semantics of the Gödel-Löb logic of provability. We have mechanically verified in Coq the soundness of our type system as applied to a von Neumann machine.},
  booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '07},
  publisher = {{ACM}},
  urldate = {2017-05-03},
  date = {2007},
  pages = {109--122},
  keywords = {impredicative polymorphism,Kripke models,mutable references,recursive types},
  author = {Appel, Andrew W. and Melliès, Paul-André and Richards, Christopher D. and Vouillon, Jérôme},
  file = {/Users/pgiarrusso/Zotero/storage/RKAITVVJ/Appel et al - 2007 - A Very Modal Model of a Modern, Major, General Type System.pdf}
}

@article{Statman1982Completeness,
  eprinttype = {jstor},
  eprint = {2273377},
  title = {Completeness, Invariance and λ-Definability},
  volume = {47},
  issn = {0022-4812},
  doi = {10.2307/2273377},
  number = {1},
  journaltitle = {The Journal of Symbolic Logic},
  date = {1982},
  pages = {17-26},
  author = {Statman, R.},
  file = {/Users/pgiarrusso/Zotero/storage/9B8UPT2T/Statman - 1982 - Completeness, Invariance and λ-Definability.pdf}
}

@article{Statman1985Logical,
  title = {Logical relations and the typed λ-calculus},
  volume = {65},
  issn = {0019-9958},
  url = {http://www.sciencedirect.com/science/article/pii/S0019995885800012},
  doi = {10.1016/S0019-9958(85)80001-2},
  number = {2},
  journaltitle = {Information and Control},
  shortjournal = {Information and Control},
  urldate = {2017-04-28},
  date = {1985-05-01},
  pages = {85-97},
  author = {Statman, R.},
  file = {/Users/pgiarrusso/Zotero/storage/KGGQ5WMP/Statman - 1985 - Logical relations and the typed λ-calculus.pdf;/Users/pgiarrusso/Zotero/storage/WJBBIGUA/S0019995885800012.html}
}

@incollection{Bruce1984semantics,
  langid = {english},
  title = {The semantics of second order polymorphic lambda calculus},
  url = {https://link.springer.com/chapter/10.1007/3-540-13346-1_6},
  booktitle = {Semantics of Data Types},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-04-28},
  date = {1984},
  pages = {131-144},
  author = {Bruce, Kim B. and Meyer, Albert R.},
  file = {/Users/pgiarrusso/Zotero/storage/KE3K4TSV/10.html},
  doi = {10.1007/3-540-13346-1_6}
}
% == BibLateX quality report for Bruce1984semantics:
% Missing required field 'editor'

@inproceedings{Mitchell1985Secondorder,
  langid = {english},
  title = {Second-order logical relations},
  url = {https://link.springer.com/chapter/10.1007/3-540-15648-8_18},
  doi = {10.1007/3-540-15648-8_18},
  abstract = {Logical relations are a generalization of homomorphisms between models of typed lambda calculus. We define logical relations for second-order typed lambda calculus and use these relations to give a semantic characterization of second-order lambda definability. Logical relations are also used to state and prove a general representation independence theorem. Representation independence implies that the meanings of expressions do not depend on whether true is represented by 1 and false by 0, as long as all the functions that manipulate truth values are represented correctly.},
  eventtitle = {Workshop on Logic of Programs},
  booktitle = {Logics of Programs},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-04-28},
  date = {1985-06-17},
  pages = {225-236},
  author = {Mitchell, John C. and Meyer, Albert R.},
  file = {/Users/pgiarrusso/Zotero/storage/T5N9VGE5/10.html}
}
% == BibLateX quality report for Mitchell1985Secondorder:
% ? Unsure about the formatting of the booktitle

@inproceedings{Wadler1989Theorems,
  location = {{New York, NY, USA}},
  title = {Theorems for Free!},
  isbn = {978-0-89791-328-7},
  url = {http://doi.acm.org/10.1145/99370.99404},
  doi = {10.1145/99370.99404},
  booktitle = {Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture},
  series = {FPCA '89},
  publisher = {{ACM}},
  urldate = {2017-04-27},
  date = {1989},
  pages = {347--359},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/2HHRW8S7/Wadler - 1989 - Theorems for Free!.pdf}
}

@inproceedings{Choi2001Compiling,
  langid = {english},
  title = {Compiling Lazy Functional Programs Based on the Spineless Tagless G-machine for the Java Virtual Machine},
  url = {https://link.springer.com/chapter/10.1007/3-540-44716-4_6},
  doi = {10.1007/3-540-44716-4_6},
  abstract = {A systematic method of compiling lazy functional programs based on the Spineless Tagless G-machine (STGM) is presented for the Java Virtual Machine (JVM). A new specification of the STGM, which consists of a compiler and a reduction machine, is presented; the compiler translates a program in the STG language, which is the source language for the STGM, into a program in an intermediate language called L-code, and our reduction machine reduces the L-code program into an answer. With our representation for the reduction machine by the Java language, an L-code program is translated into a Java program simulating the reduction machine.The translated Java programs also run at a reasonable execution speed. Our experiment shows that execution times of translated benchmarks are competitive compared with those in a traditional Haskell interpreter, Hugs, particularly when Glasgow Haskell compiler’s STG-level optimizations are applied.},
  eventtitle = {International Symposium on Functional and Logic Programming},
  booktitle = {Functional and Logic Programming},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-04-27},
  date = {2001-03-07},
  pages = {92-107},
  author = {Choi, Kwanghoon and Lim, Hyun-il and Han, Taisook},
  file = {/Users/pgiarrusso/Zotero/storage/4TN963NC/Choi et al - 2001 - Compiling Lazy Functional Programs Based on the Spineless Tagless G-machine for the Java Virtual Machine.pdf;/Users/pgiarrusso/Zotero/storage/BVGMUHHE/3-540-44716-4_6.html}
}
% == BibLateX quality report for Choi2001Compiling:
% ? Unsure about the formatting of the booktitle

@inproceedings{Ancona2017Generalizing,
  langid = {english},
  title = {Generalizing Inference Systems by Coaxioms},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-54434-1_2},
  doi = {10.1007/978-3-662-54434-1_2},
  abstract = {We introduce a generalized notion of inference system to support structural recursion on non well-founded datatypes. Besides axioms and inference rules with the usual meaning, a generalized inference system allows coaxioms, which are, intuitively, axioms which can only be applied “at infinite depth” in a proof tree. This notion nicely subsumes standard inference systems and their inductive and coinductive interpretation, while providing more flexibility. Indeed, the classical results on the existence and constructive characterization of least and greatest fixed points can be extended to our generalized framework, interpreting recursive definitions as fixed points which are not necessarily the least, nor the greatest one. This allows formal reasoning in cases where the inductive and coinductive interpretation do not provide the intended meaning, or are mixed together.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-04-27},
  date = {2017-04-25},
  pages = {29-55},
  author = {Ancona, Davide and Dagnino, Francesco and Zucca, Elena},
  file = {/Users/pgiarrusso/Zotero/storage/NGJ2TPDN/978-3-662-54434-1_2.html}
}
% == BibLateX quality report for Ancona2017Generalizing:
% ? Unsure about the formatting of the booktitle

@article{Altenkirch2016Partiality,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09254},
  primaryClass = {cs},
  title = {Partiality, Revisited: The Partiality Monad as a Quotient Inductive-Inductive Type},
  url = {http://arxiv.org/abs/1610.09254},
  shorttitle = {Partiality, Revisited},
  abstract = {Capretta's delay monad can be used to model partial computations, but it has the "wrong" notion of built-in equality, strong bisimilarity. An alternative is to quotient the delay monad by the "right" notion of equality, weak bisimilarity. However, recent work by Chapman et al. suggests that it is impossible to define a monad structure on the resulting construction in common forms of type theory without assuming the axiom of countable choice. Using an idea from homotopy type theory -a higher inductive-inductive type- we construct a partiality monad without relying on countable choice. We prove that, in the presence of countable choice, our partiality monad is equivalent to the delay monad quotiented by weak bisimilarity. Finally, we outline several applications.},
  urldate = {2017-04-26},
  date = {2016-10-28},
  keywords = {Computer Science - Logic in Computer Science,F.4.1,03B15},
  author = {Altenkirch, Thorsten and Danielsson, Nils Anders and Kraus, Nicolai},
  file = {/Users/pgiarrusso/Zotero/storage/KUP5CZEJ/Altenkirch et al - 2016 - Partiality, Revisited - The Partiality Monad as a Quotient Inductive-Inductive Type.pdf;/Users/pgiarrusso/Zotero/storage/DBQ754QJ/1610.html}
}
% == BibLateX quality report for Altenkirch2016Partiality:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Kelly2016Evolving,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03429},
  primaryClass = {cs},
  title = {Evolving the Incremental \{$\backslash$lambda\} Calculus into a Model of Forward Automatic Differentiation (AD)},
  url = {http://arxiv.org/abs/1611.03429},
  abstract = {Formal transformations somehow resembling the usual derivative are surprisingly common in computer science, with two notable examples being derivatives of regular expressions and derivatives of types. A newcomer to this list is the incremental \$$\backslash$lambda\$-calculus, or ILC, a "theory of changes" that deploys a formal apparatus allowing the automatic generation of efficient update functions which perform incremental computation. The ILC is not only defined, but given a formal machine-understandable definition---accompanied by mechanically verifiable proofs of various properties, including in particular correctness of various sorts. Here, we show how the ILC can be mutated into propagating tangents, thus serving as a model of Forward Accumulation Mode Automatic Differentiation. This mutation is done in several steps. These steps can also be applied to the proofs, resulting in machine-checked proofs of the correctness of this model of forward AD.},
  urldate = {2017-04-16},
  date = {2016-11-10},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  author = {Kelly, Robert and Pearlmutter, Barak A. and Siskind, Jeffrey Mark},
  file = {/Users/pgiarrusso/Zotero/storage/4UVZKQTX/Kelly et al - 2016 - Evolving the Incremental -lambda Calculus into a Model of Forward Automatic Differentiation (AD).pdf;/Users/pgiarrusso/Zotero/storage/JSUHGPX3/1611.html}
}
% == BibLateX quality report for Kelly2016Evolving:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Demetrescu2017Programming,
  title = {Programming Language Techniques for Incremental and Reactive Computing (Dagstuhl Seminar 16402)},
  volume = {6},
  issn = {2192-5283},
  url = {http://drops.dagstuhl.de/opus/volltexte/2017/6949},
  doi = {10.4230/DagRep.6.10.1},
  number = {10},
  journaltitle = {Dagstuhl Reports},
  urldate = {2017-04-12},
  date = {2017},
  pages = {1--12},
  keywords = {memoization,change propagation,dataflow programming,reactive programming,dynamic dependency graph,Incremental computing,live programming},
  author = {Demetrescu, Camil and Erdweg, Sebastian and Hammer, Matthew A. and Krishnamurthi, Shriram},
  editor = {Demetrescu, Camil and Erdweg, Sebastian and Hammer, Matthew A. and Krishnamurthi, Shriram},
  file = {/Users/pgiarrusso/Zotero/storage/HMHP9DJN/Demetrescu et al - 2017 - Programming Language Techniques for Incremental and Reactive Computing (Dagstuhl Seminar 16402).pdf;/Users/pgiarrusso/Zotero/storage/R45FXU7M/6949.html}
}

@article{MorihataIncremental,
  title = {Incremental computing with data structures},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642317300692},
  doi = {10.1016/j.scico.2017.04.001},
  abstract = {Incremental computing is a method of maintaining consistency between an input and output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for nontrivial data structures, such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an method of incremental computing on data structures that may consist of complex modifications. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring's incremental computing method on algebraic data structures to one on more complex data structures. The method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  urldate = {2017-04-11},
  keywords = {Data Structures,Datatype-generic programming,Incremental computing,Shortcut fusion},
  author = {Morihata, Akimasa},
  file = {/Users/pgiarrusso/Zotero/storage/9M9KJ786/Morihata - Incremental computing with data structures.pdf;/Users/pgiarrusso/Zotero/storage/IZBZBN2W/S0167642317300692.html}
}
% == BibLateX quality report for MorihataIncremental:
% Exactly one of 'date' / 'year' must be present

@article{Pottier2006Polymorphic,
  langid = {english},
  title = {Polymorphic typed defunctionalization and concretization},
  volume = {19},
  issn = {1388-3690, 1573-0557},
  url = {https://link.springer.com/article/10.1007/s10990-006-8611-7},
  doi = {10.1007/s10990-006-8611-7},
  abstract = {Defunctionalization is a program transformation that eliminates functions as first-class values. We show that defunctionalization can be viewed as a type-preserving transformation of an extension of F with guarded algebraic data types into itself. We also suggest that defunctionalization is an instance of concretization, a more general technique that allows eliminating constructs other than functions. We illustrate this point by presenting two new type-preserving transformations that can be viewed as instances of concretization. One eliminates Rémy-style polymorphic records; the other eliminates the dictionary records introduced by the standard compilation scheme for Haskell’s type classes.},
  number = {1},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order Symb Comput},
  urldate = {2017-04-04},
  date = {2006-03-01},
  pages = {125-162},
  author = {Pottier, François and Gauthier, Nadji},
  file = {/Users/pgiarrusso/Zotero/storage/QUW7JIR8/10.html}
}
% == BibLateX quality report for Pottier2006Polymorphic:
% 'issn': not a valid ISSN

@inproceedings{Osvald2016Gentrification,
  location = {{New York, NY, USA}},
  title = {Gentrification Gone Too Far? Affordable 2Nd-class Values for Fun and (Co-)Effect},
  isbn = {978-1-4503-4444-9},
  url = {http://doi.acm.org/10.1145/2983990.2984009},
  doi = {10.1145/2983990.2984009},
  shorttitle = {Gentrification Gone Too Far?},
  abstract = {First-class functions dramatically increase expressiveness, at the expense of static guarantees. In ALGOL or PASCAL, functions could be passed as arguments but never escape their defining scope. Therefore, function arguments could serve as temporary access tokens or capabilities, enabling callees to perform some action, but only for the duration of the call. In modern languages, such programming patterns are no longer available. The central thrust of this paper is to re-introduce second-class functions and other values alongside first-class entities in modern languages. We formalize second-class values with stack-bounded lifetimes as an extension to simply-typed Î» calculus, and for richer type systems such as F$<$: and systems with path-dependent types. We generalize the binary first- vs second-class distinction to arbitrary privilege lattices, with the underlying type lattice as a special case. In this setting, abstract types naturally enable privilege parametricity. We prove type soundness and lifetime properties in Coq. We implement our system as an extension of Scala, and present several case studies. First, we modify the Scala Collections library and add privilege annotations to all higher-order functions. Privilege parametricity is key to retain the high degree of code-reuse between sequential and parallel as well as lazy and eager collections. Second, we use scoped capabilities to introduce a model of checked exceptions in the Scala library, with only few changes to the code. Third, we employ second-class capabilities for memory safety in a region-based off-heap memory library.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2016},
  publisher = {{ACM}},
  urldate = {2017-03-31},
  date = {2016},
  pages = {234--251},
  keywords = {types,capabilities,effects,first-class,object lifetimes,second-class},
  author = {Osvald, Leo and Essertel, Grégory and Wu, Xilun and Alayón, Lilliam I. González and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/RDNM2GUJ/Osvald et al - 2016 - Gentrification Gone Too Far - Affordable 2Nd-class Values for Fun and (Co-)Effect.pdf}
}

@inproceedings{Ploeg2016Key,
  location = {{New York, NY, USA}},
  title = {The Key Monad: Type-safe Unconstrained Dynamic Typing},
  isbn = {978-1-4503-4434-0},
  url = {http://doi.acm.org/10.1145/2976002.2976008},
  doi = {10.1145/2976002.2976008},
  shorttitle = {The Key Monad},
  abstract = {We present a small extension to Haskell called the Key monad. With the Key monad, unique keys of different types can be created and can be tested for equality. When two keys are equal, we also obtain a concrete proof that their types are equal. This gives us a form of dynamic typing, without the need for Typeable constraints. We show that our extension allows us to safely do things we could not otherwise do: it allows us to implement the ST monad (inefficiently), to implement an embedded form of arrow notation, and to translate parametric HOAS to typed de Bruijn indices, among others. Although strongly related to the ST monad, the Key monad is simpler and might be easier to prove safe. We do not provide such a proof of the safety of the Key monad, but we note that, surprisingly, a full proof of the safety of the ST monad also remains elusive to this day. Hence, another reason for studying the Key monad is that a safety proof for it might be a stepping stone towards a safety proof of the ST monad.},
  booktitle = {Proceedings of the 9th International Symposium on Haskell},
  series = {Haskell 2016},
  publisher = {{ACM}},
  urldate = {2017-03-27},
  date = {2016},
  pages = {146--157},
  keywords = {functional programming,Haskell,higher-order state,parametric hoas,Arrow notation,ST monad},
  author = {van der Ploeg, Atze and Claessen, Koen and Buiras, Pablo},
  file = {/Users/pgiarrusso/Zotero/storage/WFVJ6ASS/Ploeg et al - 2016 - The Key Monad - Type-safe Unconstrained Dynamic Typing.pdf}
}

@inproceedings{Erdweg2014CaptureAvoiding,
  langid = {english},
  title = {Capture-Avoiding and Hygienic Program Transformations},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-44202-9_20},
  doi = {10.1007/978-3-662-44202-9_20},
  abstract = {Program transformations in terms of abstract syntax trees compromise referential integrity by introducing variable capture. Variable capture occurs when in the generated program a variable declaration accidentally shadows the intended target of a variable reference. Existing transformation systems either do not guarantee the avoidance of variable capture or impair the implementation of transformations.We present an algorithm called name-fix that automatically eliminates variable capture from a generated program by systematically renaming variables. name-fix is guided by a graph representation of the binding structure of a program, and requires name-resolution algorithms for the source language and the target language of a transformation. name-fix is generic and works for arbitrary transformations in any transformation system that supports origin tracking for names. We verify the correctness of name-fix and identify an interesting class of transformations for which name-fix provides hygiene. We demonstrate the applicability of name-fix for implementing capture-avoiding substitution, inlining, lambda lifting, and compilers for two domain-specific languages.},
  eventtitle = {European Conference on Object-Oriented Programming},
  booktitle = {ECOOP 2014 – Object-Oriented Programming},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-03-26},
  date = {2014-07-28},
  pages = {489-514},
  author = {Erdweg, Sebastian and van der Storm, Tijs and Dai, Yi},
  file = {/Users/pgiarrusso/Zotero/storage/HSTVRBKP/978-3-662-44202-9_20.html}
}
% == BibLateX quality report for Erdweg2014CaptureAvoiding:
% ? Unsure about the formatting of the booktitle

@inproceedings{Orchard2016Effects,
  location = {{New York, NY, USA}},
  title = {Effects As Sessions, Sessions As Effects},
  isbn = {978-1-4503-3549-2},
  url = {http://doi.acm.org/10.1145/2837614.2837634},
  doi = {10.1145/2837614.2837634},
  abstract = {Effect and session type systems are two expressive behavioural type systems. The former is usually developed in the context of the lambda-calculus and its variants, the latter for the pi-calculus. In this paper we explore their relative expressive power. Firstly, we give an embedding from PCF, augmented with a parameterised effect system, into a session-typed pi-calculus (session calculus), showing that session types are powerful enough to express effects. Secondly, we give a reverse embedding, from the session calculus back into PCF, by instantiating PCF with concurrency primitives and its effect system with a session-like effect algebra; effect systems are powerful enough to express sessions. The embedding of session types into an effect system is leveraged to give a new implementation of session types in Haskell, via an effect system encoding. The correctness of this implementation follows from the second embedding result. We also discuss various extensions to our embeddings.},
  booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '16},
  publisher = {{ACM}},
  urldate = {2017-03-20},
  date = {2016},
  pages = {568--581},
  keywords = {type systems,Concurrent Haskell,effect systems,encoding,PCF,session types},
  author = {Orchard, Dominic and Yoshida, Nobuko},
  file = {/Users/pgiarrusso/Zotero/storage/8D4ZI7CT/Orchard_Yoshida - 2016 - Effects As Sessions, Sessions As Effects.pdf}
}

@inproceedings{Schwaab2013Modular,
  location = {{New York, NY, USA}},
  title = {Modular Type-safety Proofs in Agda},
  isbn = {978-1-4503-1860-0},
  url = {http://doi.acm.org/10.1145/2428116.2428120},
  doi = {10.1145/2428116.2428120},
  abstract = {Methods for reusing code are widespread and well researched, but methods for reusing proofs are still emerging. We consider the use of dependent types for this purpose, introducing a modular approach for composing mechanized proofs. We show that common techniques for abstracting algorithms over data structures naturally translate to abstractions over proofs. We introduce a language composed of a series of smaller language components, each defined as functors, and tie them together by taking the fixed point of their sum [Malcom, 1990]. We then give proofs of type preservation for each language component and show how to compose these proofs into a proof for the entire language, again by taking the fixed point of a sum of functors.},
  booktitle = {Proceedings of the 7th Workshop on Programming Languages Meets Program Verification},
  series = {PLPV '13},
  publisher = {{ACM}},
  urldate = {2017-03-09},
  date = {2013},
  pages = {3--12},
  keywords = {modularity,Agda,meta-theory},
  author = {Schwaab, Christopher and Siek, Jeremy G.},
  file = {/Users/pgiarrusso/Zotero/storage/52CDZBKC/Schwaab_Siek - 2013 - Modular Type-safety Proofs in Agda.pdf}
}

@inproceedings{Altenkirch1999Monadic,
  langid = {english},
  title = {Monadic Presentations of Lambda Terms Using Generalized Inductive Types},
  isbn = {978-3-540-66536-6 978-3-540-48168-3},
  url = {http://link.springer.com/chapter/10.1007/3-540-48168-0_32},
  doi = {10.1007/3-540-48168-0_32},
  abstract = {We present a definition of untyped λ-terms using a heterogeneous datatype, i.e. an inductively defined operator. This operator can be extended to a Kleisli triple, which is a concise way to verify the substitution laws for λ-calculus. We also observe that repetitions in the definition of the monad as well as in the proofs can be avoided by using well-founded recursion and induction instead of structural induction. We extend the construction to the simply typed λ-calculus using dependent types, and show that this is an instance of a generalization of Kleisli triples. The proofs for the untyped case have been checked using the LEGO system.},
  eventtitle = {International Workshop on Computer Science Logic},
  booktitle = {Computer Science Logic},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2017-03-05},
  date = {1999-09-20},
  pages = {453-468},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,type theory,Theory of Computation,Mathematical Logic and Foundations,category theory,λ-Calculus,inductive types},
  author = {Altenkirch, Thorsten and Reus, Bernhard},
  editor = {Flum, Jörg and Rodriguez-Artalejo, Mario},
  file = {/Users/pgiarrusso/Zotero/storage/RXCQQMPK/Altenkirch_Reus - 1999 - Monadic Presentations of Lambda Terms Using Generalized Inductive Types.pdf;/Users/pgiarrusso/Zotero/storage/6SPV7TX9/3-540-48168-0_32.html}
}
% == BibLateX quality report for Altenkirch1999Monadic:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Owens2016Functional,
  location = {{New York, NY, USA}},
  title = {Functional Big-Step Semantics},
  isbn = {978-3-662-49497-4},
  url = {http://dx.doi.org/10.1007/978-3-662-49498-1_23},
  doi = {10.1007/978-3-662-49498-1_23},
  abstract = {When doing an interactive proof about a piece of software, it is important that the underlying programming language's semantics does not make the proof unnecessarily difficult or unwieldy. Both small-step and big-step semantics are commonly used, and the latter is typically given by an inductively defined relation. In this paper, we consider an alternative: using a recursive function akinï¾¿to an interpreter for the language. The advantages include a better induction theorem, less duplication, accessibility to ordinary functional programmers, and the ease of doing symbolic simulation in proofs via rewriting. We believe that this style of semantics is well suited for compiler verification, including proofs of divergence preservation. We do not claim the invention of this style of semantics: our contribution here is to clarify its value, and to explain how it supports several language features that might appear to require a relational or small-step approach. We illustrate the technique on a simple imperative language with C-like for-loops and a break statement, and compare it to a variety of other approaches. We also provide ML and lambda-calculus based examples to illustrate its generality.},
  booktitle = {Proceedings of the 25th European Symposium on Programming Languages and Systems - Volume 9632},
  publisher = {{Springer-Verlag New York, Inc.}},
  urldate = {2017-02-28},
  date = {2016},
  pages = {589--615},
  author = {Owens, Scott and Myreen, Magnus O. and Kumar, Ramana and Tan, Yong Kiam},
  file = {/Users/pgiarrusso/Zotero/storage/FMCDCGVB/Owens et al - 2016 - Functional Big-Step Semantics.pdf}
}
% == BibLateX quality report for Owens2016Functional:
% 'isbn': not a valid ISBN

@article{Dreyer2011Logical,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1103.0510},
  title = {Logical Step-Indexed Logical Relations},
  volume = {7},
  issn = {18605974},
  url = {http://arxiv.org/abs/1103.0510},
  doi = {10.2168/LMCS-7(2:16)2011},
  abstract = {Appel and McAllester's "step-indexed" logical relations have proven to be a simple and effective technique for reasoning about programs in languages with semantically interesting types, such as general recursive types and general reference types. However, proofs using step-indexed models typically involve tedious, error-prone, and proof-obscuring step-index arithmetic, so it is important to develop clean, high-level, equational proof principles that avoid mention of step indices. In this paper, we show how to reason about binary step-indexed logical relations in an abstract and elegant way. Specifically, we define a logic LSLR, which is inspired by Plotkin and Abadi's logic for parametricity, but also supports recursively defined relations by means of the modal "later" operator from Appel, Melli$\backslash$`es, Richards, and Vouillon's "very modal model" paper. We encode in LSLR a logical relation for reasoning relationally about programs in call-by-value System F extended with general recursive types. Using this logical relation, we derive a set of useful rules with which we can prove contextual equivalence and approximation results without counting steps.},
  number = {2},
  journaltitle = {Logical Methods in Computer Science},
  urldate = {2017-02-21},
  date = {2011-06-07},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,D.3.3; F.3.1; F.3.3,_tablet_modified},
  author = {Dreyer, Derek and Ahmed, Amal and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/8Z2NSG85/Dreyer-Ahmed-Birkedal - 2011 - Logical Step-Indexed Logical Relations.pdf;/Users/pgiarrusso/Zotero/storage/SFTNDRUW/1103.html}
}
% == BibLateX quality report for Dreyer2011Logical:
% Unexpected field 'archivePrefix'

@inproceedings{Castro2016Farms,
  location = {{New York, NY, USA}},
  title = {Farms, Pipes, Streams and Reforestation: Reasoning About Structured Parallel Processes Using Types and Hylomorphisms},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951920},
  doi = {10.1145/2951913.2951920},
  shorttitle = {Farms, Pipes, Streams and Reforestation},
  abstract = {The increasing importance of parallelism has motivated the creation of better abstractions for writing parallel software, including structured parallelism using nested algorithmic skeletons. Such approaches provide high-level abstractions that avoid common problems, such as race conditions, and often allow strong cost models to be defined. However, choosing a combination of algorithmic skeletons that yields good parallel speedups for a program on some specific parallel architecture remains a difficult task. In order to achieve this, it is necessary to simultaneously reason both about the costs of different parallel structures and about the semantic equivalences between them. This paper presents a new type-based mechanism that enables strong static reasoning about these properties. We exploit well-known properties of a very general recursion pattern, hylomorphisms, and give a denotational semantics for structured parallel processes in terms of these hylomorphisms. Using our approach, it is possible to determine formally whether it is possible to introduce a desired parallel structure into a program without altering its functional behaviour, and also to choose a version of that parallel structure that minimises some given cost model.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2017-02-10},
  date = {2016},
  pages = {4--17},
  keywords = {type systems,hylomorphisms,Parallelism,term rewriting systems},
  author = {Castro, David and Hammond, Kevin and Sarkar, Susmit},
  file = {/Users/pgiarrusso/Zotero/storage/4E9XJ763/Castro et al - 2016 - Farms, Pipes, Streams and Reforestation - Reasoning About Structured Parallel Processes Using Types and Hylomorphisms.pdf}
}

@inproceedings{Accattoli2014Beta,
  location = {{New York, NY, USA}},
  title = {Beta Reduction is Invariant, Indeed},
  isbn = {978-1-4503-2886-9},
  url = {http://doi.acm.org/10.1145/2603088.2603105},
  doi = {10.1145/2603088.2603105},
  abstract = {Slot and van Emde Boas' weak invariance thesis states that reasonable machines can simulate each other within a polynomially overhead in time. Is λ-calculus a reasonable machine? Is there a way to measure the computational complexity of a λ-term? This paper presents the first complete positive answer to this long-standing problem. Moreover, our answer is completely machine-independent and based over a standard notion in the theory of λ-calculus: the length of a leftmost-outermost derivation to normal form is an invariant cost model. Such a theorem cannot be proved by directly relating λ-calculus with Turing machines or random access machines, because of the size explosion problem: there are terms that in a linear number of steps produce an exponentially long output. The first step towards the solution is to shift to a notion of evaluation for which the length and the size of the output are linearly related. This is done by adopting the linear substitution calculus (LSC), a calculus of explicit substitutions modelled after linear logic proof nets and admitting a decomposition of leftmost-outermost derivations with the desired property. Thus, the LSC is invariant with respect to, say, random access machines. The second step is to show that LSC is invariant with respect to the λ-calculus. The size explosion problem seems to imply that this is not possible: having the same notions of normal form, evaluation in the LSC is exponentially longer than in the λ-calculus. We solve such an impasse by introducing a new form of shared normal form and shared reduction, deemed useful. Useful evaluation avoids those steps that only unshare the output without contributing to β-redexes, i.e. the steps that cause the blow-up in size. The main technical contribution of the paper is indeed the definition of useful reductions and the thorough analysis of their properties.},
  booktitle = {Proceedings of the Joint Meeting of the Twenty-Third EACSL Annual Conference on Computer Science Logic (CSL) and the Twenty-Ninth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)},
  series = {CSL-LICS '14},
  publisher = {{ACM}},
  urldate = {2017-02-10},
  date = {2014},
  pages = {8:1--8:10},
  keywords = {sharing,$\\lambda$-calculus,computational complexity,cost models,explicit substitutions},
  author = {Accattoli, Beniamino and Dal Lago, Ugo},
  file = {/Users/pgiarrusso/Zotero/storage/EGSQRKQC/Accattoli_Dal Lago - 2014 - Beta Reduction is Invariant, Indeed.pdf}
}

@article{Schmidt-SchaussImprovements,
  title = {Improvements in a call-by-need functional core language: Common subexpression elimination and resource preserving translations},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642317300199},
  doi = {10.1016/j.scico.2017.01.001},
  shorttitle = {Improvements in a call-by-need functional core language},
  abstract = {An improvement is a correct program transformation that optimizes the program, where the criterion is that the number of computation steps until a value is obtained is not strictly increased in any context. This paper investigates improvements in both – an untyped and a polymorphically typed variant – of a call-by-need lambda calculus with letrec, case, constructors and seq. Besides showing that several local transformations are optimizations, a main result of this paper is a proof that common subexpression elimination is correct and an improvement, which proves a conjecture and thus closes a gap in the improvement theory of Moran and Sands. The improvement relation used in this paper is generic in which essential computation steps are counted and thus the obtained results apply for several notions of improvement. Besides the small-step operational semantics, also an abstract machine semantics is considered for counting computation steps. We show for several length measures that the call-by-need calculus of Moran and Sands and our calculus are equivalent.},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  urldate = {2017-02-10},
  keywords = {functional programming,Lazy Evaluation,lambda calculus,semantics,Improvement},
  author = {Schmidt-Schauß, Manfred and Sabel, David},
  file = {/Users/pgiarrusso/Zotero/storage/GHDTR6WU/Schmidt-Schauß_Sabel - Improvements in a call-by-need functional core language - Common subexpression elimination and resource preserving translations.pdf;/Users/pgiarrusso/Zotero/storage/JDZ4KUC6/S0167642317300199.html}
}
% == BibLateX quality report for Schmidt-SchaussImprovements:
% Exactly one of 'date' / 'year' must be present

@article{Ariola2009typetheoretic,
  langid = {english},
  title = {A type-theoretic foundation of delimited continuations},
  volume = {22},
  issn = {1388-3690, 1573-0557},
  url = {http://link.springer.com/article/10.1007/s10990-007-9006-0},
  doi = {10.1007/s10990-007-9006-0},
  abstract = {There is a correspondence between classical logic and programming language calculi with first-class continuations. With the addition of control delimiters, the continuations become composable and the calculi become more expressive. We present a fine-grained analysis of control delimiters and formalise that their addition corresponds to the addition of a single dynamically-scoped variable modelling the special top-level continuation. From a type perspective, the dynamically-scoped variable requires effect annotations. In the presence of control, the dynamically-scoped variable can be interpreted in a purely functional way by applying a store-passing style. At the type level, the effect annotations are mapped within standard classical logic extended with the dual of implication, namely subtraction. A continuation-passing-style transformation of lambda-calculus with control and subtraction is defined. Combining the translations provides a decomposition of standard CPS transformations for delimited continuations. Incidentally, we also give a direct normalisation proof of the simply-typed lambda-calculus with control and subtraction.},
  number = {3},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order Symb Comput},
  urldate = {2017-02-07},
  date = {2009-09-01},
  pages = {233-273},
  author = {Ariola, Zena M. and Herbelin, Hugo and Sabry, Amr},
  file = {/Users/pgiarrusso/Zotero/storage/CQWRGBGB/Ariola et al - 2009 - A type-theoretic foundation of delimited continuations.pdf;/Users/pgiarrusso/Zotero/storage/3NXKPIID/Ariola et al. - 2009 - A type-theoretic foundation of delimited continuat.html}
}
% == BibLateX quality report for Ariola2009typetheoretic:
% 'issn': not a valid ISSN

@inproceedings{Steele2009Organizing,
  location = {{New York, NY, USA}},
  title = {Organizing Functional Code for Parallel Execution or, Foldl and Foldr Considered Slightly Harmful},
  isbn = {978-1-60558-332-7},
  url = {http://doi.acm.org/10.1145/1596550.1596551},
  doi = {10.1145/1596550.1596551},
  abstract = {Alan Perlis, inverting OscarWilde's famous quip about cynics, once suggested, decades ago, that a Lisp programmer is one who knows the value of everything and the cost of nothing. Now that the conference on Lisp and Functional Programming has become ICFP, some may think that OCaml and Haskell programmers have inherited this (now undeserved) epigram. I do believe that as multicore processors are becoming prominent, and soon ubiquitous, it behooves all programmers to rethink their programming style, strategies, and tactics, so that their code may have excellent performance. For the last six years I have been part of a team working on a programming language, Fortress, that has borrowed ideas not only from Fortran, not only from Java, not only from Algol and Alphard and CLU, not only from MADCAP and MODCAP and MIRFAC and the Klerer-May system-but also from Haskell, and I would like to repay the favor. In this talk I will discuss three ideas (none original with me) that I have found to be especially powerful in organizing Fortress programs so that they may be executed equally effectively either sequentially or in parallel: user-defined associative operators (and, more generally, user-defined monoids); conjugate transforms of data; and monoid-caching trees (as described, for example, by Hinze and Paterson). I will exhibit pleasant little code examples (some original with me) that make use of these ideas.},
  booktitle = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '09},
  publisher = {{ACM}},
  urldate = {2017-02-07},
  date = {2009},
  pages = {1--2},
  keywords = {associative operator,conjugate transform,monoid,reduction,tree},
  author = {Steele, Jr., Guy L.}
}

@inproceedings{Cicek2016Type,
  location = {{New York, NY, USA}},
  title = {A Type Theory for Incremental Computational Complexity with Control Flow Changes},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951950},
  doi = {10.1145/2951913.2951950},
  abstract = {Incremental computation aims to speed up re-runs of a program after its inputs have been modified slightly. It works by recording a trace of the program's first run and propagating changes through the trace in incremental runs, trying to re-use as much of the original trace as possible. The recent work CostIt is a type and effect system to establish the time complexity of incremental runs of a program, as a function of input changes. However, CostIt is limited in two ways. First, it prohibits input changes that influence control flow. This makes it impossible to type programs that, for instance, branch on inputs that may change. Second, the soundness of CostIt is proved relative to an abstract cost semantics, but it is unclear how the semantics can be realized.   In this paper, we address both these limitations. We present DuCostIt, a re-design of CostIt, that combines reasoning about costs of change propagation and costs of from-scratch evaluation. The latter lifts the restriction on control flow changes. To obtain the type system, we refine Flow Caml, a type system for information flow analysis, with cost effects. Additionally, we inherit from CostIt index refinements to track data structure sizes and a co-monadic type. Using a combination of binary and unary step-indexed logical relations, we prove DuCostIt's cost analysis sound relative to not only an abstract cost semantics, but also a concrete semantics, which is obtained by translation to an ML-like language.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2017-02-07},
  date = {2016},
  pages = {132--145},
  keywords = {incremental computation,type and effect systems,complexity analysis},
  author = {Çiçek, Ezgi and Paraskevopoulou, Zoe and Garg, Deepak},
  file = {/Users/pgiarrusso/Zotero/storage/BUQH5SBT/Çiçek et al - 2016 - A Type Theory for Incremental Computational Complexity with Control Flow Changes.pdf}
}

@inproceedings{Jovanovic2014Yinyang,
  location = {{New York, NY, USA}},
  title = {Yin-yang: Concealing the Deep Embedding of DSLs},
  isbn = {978-1-4503-3161-6},
  url = {http://doi.acm.org/10.1145/2658761.2658771},
  doi = {10.1145/2658761.2658771},
  shorttitle = {Yin-yang},
  abstract = {Deeply embedded domain-specific languages (EDSLs) intrinsically compromise programmer experience for improved program performance. Shallow EDSLs complement them by trading program performance for good programmer experience. We present Yin-Yang, a framework for DSL embedding that uses Scala macros to reliably translate shallow EDSL programs to the corresponding deep EDSL programs. The translation allows program prototyping and development in the user friendly shallow embedding, while the corresponding deep embedding is used where performance is important. The reliability of the translation completely conceals the deep em- bedding from the user. For the DSL author, Yin-Yang automatically generates the deep DSL embeddings from their shallow counterparts by reusing the core translation. This obviates the need for code duplication and leads to reliability by construction.},
  booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
  series = {GPCE 2014},
  publisher = {{ACM}},
  urldate = {2017-02-02},
  date = {2014},
  pages = {73--82},
  keywords = {embedded domain-specific languages,reflection,macros,Deep Embedding,Shallow Embedding},
  author = {Jovanovic, Vojin and Shaikhha, Amir and Stucki, Sandro and Nikolaev, Vladimir and Koch, Christoph and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/6GPP3X68/Jovanovic et al - 2014 - Yin-yang - Concealing the Deep Embedding of DSLs.pdf}
}

@article{Muralidharan2016Designing,
  title = {Designing a Tunable Nested Data-Parallel Programming System},
  volume = {13},
  issn = {1544-3566},
  url = {http://doi.acm.org/10.1145/3012011},
  doi = {10.1145/3012011},
  abstract = {This article describes Surge, a nested data-parallel programming system designed to simplify the porting and tuning of parallel applications to multiple target architectures. Surge decouples high-level specification of computations, expressed using a C++ programming interface, from low-level implementation details using two first-class constructs: schedules and policies. Schedules describe the valid ways in which data-parallel operators may be implemented, while policies encapsulate a set of parameters that govern platform-specific code generation. These two mechanisms are used to implement a code generation system that analyzes computations and automatically generates a search space of valid platform-specific implementations. An input and architecture-adaptive autotuning system then explores this search space to find optimized implementations. We express in Surge five real-world benchmarks from domains such as machine learning and sparse linear algebra and from the high-level specifications, Surge automatically generates CPU and GPU implementations that perform on par with or better than manually optimized versions.},
  number = {4},
  journaltitle = {ACM Trans. Archit. Code Optim.},
  urldate = {2017-01-20},
  date = {2016-12},
  pages = {47:1--47:24},
  keywords = {performance portability,nested data parallelism,autotuning},
  author = {Muralidharan, Saurav and Garland, Michael and Sidelnik, Albert and Hall, Mary},
  file = {/Users/pgiarrusso/Zotero/storage/5MRX9TB9/Muralidharan et al - 2016 - Designing a Tunable Nested Data-Parallel Programming System.pdf}
}
% == BibLateX quality report for Muralidharan2016Designing:
% ? Possibly abbreviated journal title ACM Trans. Archit. Code Optim.

@article{Danilewski2016ManyDSL,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.03488},
  primaryClass = {cs},
  title = {ManyDSL: A Host for Many Languages},
  url = {http://arxiv.org/abs/1612.03488},
  shorttitle = {ManyDSL},
  abstract = {Domain-specific languages are becoming increasingly important. Almost every application touches multiple domains. But how to define, use, and combine multiple DSLs within the same application? The most common approach is to split the project along the domain boundaries into multiple pieces and files. Each file is then compiled separately. Alternatively, multiple languages can be embedded in a flexible host language: within the same syntax a new domain semantic is provided. In this paper we follow a less explored route of metamorphic languages. These languages are able to modify their own syntax and semantics on the fly, thus becoming a more flexible host for DSLs. Our language allows for dynamic creation of grammars and switching languages where needed. We achieve this through a novel concept of Syntax-Directed Execution. A language grammar includes semantic actions that are pieces of functional code executed immediately during parsing. By avoiding additional intermediate representation, connecting actions from different languages and domains is greatly simplified. Still, actions can generate highly specialized code though lambda encapsulation and Dynamic Staging.},
  urldate = {2017-01-20},
  date = {2016-12-11},
  keywords = {Computer Science - Programming Languages},
  author = {Danilewski, Piotr and Slusallek, Philipp},
  file = {/Users/pgiarrusso/Zotero/storage/WAERX8U2/Danilewski_Slusallek - 2016 - ManyDSL - A Host for Many Languages.pdf;/Users/pgiarrusso/Zotero/storage/NIB6BVU2/1612.html}
}
% == BibLateX quality report for Danilewski2016ManyDSL:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Leijen2017Type,
  location = {{New York, NY, USA}},
  title = {Type Directed Compilation of Row-typed Algebraic Effects},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009872},
  doi = {10.1145/3009837.3009872},
  abstract = {Algebraic effect handlers, introduced by Plotkin and Power in 2002,   are recently gaining in popularity as a purely functional approach to   modeling effects. In this article, we give a full overview of   practical algebraic effects in the context of a compiled   implementation in the Koka language. In particular, we show how   algebraic effects generalize over common constructs like exception   handling, state, iterators and async-await. We give an effective type   inference algorithm based on extensible effect rows using scoped   labels, and a direct operational semantics. Finally, we show an   efficient compilation scheme to common runtime platforms (like   JavaScript) using a type directed selective CPS translation.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {486--499},
  keywords = {type inference,Algebraic Effect Handlers,CPS transformation},
  author = {Leijen, Daan},
  file = {/Users/pgiarrusso/Zotero/storage/7ITNDRFB/Leijen - 2017 - Type Directed Compilation of Row-typed Algebraic Effects.pdf}
}

@inproceedings{Amin2017LMSVerify,
  location = {{New York, NY, USA}},
  title = {LMS-Verify: Abstraction Without Regret for Verified Systems Programming},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009867},
  doi = {10.1145/3009837.3009867},
  shorttitle = {LMS-Verify},
  abstract = {Performance critical software is almost always developed in C, as programmers do not trust high-level languages to deliver the same reliable performance. This is bad because low-level code in unsafe languages attracts security vulnerabilities and because development is far less productive, with PL advances mostly lost on programmers operating under tight performance constraints. High-level languages provide memory safety out of the box, but they are deemed too slow and unpredictable for serious system software.   Recent years have seen a surge in staging and generative programming: the key idea is to use high-level languages and their abstraction power as glorified macro systems to compose code fragments in first-order, potentially domain-specific, intermediate languages, from which fast C can be emitted. But what about security? Since the end result is still C code, the safety guarantees of the high-level host language are lost.   In this paper, we extend this generative approach to emit ACSL specifications along with C code. We demonstrate that staging achieves ``abstraction without regret'' for verification: we show how high-level programming models, in particular higher-order composable contracts from dynamic languages, can be used at generation time to compose and generate first-order specifications that can be statically checked by existing tools. We also show how type classes can automatically attach invariants to data types, reducing the need for repetitive manual annotations.   We evaluate our system on several case studies that varyingly exercise verification of memory safety, overflow safety, and functional correctness. We feature an HTTP parser that is (1) fast (2) high-level: implemented using staged parser combinators (3) secure: with verified memory safety. This result is significant, as input parsing is a key attack vector, and vulnerabilities related to HTTP parsing have been documented in all widely-used web servers.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {859--873},
  keywords = {Contracts,dsls,Verification,security,blame,Frama-C,LMS,memory safety},
  author = {Amin, Nada and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/9FZSAA79/Amin_Rompf - 2017 - LMS-Verify - Abstraction Without Regret for Verified Systems Programming.pdf}
}

@inproceedings{Ahman2017Dijkstra,
  location = {{New York, NY, USA}},
  title = {Dijkstra Monads for Free},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009878},
  doi = {10.1145/3009837.3009878},
  abstract = {Dijkstra monads enable a dependent type theory to be enhanced with support for specifying and verifying effectful code via weakest preconditions. Together with their closely related counterparts, Hoare monads, they provide the basis on which verification tools like F*, Hoare Type Theory (HTT), and Ynot are built. We show that Dijkstra monads can be derived âfor freeâ by applying a continuation-passing style (CPS) translation to the standard monadic definitions of the underlying computational effects. Automatically deriving Dijkstra monads in this way provides a correct-by-construction and efficient way of reasoning about user-defined effects in dependent type theories. We demonstrate these ideas in EMF*, a new dependently typed calculus, validating it via both formal proof and a prototype implementation within F*. Besides equipping F* with a more uniform and extensible effect system, EMF* enables a novel mixture of intrinsic and extrinsic proofs within F*.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {515--529},
  keywords = {dependent types,proof assistants,Verification,effectful programming},
  author = {Ahman, Danel and Hriţcu, Cătălin and Maillard, Kenji and Martínez, Guido and Plotkin, Gordon and Protzenko, Jonathan and Rastogi, Aseem and Swamy, Nikhil},
  file = {/Users/pgiarrusso/Zotero/storage/QRIJTMNS/Ahman et al - 2017 - Dijkstra Monads for Free.pdf}
}

@inproceedings{Brown2017Typed,
  location = {{New York, NY, USA}},
  title = {Typed Self-evaluation via Intensional Type Functions},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009853},
  doi = {10.1145/3009837.3009853},
  abstract = {Many popular languages have a self-interpreter, that is, an interpreter for the language written in itself. So far, work on polymorphically-typed self-interpreters has concentrated on self-recognizers that merely recover a program from its representation. A larger and until now unsolved challenge is to implement a polymorphically-typed self-evaluator that evaluates the represented program and produces a representation of the result. We present FÏÂµi, the first Î»-calculus that supports a polymorphically-typed self-evaluator. Our calculus extends FÏ with recursive types and intensional type functions and has decidable type checking. Our key innovation is a novel implementation of type equality proofs that enables us to define a versatile representation of programs. Our results establish a new category of languages that can support polymorphically-typed self-evaluators.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {415--428},
  keywords = {lambda calculus,type equality,meta programming,Self Evaluation,Self Interpretation,Self Representation},
  author = {Brown, Matt and Palsberg, Jens},
  file = {/Users/pgiarrusso/Zotero/storage/3IMKWBMK/Brown_Palsberg - 2017 - Typed Self-evaluation via Intensional Type Functions.pdf}
}

@inproceedings{Chang2017Type,
  location = {{New York, NY, USA}},
  title = {Type Systems As Macros},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009886},
  doi = {10.1145/3009837.3009886},
  abstract = {We present Turnstile, a metalanguage for creating typed embedded languages. To implement the type system, programmers write type checking rules resembling traditional judgment syntax. To implement the semantics, they incorporate elaborations into these rules. Turnstile critically depends on the idea of linguistic reuse. It exploits a macro system in a novel way to simultaneously type check and rewrite a surface program into a target language. Reusing a macro system also yields modular implementations whose rules may be mixed and matched to create other languages. Combined with typical compiler and runtime reuse, Turnstile produces performant typed embedded languages with little effort.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {694--705},
  keywords = {type systems,macros,typed embedded DSLs},
  author = {Chang, Stephen and Knauth, Alex and Greenman, Ben},
  file = {/Users/pgiarrusso/Zotero/storage/UA4PKVA4/Chang et al - 2017 - Type Systems As Macros.pdf}
}

@inproceedings{Scherer2017Deciding,
  location = {{New York, NY, USA}},
  title = {Deciding Equivalence with Sums and the Empty Type},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009901},
  doi = {10.1145/3009837.3009901},
  abstract = {The logical technique of focusing can be applied to the Î»-calculus; in a simple type system with atomic types and negative type formers (functions, products, the unit type), its normal forms coincide with Î²Î·-normal forms. Introducing a saturation phase gives a notion of quasi-normal forms in presence of positive types (sum types and the empty type). This rich structure let us prove the decidability of Î²Î·-equivalence in presence of the empty type, the fact that it coincides with contextual equivalence, and with set-theoretic equality in all finite models.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {374--386},
  keywords = {canonicity,focusing,saturation,simply-typed lambda-calculus,sums,equivalence,empty type},
  author = {Scherer, Gabriel},
  file = {/Users/pgiarrusso/Zotero/storage/BT56XUHD/Scherer - 2017 - Deciding Equivalence with Sums and the Empty Type.pdf}
}

@inproceedings{Giarrusso2013Open,
  location = {{New York, NY, USA}},
  title = {Open GADTs and Declaration-site Variance: A Problem Statement},
  isbn = {978-1-4503-2064-1},
  url = {http://doi.acm.org/10.1145/2489837.2489842},
  doi = {10.1145/2489837.2489842},
  shorttitle = {Open GADTs and Declaration-site Variance},
  abstract = {Generalized algebraic data types (GADTs) allow embedding extensible typed ASTs and transformations on them. Such transformations on typed ASTs are useful for code optimization in deeply embedded DSLs, for instance when using Lightweight Modular Staging (LMS). However, in Scala it is hard to make transformations for typed ASTs type-safe. Therefore, AST transformations in LMS are often not fully typechecked, preventing bugs from being caught early and without extensive testing. We show that writing type-safe transformations in such embeddings is in fact not just hard, but impossible without using unsafe casts or significantly restricting extensibility: Declaration-site variance opens GADTs representing typed ASTs not only to desirable extensions, but also to extensions that introduce exotic terms. We make the problem concrete on an embedding of λ$<$: through covariant GADTs. We discuss solution approaches, and sketch a Scala extension to address this problem without either introducing unsafe casts or restricting extensibility. We believe a complete solution would significantly ease writing transformations by allowing type-checking to verify them, and thus would ease their development.},
  booktitle = {Proceedings of the 4th Workshop on Scala},
  series = {SCALA '13},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2013},
  pages = {5:1--5:4},
  keywords = {Scala,Lambda-calculus,Type-safety,DSL embedding,soundness,lightweight modular staging},
  author = {Giarrusso, Paolo G.},
  file = {/Users/pgiarrusso/Zotero/storage/HXBH86RI/Giarrusso - 2013 - Open GADTs and Declaration-site Variance - A Problem Statement.pdf}
}

@inproceedings{Augustsson1999exercise,
  title = {An exercise in dependent types: A well-typed interpreter},
  shorttitle = {An exercise in dependent types},
  abstract = {The result type of an interpreter written in a typed language is normally a tagged union. By using depent types, we can be more precise about the type of values that the intepreter returns. There is no need for tagging these values with their type, something which opens the door to more efficient interpreters.},
  booktitle = {In Workshop on Dependent Types in Programming, Gothenburg},
  date = {1999},
  author = {Augustsson, Lennart and Carlsson, Magnus},
  file = {/Users/pgiarrusso/Zotero/storage/78VRS7N7/Augustsson_Carlsson - 1999 - An exercise in dependent types - A well-typed interpreter.pdf;/Users/pgiarrusso/Zotero/storage/4CEEK5VJ/summary.html}
}
% == BibLateX quality report for Augustsson1999exercise:
% ? Unsure about the formatting of the booktitle

@inproceedings{Cicek2017Relational,
  location = {{New York, NY, USA}},
  title = {Relational Cost Analysis},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009858},
  doi = {10.1145/3009837.3009858},
  abstract = {Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis, compiler optimizations, security and privacy. Techniques based on program analysis, type systems and abstract interpretation are well-studied, but methods for analyzing how the execution costs of two programs compare to each other have not received attention. Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs. In this work, we propose a relational cost analysis technique that is capable of establishing precise bounds on the difference in the execution cost of two programs by making use of relational properties of programs and inputs. We develop , a refinement type and effect system for a higher-order functional language with recursion and subtyping. The key novelty of our technique is the combination of relational refinements with two modes of typingârelational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs. This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach. We prove our type system sound using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs. We demonstrate the precision and generality of our technique through examples.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {316--329},
  keywords = {type and effect systems,complexity analysis,Relational reasoning},
  author = {Çiçek, Ezgi and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Hoffmann, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/A5TV2MHN/Çiçek et al - 2017 - Relational Cost Analysis.pdf}
}

@inproceedings{Kiselyov2017Stream,
  location = {{New York, NY, USA}},
  title = {Stream Fusion, to Completeness},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009880},
  doi = {10.1145/3009837.3009880},
  abstract = {Stream processing is mainstream (again): Widely-used stream libraries are now available for virtually all modern OO and functional languages, from Java to C\# to Scala to OCaml to Haskell. Yet expressivity and performance are still lacking. For instance, the popular, well-optimized Java 8 streams do not support the zip operator and are still an order of magnitude slower than hand-written loops.   We present the first approach that represents the full generality of stream processing and eliminates overheads, via the use of staging. It is based on an unusually rich semantic model of stream interaction. We support any combination of zipping, nesting (or flat-mapping), sub-ranging, filtering, mapping—of finite or infinite streams. Our model captures idiosyncrasies that a programmer uses in optimizing stream pipelines, such as rate differences and the choice of a “for” vs. “while” loops. Our approach delivers hand-written–like code, but automatically. It explicitly avoids the reliance on black-box optimizers and sufficiently-smart compilers, offering highest, guaranteed and portable performance.   Our approach relies on high-level concepts that are then readily mapped into an implementation. Accordingly, we have two distinct implementations: an OCaml stream library, staged via MetaOCaml, and a Scala library for the JVM, staged via LMS. In both cases, we derive libraries richer and simultaneously many tens of times faster than past work. We greatly exceed in performance the standard stream libraries available in Java, Scala and OCaml, including the well-optimized Java 8 streams.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {285--299},
  keywords = {code generation,optimization,stream fusion,multi-stage programming,streams},
  author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
  file = {/Users/pgiarrusso/Zotero/storage/8FWACEKE/Kiselyov et al - 2017 - Stream Fusion, to Completeness.pdf}
}

@inproceedings{Amin2017Type,
  location = {{New York, NY, USA}},
  title = {Type Soundness Proofs with Definitional Interpreters},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009866},
  doi = {10.1145/3009837.3009866},
  abstract = {While type soundness proofs are taught in every graduate PL class, the gap between realistic languages and what is accessible to formal proofs is large. In the case of Scala, it has been shown that its formal model, the Dependent Object Types (DOT) calculus, cannot simultaneously support key metatheoretic properties such as environment narrowing and subtyping transitivity, which are usually required for a type soundness proof. Moreover, Scala and many other realistic languages lack a general substitution property. The first contribution of this paper is to demonstrate how type soundness proofs for advanced, polymorphic, type systems can be carried out with an operational semantics based on high-level, definitional interpreters, implemented in Coq. We present the first mechanized soundness proofs in this style for System F and several extensions, including mutable references. Our proofs use only straightforward induction, which is significant, as the combination of big-step semantics, mutable references, and polymorphism is commonly believed to require coinductive proof techniques. The second main contribution of this paper is to show how DOT-like calculi emerge from straightforward generalizations of the operational aspects of F, exposing a rich design space of calculi with path-dependent types inbetween System F and DOT, which we dub the System D Square. By working directly on the target language, definitional interpreters can focus the design space and expose the invariants that actually matter at runtime. Looking at such runtime invariants is an exciting new avenue for type system design.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {666--679},
  keywords = {_tablet,Scala,Type soundness,dependent object types,DOT,Definitional interpreters},
  author = {Amin, Nada and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/U3UWXBGU/Amin-Rompf - 2017 - Type Soundness Proofs with Definitional Interpreters.pdf}
}

@inproceedings{Crary2017Modules,
  location = {{New York, NY, USA}},
  title = {Modules, Abstraction, and Parametric Polymorphism},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009892},
  doi = {10.1145/3009837.3009892},
  abstract = {Reynolds's Abstraction theorem forms the mathematical foundation for data abstraction. His setting was the polymorphic lambda calculus. Today, many modern languages, such as the ML family, employ rich module systems designed to give more expressive support for data abstraction than the polymorphic lambda calculus, but analogues of the Abstraction theorem for such module systems have lagged far behind.   We give an account of the Abstraction theorem for a modern module calculus supporting generative and applicative functors, higher-order functors, sealing, and translucent signatures. The main issues to be overcome are: (1) the fact that modules combine both types and terms, so they must be treated as both simultaneously, (2) the effect discipline that models the distinction between transparent and opaque modules, and (3) a very rich language of type constructors supporting singleton kinds. We define logical equivalence for modules and show that it coincides with contextual equivalence. This substantiates the folk theorem that modules are good for data abstraction. All our proofs are formalized in Coq.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {100--113},
  keywords = {logical relations,modules,parametricity,Abstraction},
  author = {Crary, Karl},
  file = {/Users/pgiarrusso/Zotero/storage/5ATZACGN/Crary - 2017 - Modules, Abstraction, and Parametric Polymorphism.pdf}
}

@inproceedings{Dolan2017Polymorphism,
  location = {{New York, NY, USA}},
  title = {Polymorphism, Subtyping, and Type Inference in MLsub},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009882},
  doi = {10.1145/3009837.3009882},
  abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs.   This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2017-01-18},
  date = {2017},
  pages = {60--72},
  keywords = {polymorphism,type inference,subtyping,Algebra},
  author = {Dolan, Stephen and Mycroft, Alan},
  file = {/Users/pgiarrusso/Zotero/storage/FBWJNQP3/Dolan_Mycroft - 2017 - Polymorphism, Subtyping, and Type Inference in MLsub.pdf}
}

@inproceedings{Ahmed2006StepIndexed,
  langid = {english},
  title = {Step-Indexed Syntactic Logical Relations for Recursive and Quantified Types},
  url = {http://link.springer.com/chapter/10.1007/11693024_6},
  abstract = {We present a sound and complete proof technique, based on syntactic logical relations, for showing contextual equivalence of expressions in a λ-calculus with recursive types and impredicative universal and existential types. Our development builds on the step-indexed PER model of recursive types presented by Appel and McAllester. We have discovered that a direct proof of transitivity of that model does not go through, leaving the “PER” status of the model in question. We show how to extend the Appel-McAllester model to obtain a logical relation that we can prove is transitive, as well as sound and complete with respect to contextual equivalence. We then augment this model to support relational reasoning in the presence of quantified types.Step-indexed relations are indexed not just by types, but also by the number of steps available for future evaluation. This stratification is essential for handling various circularities, from recursive functions, to recursive types, to impredicative polymorphism. The resulting construction is more elementary than existing logical relations which require complex machinery such as domain theory, admissibility, syntactic minimal invariance, and ⊤ ⊤-closure.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-01-13},
  date = {2006-03-27},
  pages = {69-83},
  keywords = {_tablet},
  author = {Ahmed, Amal},
  file = {/Users/pgiarrusso/Zotero/storage/RZBITID3/Ahmed - 2006 - Step-Indexed Syntactic Logical Relations for Recursive and Quantified Types.pdf;/Users/pgiarrusso/Zotero/storage/SB5H46Q8/11693024_6.html},
  doi = {10.1007/11693024_6}
}
% == BibLateX quality report for Ahmed2006StepIndexed:
% ? Unsure about the formatting of the booktitle

@article{Leinster2016Basic,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.09375},
  primaryClass = {math},
  title = {Basic Category Theory},
  url = {http://arxiv.org/abs/1612.09375},
  abstract = {This short introduction to category theory is for readers with relatively little mathematical background. At its heart is the concept of a universal property, important throughout mathematics. After a chapter introducing the basic definitions, separate chapters present three ways of expressing universal properties: via adjoint functors, representable functors, and limits. A final chapter ties the three together. For each new categorical concept, a generous supply of examples is provided, taken from different parts of mathematics. At points where the leap in abstraction is particularly great (such as the Yoneda lemma), the reader will find careful and extensive explanations.},
  urldate = {2017-01-12},
  date = {2016-12-29},
  keywords = {Mathematics - Category Theory,Mathematics - Logic,Mathematics - Algebraic Topology},
  author = {Leinster, Tom}
}
% == BibLateX quality report for Leinster2016Basic:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Rapoport2016Mutable,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.07610},
  primaryClass = {cs},
  title = {Mutable WadlerFest DOT},
  url = {http://arxiv.org/abs/1611.07610},
  abstract = {The Dependent Object Types (DOT) calculus aims to model the essence of Scala, with a focus on abstract type members, path-dependent types, and subtyping. Other Scala features could be defined by translation to DOT. Mutation is a fundamental feature of Scala currently missing in DOT. Mutation in DOT is needed not only to model effectful computation and mutation in Scala programs, but even to precisely specify how Scala initializes immutable variables and fields (vals). We present an extension to DOT that adds typed mutable reference cells. We have proven the extension sound with a mechanized proof in Coq. We present the key features of our extended calculus and its soundness proof, and discuss the challenges that we encountered in our search for a sound design and the alternative solutions that we considered.},
  urldate = {2017-01-06},
  date = {2016-11-22},
  keywords = {Computer Science - Programming Languages,_tablet},
  author = {Rapoport, Marianna and Lhoták, Ondřej},
  file = {/Users/pgiarrusso/Zotero/storage/ZMG67T43/Rapoport-Lhoták - 2016 - Mutable WadlerFest DOT.pdf;/Users/pgiarrusso/Zotero/storage/44R5R43B/1611.html}
}
% == BibLateX quality report for Rapoport2016Mutable:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Krishnan2016Marriage,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.08573},
  primaryClass = {cs},
  title = {The Marriage of Incremental and Approximate Computing},
  url = {http://arxiv.org/abs/1611.08573},
  abstract = {Most data analytics systems that require low-latency execution and efficient utilization of computing resources, increasingly adopt two computational paradigms, namely, incremental and approximate computing. Incremental computation updates the output incrementally instead of re-computing everything from scratch for successive runs of a job with input changes. Approximate computation returns an approximate output for a job instead of the exact output. Both paradigms rely on computing over a subset of data items instead of computing over the entire dataset, but they differ in their means for skipping parts of the computation. Incremental computing relies on the memoization of intermediate results of sub-computations, and reusing these memoized results across jobs for sub-computations that are unaffected by the changed input. Approximate computing relies on representative sampling of the entire dataset to compute over a subset of data items. In this thesis, we make the observation that these two computing paradigms are complementary, and can be married together! The high level idea is to: design a sampling algorithm that biases the sample selection to the memoized data items from previous runs. To concretize this idea, we designed an online stratified sampling algorithm that uses self-adjusting computation to produce an incrementally updated approximate output with bounded error. We implemented our algorithm in a data analytics system called IncAppox based on Apache Spark Streaming. Our evaluation of the system shows that IncApprox achieves the benefits of both incremental and approximate computing.},
  urldate = {2016-12-15},
  date = {2016-11-25},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  author = {Krishnan, Dhanya R.},
  file = {/Users/pgiarrusso/Zotero/storage/8SU8H7XD/Krishnan - 2016 - The Marriage of Incremental and Approximate Computing.pdf;/Users/pgiarrusso/Zotero/storage/CFUZNR7D/1611.html}
}
% == BibLateX quality report for Krishnan2016Marriage:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Schmid2016SMTbased,
  location = {{New York, NY, USA}},
  title = {SMT-based Checking of Predicate-qualified Types for Scala},
  isbn = {978-1-4503-4648-1},
  url = {http://doi.acm.org/10.1145/2998392.2998398},
  doi = {10.1145/2998392.2998398},
  abstract = {We present *qualified types* for Scala, a form of refinement types adapted to the Scala language. Qualified types allow users to refine base types and classes using predicate expressions. We implemented a type checker for qualified types that is embedded in Scala's next-generation compiler Dotty and delegates constraint checking to an SMT solver. Our system supports many of Scala's functional as well as its object-oriented constructs. To propagate user-provided qualifier ascriptions we utilize both Scala's own type system and an incomplete, but effective qualifier inference algorithm. Our evaluation shows that for a series of examples exerting various of Scala's language features, the additional compile-time overhead is manageable. By combining these features we show that one can verify essential safety properties such as static bounds-checks while retaining several of Scala's advanced features.},
  booktitle = {Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
  series = {SCALA 2016},
  publisher = {{ACM}},
  urldate = {2016-12-15},
  date = {2016},
  pages = {31--40},
  keywords = {refinement types,Scala},
  author = {Schmid, Georg Stefan and Kuncak, Viktor},
  file = {/Users/pgiarrusso/Zotero/storage/QNAQA9IE/Schmid_Kuncak - 2016 - SMT-based Checking of Predicate-qualified Types for Scala.pdf}
}

@inproceedings{Kiselyov2012Iteratees,
  location = {{Berlin, Heidelberg}},
  title = {Iteratees},
  isbn = {978-3-642-29821-9},
  url = {http://dx.doi.org/10.1007/978-3-642-29822-6_15},
  doi = {10.1007/978-3-642-29822-6_15},
  abstract = {Iteratee IO is a style of incremental input processing with precise resource control. The style encourages building input processors from a user-extensible set of primitives by chaining, layering, pairing and other modes of compositions. The programmer is still able, where needed, to precisely control look-ahead, the allocation of buffers, file descriptors and other resources. The style is especially suitable for processing of communication streams, large amount of data, and data undergone several levels of encoding such as pickling, compression, chunking, framing. It has been used for programming high-performance (HTTP) servers and web frameworks, in computational linguistics and financial trading. We exposit programming with iteratees, contrasting them with Lazy IO and the Handle-based, |stdio|-like IO. We relate them to online parser combinators. We introduce a simple implementation as free monads, which lets us formally reason with iteratees. As an example, we validate several equational laws and use them to optimize iteratee programs. The simple implementation helps understand existing implementations of iteratees and derive new ones.},
  booktitle = {Proceedings of the 11th International Conference on Functional and Logic Programming},
  series = {FLOPS'12},
  publisher = {{Springer-Verlag}},
  urldate = {2016-12-13},
  date = {2012},
  pages = {166--181},
  author = {Kiselyov, Oleg},
  file = {/Users/pgiarrusso/Zotero/storage/HX4WG5AM/Kiselyov - 2012 - Iteratees.pdf}
}
% == BibLateX quality report for Kiselyov2012Iteratees:
% 'isbn': not a valid ISBN

@article{Danilewski2016Building,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.01325},
  primaryClass = {cs},
  title = {Building Code with Dynamic Staging},
  url = {http://arxiv.org/abs/1612.01325},
  abstract = {When creating a new domain-specific language (DSL) it is common to embed it as a part of a flexible host language, rather than creating it entirely from scratch. The semantics of an embedded DSL (EDSL) is either given directly as a set of functions (shallow embedding), or an AST is constructed that is later processed (deep embedding). Typically, the deep embedding is used when the EDSL specifies domain-specific optimizations (DSO) in a form of AST transformations. In this paper we show that deep embedding is not necessary to specify most optimizations. We define language semantics as action functions that are executed during parsing. These actions build incrementally a new, arbitrary complex program function. The EDSL designer is able to specify many aspects of the semantics as a runnable code, such as variable scoping rules, custom type checking, arbitrary control flow structures, or DSO. A sufficiently powerful staging mechanism helps assembling the code from different actions, as well as evaluate the semantics in arbitrarily many stages. In the end, we obtain code that is as efficient as one written by hand. We never create any object representation of the code. No external traversing algorithm is used to process the code. All program fragments are functions with their entire semantics embedded within the function bodies. This approach allows reusing the code between EDSL and the host language, as well as combining actions of many different EDSLs.},
  urldate = {2016-12-12},
  date = {2016-12-05},
  keywords = {Computer Science - Programming Languages},
  author = {Danilewski, Piotr and Slusallek, Philipp},
  file = {/Users/pgiarrusso/Zotero/storage/ADN647VB/Danilewski_Slusallek - 2016 - Building Code with Dynamic Staging.pdf;/Users/pgiarrusso/Zotero/storage/8BFHHSUD/1612.html}
}
% == BibLateX quality report for Danilewski2016Building:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Lewis2000Implicit,
  location = {{New York, NY, USA}},
  title = {Implicit Parameters: Dynamic Scoping with Static Types},
  isbn = {978-1-58113-125-3},
  url = {http://doi.acm.org/10.1145/325694.325708},
  doi = {10.1145/325694.325708},
  shorttitle = {Implicit Parameters},
  abstract = {This paper introduces a language feature, called implicit parameters, that provides dynamically scoped variables within a statically-typed Hindley-Milner framework. Implicit parameters are lexically distinct from regular identifiers, and are bound by a special with construct whose scope is dynamic, rather than static as with let. Implicit parameters are treated by the type system as parameters that are not explicitly declared, but are inferred from their use.
We present implicit parameters within a small call-by-name \&lgr;-calculus. We give a type system, a type inference algorithm, and several semantics. We also explore implicit parameters in the wider settings of call-by-need languages with overloading, and call-by-value languages with effects. As a witness to the former, we have implemented implicit parameters as an extension of Haskell within the Hugs interpreter, which we use to present several motivating examples.},
  booktitle = {Proceedings of the 27th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '00},
  publisher = {{ACM}},
  urldate = {2016-12-08},
  date = {2000},
  pages = {108--118},
  author = {Lewis, Jeffrey R. and Launchbury, John and Meijer, Erik and Shields, Mark B.},
  file = {/Users/pgiarrusso/Zotero/storage/KTZMXDSZ/Lewis et al - 2000 - Implicit Parameters - Dynamic Scoping with Static Types.pdf}
}

@article{McBRIDE2002Faking,
  title = {Faking it Simulating dependent types in Haskell},
  volume = {12},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/faking-it-simulating-dependent-types-in-haskell/A904B84CA962F2D75578445B703F199A},
  doi = {10.1017/S0956796802004355},
  abstract = {Dependent types reflect the fact that validity of data is often a relative notion by allowing
prior data to affect the types of subsequent data. Not only does this make for a precise
type system, but also a highly generic one: both the type and the program for each instance
of a family of operations can be computed from the data which codes for that instance.
Recent experimental extensions to the Haskell type class mechanism give us strong tools to
relativize types to other types. We may simulate some aspects of dependent typing by making
counterfeit type-level copies of data, with type constructors simulating data constructors and
type classes simulating datatypes. This paper gives examples of the technique and discusses
its potential.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  urldate = {2016-11-28},
  date = {2002-07},
  pages = {375-392},
  author = {McBRIDE, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/6W63H8UR/McBRIDE - 2002 - Faking it Simulating dependent types in Haskell.pdf;/Users/pgiarrusso/Zotero/storage/33BXXN3U/A904B84CA962F2D75578445B703F199A.html}
}
% == BibLateX quality report for McBRIDE2002Faking:
% 'issn': not a valid ISSN

@inproceedings{Salvaneschi2013Reactive,
  location = {{New York, NY, USA}},
  title = {Reactive Behavior in Object-oriented Applications: An Analysis and a Research Roadmap},
  isbn = {978-1-4503-1766-5},
  url = {http://doi.acm.org/10.1145/2451436.2451442},
  doi = {10.1145/2451436.2451442},
  shorttitle = {Reactive Behavior in Object-oriented Applications},
  abstract = {Reactive applications are difficult to implement. Traditional solutions based on event systems and the Observer pattern have a number of inconveniences, but programmers bear them in return for the benefits of OO design. On the other hand, reactive approaches based on automatic updates of dependencies - like functional reactive programming and dataflow languages - provide undoubted advantages but do not fit well with mutable objects. In this paper, we provide a research roadmap to overcome the limitations of the current approaches and to support reactive applications in the OO setting. To establish a solid background for our investigation, we propose a conceptual framework to model the design space of reactive applications and we study the flaws of the existing solutions. Then we highlight how reactive languages have the potential to address those issues and we formulate our research plan.},
  booktitle = {Proceedings of the 12th Annual International Conference on Aspect-oriented Software Development},
  series = {AOSD '13},
  publisher = {{ACM}},
  urldate = {2016-11-28},
  date = {2013},
  pages = {37--48},
  keywords = {object-oriented programming,incremental computation,reactive programming,functional-reactive programming},
  author = {Salvaneschi, Guido and Mezini, Mira},
  file = {/Users/pgiarrusso/Zotero/storage/RBB98KNJ/Salvaneschi_Mezini - 2013 - Reactive Behavior in Object-oriented Applications - An Analysis and a Research Roadmap.pdf}
}

@inproceedings{Shaikhha2016How,
  title = {How to Architect a Query Compiler},
  url = {https://infoscience.epfl.ch/record/218087},
  doi = {10.1145/2882903.2915244},
  eventtitle = {SIGMOD 2016},
  urldate = {2016-11-24},
  date = {2016},
  author = {Shaikhha, Amir and Klonatos, Ioannis and Parreaux, Lionel Emile Vincent and Brown, Lewis and Dashti Rahmat Abadi, Mohammad and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/JB6PV5FS/Shaikhha et al - 2016 - How to Architect a Query Compiler.pdf;/Users/pgiarrusso/Zotero/storage/4FFGUBCQ/218087.html}
}
% == BibLateX quality report for Shaikhha2016How:
% Missing required field 'booktitle'

@inproceedings{Koch2010Incremental,
  location = {{New York, NY, USA}},
  title = {Incremental Query Evaluation in a Ring of Databases},
  isbn = {978-1-4503-0033-9},
  url = {http://doi.acm.org/10.1145/1807085.1807100},
  doi = {10.1145/1807085.1807100},
  abstract = {This paper approaches the incremental view maintenance problem from an algebraic perspective. We construct the algebraic structure of a ring of databases and use it as the foundation of the design of a query calculus that allows to express powerful aggregate queries. The query calculus inherits key properties of the ring, such as having a normal form of polynomials and being closed under computing inverses and delta queries. The k-th delta of a polynomial query of degree k without nesting is purely a function of the update, not of the database. This gives rise to a method of eliminating expensive query operators such as joins from programs that perform incremental view maintenance. The main result is that, for non-nested queries, each individual aggregate value can be incrementally maintained using a constant amount of work. This is not possible for nonincremental evaluation.},
  booktitle = {Proceedings of the Twenty-ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  series = {PODS '10},
  publisher = {{ACM}},
  urldate = {2016-11-18},
  date = {2010},
  pages = {87--98},
  keywords = {Algebra,Incremental view maintenance},
  author = {Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/JDB5I6FV/Koch - 2010 - Incremental Query Evaluation in a Ring of Databases.pdf}
}

@inproceedings{Godefroid2005DART,
  location = {{New York, NY, USA}},
  title = {DART: Directed Automated Random Testing},
  isbn = {978-1-59593-056-9},
  url = {http://doi.acm.org/10.1145/1065010.1065036},
  doi = {10.1145/1065010.1065036},
  shorttitle = {DART},
  abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '05},
  publisher = {{ACM}},
  urldate = {2016-11-16},
  date = {2005},
  pages = {213--223},
  keywords = {interfaces,automated test generation,program verification,random testing,software testing},
  author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  file = {/Users/pgiarrusso/Zotero/storage/ZFHJ935E/Godefroid et al - 2005 - DART - Directed Automated Random Testing.pdf}
}

@inproceedings{Rompf2016Type,
  location = {{New York, NY, USA}},
  title = {Type Soundness for Dependent Object Types (DOT)},
  isbn = {978-1-4503-4444-9},
  url = {http://doi.acm.org/10.1145/2983990.2984008},
  doi = {10.1145/2983990.2984008},
  abstract = {Scala’s type system unifies aspects of ML modules, object- oriented, and functional programming. The Dependent Object Types (DOT) family of calculi has been proposed as a new theoretic foundation for Scala and similar expressive languages. Unfortunately, type soundness has only been established for restricted subsets of DOT. In fact, it has been shown that important Scala features such as type refinement or a subtyping relation with lattice structure break at least one key metatheoretic property such as environment narrowing or invertible subtyping transitivity, which are usually required for a type soundness proof. The main contribution of this paper is to demonstrate how, perhaps surprisingly, even though these properties are lost in their full generality, a rich DOT calculus that includes recursive type refinement and a subtyping lattice with intersection types can still be proved sound. The key insight is that subtyping transitivity only needs to be invertible in code paths executed at runtime, with contexts consisting entirely of valid runtime objects, whereas inconsistent subtyping contexts can be permitted for code that is never executed.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2016},
  publisher = {{ACM}},
  urldate = {2016-11-01},
  date = {2016},
  pages = {624--641},
  keywords = {_tablet,Scala,soundness,dependent object types,DOT},
  author = {Rompf, Tiark and Amin, Nada},
  file = {/Users/pgiarrusso/Zotero/storage/SJ7HCPXJ/Rompf-Amin - 2016 - Type Soundness for Dependent Object Types (DOT).pdf}
}

@incollection{Lagorio2009Featherweight,
  langid = {english},
  title = {Featherweight Jigsaw: A Minimal Core Calculus for Modular Composition of Classes},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-03013-0_12},
  shorttitle = {Featherweight Jigsaw},
  abstract = {We present FJig, a simple calculus where basic building blocks are classes in the style of Featherweight Java, declaring fields, methods and one constructor. However, inheritance has been generalized to the much more flexible notion originally proposed in Bracha’s Jigsaw framework. That is, classes play also the role of modules, that can be composed by a rich set of operators, all of which can be expressed by a minimal core. We keep the nominal approach of Java-like languages, that is, types are class names. However, a class is not necessarily a structural subtype of any class used in its defining expression. The calculus allows the encoding of a large variety of different mechanisms for software composition in class-based languages, including standard inheritance, mixin classes, traits and hiding. Hence, FJig can be used as a unifying framework for analyzing existing mechanisms and proposing new extensions. We provide two different semantics of an FJig program: flattening and direct semantics. The difference is analogous to that between two intuitive models to understand inheritance: the former where inherited methods are copied into heir classes, and the latter where member lookup is performed by ascending the inheritance chain. Here we address equivalence of these two views for a more sophisticated composition mechanism.},
  number = {5653},
  booktitle = {ECOOP 2009 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2016-10-28},
  date = {2009-07-06},
  pages = {244-268},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Software Engineering/Programming and Operating Systems,Computer Communication Networks},
  author = {Lagorio, Giovanni and Servetto, Marco and Zucca, Elena},
  editor = {Drossopoulou, Sophia},
  file = {/Users/pgiarrusso/Zotero/storage/N627WQTI/Lagorio et al - 2009 - Featherweight Jigsaw - A Minimal Core Calculus for Modular Composition of Classes.pdf;/Users/pgiarrusso/Zotero/storage/6H2JADAA/10.html},
  doi = {10.1007/978-3-642-03013-0_12}
}
% == BibLateX quality report for Lagorio2009Featherweight:
% 'isbn': not a valid ISBN

@article{Hammer2016Typeda,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00097},
  primaryClass = {cs},
  title = {Typed Adapton: Refinement types for nominal memoization of purely functional incremental programs},
  url = {http://arxiv.org/abs/1610.00097},
  shorttitle = {Typed Adapton},
  abstract = {Nominal memoization combines memoized functional programming with a controlled form of imperative cache effects. By leveraging these imperative effects, nominal memoization can dramatically outperform traditional ("structural") memoization. However, the nominal memoization programming model is error-prone: when the programmer unintentionally misuses names, their incremental program ceases to correspond with a purely functional specification. This paper develops a refinement type system for nominal memoization that enforces a program's correspondence with a purely functional specification. Our type system employs set-indexed types in the style of DML (Xi and Pfenning 1999), extended with polymorphism over kinds and index functions. We prove that our type system enforces the dynamic side conditions proposed by Hammer et al 2015. Past work shows that these conditions suffice to write useful examples of nominal memoization while also guaranteeing from-scratch consistency of the incremental programs. These features contribute to its overall goal of expressing generic naming strategies in type-generic incremental code. In particular, we show various forms of namespace parametricity and illustrate through these examples its importance for expressing nominal memoization in library code. We also show how extensions to our type system can permit controlled forms of naming strategies that encode incremental churn and feedback. We speculate that certain feedback and churn patterns constitute naming strategies that encode existing forms of functional reactive computation as a mode of use of nominal memoization.},
  urldate = {2016-10-17},
  date = {2016-10-01},
  keywords = {Computer Science - Programming Languages},
  author = {Hammer, Matthew A. and Dunfield, Joshua},
  file = {/Users/pgiarrusso/Zotero/storage/VUUU2ZHF/Hammer_Dunfield - 2016 - Typed Adapton - Refinement types for nominal memoization of purely functional incremental programs.pdf;/Users/pgiarrusso/Zotero/storage/3GVWIKDA/1610.html}
}
% == BibLateX quality report for Hammer2016Typeda:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Bracker2016Supermonads,
  location = {{New York, NY, USA}},
  title = {Supermonads: One Notion to Bind Them All},
  isbn = {978-1-4503-4434-0},
  url = {http://doi.acm.org/10.1145/2976002.2976012},
  doi = {10.1145/2976002.2976012},
  shorttitle = {Supermonads},
  abstract = {Several popular generalizations of monads have been implemented in Haskell. Unfortunately, because the shape of the associated type constructors do not match the standard Haskell monad interface, each such implementation provides its own type class and versions of associated library functions. Furthermore, simultaneous use of different monadic notions can be cumbersome as it in general is necessary to be explicit about which notion is used where. In this paper we introduce supermonads: an encoding of monadic notions that captures several different generalizations along with a version of the standard library of monadic functions that work uniformly with all of them. As standard Haskell type inference does not work for supermonads due to their generality, our supermonad implementation is accompanied with a language extension, in the form of a plugin for the Glasgow Haskell Compiler (GHC), that allows type inference for supermonads, obviating the need for manual annotations.},
  booktitle = {Proceedings of the 9th International Symposium on Haskell},
  series = {Haskell 2016},
  publisher = {{ACM}},
  urldate = {2016-10-16},
  date = {2016},
  pages = {158--169},
  keywords = {functional programming,Haskell,Monads,Glasgow Haskell Compiler,syntactic support,type checker plugin},
  author = {Bracker, Jan and Nilsson, Henrik},
  file = {/Users/pgiarrusso/Zotero/storage/4F88ZI3N/Bracker_Nilsson - 2016 - Supermonads - One Notion to Bind Them All.pdf}
}

@article{Jay2016Programs,
  title = {Programs as Data Structures in λSF-Calculus},
  volume = {325},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066116300913},
  doi = {10.1016/j.entcs.2016.09.040},
  abstract = {Lambda-SF-calculus can represent programs as closed normal forms. In turn, all closed normal forms are data structures, in the sense that their internal structure is accessible through queries defined in the calculus, even to the point of constructing the Goedel number of a program. Thus, program analysis and optimisation can be performed entirely within the calculus, without requiring any meta-level process of quotation to produce a data structure.

Lambda-SF-calculus is a confluent, applicative rewriting system derived from lambda-calculus, and the combinatory SF-calculus. Its superior expressive power relative to lambda-calculus is demonstrated by the ability to decide if two programs are syntactically equal, or to determine if a program uses its input. Indeed, there is no homomorphism of applicative rewriting systems from lambda-SF-calculus to lambda-calculus.

Program analysis and optimisation can be illustrated by considering the conversion of a programs to combinators. Traditionally, a program p is interpreted using fixpoint constructions that do not have normal forms, but combinatory techniques can be used to block reduction until the program arguments are given. That is, p is interpreted by a closed normal form M. Then factorisation (by F) adapts the traditional account of lambda-abstraction in combinatory logic to convert M to a combinator N that is equivalent to M in the following two senses. First, N is extensionally equivalent to M where extensional equivalence is defined in terms of eta-reduction. Second, the conversion is an intensional equivalence in that it does not lose any information, and so can be reversed by another definable conversion. Further, the standard optimisations of the conversion process are all definable within lambda-SF-calculus, even those involving free variable analysis.

Proofs of all theorems in the paper have been verified using the Coq theorem prover.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {The Thirty-second Conference on the Mathematical Foundations of Programming Semantics (MFPS XXXII)},
  urldate = {2016-10-13},
  date = {2016-10-05},
  pages = {221-236},
  keywords = {Lambda-calculus,self-interpretation,SF-calculus,xi-rule},
  author = {Jay, Barry},
  file = {/Users/pgiarrusso/Zotero/storage/UVGW44G4/Jay - 2016 - Programs as Data Structures in λSF-Calculus.pdf;/Users/pgiarrusso/Zotero/storage/QNEGU88R/S1571066116300913.html}
}

@inproceedings{Sculthorpe2013Constrainedmonad,
  location = {{New York, NY, USA}},
  title = {The Constrained-monad Problem},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500602},
  doi = {10.1145/2500365.2500602},
  abstract = {In Haskell, there are many data types that would form monads were it not for the presence of type-class constraints on the operations on that data type. This is a frustrating problem in practice, because there is a considerable amount of support and infrastructure for monads that these data types cannot use. Using several examples, we show that a monadic computation can be restructured into a normal form such that the standard monad class can be used. The technique is not specific to monads, and we show how it can also be applied to other structures, such as applicative functors. One significant use case for this technique is domain-specific languages, where it is often desirable to compile a deep embedding of a computation to some other language, which requires restricting the types that can appear in that computation.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2016-10-13},
  date = {2013},
  pages = {287--298},
  keywords = {Haskell,Monads,class constraints,deep embeddings},
  author = {Sculthorpe, Neil and Bracker, Jan and Giorgidze, George and Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/MIJURRZJ/Sculthorpe et al - 2013 - The Constrained-monad Problem.pdf}
}

@article{Koch2014Incremental,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.4320},
  primaryClass = {cs},
  title = {Incremental View Maintenance For Collection Programming},
  url = {http://arxiv.org/abs/1412.4320},
  abstract = {In the context of incremental view maintenance (IVM), delta query derivation is an essential technique for speeding up the processing of large, dynamic datasets. The goal is to generate delta queries that, given a small change in the input, can update the materialized view more efficiently than via recomputation. In this work we propose the first solution for the efficient incrementalization of positive nested relational calculus (NRC+) on bags (with integer multiplicities). More precisely, we model the cost of NRC+ operators and classify queries as efficiently incrementalizable if their delta has a strictly lower cost than full re-evaluation. Then, we identify IncNRC+; a large fragment of NRC+ that is efficiently incrementalizable and we provide a semantics-preserving translation that takes any NRC+ query to a collection of IncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is within the complexity class NC0 and we showcase how recursive IVM, a technique that has provided significant speedups over traditional IVM in the case of flat queries [25], can also be applied to IncNRC+.},
  urldate = {2016-10-12},
  date = {2014-12-14},
  keywords = {Computer Science - Databases},
  author = {Koch, Christoph and Lupei, Daniel and Tannen, Val},
  file = {/Users/pgiarrusso/Zotero/storage/7228TF5T/Koch et al - 2014 - Incremental View Maintenance For Collection Programming.pdf;/Users/pgiarrusso/Zotero/storage/PBEFC3H6/1412.html}
}
% == BibLateX quality report for Koch2014Incremental:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Koch2016Incremental,
  location = {{New York, NY, USA}},
  title = {Incremental View Maintenance For Collection Programming},
  isbn = {978-1-4503-4191-2},
  url = {http://doi.acm.org/10.1145/2902251.2902286},
  doi = {10.1145/2902251.2902286},
  abstract = {In the context of incremental view maintenance (IVM), delta query derivation is an essential technique for speeding up the processing of large, dynamic datasets. The goal is to generate delta queries that, given a small change in the input, can update the materialized view more efficiently than via recomputation. In this work we propose the first solution for the efficient incrementalization of positive nested relational calculus (NRC+) on bags (with integer multiplicities). More precisely, we model the cost of NRC+ operators and classify queries as efficiently incrementalizable if their delta has a strictly lower cost than full re-evaluation. Then, we identify NRC+, a large fragment of NRC+ that is efficiently incrementalizable and we provide a semantics-preserving translation that takes any NRC+ query to a collection of IncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is within the complexity class NC0 and we showcase how recursive IVM, a technique that has provided significant speedups over traditional IVM in the case of flat queries [25], can also be applied to IncNRC+.},
  booktitle = {Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
  series = {PODS '16},
  publisher = {{ACM}},
  urldate = {2016-10-12},
  date = {2016},
  pages = {75--90},
  keywords = {incremental computation,Incremental view maintenance,collection programming,higher-order delta derivation,nested relational algebra,nested relational calculus,recursive ivm,shredding transformation},
  author = {Koch, Christoph and Lupei, Daniel and Tannen, Val},
  file = {/Users/pgiarrusso/Zotero/storage/7R9T3699/Koch et al - 2016 - Incremental View Maintenance For Collection Programming.pdf}
}

@article{Dargaye2010verified,
  langid = {english},
  title = {A verified framework for higher-order uncurrying optimizations},
  volume = {22},
  issn = {1388-3690, 1573-0557},
  url = {http://link.springer.com/article/10.1007/s10990-010-9050-z},
  doi = {10.1007/s10990-010-9050-z},
  abstract = {Function uncurrying is an important optimization for the efficient execution of functional programming languages. This optimization replaces curried functions by uncurried, multiple-argument functions, while preserving the ability to evaluate partial applications. First-order uncurrying (where curried functions are optimized only in the static scopes of their definitions) is well understood and implemented by many compilers, but its extension to higher-order functions (where uncurrying can also be performed on parameters and results of higher-order functions) is challenging. This article develops a generic framework that expresses higher-order uncurrying optimizations as type-directed insertion of coercions, and prove its correctness. The proof uses step-indexed logical relations and was entirely mechanized using the Coq proof assistant.},
  number = {3},
  journaltitle = {Higher-Order and Symbolic Computation},
  shortjournal = {Higher-Order Symb Comput},
  urldate = {2016-10-11},
  date = {2010-01-19},
  pages = {199-231},
  author = {Dargaye, Zaynah and Leroy, Xavier},
  file = {/Users/pgiarrusso/Zotero/storage/6ECEX4PR/Dargaye_Leroy - 2010 - A verified framework for higher-order uncurrying optimizations.pdf;/Users/pgiarrusso/Zotero/storage/HBGVB6MH/s10990-010-9050-z.html}
}
% == BibLateX quality report for Dargaye2010verified:
% 'issn': not a valid ISSN

@article{Headley2016Random,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06009},
  primaryClass = {cs},
  title = {The Random Access Zipper: Simple, Purely-Functional Sequences},
  url = {http://arxiv.org/abs/1608.06009},
  shorttitle = {The Random Access Zipper},
  abstract = {We introduce the Random Access Zipper (RAZ), a simple, purely-functional data structure for editable sequences. A RAZ combines the structure of a zipper with that of a tree: like a zipper, edits at the cursor require constant time; by leveraging tree structure, relocating the edit cursor in the sequence requires logarithmic time. While existing data structures provide these time bounds, none do so with the same simplicity and brevity of code as the RAZ. The simplicity of the RAZ provides the opportunity for more programmers to extend the structure to their own needs, and we provide some suggestions for how to do so.},
  urldate = {2016-10-06},
  date = {2016-08-21},
  keywords = {Computer Science - Programming Languages,_tablet,Computer Science - Data Structures and Algorithms},
  author = {Headley, Kyle and Hammer, Matthew A.},
  file = {/Users/pgiarrusso/Zotero/storage/Q3WTTVTF/Headley-Hammer - 2016 - The Random Access Zipper - Simple, Purely-Functional Sequences.pdf;/Users/pgiarrusso/Zotero/storage/74E92W5B/1608.html}
}
% == BibLateX quality report for Headley2016Random:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@incollection{Firsov2016Purely,
  langid = {english},
  title = {Purely Functional Incremental Computing},
  isbn = {978-3-319-45278-4 978-3-319-45279-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-45279-1_5},
  abstract = {Many applications have to maintain evolving data sources as well as views on these sources. If sources change, the corresponding views have to be adapted. Complete recomputation of views is typically too expensive. An alternative is to convert source changes into view changes and apply these to the views. This is the key idea of incremental computing. In this paper, we use Haskell to develop an incremental computing framework. We illustrate the concepts behind this framework by implementing several example computations on sequences. Our framework allows the user to implement incremental computations using arbitrary monad families that encapsulate mutable state. This makes it possible to use highly efficient algorithms for core computations.},
  number = {9889},
  booktitle = {Programming Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  urldate = {2016-10-05},
  date = {2016-09-22},
  pages = {62-77},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Firsov, Denis and Jeltsch, Wolfgang},
  editor = {Castor, Fernando and Liu, Yu David},
  file = {/Users/pgiarrusso/Zotero/storage/KNRAJ55X/Firsov_Jeltsch - 2016 - Purely Functional Incremental Computing.pdf;/Users/pgiarrusso/Zotero/storage/63WJJCHZ/978-3-319-45279-1_5.html},
  doi = {10.1007/978-3-319-45279-1_5}
}
% == BibLateX quality report for Firsov2016Purely:
% 'isbn': not a valid ISBN

@article{Fisher2016miniAdapton,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05337},
  primaryClass = {cs},
  title = {miniAdapton: A Minimal Implementation of Incremental Computation in Scheme},
  url = {http://arxiv.org/abs/1609.05337},
  shorttitle = {miniAdapton},
  abstract = {We describe a complete Scheme implementation of miniAdapton, which implements the core functionality of the Adapton system for incremental computation (also known as self-adjusting computation). Like Adapton, miniAdapton allows programmers to safely combine mutation and memoization. miniAdapton is built on top of an even simpler system, microAdapton. Both miniAdapton and microAdapton are designed to be easy to understand, extend, and port to host languages other than Scheme. We also present adapton variables, a new interface in Adapton for variables intended to represent expressions.},
  urldate = {2016-09-27},
  date = {2016-09-17},
  keywords = {Computer Science - Programming Languages},
  author = {Fisher, Dakota and Hammer, Matthew A. and Byrd, William and Might, Matthew},
  file = {/Users/pgiarrusso/Zotero/storage/S8JV5PB9/Fisher et al - 2016 - miniAdapton - A Minimal Implementation of Incremental Computation in Scheme.pdf;/Users/pgiarrusso/Zotero/storage/V582UXPH/1609.html}
}
% == BibLateX quality report for Fisher2016miniAdapton:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Taha1997multistage,
  location = {{New York, NY, USA}},
  title = {Multi-stage Programming with Explicit Annotations},
  isbn = {0-89791-917-3},
  url = {http://doi.acm.org/10.1145/258993.259019},
  doi = {10.1145/258993.259019},
  abstract = {We introduce MetaML, a statically-typed multi-stage programming language extending Nielson and Nielson's two stage notation to an arbitrary number of stages. MetaML extends previous work by introducing four distinct staging annotations which generalize those published previously [25, 12, 7, 6]We give a static semantics in which type checking is done once and for all before the first stage, and a dynamic semantics which introduces a new concept of cross-stage persistence, which requires that variables available in any stage are also available in all future stages.We illustrate that staging is a manual form of binding time analysis. We explain why, even in the presence of automatic binding time analysis, explicit annotations are useful, especially for programs with more than two stages.A thesis of this paper is that multi-stage languages are useful as programming languages in their own right, and should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three stage example, and an extended two-stage example elaborating a number of practical issues.},
  booktitle = {Proceedings of the 1997 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation},
  series = {PEPM '97},
  publisher = {{ACM}},
  urldate = {2014-10-21},
  date = {1997},
  pages = {203--217},
  author = {Taha, Walid and Sheard, Tim},
  file = {/Users/pgiarrusso/Zotero/storage/8KPMU7A3/Taha_Sheard - 1997 - Multi-stage Programming with Explicit Annotations.pdf}
}

@inproceedings{Vassena2016Generic,
  location = {{New York, NY, USA}},
  title = {Generic Diff3 for Algebraic Datatypes},
  isbn = {978-1-4503-4435-7},
  url = {http://doi.acm.org/10.1145/2976022.2976026},
  doi = {10.1145/2976022.2976026},
  abstract = {Many version control systems, including Git and Mercurial, rely on diff3 to merge different revisions of the same file. More precisely diff3 automatically merges two text files, given a common base version, comparing them line by line and raising conflicts when the changes made are irreconcilable. The program ignores the actual structure of the data stored in the files, hence it might generate spurious conflicts, which must be manually resolved by the user. In this paper, we present a state-based, three-way, persistent, data-type generic diff3 algorithm whose increased precision in detecting changes reduces the number of false conflicts raised and improves its merging capabilities. We have implemented the algorithm in Agda, a proof assistant with dependent types, and developed a model to reason about “diffing” and merging. We have formalized sanity properties and specifications of diff3 and proved that our algorithm meets them. Furthermore, we have identified the minimal conditions under which the merging algorithm raises a conflict and established a structural invariant preserved.},
  booktitle = {Proceedings of the 1st International Workshop on Type-Driven Development},
  series = {TyDe 2016},
  publisher = {{ACM}},
  urldate = {2016-09-20},
  date = {2016},
  pages = {62--71},
  keywords = {Datatype-generic programming,dependent types,Diff3},
  author = {Vassena, Marco},
  file = {/Users/pgiarrusso/Zotero/storage/MQBI9XHA/Vassena - 2016 - Generic Diff3 for Algebraic Datatypes.pdf}
}

@article{Backus1978Can,
  title = {Can Programming Be Liberated from the Von Neumann Style?: A Functional Style and Its Algebra of Programs},
  volume = {21},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/359576.359579},
  doi = {10.1145/359576.359579},
  shorttitle = {Can Programming Be Liberated from the Von Neumann Style?},
  abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor—the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.
An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.
Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose “unknowns” are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.
 A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states—only one state transition occurs per major computation.},
  number = {8},
  journaltitle = {Commun. ACM},
  urldate = {2016-09-15},
  date = {1978-08},
  pages = {613--641},
  keywords = {functional programming,program transformation,programming languages,algebra of programs,applicative computing systems,applicative state transition systems,combining forms,functional forms,metacomposition,models of computing systems,program correctness,program termination,von Neumann computers,von Neumann languages},
  author = {Backus, John},
  file = {/Users/pgiarrusso/Zotero/storage/M39KT3VJ/Backus - 1978 - Can Programming Be Liberated from the Von Neumann Style - - A Functional Style and Its Algebra of Programs.pdf}
}
% == BibLateX quality report for Backus1978Can:
% ? Possibly abbreviated journal title Commun. ACM

@incollection{Hofmann1997syntax,
  langid = {english},
  title = {Syntax and semantics of dependent types},
  isbn = {978-1-4471-1243-3 978-1-4471-0963-1},
  url = {http://link.springer.com/chapter/10.1007/978-1-4471-0963-1_2},
  abstract = {In this chapter we fix a particular syntax for a dependently typed calculus and define an abstract notion of model as well as a general interpretation function mapping syntactical objects to entities in a model. This interpretation function is shown to be sound with respect to the syntax.},
  booktitle = {Extensional Concepts in Intensional Type Theory},
  series = {CPHC/BCS Distinguished Dissertations},
  publisher = {{Springer London}},
  urldate = {2015-05-04},
  date = {1997},
  pages = {13-54},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  author = {Hofmann, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/5HZZVA36/978-1-4471-0963-1_2.html}
}
% == BibLateX quality report for Hofmann1997syntax:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@article{Omar2016Hazelnut,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.04180},
  primaryClass = {cs},
  title = {Hazelnut: A Bidirectionally Typed Structure Editor Calculus},
  url = {http://arxiv.org/abs/1607.04180},
  shorttitle = {Hazelnut},
  abstract = {Structure editors allow programmers to edit the tree structure of a program directly. This can have cognitive benefits, particularly for novice and end-user programmers (as evidenced by the popularity of structure editors like Scratch.) It also simplifies matters for tool designers, because they do not need to contend with malformed program text. This paper defines Hazelnut, a structure editor based on a small bidirectionally typed lambda calculus extended with holes and a cursor (a la Huet's zipper.) Hazelnut goes one step beyond syntactic well-formedness: it's edit actions operate over statically meaningful (i.e. well-typed) terms. Naively, this prohibition on ill-typed edit states would force the programmer to construct terms in a rigid "outside-in" manner. To avoid this problem, the action semantics automatically places terms assigned a type that is inconsistent with the expected type inside a hole. This safely defers the type consistency check until the term inside the hole is finished. Hazelnut is a foundational type-theoretic account of typed structure editing, rather than an end-user tool itself. To that end, we describe how Hazelnut's rich metatheory, which we have mechanized in Agda, guides the definition of an extension to the calculus. We also discuss various plausible evaluation strategies for terms with holes, and in so doing reveal connections with gradual typing and contextual modal type theory (the Curry-Howard interpretation of contextual modal logic.) Finally, we discuss how Hazelnut's semantics lends itself to implementation as a functional reactive program. Our reference implementation is written using js\_of\_ocaml.},
  urldate = {2016-09-07},
  date = {2016-07-14},
  keywords = {Computer Science - Programming Languages},
  author = {Omar, Cyrus and Voysey, Ian and Hilton, Michael and Aldrich, Jonathan and Hammer, Matthew A.},
  file = {/Users/pgiarrusso/Zotero/storage/MF48RUXC/Omar et al - 2016 - Hazelnut - A Bidirectionally Typed Structure Editor Calculus.pdf;/Users/pgiarrusso/Zotero/storage/ZNAX9PUX/1607.html}
}
% == BibLateX quality report for Omar2016Hazelnut:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Petrashko2017Design,
  langid = {english},
  title = {Design and implementation of an optimizing type-centric compiler for a high-level language},
  url = {http://infoscience.epfl.ch/record/232671},
  doi = {10.5075/epfl-thesis-7979},
  abstract = {Production compilers for programming languages face multiple requirements. They should be correct, as we rely on them to produce code. They should be fast, in order to provide a good developer experience. They should also be easy to maintain and evolve. This thesis shows how an expressive high level type system can be used to simplify the development of a compiler and demonstrates this on a compiler for Scala. First, it shows how expressive types of high level languages can be used to build internal data structures that provide a statically checked API, ensuring that important properties hold at compile time. Second, we also show how high level language features can be used to abstract the components of a compiler. We demonstrate this by introducing a type-safe layer on top of the bytecode emission phase. This makes it possible to abstract away the implementation details of the compiler frontend and run the same bytecode emission phase in two different Scala compilers. Third, it presents MiniPhases, a novel way to organize transformation passes in a compiler. MiniPhases impose constraints on the organization of passes that are beneficial for maintainability, performance, and testability. We include a detailed performance evaluation of MiniPhases which indicates that their speedup is due to improved cache friendliness and to a lower rate of promotions of objects into the old generations of garbage collectors. Finally, we demonstrate how the expressive type system of the language being compiled can be used for static analysis. We present a novel call graph construction algorithm which uses the typing context for context sensitivity. The resulting algorithm is both substantially faster and more precise than existing alternatives. We demonstrate the applicability of this analysis by extending common subexpression elimination to idempotent expression elimination. Petrashko, Dmytro},
  urldate = {2018-02-21},
  date = {2017},
  author = {Petrashko, Dmytro},
  file = {/Users/pgiarrusso/Zotero/storage/4SKGI2JI/Petrashko - 2017 - Design and implementation of an optimizing type-centric compiler for a high-level language.pdf}
}
% == BibLateX quality report for Petrashko2017Design:
% Missing required field 'journaltitle'

@article{Stump2017calculus,
  langid = {english},
  title = {The calculus of dependent lambda eliminations},
  volume = {27},
  issn = {0956-7968, 1469-7653},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/calculus-of-dependent-lambda-eliminations/1D0BDA070E9273AC56C108D8F6F2B078#},
  doi = {10.1017/S0956796817000053},
  abstract = {Abstract
Modern constructive type theory is based on pure dependently typed lambda calculus, augmented with user-defined datatypes. This paper presents an alternative called the Calculus of Dependent Lambda Eliminations, based on pure lambda encodings with no auxiliary datatype system. New typing constructs are defined that enable induction, as well as large eliminations with lambda encodings. These constructs are constructor-constrained recursive types, and a lifting operation to lift simply typed terms to the type level. Using a lattice-theoretic denotational semantics for types, the language is proved logically consistent. The power of CDLE is demonstrated through several examples, which have been checked with a prototype implementation called Cedille.},
  journaltitle = {Journal of Functional Programming},
  urldate = {2018-02-23},
  year = {2017/ed},
  author = {Stump, Aaron},
  file = {/Users/pgiarrusso/Zotero/storage/UDACNQ9I/Stump - 2017 - The calculus of dependent lambda eliminations.pdf;/Users/pgiarrusso/Zotero/storage/ZUZW33KU/Stump - 2017 - The calculus of dependent lambda eliminations.pdf;/Users/pgiarrusso/Zotero/storage/2US8ZS42/1D0BDA070E9273AC56C108D8F6F2B078.html;/Users/pgiarrusso/Zotero/storage/SJMHRN8M/1D0BDA070E9273AC56C108D8F6F2B078.html}
}
% == BibLateX quality report for Stump2017calculus:
% 'issn': not a valid ISSN

@article{Najd2017Trees,
  title = {Trees that Grow},
  url = {http://www.jucs.org/jucs_23_1/trees_that_grow/},
  abstract = {We study the notion of extensibility in functional data types, as a new approach to the problem of decorating abstract syntax trees with additional information. We observed the need for such extensibility while redesigning the data types representing Haskell abstract syntax inside Glasgow Haskell Compiler (GHC).

Specifically, we describe a programming idiom that exploits type-level functions to allow a particular form of extensibility. The approach scales to support existentials and generalised algebraic data types, and we can use pattern synonyms to make it convenient in practice.},
  urldate = {2018-02-23},
  date = {2017},
  author = {Najd, Shayan and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/KDC7X6PS/Najd and Peyton Jones - 2017 - Trees that Grow.pdf;/Users/pgiarrusso/Zotero/storage/DRM9WIWM/trees_that_grow.html}
}
% == BibLateX quality report for Najd2017Trees:
% Missing required field 'journaltitle'

@article{Severi2017Two,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.00285},
  primaryClass = {cs},
  title = {Two Light Modalities for Recursion},
  url = {http://arxiv.org/abs/1801.00285},
  abstract = {We investigate the interplay between two modalities for controlling the behaviour of recursive functional programs on infinite structures which are completely silent in the syntax. The latter means that programs do not contain "marks" showing the application of the introduction and elimination rules for the modalities. This shifts the burden of controlling recursion from the programmer to the compiler. To do this, we introduce a typed lambda calculus a la Curry with two silent modalities and guarded recursive types. The first modality delays the data while the second one removes the delays. The typing discipline guarantees normalisation and can be transformed into an algorithm which infers the type of a program.},
  urldate = {2018-02-25},
  date = {2017-12-31},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  author = {Severi, Paula},
  file = {/Users/pgiarrusso/Zotero/storage/9JJSXS32/Severi - 2017 - Two Light Modalities for Recursion.pdf;/Users/pgiarrusso/Zotero/storage/L4TR9YIR/1801.html}
}
% == BibLateX quality report for Severi2017Two:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@report{Coquand1986analysis,
  langid = {english},
  title = {An analysis of Girard's paradox},
  url = {https://hal.inria.fr/inria-00076023/document},
  institution = {{INRIA}},
  type = {report},
  urldate = {2018-03-01},
  date = {1986-05},
  author = {Coquand, T.},
  file = {/Users/pgiarrusso/Zotero/storage/CK4EPWGY/Coquand - 1986 - An analysis of Girard's paradox.pdf;/Users/pgiarrusso/Zotero/storage/K3RZ79IY/inria-00076023.html}
}

@book{Stump1999Coquand,
  title = {On Coquand's "An Analysis of Girard's Paradox"},
  abstract = {In his paper "An Analysis of Girard's Paradox" [3], Coquand presents a result  of Girard that minimal higher-order logic extended with quantification over types is  inconsistent. Using Girard's idea, he shows that some other extensions of minimal  higher-order logic, several extensions of the calculus of constructions, and an early  calculus of Martin-Lof with type:type are also inconsistent. He also presents several  consistent extensions of minimal higher-order logic and the calculus of constructions.  In this paper, I survey relevant background material and present two of Coquand's  proofs of inconsistency.  1 Background  In this section, I present some material that is relevant for understanding Coquand's results. Readers familiar with this material may wish to skip to section 2 and refer to this section later as necessary.  1.1 Styles of Axiomatization  There are two styles commonly used for axiomatizing various logics; the style of natural deduction and the style of Hilbert. Syst...},
  date = {1999},
  author = {Stump, Aaron},
  file = {/Users/pgiarrusso/Zotero/storage/9LRPWQ2I/Stump - On Coquand's An Analysis of Girard's Paradox.pdf;/Users/pgiarrusso/Zotero/storage/R7642M63/summary.html}
}

@article{Finn1997Partial,
  langid = {english},
  title = {Partial Functions in a Total Setting},
  volume = {18},
  issn = {0168-7433, 1573-0670},
  url = {https://link.springer.com/article/10.1023/A:1005702928286},
  doi = {10.1023/A:1005702928286},
  abstract = {We discuss a scheme for defining and reasoning about partial recursive functions within a classical two-valued logic in which all terms denote. We show how a total extension of the partial function introduced by a recursive declaration may be axiomatized within a classical logic, and illustrate by an example the kind of reasoning that our scheme supports. By presenting a naive set-theoretic semantics, we show that the system we propose is logically consistent. Our work is motivated largely by the pragmatic issues arising from mechanical theorem proving – we discuss some of the practical benefits and limitations of our scheme for mechanical verification of software and hardware systems.},
  number = {1},
  journaltitle = {Journal of Automated Reasoning},
  shortjournal = {Journal of Automated Reasoning},
  urldate = {2018-03-01},
  date = {1997-02-01},
  pages = {85-104},
  author = {Finn, Simon and Fourman, Michael P. and Longley, John},
  file = {/Users/pgiarrusso/Zotero/storage/L5ZE8DZ6/Finn-Fourman-Longley - 1997 - Partial Functions in a Total Setting.pdf;/Users/pgiarrusso/Zotero/storage/UBETY3E5/A1005702928286.html}
}
% == BibLateX quality report for Finn1997Partial:
% 'issn': not a valid ISSN

@inproceedings{Hurkens1995simplification,
  langid = {english},
  title = {A simplification of Girard's paradox},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  url = {https://link.springer.com/chapter/10.1007/BFb0014058},
  doi = {10.1007/BFb0014058},
  abstract = {In 1972 J.-Y. Girard showed that the Burali-Forti paradox can be formalised in the type system U. In 1991 Th. Coquand formalised another paradox in U−. The corresponding proof terms (that have no normal form) are large. We present a shorter term of type ⊥ in the Pure Type System λU− and analyse its reduction behaviour. The idea is to construct a universe U and two functions such that a certain equality holds. Using this equality, we prove and disprove that a certain object in U is well-founded.},
  eventtitle = {International Conference on Typed Lambda Calculi and Applications},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-03-01},
  date = {1995-04-10},
  pages = {266-278},
  author = {Hurkens, Antonius J. C.},
  file = {/Users/pgiarrusso/Zotero/storage/BW68ZNLI/Hurkens - 1995 - A simplification of Girard's paradox.pdf;/Users/pgiarrusso/Zotero/storage/X9B7ZE4R/BFb0014058.html}
}
% == BibLateX quality report for Hurkens1995simplification:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Odersky2017Simplicitlya,
  title = {Simplicitly: Foundations and Applications of Implicit Function Types},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3158130},
  doi = {10.1145/3158130},
  shorttitle = {Simplicitly},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  issue = {POPL},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-03-03},
  date = {2017-12},
  pages = {42:1--42:29},
  keywords = {Scala,Dotty,implicit parameters},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  file = {/Users/pgiarrusso/Zotero/storage/PS7URS79/Odersky-Blanvillain-Liu-Biboudis-Miller-Stucki - 2017 - Simplicitly - Foundations and Applications of Implicit Function Types.pdf}
}
% == BibLateX quality report for Odersky2017Simplicitlya:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Delling2016Rethinking,
  title = {Rethinking Experimental Methods in Computing (Dagstuhl Seminar 16111)},
  volume = {6},
  issn = {2192-5283},
  url = {http://drops.dagstuhl.de/opus/volltexte/2016/6146},
  doi = {10.4230/DagRep.6.3.24},
  number = {3},
  journaltitle = {Dagstuhl Reports},
  urldate = {2018-03-03},
  date = {2016},
  pages = {24--43},
  keywords = {Algorithms,Benchmarks,Data sets,Experiments,Repeatability,Reproducibility,Software Artifacts,Statistics},
  author = {Delling, Daniel and Demetrescu, Camil and Johnson, David S. and Vitek, Jan},
  editor = {Delling, Daniel and Demetrescu, Camil and Johnson, David S. and Vitek, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/Y833UYF8/Delling-Demetrescu-Johnson-Vitek - 2016 - Rethinking Experimental Methods in Computing (Dagstuhl Seminar 16111).pdf;/Users/pgiarrusso/Zotero/storage/HRPUICSJ/6146.html}
}

@inproceedings{Plociniczak2013Scalad,
  location = {{New York, NY, USA}},
  title = {Scalad: An Interactive Type-level Debugger},
  isbn = {978-1-4503-2064-1},
  url = {http://doi.acm.org/10.1145/2489837.2489845},
  doi = {10.1145/2489837.2489845},
  shorttitle = {Scalad},
  abstract = {Dealing with statically-typed languages and their complex type systems is problematic for programmers of all levels. Resolving confusing type errors is a time consuming and not always successful process. In this tool demonstration we give an overview of Scalad, an interactive tool that can explain decisions made by the existing typechecker of a multi-paradigm programming language by visualizing the whole process in the form of a proof tree. The tool works for both type correct and incorrect programs, making it suitable for educational purposes as well as debugging. We provide examples on how the tool can be used to understand typing puzzles. The debugger comes with an expandable search mechanism that can precisely guide users in finding answers to the typechecking problems and improve exploration time.},
  booktitle = {Proceedings of the 4th Workshop on Scala},
  series = {SCALA '13},
  publisher = {{ACM}},
  urldate = {2018-03-04},
  date = {2013},
  pages = {8:1--8:4},
  keywords = {debugging,errors,type system,visualization},
  author = {Plociniczak, Hubert},
  file = {/Users/pgiarrusso/Zotero/storage/CDB7EUYI/Plociniczak - 2013 - Scalad - An Interactive Type-level Debugger.pdf}
}

@article{Orchard2014semantic,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.5391},
  primaryClass = {cs},
  title = {The semantic marriage of monads and effects},
  url = {http://arxiv.org/abs/1401.5391},
  abstract = {Wadler and Thiemann unified type-and-effect systems with monadic semantics via a syntactic correspondence and soundness results with respect to an operational semantics. They conjecture that a general, "coherent" denotational semantics can be given to unify effect systems with a monadic-style semantics. We provide such a semantics based on the novel structure of an indexed monad, which we introduce. We redefine the semantics of Moggi's computational lambda-calculus in terms of (strong) indexed monads which gives a one-to-one correspondence between indices of the denotations and the effect annotations of traditional effect systems. Dually, this approach yields indexed comonads which gives a unified semantics and effect system to contextual notions of effect (called coeffects), which we have previously described.},
  urldate = {2018-03-07},
  date = {2014-01-21},
  keywords = {Computer Science - Programming Languages},
  author = {Orchard, Dominic and Petricek, Tomas and Mycroft, Alan},
  file = {/Users/pgiarrusso/Zotero/storage/96FE348D/Orchard-Petricek-Mycroft - 2014 - The semantic marriage of monads and effects.pdf;/Users/pgiarrusso/Zotero/storage/8LTX6FQR/1401.html}
}
% == BibLateX quality report for Orchard2014semantic:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Wadler1998Marriage,
  location = {{New York, NY, USA}},
  title = {The Marriage of Effects and Monads},
  isbn = {978-1-58113-024-9},
  url = {http://doi.acm.org/10.1145/289423.289429},
  doi = {10.1145/289423.289429},
  abstract = {Gifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect systems into a corresponding monad system.},
  booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '98},
  publisher = {{ACM}},
  urldate = {2018-03-07},
  date = {1998},
  pages = {63--74},
  author = {Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/IZF82HEA/Wadler - 1998 - The Marriage of Effects and Monads.pdf}
}

@thesis{Ahmed2004Semantics,
  title = {Semantics of Types for Mutable State (Thesis) | Computer Science Department at Princeton University},
  url = {https://www.cs.princeton.edu/research/techreps/TR-713-04},
  abstract = {Proof-carrying code (PCC) is a framework for mechanically verifying the safety of machine language programs. A program that is successfully verified by a PCC system is guaranteed to be safe to execute, but this safety guarantee is contingent upon the correctness of various trusted components. For instance, in traditional PCC systems the trusted computing base includes a large set of low-level typing rules. Foundational PCC systems seek to minimize the size of the trusted computing base. In particular, they eliminate the need to trust complex, low-level type systems by providing machine-checkable proofs of type soundness for real machine languages.

In this thesis, I demonstrate the use of logical relations for proving
the soundness of type systems for mutable state. Specifically, I
focus on type systems that ensure the safe allocation, update, and
reuse of memory. For each type in the language, I define logical
relations that explain the meaning of the type in terms of the
operational semantics of the language. Using this model of types, I
prove each typing rule as a lemma.

The major contribution is a model of System F with general references
-- that is, mutable cells that can hold values of any closed
type including other references, functions, recursive types, and
impredicative quantified types. The model is based on ideas from both
possible worlds and the indexed model of Appel and McAllester.

I show how the model of mutable references is encoded in higher-order
logic. I also show how to construct an indexed possible-worlds model for a von Neumann machine. The latter is used in the Princeton Foundational PCC system to prove type safety for a full-fledged low-level typed assembly language. Finally, I present a semantic model for a region calculus that supports type-invariant references as well as memory reuse.},
  institution = {{Princeton}},
  urldate = {2018-03-09},
  date = {2004-06},
  author = {Ahmed, Amal Jamil},
  file = {/Users/pgiarrusso/Zotero/storage/9FUTBVB9/Ahmed - 2004 - Semantics of Types for Mutable State (Thesis) Computer Science Department at Princeton University.pdf;/Users/pgiarrusso/Zotero/storage/AGXTKFV2/TR-713-04.html}
}
% == BibLateX quality report for Ahmed2004Semantics:
% Missing required field 'type'

@article{Ahmed2003Indexed,
  title = {An Indexed Model of Impredicative Polymorphism and Mutable References},
  abstract = {We present a semantic model of the polymorphic lambda calculus augmented with a higher-order store, allowing the storage of values of any type, including impredicative quantified types, mutable references, recursive types, and functions. Our model provides the first denotational semantics for a type system with updatable references to values of impredicative quantified types. The central idea behind our semantics is that instead of tracking the exact type of a mutable reference in a possible world our model keeps track of the approximate type. While high-level languages like ML and Java do not themselves support storage of impredicative existential packages in mutable cells, this feature is essential when representing ML function closures, that is, in a target language for typed closure conversion of ML programs. 1},
  date = {2003-12-24},
  author = {Ahmed, Amal and Appel, Andrew W. and Virga, Roberto},
  file = {/Users/pgiarrusso/Zotero/storage/WWSJ6V83/Ahmed et al. - 2003 - An Indexed Model of Impredicative Polymorphism and.pdf}
}
% == BibLateX quality report for Ahmed2003Indexed:
% Missing required field 'journaltitle'

@inproceedings{Lucassen1988Polymorphic,
  location = {{New York, NY, USA}},
  title = {Polymorphic Effect Systems},
  isbn = {978-0-89791-252-5},
  url = {http://doi.acm.org/10.1145/73560.73564},
  doi = {10.1145/73560.73564},
  abstract = {We present a new approach to programming languages for parallel computers that uses an effect system to discover expression scheduling constraints. This effect system is part of a 'kinded' type system with three base kinds: types, which describe the value that an expression may return; effects, which describe the side-effects that an expression may have; and regions, which describe the area of the store in which side-effects may occur. Types, effects and regions are collectively called descriptions.
Expressions can be abstracted over any kind of description variable -- this permits type, effect and region polymorphism. Unobservable side-effects can be masked by the effect system; an effect soundness property guarantees that the effects computed statically by the effect system are a conservative approximation of the actual side-effects that a given expression may have.
The effect system we describe performs certain kinds of side-effect analysis that were not previously feasible. Experimental data from the programming language FX indicate that an effect system can be used effectively to compile programs for parallel computers.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '88},
  publisher = {{ACM}},
  urldate = {2018-03-12},
  date = {1988},
  pages = {47--57},
  author = {Lucassen, J. M. and Gifford, D. K.},
  file = {/Users/pgiarrusso/Zotero/storage/LZQK6ZK5/Lucassen-Gifford - 1988 - Polymorphic Effect Systems.pdf}
}

@inproceedings{Gifford1986Integrating,
  location = {{New York, NY, USA}},
  title = {Integrating Functional and Imperative Programming},
  isbn = {978-0-89791-200-6},
  url = {http://doi.acm.org/10.1145/319838.319848},
  doi = {10.1145/319838.319848},
  booktitle = {Proceedings of the 1986 ACM Conference on LISP and Functional Programming},
  series = {LFP '86},
  publisher = {{ACM}},
  urldate = {2018-03-12},
  date = {1986},
  pages = {28--38},
  author = {Gifford, David K. and Lucassen, John M.},
  file = {/Users/pgiarrusso/Zotero/storage/Z45WEUYU/Gifford-Lucassen - 1986 - Integrating Functional and Imperative Programming.pdf}
}

@inproceedings{Toro2015Customizable,
  location = {{New York, NY, USA}},
  title = {Customizable Gradual Polymorphic Effects for Scala},
  isbn = {978-1-4503-3689-5},
  url = {http://doi.acm.org/10.1145/2814270.2814315},
  doi = {10.1145/2814270.2814315},
  abstract = {Despite their obvious advantages in terms of static reasoning, the adoption of effect systems is still rather limited in practice. Recent advances such as generic effect systems, lightweight effect polymorphism, and gradual effect checking, all represent promising steps towards making effect systems suitable for widespread use. However, no existing system combines these approaches: the theory of gradual polymorphic effects has not been developed, and there are no implementations of gradual effect checking. In addition, a limiting factor in the adoption of effect systems is their unsuitability for localized and customized effect disciplines. This paper addresses these issues by presenting the first implementation of gradual effect checking, for Scala, which supports both effect polymorphism and a domain-specific language called Effscript to declaratively define and customize effect disciplines. We report on the theory, implementation, and practical application of the system.},
  booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  series = {OOPSLA 2015},
  publisher = {{ACM}},
  urldate = {2018-03-12},
  date = {2015},
  pages = {935--953},
  keywords = {gradual typing,Scala,effect polymorphism,effscript,Type-and-effect systems},
  author = {Toro, Matías and Tanter, Éric},
  file = {/Users/pgiarrusso/Zotero/storage/D9PA467N/Toro-Tanter - 2015 - Customizable Gradual Polymorphic Effects for Scala.pdf}
}

@inproceedings{Morris2016Best,
  location = {{New York, NY, USA}},
  title = {The Best of Both Worlds: Linear Functional Programming Without Compromise},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951925},
  doi = {10.1145/2951913.2951925},
  shorttitle = {The Best of Both Worlds},
  abstract = {We present a linear functional calculus with both the safety guarantees expressible with linear types and the rich language of combinators and composition provided by functional programming. Unlike previous combinations of linear typing and functional programming, we compromise neither the linear side (for example, our linear values are first-class citizens of the language) nor the functional side (for example, we do not require duplicate definitions of compositions for linear and unrestricted functions). To do so, we must generalize abstraction and application to encompass both linear and unrestricted functions. We capture the typing of the generalized constructs with a novel use of qualified types. Our system maintains the metatheoretic properties of the theory of qualified types, including principal types and decidable type inference. Finally, we give a formal basis for our claims of expressiveness, by showing that evaluation respects linearity, and that our language is a conservative extension of existing functional calculi.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2018-03-12},
  date = {2016},
  pages = {448--461},
  keywords = {linear types,qualified types,substructural types},
  author = {Morris, J. Garrett},
  file = {/Users/pgiarrusso/Zotero/storage/85C7CVZY/Morris - 2016 - The Best of Both Worlds - Linear Functional Programming Without Compromise.pdf}
}

@incollection{Coquand1995new,
  title = {A new paradox in type theory},
  volume = {134},
  url = {http://www.sciencedirect.com/science/article/pii/S0049237X06800625},
  abstract = {This chapter presents a new paradox in type theory that is a type-theoretic refinement of Reynolds' result that there is no set-theoretic model of polymorphism. One application of this paradox, which shows unexpected connections between the principles of excluded middle and the axiom of description in impredicative type theories, is discussed in the chapter. The lambda terms will always be considered up to β-conversion. The types of minimal higher order logic consist of one basic type “o” and function types of the form α → β. The chapter also defines other logical connectives. The original logic of Church was formulated for classical logic and had a ground type of individuals. Yet another difference was the introduction of a description operator. It is possible to interpret classical higher order propositional logic in minimal higher order logic. Minimal higher order logic has a direct set-theoretic semantics. Second-order lambda calculus is also introduced in the chapter. One motivation is to provide syntax for polymorphic procedure.},
  booktitle = {Studies in Logic and the Foundations of Mathematics},
  series = {Logic, Methodology and Philosophy of Science IX},
  publisher = {{Elsevier}},
  urldate = {2018-03-14},
  date = {1995-01-01},
  pages = {555-570},
  author = {Coquand, Thierry},
  editor = {Prawitz, Dag and Skyrms, Brian and Westerståhl, Dag},
  file = {/Users/pgiarrusso/Zotero/storage/C2I6U7A3/Coquand - 1995 - A new paradox in type theory.pdf;/Users/pgiarrusso/Zotero/storage/7ZJSCR48/S0049237X06800625.html},
  doi = {10.1016/S0049-237X(06)80062-5}
}

@inproceedings{Jones1995Functional,
  langid = {english},
  title = {Functional programming with overloading and higher-order polymorphism},
  isbn = {978-3-540-59451-2 978-3-540-49270-2},
  url = {https://link.springer.com/chapter/10.1007/3-540-59451-5_4},
  doi = {10.1007/3-540-59451-5_4},
  abstract = {The Hindley/Milner type system has been widely adopted as a basis for statically typed functional languages. One of the main reasons for this is that it provides an elegant compromise between flexibility, allowing a single value to be used in different ways, and practicality, freeing the programmer from the need to supply explicit type information. Focusing on practical applications rather than implementation or theoretical details, these notes examine a range of extensions that provide more flexible type systems while retaining many of the properties that have made the original Hindley/Milner system so popular. The topics discussed, some old, but most quite recent, include higher-order polymorphism and type and constructor class overloading. Particular emphasis is placed on the use of these features to promote modularity and reusability.},
  eventtitle = {International School on Advanced Functional Programming},
  booktitle = {Advanced Functional Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-03-15},
  date = {1995-05-24},
  pages = {97-136},
  author = {Jones, Mark P.},
  file = {/Users/pgiarrusso/Zotero/storage/YLEAL46D/Jones - 1995 - Functional programming with overloading and higher-order polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/ZRJDFA2C/3-540-59451-5_4.html}
}
% == BibLateX quality report for Jones1995Functional:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@book{Abel1998foetus,
  title = {foetus – Termination Checker for Simple Functional Programs. www.tcs.informatik. uni-muenchen.de/ abel/foetus},
  abstract = {We introduce a simple functional language foetus (lambda calculus with tuples, constructors and pattern matching) supplied with a termination checker. This checker tries to find a well-founded structural order on the parameters on the given function to prove termination. The components of the check algorithm are: function call extraction out of the program text, call graph completion and finding a lexical order for the function parameters. The HTML version of this paper contains many ready-to-run Web-based examples. 1},
  date = {1998},
  author = {Abel, Andreas},
  file = {/Users/pgiarrusso/Zotero/storage/5RD657LS/Abel - 1998 - foetus – Termination Checker for Simple Functional Programs. www.tcs.informatik. uni-muenchen.de- abel-foetus.pdf;/Users/pgiarrusso/Zotero/storage/BGSH3ZJW/summary.html}
}

@inproceedings{Weirich2013System,
  location = {{New York, NY, USA}},
  title = {System FC with Explicit Kind Equality},
  isbn = {978-1-4503-2326-0},
  url = {http://doi.acm.org/10.1145/2500365.2500599},
  doi = {10.1145/2500365.2500599},
  abstract = {System FC, the core language of the Glasgow Haskell Compiler, is an explicitly-typed variant of System F with first-class type equality proofs called coercions. This extensible proof system forms the foundation for type system extensions such as type families (type-level functions) and Generalized Algebraic Datatypes (GADTs). Such features, in conjunction with kind polymorphism and datatype promotion, support expressive compile-time reasoning. However, the core language lacks explicit kind equality proofs. As a result, type-level computation does not have access to kind-level functions or promoted GADTs, the type-level analogues to expression-level features that have been so useful. In this paper, we eliminate such discrepancies by introducing kind equalities to System FC. Our approach is based on dependent type systems with heterogeneous equality and the "Type-in-Type" axiom, yet it preserves the metatheoretic properties of FC. In particular, type checking is simple, decidable and syntax directed. We prove the preservation and progress theorems for the extended language.},
  booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '13},
  publisher = {{ACM}},
  urldate = {2018-03-20},
  date = {2013},
  pages = {275--286},
  keywords = {dependent types,equality,haskell},
  author = {Weirich, Stephanie and Hsu, Justin and Eisenberg, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/878FBPQA/Weirich-Hsu-Eisenberg - 2013 - System FC with Explicit Kind Equality.pdf}
}

@article{Coquand1988calculus,
  title = {The calculus of constructions},
  volume = {76},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/0890540188900053},
  doi = {10.1016/0890-5401(88)90005-3},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2018-03-23},
  date = {1988-02-01},
  pages = {95-120},
  author = {Coquand, Thierry and Huet, Gérard},
  file = {/Users/pgiarrusso/Zotero/storage/52SEWZ4D/Coquand-Huet - 1988 - The calculus of constructions.pdf;/Users/pgiarrusso/Zotero/storage/CDDGXLTI/0890540188900053.html}
}

@article{Zhang2017Familia,
  title = {Familia: Unifying Interfaces, Type Classes, and Family Polymorphism},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3133894},
  doi = {10.1145/3133894},
  shorttitle = {Familia},
  abstract = {Parametric polymorphism and inheritance are both important, extensively explored language mechanisms for providing code reuse and extensibility. But harmoniously integrating these apparently distinct mechanisms—and powerful recent forms of them, including type classes and family polymorphism—in a single language remains an elusive goal. In this paper, we show that a deep unification can be achieved by generalizing the semantics of interfaces and classes. The payoff is a significant increase in expressive power with little increase in programmer-visible complexity. Salient features of the new programming language include retroactive constraint modeling, underpinning both object-oriented programming and generic programming, and module-level inheritance with further-binding, allowing family polymorphism to be deployed at large scale. The resulting mechanism is syntactically light, and the more advanced features are transparent to the novice programmer. We describe the design of a programming language that incorporates this mechanism; using a core calculus, we show that the type system is sound. We demonstrate that this language is highly expressive by illustrating how to use it to implement highly extensible software and by showing that it can not only concisely model state-of-the-art features for code reuse, but also go beyond them.},
  issue = {OOPSLA},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-04-02},
  date = {2017-10},
  pages = {70:1--70:31},
  keywords = {_tablet,extensibility,type classes,language design,family polymorphism,Familia,genericity,type-safety},
  author = {Zhang, Yizhou and Myers, Andrew C.},
  file = {/Users/pgiarrusso/Zotero/storage/5UVSCYR9/Zhang-Myers - 2017 - Familia - Unifying Interfaces, Type Classes, and Family Polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/NBQJCFVU/Zhang-Myers - 2017 - Familia - Unifying Interfaces, Type Classes, and Family Polymorphism.pdf}
}
% == BibLateX quality report for Zhang2017Familia:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Svendsen2014Impredicative,
  langid = {english},
  title = {Impredicative Concurrent Abstract Predicates},
  isbn = {978-3-642-54832-1 978-3-642-54833-8},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-54833-8_9},
  doi = {10.1007/978-3-642-54833-8_9},
  abstract = {We present impredicative concurrent abstract predicates – iCAP – a program logic for modular reasoning about concurrent, higher-order, reentrant, imperative code. Building on earlier work, iCAP uses protocols to reason about shared mutable state. A key novel feature of iCAP is the ability to define impredicative protocols; protocols that are parameterized on arbitrary predicates, including predicates that themselves refer to protocols. We demonstrate the utility of impredicative protocols through a series of examples, including the specification and verification, in the logic, of a spin-lock, a reentrant event loop, and a concurrent bag implemented using cooperation, against modular specifications.},
  eventtitle = {European Symposium on Programming Languages and Systems},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-04-04},
  date = {2014-04-05},
  pages = {149-168},
  author = {Svendsen, Kasper and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/55I3KLEL/Svendsen-Birkedal - 2014 - Impredicative Concurrent Abstract Predicates.pdf;/Users/pgiarrusso/Zotero/storage/C8AVZ93M/978-3-642-54833-8_9.html}
}
% == BibLateX quality report for Svendsen2014Impredicative:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Jung2015Iris,
  location = {{New York, NY, USA}},
  title = {Iris: Monoids and Invariants As an Orthogonal Basis for Concurrent Reasoning},
  isbn = {978-1-4503-3300-9},
  url = {http://doi.acm.org/10.1145/2676726.2676980},
  doi = {10.1145/2676726.2676980},
  shorttitle = {Iris},
  abstract = {We present Iris, a concurrent separation logic with a simple premise: monoids and invariants are all you need. Partial commutative monoids enable us to express---and invariants enable us to enforce---user-defined *protocols* on shared state, which are at the conceptual core of most recent program logics for concurrency. Furthermore, through a novel extension of the concept of a *view shift*, Iris supports the encoding of *logically atomic specifications*, i.e., Hoare-style specs that permit the client of an operation to treat the operation essentially as if it were atomic, even if it is not.},
  booktitle = {Proceedings of the 42Nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '15},
  publisher = {{ACM}},
  urldate = {2018-04-04},
  date = {2015},
  pages = {637--650},
  keywords = {invariants,higher-order logic,atomicity,compositional verification,fine-grained concurrency,partial commutative monoids,separation logic},
  author = {Jung, Ralf and Swasey, David and Sieczkowski, Filip and Svendsen, Kasper and Turon, Aaron and Birkedal, Lars and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/7MNGH37Z/Jung-Swasey-Sieczkowski-Svendsen-Turon-Birkedal-Dreyer - 2015 - Iris - Monoids and Invariants As an Orthogonal Basis for Concurrent Reasoning.pdf}
}

@article{Birkedal2010categorytheoretic,
  title = {The category-theoretic solution of recursive metric-space equations},
  volume = {411},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397510003968},
  doi = {10.1016/j.tcs.2010.07.010},
  abstract = {It is well known that one can use an adaptation of the inverse-limit construction to solve recursive equations in the category of complete ultrametric spaces. We show that this construction generalizes to a large class of categories with metric-space structure on each set of morphisms: the exact nature of the objects is less important. In particular, the construction immediately applies to categories where the objects are ultrametric spaces with ‘extra structure’, and where the morphisms preserve this extra structure. The generalization is inspired by classical domain-theoretic work by Smyth and Plotkin. For many of the categories we consider, there is a natural subcategory in which each set of morphisms is required to be a compact metric space. Our setting allows for a proof that such a subcategory always inherits solutions of recursive equations from the full category. As another application, we present a construction that relates solutions of generalized domain equations in the sense of Smyth and Plotkin to solutions of equations in our class of categories. Our primary motivation for solving generalized recursive metric-space equations comes from recent and ongoing work on Kripke-style models in which the sets of worlds must be recursively defined. We show a series of examples motivated by this line of work.},
  number = {47},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2018-04-04},
  date = {2010-10-29},
  pages = {4102-4122},
  keywords = {Fixed point,Metric space,Recursive equation},
  author = {Birkedal, Lars and Støvring, Kristian and Thamsborg, Jacob},
  file = {/Users/pgiarrusso/Zotero/storage/PUHI74FK/Birkedal-Støvring-Thamsborg - 2010 - The category-theoretic solution of recursive metric-space equations.pdf;/Users/pgiarrusso/Zotero/storage/AKH5I9VH/S0304397510003968.html}
}

@article{Schrijvers2017Cochis,
  langid = {english},
  title = {Cochis: Deterministic and coherent implicits},
  url = {https://lirias.kuleuven.be/handle/123456789/582749},
  shorttitle = {Cochis},
  urldate = {2018-04-04},
  date = {2017-05-01},
  author = {Schrijvers, Tom and C. d. S. Oliveira, Bruno and Wadler, Philip},
  file = {/Users/pgiarrusso/Zotero/storage/35ATQXZE/Schrijvers-C. d. S. Oliveira-Wadler - 2017 - Cochis - Deterministic and coherent implicits.pdf;/Users/pgiarrusso/Zotero/storage/I8IYJ2ZC/582749.html}
}
% == BibLateX quality report for Schrijvers2017Cochis:
% Missing required field 'journaltitle'

@inproceedings{Bernardy2010Parametricity,
  location = {{New York, NY, USA}},
  title = {Parametricity and Dependent Types},
  isbn = {978-1-60558-794-3},
  url = {http://doi.acm.org/10.1145/1863543.1863592},
  doi = {10.1145/1863543.1863592},
  abstract = {Reynolds' abstraction theorem shows how a typing judgement in System F can be translated into a relational statement (in second order predicate logic) about inhabitants of the type. We (in second order predicate logic) about inhabitants of the type. We obtain a similar result for a single lambda calculus (a pure type system), in which terms, types and their relations are expressed. Working within a single system dispenses with the need for an interpretation layer, allowing for an unusually simple presentation. While the unification puts some constraints on the type system (which we spell out), the result applies to many interesting cases, including dependently-typed ones.},
  booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '10},
  publisher = {{ACM}},
  urldate = {2018-04-07},
  date = {2010},
  pages = {345--356},
  keywords = {abstraction theorem,free theorems,pure type system},
  author = {Bernardy, Jean-Philippe and Jansson, Patrik and Paterson, Ross},
  file = {/Users/pgiarrusso/Zotero/storage/Z6QNWJIE/Bernardy-Jansson-Paterson - 2010 - Parametricity and Dependent Types.pdf}
}

@inproceedings{McBride2015TuringCompleteness,
  langid = {english},
  title = {Turing-Completeness Totally Free},
  isbn = {978-3-319-19796-8 978-3-319-19797-5},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-19797-5_13},
  doi = {10.1007/978-3-319-19797-5_13},
  abstract = {In this paper, I show that general recursive definitions can be represented in the free monad which supports the ‘effect’ of making a recursive call, without saying how these calls should be executed. Diverse semantics can be given within a total framework by suitable monad morphisms. The Bove-Capretta construction of the domain of a general recursive function can be presented datatype-generically as an instance of this technique. The paper is literate Agda, but its key ideas are more broadly transferable.},
  eventtitle = {International Conference on Mathematics of Program Construction},
  booktitle = {Mathematics of Program Construction},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2018-04-09},
  date = {2015-06-29},
  pages = {257-275},
  author = {McBride, Conor},
  file = {/Users/pgiarrusso/Zotero/storage/SGQJYEMM/McBride - 2015 - Turing-Completeness Totally Free.pdf;/Users/pgiarrusso/Zotero/storage/V2LVV5P7/978-3-319-19797-5_13.html}
}
% == BibLateX quality report for McBride2015TuringCompleteness:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Grebe2017Rewriting,
  location = {{New York, NY, USA}},
  title = {Rewriting a Shallow DSL Using a GHC Compiler Extension},
  isbn = {978-1-4503-5524-7},
  url = {http://doi.acm.org/10.1145/3136040.3136048},
  doi = {10.1145/3136040.3136048},
  abstract = {Embedded Domain Specific Languages are a powerful tool for developing customized languages to fit specific problem domains. Shallow EDSLs allow a programmer to program using many of the features of a host language and its syntax, but sacrifice performance. Deep EDSLs provide better performance and flexibility, through the ability to manipulate the abstract syntax tree of the DSL program, but sacrifice syntactical similarity to the host language. Using Haskino, an EDSL designed for small embedded systems based on the Arduino line of microcontrollers, and a compiler plugin for the Haskell GHC compiler, we show a method for combining the best aspects of shallow and deep EDSLs. The programmer is able to write in the shallow EDSL, and have it automatically transformed into the deep EDSL. This allows the EDSL user to benefit from powerful aspects of the host language, Haskell, while meeting the demanding resource constraints of the small embedded processing environment.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
  series = {GPCE 2017},
  publisher = {{ACM}},
  urldate = {2018-04-17},
  date = {2017},
  pages = {246--258},
  keywords = {Haskell,_tablet,EDSL,Arduino,GHC,Transformations},
  author = {Grebe, Mark and Young, David and Gill, Andy},
  file = {/Users/pgiarrusso/Zotero/storage/59LAFWU3/Grebe-Young-Gill - 2017 - Rewriting a Shallow DSL Using a GHC Compiler Extension.pdf}
}

@article{Barbanera1996Proofirrelevance,
  langid = {english},
  title = {Proof-irrelevance out of excluded-middle and choice in the calculus of constructions},
  volume = {6},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/proofirrelevance-out-of-excludedmiddle-and-choice-in-the-calculus-of-constructions/698316A70788A331B3E10B2D3235255C},
  doi = {10.1017/S0956796800001829},
  abstract = {We present a short and direct syntactic proof of the fact that adding the axiom of choice and the principle of excluded-middle to Coquand–Huet's Calculus of Constructions gives proof-irrelevance.},
  number = {3},
  journaltitle = {Journal of Functional Programming},
  urldate = {2018-04-19},
  date = {1996-05},
  pages = {519-526},
  keywords = {_tablet},
  author = {Barbanera, Franco and Berardi, Stefano},
  file = {/Users/pgiarrusso/Zotero/storage/GLKE8JNZ/Barbanera-Berardi - 1996 - Proof-irrelevance out of excluded-middle and choice in the calculus of constructions.pdf;/Users/pgiarrusso/Zotero/storage/4TGIQTH4/698316A70788A331B3E10B2D3235255C.html}
}
% == BibLateX quality report for Barbanera1996Proofirrelevance:
% 'issn': not a valid ISSN

@inproceedings{Ostermann2018Dualizing,
  langid = {english},
  title = {Dualizing Generalized Algebraic Data Types by Matrix Transposition},
  isbn = {978-3-319-89883-4 978-3-319-89884-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-89884-1_3},
  doi = {10.1007/978-3-319-89884-1_3},
  abstract = {We characterize the relation between generalized algebraic datatypes (GADTs) with pattern matching on their constructors one hand, and generalized algebraic co-datatypes (GAcoDTs) with copattern matching on their destructors on the other hand: GADTs can be converted mechanically to GAcoDTs by refunctionalization, GAcoDTs can be converted mechanically to GADTs by defunctionalization, and both defunctionalization and refunctionalization correspond to a transposition of the matrix in which the equations for each constructor/destructor pair of the (co-)datatype are organized. We have defined a calculus, GADTTGADTTGADT\^T, which unifies GADTs and GAcoDTs in such a way that GADTs and GAcoDTs are merely different ways to partition the program.We have formalized the type system and operational semantics of GADTTGADTTGADT\^T in the Coq proof assistant and have mechanically verified the following results: (1) The type system of GADTTGADTTGADT\^T is sound, (2) defunctionalization and refunctionalization can translate GADTs to GAcoDTs and back, (3) both transformations are type- and semantics-preserving and are inverses of each other, (4) (co-)datatypes can be represented by matrices in such a way the aforementioned transformations correspond to matrix transposition, (5) GADTs are extensible in an exactly dual way to GAcoDTs; we thereby clarify folklore knowledge about the “expression problem”.We believe that the identification of this relationship can guide future language design of “dual features” for data and codata.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2018-04-19},
  date = {2018-04-16},
  pages = {60-85},
  author = {Ostermann, Klaus and Jabs, Julian},
  file = {/Users/pgiarrusso/Zotero/storage/6MPGLA6B/Ostermann-Jabs - 2018 - Dualizing Generalized Algebraic Data Types by Matrix Transposition.pdf;/Users/pgiarrusso/Zotero/storage/C8DKUXJX/978-3-319-89884-1_3.html}
}
% == BibLateX quality report for Ostermann2018Dualizing:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Xie2018Let,
  langid = {english},
  title = {Let Arguments Go First},
  isbn = {978-3-319-89883-4 978-3-319-89884-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-89884-1_10},
  doi = {10.1007/978-3-319-89884-1_10},
  abstract = {Bi-directional type checking has proved to be an extremely useful and versatile tool for type checking and type inference. The conventional presentation of bi-directional type checking consists of two modes: inference mode and checked mode. In traditional bi-directional type-checking, type annotations are used to guide (via the checked mode) the type inference/checking procedure to determine the type of an expression, and type information flows from functions to arguments.This paper presents a variant of bi-directional type checking where the type information flows from arguments to functions. This variant retains the inference mode, but adds a so-called application mode. Such design can remove annotations that basic bi-directional type checking cannot, and is useful when type information from arguments is required to type-check the functions being applied. We present two applications and develop the meta-theory (mostly verified in Coq) of the application mode.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2018-04-19},
  date = {2018-04-16},
  pages = {272-299},
  author = {Xie, Ningning and Oliveira, Bruno C. d S.},
  file = {/Users/pgiarrusso/Zotero/storage/RTZ6W3L2/Xie-Oliveira - 2018 - Let Arguments Go First.pdf;/Users/pgiarrusso/Zotero/storage/U6KMFQXJ/978-3-319-89884-1_10.html}
}
% == BibLateX quality report for Xie2018Let:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Bizjak2018Denotational,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03744},
  primaryClass = {cs},
  title = {Denotational semantics for guarded dependent type theory},
  url = {http://arxiv.org/abs/1802.03744},
  abstract = {We present a new model of Guarded Dependent Type Theory (GDTT), a type theory with guarded recursion and multiple clocks in which one can program with, and reason about coinductive types. Productivity of recursively defined coinductive programs and proofs is encoded in types using guarded recursion, and can therefore be checked modularly, unlike the syntactic checks implemented in modern proof assistants. The model is based on a category of covariant presheaves over a category of time objects, and quantification over clocks is modelled using a presheaf of clocks. To model the clock irrelevance axiom, crucial for programming with coinductive types, types must be interpreted as presheaves orthogonal to the object of clocks. In the case of dependent types, this translates to a unique lifting condition similar to the one found in homotopy theoretic models of type theory. Since the universes defined by the standard Hofmann-Streicher construction in this model do not satisfy this property, the universes in GDTT must be indexed by contexts of clock variables. A large and technical part of the paper is devoted to showing that these can be constructed in such a way that inclusions between universes induced by inclusions of clock variable contexts commute on the nose with type operations on the universes.},
  urldate = {2018-04-19},
  date = {2018-02-11},
  keywords = {Computer Science - Logic in Computer Science},
  author = {Bizjak, Aleš and Møgelberg, Rasmus Ejlers},
  file = {/Users/pgiarrusso/Zotero/storage/R5PX4K6Q/Bizjak-Møgelberg - 2018 - Denotational semantics for guarded dependent type theory.pdf;/Users/pgiarrusso/Zotero/storage/U5HB2I59/1802.html}
}
% == BibLateX quality report for Bizjak2018Denotational:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Odersky2003Nominal,
  langid = {english},
  title = {A Nominal Theory of Objects with Dependent Types},
  isbn = {978-3-540-40531-3 978-3-540-45070-2},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-45070-2_10},
  doi = {10.1007/978-3-540-45070-2_10},
  abstract = {We design and study vObj, a calculus and dependent type system for objects and classes which can have types as members. Type members can be aliases, abstract types, or new types. The type system can model the essential concepts of JAVA’s inner classes as well as virtual types and family polymorphism found in BETA or GBETA. It can also model most concepts of SML-style module systems, including sharing constraints and higher-order functors, but excluding applicative functors. The type system can thus be used as a basis for unifying concepts that so far existed in parallel in advanced object systems and in module systems. The paper presents results on confluence of the calculus, soundness of the type system, and undecidability of type checking.},
  eventtitle = {European Conference on Object-Oriented Programming},
  booktitle = {ECOOP 2003 – Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-04-22},
  date = {2003-07-21},
  pages = {201-224},
  author = {Odersky, Martin and Cremet, Vincent and Röckl, Christine and Zenger, Matthias},
  file = {/Users/pgiarrusso/Zotero/storage/FP9PD2A5/978-3-540-45070-2_10.html}
}
% == BibLateX quality report for Odersky2003Nominal:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Summers2011Freedom,
  location = {{New York, NY, USA}},
  title = {Freedom Before Commitment: A Lightweight Type System for Object Initialisation},
  isbn = {978-1-4503-0940-0},
  url = {http://doi.acm.org/10.1145/2048066.2048142},
  doi = {10.1145/2048066.2048142},
  shorttitle = {Freedom Before Commitment},
  abstract = {One of the main purposes of object initialisation is to establish invariants such as a field being non-null or an immutable data structure containing specific values. These invariants are then implicitly assumed by the rest of the implementation, for instance, to ensure that a field may be safely dereferenced or that immutable data may be accessed concurrently. Consequently, letting an object escape from its constructor is dangerous; the escaping object might not yet satisfy its invariants, leading to errors in code that relies on them. Nevertheless, preventing objects entirely from escaping from their constructors is too restrictive; it is often useful to call auxiliary methods on the object under initialisation or to pass it to another constructor to set up mutually-recursive structures. We present a type system that tracks which objects are fully initialised and which are still under initialisation. The system can be used to prevent objects from escaping, but also to allow safe escaping by making explicit which objects might not yet satisfy their invariants. We designed, formalised and implemented our system as an extension to a non-null type system, but it is not limited to this application. Our system is conceptually simple and requires little annotation overhead; it is sound and sufficiently expressive for many common programming idioms. Therefore, we believe it to be the first such system suitable for mainstream use.},
  booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '11},
  publisher = {{ACM}},
  urldate = {2018-04-25},
  date = {2011},
  pages = {1013--1032},
  keywords = {expressive,initialisation,modular,non-null,simple,sound,type-system},
  author = {Summers, Alexander J. and Mueller, Peter},
  file = {/Users/pgiarrusso/Zotero/storage/B8UEL9B6/Summers-Mueller - 2011 - Freedom Before Commitment - A Lightweight Type System for Object Initialisation.pdf}
}

@inproceedings{Brotherston2017Granullar,
  location = {{New York, NY, USA}},
  title = {Granullar: Gradual Nullable Types for Java},
  isbn = {978-1-4503-5233-8},
  url = {http://doi.acm.org/10.1145/3033019.3033032},
  doi = {10.1145/3033019.3033032},
  shorttitle = {Granullar},
  abstract = {Object-oriented languages like Java and C\# allow the null value for all references. This supports many flexible patterns, but has led to many errors, security vulnerabilities, and system crashes. \% Static type systems can prevent null-pointer exceptions at compile time, but require annotations, in particular for used libraries. Conservative defaults choose the most restrictive typing, preventing many errors, but requiring a large annotation effort. Liberal defaults choose the most flexible typing, requiring less annotations, but giving weaker guarantees. Trusted annotations can be provided, but are not checked and require a large manual effort. None of these approaches provide a strong guarantee that the checked part of the program is isolated from the unchecked part: even with conservative defaults, null-pointer exceptions can occur in the checked part.   This paper presents Granullar, a gradual type system for null-safety. Developers start out verifying null-safety for the most important components of their applications. At the boundary to unchecked components, runtime checks are inserted by Granullar to guard the verified system from being polluted by unexpected null values. This ensures that null-pointer exceptions can only occur within the unchecked code or at the boundary to checked code; the checked code is free of null-pointer exceptions.   We present Granullar for Java, define the checked-unchecked boundary, and how runtime checks are generated. We evaluate our approach on real world software annotated for null-safety. We demonstrate the runtime checks, and acceptable compile-time and run-time performance impacts. Granullar enables combining a checked core with untrusted libraries in a safe manner, improving on the practicality of such a system.},
  booktitle = {Proceedings of the 26th International Conference on Compiler Construction},
  series = {CC 2017},
  publisher = {{ACM}},
  urldate = {2018-04-25},
  date = {2017},
  pages = {87--97},
  keywords = {gradual type systems,nullness,pluggable type systems,runtime checks},
  author = {Brotherston, Dan and Dietl, Werner and Lhoták, Ondřej},
  file = {/Users/pgiarrusso/Zotero/storage/V22TMFIG/Brotherston-Dietl-Lhoták - 2017 - Granullar - Gradual Nullable Types for Java.pdf}
}

@article{Dunfield2013Complete,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.6032},
  primaryClass = {cs},
  title = {Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism},
  url = {http://arxiv.org/abs/1306.6032},
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike Damas-Milner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higher-rank polymorphism, grounded in proof theory; this calculus enjoys many properties such as eta-reduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and well-behaved, despite being both sound and complete.},
  urldate = {2018-04-25},
  date = {2013-06-25},
  keywords = {Computer Science - Programming Languages},
  author = {Dunfield, Joshua and Krishnaswami, Neelakantan R.},
  file = {/Users/pgiarrusso/Zotero/storage/EI5GM3LE/Dunfield-Krishnaswami - 2013 - Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/G8JNJXVR/1306.html}
}
% == BibLateX quality report for Dunfield2013Complete:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Liu2016generic,
  langid = {english},
  title = {A generic algorithm for checking exhaustivity of pattern matching},
  url = {https://infoscience.epfl.ch/record/225497},
  doi = {10.1145/2998392.2998401},
  abstract = {Algebraic data types and pattern matching are key features of functional programming languages. Exhaustivity checking of pattern matching is a safety belt that defends against unmatched exceptions at runtime and boosts type safety. However, the presence of language features like inheritance, typecase, traits, GADTs, path-dependent types and union types makes the checking difficult and the algorithm complex. In this paper we propose a generic algorithm that decouples the checking algorithm from specific type theories. The decoupling makes the algorithm simple and enables easy customization for specific type systems. Liu, Fengyun},
  journaltitle = {SCALA 2016 Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
  urldate = {2018-04-26},
  date = {2016},
  author = {Liu, Fengyun},
  file = {/Users/pgiarrusso/Zotero/storage/RGJLVHT7/Liu - 2016 - A generic algorithm for checking exhaustivity of pattern matching.pdf}
}

@inproceedings{Awodey2009Kripke,
  langid = {english},
  title = {Kripke Semantics for Martin-Löf’s Extensional Type Theory},
  isbn = {978-3-642-02272-2 978-3-642-02273-9},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-02273-9_19},
  doi = {10.1007/978-3-642-02273-9_19},
  abstract = {It is well-known that simple type theory is complete with respect to non-standard models. Completeness for standard models only holds when increasing the class of models, e.g., to cartesian closed categories. Similarly, dependent type theory is complete for locally cartesian closed categories. However, it is usually difficult to establish the coherence of interpretations of dependent type theory, i.e., to show that the interpretations of equal expressions are indeed equal. Several classes of models have been used to remedy this problem.We contribute to this investigation by giving a semantics that is both coherent and sufficiently general for completeness while remaining relatively easy to compute with. Our models interpret types of Martin-Löf’s extensional dependent type theory as sets indexed over posets or, equivalently, as fibrations over posets. This semantics can be seen as a generalization to dependent type theory of the interpretation of intuitionistic first-order logic in Kripke models. This yields a simple coherent model theory with respect to which simple and dependent type theory are sound and complete.},
  eventtitle = {International Conference on Typed Lambda Calculi and Applications},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-04-27},
  date = {2009-07-01},
  pages = {249-263},
  author = {Awodey, Steve and Rabe, Florian},
  file = {/Users/pgiarrusso/Zotero/storage/MMQCUW34/Awodey-Rabe - 2009 - Kripke Semantics for Martin-Löf’s Extensional Type Theory.pdf;/Users/pgiarrusso/Zotero/storage/GJSZKG3B/10.html}
}
% == BibLateX quality report for Awodey2009Kripke:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Shao2000Efficient,
  title = {Efficient and Safe-for-space Closure Conversion},
  volume = {22},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/345099.345125},
  doi = {10.1145/345099.345125},
  abstract = {Modern compilers often implement function calls (or returns) in two steps: first, a “closure” environment is properly installed to provide access for free variables in the target program fragment; second, the control is transferred to the target by a “jump with arguments (for results).” Closure conversion—which decides where and how to represent closures at runtime—is a crucial step in the compilation of functional languages. This paper presents a new algorithm that exploits the use of compile-time control and data-flow information to optimize funtion calls. By extensive closure sharing and allocation by 36\% and memory fetches for local and global variables by 43\%; and improves the already efficient code generated by an earlier version of the Standard ML of New Jersey compiler by about 17\% on a DECstation 5000. Moreover, unlike most other approaches, our new closure-allocation scheme the strong safe-for-space-complexity rule, thus achieving good asymptotic space usage.},
  number = {1},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2018-05-03},
  date = {2000-01},
  pages = {129--161},
  keywords = {compiler optimization,closure conversion,callee-save registers,closure representation,flow analysis,heap-based compilation,space safety},
  author = {Shao, Zhong and Appel, Andrew W.},
  file = {/Users/pgiarrusso/Zotero/storage/URFUSRLB/Shao-Appel - 2000 - Efficient and Safe-for-space Closure Conversion.pdf}
}
% == BibLateX quality report for Shao2000Efficient:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@inproceedings{Leijen2005Qualified,
  location = {{New York, NY, USA}},
  title = {Qualified Types for MLF},
  isbn = {978-1-59593-064-4},
  url = {http://doi.acm.org/10.1145/1086365.1086385},
  doi = {10.1145/1086365.1086385},
  abstract = {MLF is a type system that extends a functional language with impredicative rank-n polymorphism. Type inference remains possible and only in some clearly defined situations, a local type annotation is required. Qualified types are a general concept that can accommodate a wide range of type systems extension, for example, type classes in Haskell. We show how the theory of qualified types can be used seamlessly with the higher-ranked impredicative polymorphism of MLF, and give a solution to the non-trivial problem of evidence translation in the presence of impredicative datatypes.},
  booktitle = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '05},
  publisher = {{ACM}},
  urldate = {2018-05-10},
  date = {2005},
  pages = {144--155},
  keywords = {higher-rank polymorphism,qualified types,impredicativity,MLF},
  author = {Leijen, Daan and Löh, Andres},
  file = {/Users/pgiarrusso/Zotero/storage/TJB5LT7C/Leijen-Löh - 2005 - Qualified Types for MLF.pdf}
}

@inproceedings{Maher2005Herbrand,
  title = {Herbrand constraint abduction},
  doi = {10.1109/LICS.2005.21},
  abstract = {In this paper we explore abduction over the Herbrand domain - equations on the algebra of finite terms (or finite trees) - which is a central element of logic programming and first-order automated reasoning. This paper is a case study of constraint abduction in the Herbrand domain. The direct relationship between Herbrand constraint abduction and type inference outlined above should make it easy to interpret the results of this paper in the context of type inference.},
  eventtitle = {20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05)},
  booktitle = {20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05)},
  date = {2005-06},
  pages = {397-406},
  keywords = {type theory,type inference,Algebra,Concrete,process algebra,Logic programming,Equations,Australia,automated reasoning,Databases,Herbrand domain,logic programming,Query processing},
  author = {Maher, M.},
  file = {/Users/pgiarrusso/Zotero/storage/9WP3AZCR/1509245.html}
}
% == BibLateX quality report for Maher2005Herbrand:
% ? Unsure about the formatting of the booktitle

@article{Sulzmann2008Type,
  langid = {english},
  title = {Type inference for GADTs via Herbrand constraint abduction},
  url = {https://lirias.kuleuven.be/handle/123456789/161887},
  urldate = {2018-05-11},
  date = {2008-01-01},
  author = {Sulzmann, Martin and Schrijvers, Tom and Stuckey, Peter J.},
  file = {/Users/pgiarrusso/Zotero/storage/5Q5E9IQX/Sulzmann-Schrijvers-Stuckey - 2008 - Type inference for GADTs via Herbrand constraint abduction.pdf;/Users/pgiarrusso/Zotero/storage/5QBSFSRH/161887.html}
}
% == BibLateX quality report for Sulzmann2008Type:
% Missing required field 'journaltitle'

@article{Odersky2002Nominal,
  langid = {english},
  title = {A Nominal Theory of Objects with Dependent Types},
  url = {https://infoscience.epfl.ch/record/64407},
  abstract = {We design and study newObj, a calculus and dependent type system for objects and classes which can have types as members. Type members can be aliases, abstract types, or new types. The type system can model the essential concepts of Java's inner classes as well as virtual types and family polymorphism found in BETA or gbeta. It can also model most concepts of SML-style module systems, including sharing constraints and higher-order functors, but excluding applicative functors. The type system can thus be used as a basis for unifying concepts that so far existed in parallel in advanced object systems and in module systems. The technical report presents results on confluence of the calculus, soundness of the type system, and undecidability of type checking. Odersky, Martin; Cremet, Vincent; Röckl, Christine; Zenger, Matthias},
  urldate = {2018-05-31},
  date = {2002},
  keywords = {_tablet},
  author = {Odersky, Martin and Cremet, Vincent and Röckl, Christine and Zenger, Matthias},
  file = {/Users/pgiarrusso/Zotero/storage/MKH3EHVF/Odersky-Cremet-Röckl-Zenger - 2002 - A Nominal Theory of Objects with Dependent Types.pdf}
}
% == BibLateX quality report for Odersky2002Nominal:
% Missing required field 'journaltitle'

@inproceedings{Tahboub2018How,
  location = {{New York, NY, USA}},
  title = {How to Architect a Query Compiler, Revisited},
  isbn = {978-1-4503-4703-7},
  url = {http://doi.acm.org/10.1145/3183713.3196893},
  doi = {10.1145/3183713.3196893},
  abstract = {To leverage modern hardware platforms to their fullest, more and more database systems embrace compilation of query plans to native code. In the research community, there is an ongoing debate about the best way to architect such query compilers. This is perceived to be a difficult task, requiring techniques fundamentally different from traditional interpreted query execution.   We aim to contribute to this discussion by drawing attention to an old but underappreciated idea known as Futamura projections, which fundamentally link interpreters and compilers. Guided by this idea, we demonstrate that efficient query compilation can actually be very simple, using techniques that are no more difficult than writing a query interpreter in a high-level language. Moreover, we demonstrate how intricate compilation patterns that were previously used to justify multiple compiler passes can be realized in one single, straightforward, generation pass. Key examples are injection of specialized index structures, data representation changes such as string dictionaries, and various kinds of code motion to reduce the amount of work on the critical path.  We present LB2: a high-level query compiler developed in this style that performs on par with, and sometimes beats, the best compiled query engines on the standard TPC-H benchmark.},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  series = {SIGMOD '18},
  publisher = {{ACM}},
  urldate = {2018-06-02},
  date = {2018},
  pages = {307--322},
  keywords = {futamura projections,query compilation},
  author = {Tahboub, Ruby Y. and Essertel, Grégory M. and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/7I5QDLSI/Tahboub-Essertel-Rompf - 2018 - How to Architect a Query Compiler, Revisited.pdf;/Users/pgiarrusso/Zotero/storage/DUW7EJD5/Tahboub-Essertel-Rompf - 2018 - How to Architect a Query Compiler, Revisited.pdf}
}

@inproceedings{Schurmann2008Structural,
  title = {Structural Logical Relations},
  doi = {10.1109/LICS.2008.44},
  abstract = {Tait's method (a.k.a. proof by logical relations) is a powerful proof technique frequently used for showing foundational properties of languages based on typed lambda-calculi. Historically, these proofs have been extremely difficult to formalize in proof assistants with weak meta-logics, such as Twelf, and yet they are often straightforward in proof assistants with stronger meta-logics. In this paper, we propose structural logical relations as a technique for conducting these proofs in systems with limited meta-logical strength by explicitly representing and reasoning about an auxiliary logic. In support of our claims, we give a Twelf-checked proof of the completeness of an algorithm for checking equality of simply typed lambda-terms.},
  eventtitle = {2008 23rd Annual IEEE Symposium on Logic in Computer Science},
  booktitle = {2008 23rd Annual IEEE Symposium on Logic in Computer Science},
  date = {2008-06},
  pages = {69-80},
  keywords = {lambda calculus,Computer science,Logic,proof assistants,inference mechanisms,auxiliary logic,Cut-Elimination,Encoding,Logical Frameworks,Logical Relations,meta-logics,Normalization,structural logical relations,Turning,Twelf,Twelf-checked proof,typed lambda-calculi,USA Councils},
  author = {Schürmann, C. and Sarnat, J.},
  file = {/Users/pgiarrusso/Zotero/storage/JZLNZ9ZN/Schürmann-Sarnat - 2008 - Structural Logical Relations.pdf;/Users/pgiarrusso/Zotero/storage/X5DMZXEC/4557901.html}
}
% == BibLateX quality report for Schurmann2008Structural:
% ? Unsure about the formatting of the booktitle

@inproceedings{Homer2012Patterns,
  location = {{New York, NY, USA}},
  title = {Patterns as objects in Grace},
  isbn = {978-1-4503-1564-7},
  url = {http://doi.acm.org/10.1145/2384577.2384581},
  doi = {10.1145/2384577.2384581},
  abstract = {Object orientation and pattern matching are often seen as conflicting approaches to program design. Object-oriented programs place type-dependent behavior inside objects and invoke it via dynamic dispatch, while pattern-matching programs place type-dependent behavior outside data structures and invoke it via multiway conditionals (case statements). Grace is a new, dynamic, object-oriented language designed to support teaching: to this end, Grace needs to support both styles. We explain how this conflict can be resolved gracefully: by modelling patterns and cases as partial functions, reifying those functions as objects, and then building up complex patterns from simpler ones using pattern combinators. We describe the implementation of this design as an object-oriented framework, and a case study of its effectiveness.},
  booktitle = {Proceedings of the 8th Symposium on Dynamic Languages},
  series = {DLS '12},
  publisher = {{ACM}},
  urldate = {2018-06-10},
  date = {2012},
  pages = {17--28},
  keywords = {pattern matching,education,grace,minigrace,object orientation},
  author = {Homer, Michael and Noble, James and Bruce, Kim B. and Black, Andrew P. and Pearce, David J.},
  file = {/Users/pgiarrusso/Zotero/storage/LWR48VKK/Homer-Noble-Bruce-Black-Pearce - 2012 - Patterns As Objects in Grace.pdf}
}

@inproceedings{Syme2007Extensible,
  location = {{New York, NY, USA}},
  title = {Extensible pattern matching via a lightweight language extension},
  isbn = {978-1-59593-815-2},
  url = {http://doi.acm.org/10.1145/1291151.1291159},
  doi = {10.1145/1291151.1291159},
  abstract = {Pattern matching of algebraic data types (ADTs) is a standard feature in typed functional programming languages, but it is well known that it interacts poorly with abstraction. While several partial solutions to this problem have been proposed, few have been implemented or used. This paper describes an extension to the .NET language F\# called active patterns, which supports pattern matching over abstract representations of generic heterogeneous data such as XML and term structures, including where these are represented via object models in other .NET languages. Our design is the first to incorporate both ad hoc pattern matching functions for partial decompositions and "views" for total decompositions, and yet remains a simple and lightweight extension. We give a description of the language extension along with numerous motivating examples. Finally we describe how this feature would interact with other reasonable and related language extensions: existential types quantified at data discrimination tags, GADTs, and monadic generalizations of pattern matching.},
  booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '07},
  publisher = {{ACM}},
  urldate = {2018-06-10},
  date = {2007},
  pages = {29--40},
  keywords = {functional programming,pattern matching,F\#,ML},
  author = {Syme, Don and Neverov, Gregory and Margetson, James},
  file = {/Users/pgiarrusso/Zotero/storage/S3S4MDUI/Syme-Neverov-Margetson - 2007 - Extensible Pattern Matching via a Lightweight Language Extension.pdf}
}

@inproceedings{Blume2006Extensible,
  location = {{New York, NY, USA}},
  title = {Extensible programming with first-class cases},
  isbn = {978-1-59593-309-6},
  url = {http://doi.acm.org/10.1145/1159803.1159836},
  doi = {10.1145/1159803.1159836},
  abstract = {We present language mechanisms for polymorphic, extensible records and their exact dual, polymorphic sums with extensible first-class cases. These features make it possible to easily extend existing code with new cases. In fact, such extensions do not require any changes to code that adheres to a particular programming style. Using that style, individual extensions can be written independently and later be composed to form larger components. These language mechanisms provide a solution to the expression problem.We study the proposed mechanisms in the context of an implicitly typed, purely functional language PolyR. We give a type system for the language and provide rules for a 2-phase transformation: first into an explicitly typed λ-calculus with record polymorphism, and finally to efficient index-passing code. The first phase eliminates sums and cases by taking advantage of the duality with records.We implement a version of PolyR extended with imperative features and pattern matching - we call this language MLPolyR. Programs in MLPolyR require no type annotations - the implementation employs a reconstruction algorithm to infer all types. The compiler generates machine code (currently for PowerPC) and optimizes the representation of sums by eliminating closures generated by the dual construction.},
  booktitle = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '06},
  publisher = {{ACM}},
  urldate = {2018-06-10},
  date = {2006},
  pages = {239--250},
  keywords = {sums,duality,first-class cases,records},
  author = {Blume, Matthias and Acar, Umut A. and Chae, Wonseok},
  file = {/Users/pgiarrusso/Zotero/storage/ZZ42QB9Z/Blume-Acar-Chae - 2006 - Extensible Programming with First-class Cases.pdf}
}

@inproceedings{Bloom2009Thorn,
  location = {{New York, NY, USA}},
  title = {Thorn: Robust, concurrent, extensible scripting on the JVM},
  isbn = {978-1-60558-766-0},
  url = {http://doi.acm.org/10.1145/1640089.1640098},
  doi = {10.1145/1640089.1640098},
  shorttitle = {Thorn},
  abstract = {Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.},
  booktitle = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications},
  series = {OOPSLA '09},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {2009},
  pages = {117--136},
  keywords = {pattern matching,actors,scripting},
  author = {Bloom, Bard and Field, John and Nystrom, Nathaniel and Östlund, Johan and Richards, Gregor and Strniša, Rok and Vitek, Jan and Wrigstad, Tobias},
  file = {/Users/pgiarrusso/Zotero/storage/4SQ2MIAL/Bloom-Field-Nystrom-Östlund-Richards-Strniša-Vitek-Wrigstad - 2009 - Thorn - Robust, Concurrent, Extensible Scripting on the JVM.pdf}
}

@incollection{Haller2008Implementing,
  langid = {english},
  title = {Implementing joins using extensible pattern matching},
  isbn = {978-3-540-68264-6 978-3-540-68265-3},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-68265-3_9},
  abstract = {Join patterns are an attractive declarative way to synchronize both threads and asynchronous distributed computations. We explore joins in the context of extensible pattern matching that recently appeared in languages such as F\# and Scala. Our implementation supports join patterns with multiple synchronous events, and guards. Furthermore, we integrated joins into an existing actor-based concurrency framework. It enables join patterns to be used in the context of more advanced synchronization modes, such as future-type message sending and token-passing continuations.},
  booktitle = {Coordination Models and Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-06-11},
  date = {2008-06-04},
  pages = {135-152},
  author = {Haller, Philipp and Cutsem, Tom Van},
  file = {/Users/pgiarrusso/Zotero/storage/YE6P82WD/Haller-Cutsem - 2008 - Implementing Joins Using Extensible Pattern Matching.pdf;/Users/pgiarrusso/Zotero/storage/SF4STPKX/978-3-540-68265-3_9.html},
  doi = {10.1007/978-3-540-68265-3_9}
}
% == BibLateX quality report for Haller2008Implementing:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@inproceedings{Richard2007OOMatch,
  location = {{New York, NY, USA}},
  title = {OOMatch: Pattern matching as dispatch in Java},
  isbn = {978-1-59593-865-7},
  url = {http://doi.acm.org/10.1145/1297846.1297880},
  doi = {10.1145/1297846.1297880},
  shorttitle = {OOMatch},
  abstract = {We present an extension to Java, dubbed OOMatch. It allows method parameters to be specified as patterns, which are matched against the arguments to the method call. When matches occur, the method applies; if multiple methods apply, the method with the more specific pattern overrides the others.},
  booktitle = {Companion to the 22Nd ACM SIGPLAN Conference on Object-oriented Programming Systems and Applications Companion},
  series = {OOPSLA '07},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {2007},
  pages = {771--772},
  keywords = {pattern matching,dynamic dispatch,Java,multimethods,predicate dispatch},
  author = {Richard, Adam and Lhotak, Ondrej},
  file = {/Users/pgiarrusso/Zotero/storage/W7MKG9TN/Richard-Lhotak - 2007 - OOMatch - Pattern Matching As Dispatch in Java.pdf}
}

@article{Millstein2009Expressive,
  title = {Expressive and modular predicate dispatch for Java},
  volume = {31},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/1462166.1462168},
  doi = {10.1145/1462166.1462168},
  abstract = {Predicate dispatch is an object-oriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functional-style pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit the practical utility of this language feature. We describe JPred, a backward-compatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included special-purpose algorithms for reasoning about predicates, JPred employs general-purpose, off-the-shelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language informally, we present an extension to Featherweight Java that formalizes the language and its modular type system, which we have proven sound. Finally, we discuss two case studies that illustrate the practical utility of JPred, including its use in the detection of several errors.},
  number = {2},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2018-06-11},
  date = {2009-02},
  pages = {7:1--7:54},
  keywords = {modular typechecking,dynamic dispatch,Predicate dispatch},
  author = {Millstein, Todd and Frost, Christopher and Ryder, Jason and Warth, Alessandro},
  file = {/Users/pgiarrusso/Zotero/storage/DR7B6T3P/Millstein-Frost-Ryder-Warth - 2009 - Expressive and Modular Predicate Dispatch for Java.pdf}
}
% == BibLateX quality report for Millstein2009Expressive:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@inproceedings{Warth2007OMeta,
  location = {{New York, NY, USA}},
  title = {OMeta: An object-oriented language for pattern matching},
  isbn = {978-1-59593-868-8},
  url = {http://doi.acm.org/10.1145/1297081.1297086},
  doi = {10.1145/1297081.1297086},
  shorttitle = {OMeta},
  abstract = {This paper introduces OMeta, a new object-oriented language for pattern matching. OMeta is based on a variant of Parsing Expression Grammars (PEGs) [5]---a recognition-based foundation for describing syntax---which we have extended to handle arbitrary kinds of data. We show that OMeta's general-purpose pattern matching provides a natural and convenient way for programmers to implement tokenizers, parsers, visitors, and tree transformers, all of which can be extended in interesting ways using familiar object-oriented mechanisms. This makes OMeta particularly well-suited as a medium for experimenting with new designs for programming languages and extensions to existing languages.},
  booktitle = {Proceedings of the 2007 Symposium on Dynamic Languages},
  series = {DLS '07},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {2007},
  pages = {11--19},
  keywords = {pattern matching,parsing,metacircular implementation},
  author = {Warth, Alessandro and Piumarta, Ian},
  file = {/Users/pgiarrusso/Zotero/storage/BGQIFQBD/Warth-Piumarta - 2007 - OMeta - An Object-oriented Language for Pattern Matching.pdf}
}

@inproceedings{Wadler1987Views,
  location = {{New York, NY, USA}},
  title = {Views: A way for pattern matching to cohabit with data abstraction},
  isbn = {978-0-89791-215-0},
  url = {http://doi.acm.org/10.1145/41625.41653},
  doi = {10.1145/41625.41653},
  shorttitle = {Views},
  abstract = {Pattern matching and data abstraction are important concepts in designing programs, but they do not fit well together. Pattern matching depends on making public a free data type representation, while data abstraction depends on hiding the representation. This paper proposes the views mechanism as a means of reconciling this conflict. A view allows any type to be viewed as a free data type, thus combining the clarity of pattern matching with the efficiency of data abstraction.},
  booktitle = {Proceedings of the 14th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '87},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {1987},
  pages = {307--313},
  author = {Wadler, P.},
  file = {/Users/pgiarrusso/Zotero/storage/J9J6P94R/Wadler - 1987 - Views - A Way for Pattern Matching to Cohabit with Data Abstraction.pdf}
}

@inproceedings{Ryu2010Adding,
  title = {Adding pattern matching to existing object-oriented languages},
  abstract = {While object-oriented languages are designed for information hiding and data encapsulation, intensive data manipulation often calls for pattern matching, one of the main features of functional programming languages. Pattern matching provides a concise way to describe specific structures or conditions of objects so that programmers can clearly identify and easily access the corresponding objects. The need for both object information hiding and pattern matching over objects, which are seemingly conflicting features, shows up very well in the problem of language manipulation. In the compiler development of Fortress, a new object-oriented language with extensive support for functional programming, we originally implemented the Fortress type checker in Java. However, because Java does not provide pattern matching, we reimplemented the type checker in Scala. Compared to the pattern matching mechanism in Scala, Java’s visitor patterns are verbose, make it hard to capture the high-level specification of objects, and are nontrivial to maintain. In this paper, we introduce a new pattern-matching mechanism which can be added to existing object-oriented languages. We present the mechanism currently being implemented as an addition to the Fortress programming language. The mechanism provides a concise way of describing and using patterns in the presence of runtime manipulation of types using typecase and multimethods, unlike Scala which has type-erasure semantics. Even though it is premature to discuss the performance and scalability of the presented mechanism, we believe that the same optimization techniques used in Scala will apply to our mechanism.},
  booktitle = {FOOL},
  date = {2010},
  author = {Ryu, Sukyoung and Park, Changhee and Steele, Guy L.},
  file = {/Users/pgiarrusso/Zotero/storage/IQTBNH2N/Ryu-Park-Steele - Adding Pattern Matching to Existing Object-Oriented Languages.pdf;/Users/pgiarrusso/Zotero/storage/Y7USUYK5/summary.html}
}
% == BibLateX quality report for Ryu2010Adding:
% ? Unsure about the formatting of the booktitle

@inproceedings{Pickering2016Pattern,
  location = {{New York, NY, USA}},
  title = {Pattern synonyms},
  isbn = {978-1-4503-4434-0},
  url = {http://doi.acm.org/10.1145/2976002.2976013},
  doi = {10.1145/2976002.2976013},
  abstract = {Pattern matching has proven to be a convenient, expressive way of inspecting data. Yet this language feature, in its traditional form, is limited: patterns must be data constructors of concrete data types. No computation or abstraction is allowed. The data type in question must be concrete, with no ability to enforce any invariants. Any change in this data type requires all clients to update their code.   This paper introduces pattern synonyms, which allow programmers to abstract over patterns, painting over all the shortcomings listed above. Pattern synonyms are assigned types, enabling a compiler to check the validity of a synonym independent of its definition. These types are intricate; detailing how to assign a type to a pattern synonym is a key contribution of this work. We have implemented pattern synonyms in the Glasgow Haskell Compiler, where they have enjoyed immediate popularity, but we believe this feature could easily be exported to other languages that support pattern matching.},
  booktitle = {Proceedings of the 9th International Symposium on Haskell},
  series = {Haskell 2016},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {2016},
  pages = {80--91},
  keywords = {functional programming,Haskell,pattern matching},
  author = {Pickering, Matthew and Érdi, Gergő and Peyton Jones, Simon and Eisenberg, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/MAFPS7G6/Pickering-Érdi-Peyton Jones-Eisenberg - 2016 - Pattern Synonyms.pdf}
}

@inproceedings{Isradisaikul2013Reconciling,
  location = {{New York, NY, USA}},
  title = {Reconciling exhaustive pattern matching with objects},
  isbn = {978-1-4503-2014-6},
  url = {http://doi.acm.org/10.1145/2491956.2462194},
  doi = {10.1145/2491956.2462194},
  abstract = {Pattern matching, an important feature of functional languages, is in conflict with data abstraction and extensibility, which are central to object-oriented languages. Modal abstraction offers an integration of deep pattern matching and convenient iteration abstractions into an object-oriented setting; however, because of data abstraction, it is challenging for a compiler to statically verify properties such as exhaustiveness. In this work, we extend modal abstraction in the JMatch language to support static, modular reasoning about exhaustiveness and redundancy. New matching specifications allow these properties to be checked using an SMT solver. We also introduce expressive pattern-matching constructs. Our evaluation shows that these new features enable more concise code and that the performance of checking exhaustiveness and redundancy is acceptable.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '13},
  publisher = {{ACM}},
  urldate = {2018-06-11},
  date = {2013},
  pages = {343--354},
  keywords = {pattern matching,java,subtyping,data abstraction,equality constructor,exhaustiveness,jmatch,matching specification,modal abstraction,named constructor,redundancy},
  author = {Isradisaikul, Chinawat and Myers, Andrew C.},
  file = {/Users/pgiarrusso/Zotero/storage/FGBDGLWW/Isradisaikul-Myers - 2013 - Reconciling Exhaustive Pattern Matching with Objects.pdf}
}

@article{Emir2007Objectoriented,
  langid = {english},
  title = {Object-oriented pattern matching},
  url = {https://infoscience.epfl.ch/record/109881},
  doi = {10.5075/epfl-thesis-3899},
  abstract = {Pattern matching is a programming language construct considered essential in functional programming. Its purpose is to inspect and decompose data. Instead, object-oriented programming languages do not have a dedicated construct for this purpose. A possible reason for this is that pattern matching is useful when data is defined separately from operations on the data – a scenario that clashes with the object-oriented motto of grouping data and operations. However, programmers are frequently confronted with situations where there is no alternative to expressing data and operations separately – because most data is neither stored in nor does it originate from an object-oriented context. Consequently, object-oriented programmers, too, are in need for elegant and concise solutions to the problem of decomposing data. To this end, we propose a built-in pattern matching construct compatible with object-oriented programming. We claim that it leads to more concise and readable code than standard object-oriented approaches. A pattern in our approach is any computable way of testing and deconstructing an object and binding relevant parts to local names. We introduce pattern matching in two variants, case classes and extractors. We compare the readability, extensibility and performance of built-in pattern matching in these two variants with standard decomposition techniques. It turns out that standard object-oriented approaches to decomposing data are not extensible. Case classes, which have been studied before, require a low notational overhead, but expose their representation, making them hard to change later. The novel extractor mechanism offers loose coupling and extensibility, but comes with a performance overhead. We present a formalization of object-oriented pattern matching with extractors. This is done by giving definitions and proving standard properties for a calculus that provides pattern matching as described before. We then give a formal, optimizing translation from the calculus including pattern matching to its fragment without pattern matching, and prove it correct. Finally, we consider non-obvious interactions between the pattern matching and parametric polymorphism. We review the technique of generalized algebraic data types from functional programming, and show how it can be carried over to the object-oriented style. The main tool is the extension of the type system with subtype constraints, which leads to a very expressive metatheory. Through this theory, we are able to express patterns that operate on existentially quantified types purely by universally quantified extractors. Emir, Burak},
  urldate = {2018-06-11},
  date = {2007},
  author = {Emir, Burak},
  file = {/Users/pgiarrusso/Zotero/storage/5IRIR93K/Emir - 2007 - Object-oriented pattern matching.pdf}
}
% == BibLateX quality report for Emir2007Objectoriented:
% Missing required field 'journaltitle'

@article{Volanschi2012Pattern,
  title = {Pattern matching for the masses using custom notations},
  volume = {77},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642311002243},
  doi = {10.1016/j.scico.2011.12.002},
  abstract = {For many programmers, the notion of “pattern matching” evokes nothing more than regular expressions for matching unstructured text, or technologies such as XPath to match semi-structured data in XML. This common perception of pattern matching is partly due to the success of regular expressions and XPath, which are supported in many popular programming languages today, either as standard libraries or as part of the language. But it is also due to the fact that many programmers never used another elegant form of pattern matching—on structured data, i.e., the native data structures of a programming language. This form of matching is common in functional or logic languages used in the research, but unfortunately much less used in the software industry. It is indeed very surprising that none of the popular languages in use today support, in their standard form, a nearly general form of structured data matching, decades after this technology has been discovered and continuously improved. This paper shows that programmers do not have to wait for next generation languages to integrate pattern matching, neither need they use non-standard pre-processors, thereby losing some advantages that are most important in an industrial setting: official support, compatibility, standardization, etc. Instead, pattern matching of native data in custom notations can be implemented as a minimalist library in popular object languages. Thus, some of the comfortable existing notations from logic languages can be reused, existing standard notations for structured data such as JSON (JavaScript Object Notation) can be smoothly extended to support pattern matching, and new notations can be designed. As in most library implementations of regular expressions, custom notation patterns are simply represented as strings. They can be used in two different modes: interpreted and compiled. This paper presents two open-source implementations of custom matching notations, for Java and JavaScript, exhibiting a reasonable overhead compared to other forms of pattern matching.},
  number = {5},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  urldate = {2018-06-11},
  date = {2012-05-01},
  pages = {609-635},
  keywords = {Customization,Data notations,Pattern matching},
  author = {Volanschi, Nic},
  file = {/Users/pgiarrusso/Zotero/storage/3YSD4IHQ/Volanschi - 2012 - Pattern matching for the masses using custom notations.pdf;/Users/pgiarrusso/Zotero/storage/HCI7HN6C/S0167642311002243.html}
}

@incollection{Hirzel2008Matchete,
  langid = {english},
  title = {Matchete: paths through the pattern matching jungle},
  isbn = {978-3-540-77441-9 978-3-540-77442-6},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-77442-6_11},
  shorttitle = {Matchete},
  abstract = {Pattern matching is a programming language feature for selecting a handler based on the structure of data while binding names to sub-structures. By combining selection and binding, pattern matching facilitates many common tasks such as date normalization, red-black tree manipulation, conversion of XML documents, or decoding TCP/IP packets. Matchete is a language extension to Java that unifies different approaches to pattern matching: regular expressions, structured term patterns, XPath, and bit-level patterns. Matchete naturally allows nesting of these different patterns to form composite patterns. We present the Matchete syntax and describe a prototype implementation.},
  booktitle = {Practical Aspects of Declarative Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-06-11},
  date = {2008-01-07},
  pages = {150-166},
  author = {Hirzel, Martin and Nystrom, Nathaniel and Bloom, Bard and Vitek, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/5H7ZBLZ8/Hirzel-Nystrom-Bloom-Vitek - 2008 - Matchete - Paths through the Pattern Matching Jungle.pdf;/Users/pgiarrusso/Zotero/storage/3UMDY3MF/978-3-540-77442-6_11.html},
  doi = {10.1007/978-3-540-77442-6_11}
}
% == BibLateX quality report for Hirzel2008Matchete:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@inproceedings{Monin2010Proof,
  location = {{Edinburgh, United Kingdom}},
  title = {Proof Trick: Small Inversions},
  url = {https://hal.inria.fr/inria-00489412},
  shorttitle = {Proof Trick},
  abstract = {We show how an inductive hypothesis can be inverted with small proof terms, using just dependent elimination with a diagonal predicate. The technique works without any auxiliary type such as True, False, eq. It can also be used to discriminate, in some sense, the constructors of an inductive type of sort Prop in Coq.},
  booktitle = {Second Coq Workshop},
  publisher = {{Yves Bertot}},
  urldate = {2018-06-12},
  date = {2010-07},
  author = {Monin, Jean-François},
  editor = {Bertot, Yves},
  file = {/Users/pgiarrusso/Zotero/storage/ESWNDV2W/Monin - 2010 - Proof Trick - Small Inversions.pdf}
}
% == BibLateX quality report for Monin2010Proof:
% ? Unsure about the formatting of the booktitle

@article{Eisenberg2018Type,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.03476},
  primaryClass = {cs},
  title = {Type variables in patterns},
  url = {http://arxiv.org/abs/1806.03476},
  abstract = {For many years, GHC has implemented an extension to Haskell that allows type variables to be bound in type signatures and patterns, and to scope over terms. This extension was never properly specified. We rectify that oversight here. With the formal specification in hand, the otherwise-labyrinthine path toward a design for binding type variables in patterns becomes blindingly clear. We thus extend ScopedTypeVariables to bind type variables explicitly, obviating the Proxy workaround to the dustbin of history.},
  urldate = {2018-06-13},
  date = {2018-06-09},
  keywords = {Computer Science - Programming Languages},
  author = {Eisenberg, Richard A. and Breitner, Joachim and Jones, Simon Peyton},
  file = {/Users/pgiarrusso/Zotero/storage/7JWSA98W/Eisenberg-Breitner-Jones - 2018 - Type variables in patterns.pdf;/Users/pgiarrusso/Zotero/storage/5AP68DY4/1806.html;/Users/pgiarrusso/Zotero/storage/B3I8MQJS/1806.html}
}
% == BibLateX quality report for Eisenberg2018Type:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Dreyer2007Type,
  location = {{New York, NY, USA}},
  title = {A Type System for Recursive Modules},
  isbn = {978-1-59593-815-2},
  url = {http://doi.acm.org/10.1145/1291151.1291196},
  doi = {10.1145/1291151.1291196},
  abstract = {There has been much work in recent years on extending ML with recursive modules. One of the most difficult problems in the development of such an extension is the double vision problem, which concerns the interaction of recursion and data abstraction. In previous work, I defined a type system called RTG, which solves the double vision problem at the level of a System-F-style core calculus. In this paper, I scale the ideas and techniques of RTG to the level of a recursive ML-style module calculus called RMC, thus establishing that no tradeoff between data abstraction and recursive modules is necessary. First, I describe RMC's typing rules for recursive modules informally and discuss some of the design questions that arose in developing them. Then, I present the formal semantics of RMC, which is interesting in its own right. The formalization synthesizes aspects of both the Definition and the Harper-Stone interpretation of Standard ML, and includes a novel two-pass algorithm for recursive module typechecking in which the coherence of the two passes is emphasized by their representation in terms of the same set of inference rules.},
  booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '07},
  publisher = {{ACM}},
  urldate = {2018-06-13},
  date = {2007},
  pages = {289--302},
  keywords = {recursion,abstract data types,type systems,modules},
  author = {Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/QL26B9SJ/Dreyer - 2007 - A Type System for Recursive Modules.pdf}
}

@inproceedings{Nakata2006Recursive,
  location = {{New York, NY, USA}},
  title = {Recursive Modules for Programming},
  isbn = {978-1-59593-309-6},
  url = {http://doi.acm.org/10.1145/1159803.1159813},
  doi = {10.1145/1159803.1159813},
  abstract = {TheML module system is useful for building large-scale programs. The programmer can factor programs into nested and parameterized modules, and can control abstraction with signatures. Yet ML prohibits recursion between modules. As a result of this constraint, the programmer may have to consolidate conceptually separate components into a single module, intruding on modular programming. Introducing recursive modules is a natural way out of this predicament. Existing proposals, however, vary in expressiveness and verbosity. In this paper, we propose a type system for recursive modules, which can infer their signatures. Opaque signatures can also be given explicitly, to provide type abstraction either inside or outside the recursion. The type system is decidable, and is sound for a call-by-value semantics. We also present a solution to the expression problem, in support of our design choices.},
  booktitle = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '06},
  publisher = {{ACM}},
  urldate = {2018-06-13},
  date = {2006},
  pages = {74--86},
  keywords = {type systems,type inference,recursive modules,the expression problem,applicative functors},
  author = {Nakata, Keiko and Garrigue, Jacques},
  file = {/Users/pgiarrusso/Zotero/storage/ZRYVR9Z8/Nakata-Garrigue - 2006 - Recursive Modules for Programming.pdf}
}

@incollection{Kuan2009Engineering,
  langid = {english},
  title = {Engineering Higher-Order Modules in SML/NJ},
  isbn = {978-3-642-16477-4 978-3-642-16478-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-16478-1_13},
  abstract = {SML/NJ and other Standard ML variants extend the ML module system with higher-order functors, elevating the module language to a full functional language. In this paper, we describe the implementation of higher-order modules in SML/NJ, which is unique in providing “true” higher-order static behavior. This implementation is based on three key ideas: unique internal variables (entity variables) for naming static entities, factorization of the static information in both basic modules and functors into signatures and realizations, and representing the static “effects” and type-level mapping performed by a functor using a static lambda calculus (the entity calculus). This design conforms to MacQueen-Tofte’s re-elaboration semantics without having to re-elaborate functor bodies at functor applications.},
  booktitle = {Implementation and Application of Functional Languages},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-06-13},
  date = {2009-09-23},
  pages = {218-235},
  author = {Kuan, George and MacQueen, David},
  file = {/Users/pgiarrusso/Zotero/storage/9EHY5XWZ/Kuan-MacQueen - 2009 - Engineering Higher-Order Modules in SML-NJ.pdf;/Users/pgiarrusso/Zotero/storage/GBTM5GKR/978-3-642-16478-1_13.html},
  doi = {10.1007/978-3-642-16478-1_13}
}
% == BibLateX quality report for Kuan2009Engineering:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@inproceedings{Dreyer2004Type,
  location = {{New York, NY, USA}},
  title = {A Type System for Well-founded Recursion},
  isbn = {978-1-58113-729-3},
  url = {http://doi.acm.org/10.1145/964001.964026},
  doi = {10.1145/964001.964026},
  abstract = {In the interest of designing a recursive module extension to ML that is as simple and general as possible, we propose a novel type system for general recursion over effectful expressions. The presence of effects seems to necessitate a backpatching semantics for recursion similar to that of Scheme. Our type system ensures statically that recursion is well-founded---that the body of a recursive expression will evaluate without attempting to access the undefined recursive variable---which avoids some unnecessary run-time costs associated with backpatching. To ensure well-founded recursion in the presence of multiple recursive variables and separate compilation, we track the usage of individual recursive variables, represented statically by "names". So that our type system may eventually be integrated smoothly into ML's, reasoning involving names is only required inside code that uses our recursive construct and need not infect existing ML code, although instrumentation of some existing code can help to improve the precision of our type system.},
  booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '04},
  publisher = {{ACM}},
  urldate = {2018-06-13},
  date = {2004},
  pages = {293--305},
  keywords = {recursion,recursive modules,effect systems,yype systems},
  author = {Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/KGLME73J/Dreyer - 2004 - A Type System for Well-founded Recursion.pdf}
}

@inproceedings{Dreyer2005Recursive,
  location = {{New York, NY, USA}},
  title = {Recursive Type Generativity},
  isbn = {978-1-59593-064-4},
  url = {http://doi.acm.org/10.1145/1086365.1086372},
  doi = {10.1145/1086365.1086372},
  abstract = {Existential types provide a simple and elegant foundation for understanding generative abstract data types, of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so.We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first "forward-declare" them by generating their names, and then define each one secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among "transparent" type definitions.In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a destination-passing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.},
  booktitle = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP '05},
  publisher = {{ACM}},
  urldate = {2018-06-13},
  date = {2005},
  pages = {41--53},
  keywords = {recursion,abstract data types,type systems,generativity,recursive modules,effect systems},
  author = {Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/HQJSZDEC/Dreyer - 2005 - Recursive Type Generativity.pdf}
}

@article{Dreyer2007Recursive,
  langid = {english},
  title = {Recursive type generativity},
  volume = {17},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/recursive-type-generativity/897191BEE5243EEE18FE5738728350D3},
  doi = {10.1017/S0956796807006429},
  abstract = {Existential types provide a simple and elegant foundation for understanding generative abstract data types of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so. We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first ‘forward-declare’ them by generating their names, and then supply each one's identity secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among ‘transparent’ type definitions. In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a destination-passing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.},
  number = {4-5},
  journaltitle = {Journal of Functional Programming},
  urldate = {2018-06-13},
  date = {2007-07},
  pages = {433-471},
  author = {Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/RNQBTZX3/Dreyer - 2007 - Recursive type generativity.pdf;/Users/pgiarrusso/Zotero/storage/4GBAAX29/897191BEE5243EEE18FE5738728350D3.html}
}
% == BibLateX quality report for Dreyer2007Recursive:
% 'issn': not a valid ISSN

@inproceedings{Harper1990Higherorder,
  location = {{New York, NY, USA}},
  title = {Higher-order Modules and the Phase Distinction},
  isbn = {978-0-89791-343-0},
  url = {http://doi.acm.org/10.1145/96709.96744},
  doi = {10.1145/96709.96744},
  abstract = {In earlier work, we used a typed function calculus, XML, with dependent types to analyze several aspects of the Standard ML type system. In this paper, we introduce a refinement of XML with a clear compile-time/run-time phase distinction, and a direct compile-time type checking algorithm. The calculus uses a finer separation of types into universes than XML and enforces the phase distinction using a nonstandard equational theory for module and signature expressions. While unusual from a type-theoretic point of view, the nonstandard equational theory arises naturally from the well-known Grothendieck construction on an indexed category.},
  booktitle = {Proceedings of the 17th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '90},
  publisher = {{ACM}},
  urldate = {2018-06-13},
  date = {1990},
  pages = {341--354},
  author = {Harper, Robert and Mitchell, John C. and Moggi, Eugenio},
  file = {/Users/pgiarrusso/Zotero/storage/GBES7XYS/Harper-Mitchell-Moggi - 1990 - Higher-order Modules and the Phase Distinction.pdf}
}

@article{Harper1993Type,
  title = {On the Type Structure of Standard ML},
  volume = {15},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/169701.169696},
  doi = {10.1145/169701.169696},
  abstract = {Standard ML is a useful programming language with a polymorphic type system and a flexible module facility. One notable feature of the core expression language of ML is that it is implicitly typed: no explicit type information need be supplied by the programmer. In contrast, the module language of ML is explicitly typed; in particular, the types of parameters in parametric modules must be supplied by the programmer. We study the type structure of Standard ML by giving an explicitly-typed, polymorphic function calculus that captures many of the essential aspects of both the core and module language. In this setting, implicitly-typed core language expressions are regarded as a convenient short-hand for an explicitly-typed counterpart in our function calculus. In contrast to the Girard-Reynolds polymorphic calculus, our function calculus is predicative: the type system may be built up by induction on type levels. We show that, in a precise sense, the language becomes inconsistent if restrictions imposed by type levels are relaxed. More specifically, we prove that the important programming features of ML cannot be added to any impredicative language, such as the Girard-Reynolds calculus, without implicitly assuming a type of all types.},
  number = {2},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2018-06-13},
  date = {1993-04},
  pages = {211--252},
  author = {Harper, Robert and Mitchell, John C.},
  file = {/Users/pgiarrusso/Zotero/storage/YFBB9Q3D/Harper-Mitchell - 1993 - On the Type Structure of Standard ML.pdf}
}
% == BibLateX quality report for Harper1993Type:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@inproceedings{MacQueen1986Using,
  location = {{New York, NY, USA}},
  title = {Using Dependent Types to Express Modular Structure},
  url = {http://doi.acm.org/10.1145/512644.512670},
  doi = {10.1145/512644.512670},
  abstract = {Writing any large program poses difficult problems of organization. In many modern programming languages these problems are addressed by special linguistic constructs, variously known as modules, packages, or clusters, which provide for partitioning programs into manageable components and for securely combining these components to form complete programs. Some general purpose components are able to take on a life of their own, being separately compiled and stored in libraries of generic, reusable program units. Usually modularity constructs also support some form of information hiding, such as "abstract data types." "Programming in the large" is concerned with using such constructs to impose structure on large programs, in contrast to "programming in the small", which deals with the detailed implementation of algorithms in terms of data structures and control constructs. Our goal here is to examine some of the proposed linguistic notions with respect to how they meet the pragmatic requirements of programming in the large.},
  booktitle = {Proceedings of the 13th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL '86},
  publisher = {{ACM}},
  urldate = {2018-06-14},
  date = {1986},
  pages = {277--286},
  author = {MacQueen, David B.},
  file = {/Users/pgiarrusso/Zotero/storage/QM5SZEYP/MacQueen - 1986 - Using Dependent Types to Express Modular Structure.pdf}
}

@inproceedings{Serrano2018Guarded,
  location = {{New York, NY, USA}},
  title = {Guarded Impredicative Polymorphism},
  isbn = {978-1-4503-5698-5},
  url = {http://doi.acm.org/10.1145/3192366.3192389},
  doi = {10.1145/3192366.3192389},
  abstract = {The design space for type systems that support impredicative instantiation is extremely complicated. One needs to strike a balance between expressiveness, simplicity for both the end programmer and the type system implementor, and how easily the system can be integrated with other advanced type system concepts. In this paper, we propose a new point in the design space, which we call guarded impredicativity. Its key idea is that impredicative instantiation in an application is allowed for type variables that occur under a type constructor. The resulting type system has a clean declarative specification — making it easy for programmers to predict what will type and what will not —, allows for a smooth integration with GHC’s OutsideIn(X) constraint solving framework, while giving up very little in terms of expressiveness compared to systems like HMF, HML, FPH and MLF. We give a sound and complete inference algorithm, and prove a principal type property for our system.},
  booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI 2018},
  publisher = {{ACM}},
  urldate = {2018-06-16},
  date = {2018},
  pages = {783--796},
  keywords = {impredicative polymorphism,constraint-based inference,Type systems},
  author = {Serrano, Alejandro and Hage, Jurriaan and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  file = {/Users/pgiarrusso/Zotero/storage/74326XTF/Serrano-Hage-Vytiniotis-Peyton Jones - 2018 - Guarded Impredicative Polymorphism.pdf}
}

@incollection{Hughes1996Type,
  langid = {english},
  title = {Type specialisation for the λ-calculus; or, a new paradigm for partial evaluation based on type inference},
  isbn = {978-3-540-61580-4 978-3-540-70589-5},
  url = {https://link.springer.com/chapter/10.1007/3-540-61580-6_10},
  booktitle = {Partial Evaluation},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-06-16},
  date = {1996},
  pages = {183-215},
  author = {Hughes, John},
  file = {/Users/pgiarrusso/Zotero/storage/9M9CTSF9/Hughes - 1996 - Type specialisation for the λ-calculus\; or, a new paradigm for partial evaluation based on type inference.pdf;/Users/pgiarrusso/Zotero/storage/95UD2WVM/10.html},
  doi = {10.1007/3-540-61580-6_10}
}
% == BibLateX quality report for Hughes1996Type:
% Missing required field 'editor'
% 'isbn': not a valid ISBN

@article{Casinghino2012StepIndexed,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1202.2918},
  title = {Step-Indexed Normalization for a Language with General Recursion},
  volume = {76},
  issn = {2075-2180},
  url = {http://arxiv.org/abs/1202.2918},
  doi = {10.4204/EPTCS.76.4},
  abstract = {The Trellys project has produced several designs for practical dependently typed languages. These languages are broken into two fragments-a\_logical\_fragment where every term normalizes and which is consistent when interpreted as a logic, and a\_programmatic\_fragment with general recursion and other convenient but unsound features. In this paper, we present a small example language in this style. Our design allows the programmer to explicitly mention and pass information between the two fragments. We show that this feature substantially complicates the metatheory and present a new technique, combining the traditional Girard-Tait method with step-indexed logical relations, which we use to show normalization for the logical fragment.},
  journaltitle = {Electronic Proceedings in Theoretical Computer Science},
  urldate = {2018-06-19},
  date = {2012-02-11},
  pages = {25-39},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  author = {Casinghino, Chris and Sjöberg, Vilhelm and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/TK85Y6LY/Casinghino-Sjöberg-Weirich - 2012 - Step-Indexed Normalization for a Language with General Recursion.pdf;/Users/pgiarrusso/Zotero/storage/CB89BLGA/1202.html}
}
% == BibLateX quality report for Casinghino2012StepIndexed:
% Unexpected field 'archivePrefix'

@inproceedings{Altenkirch2010PS,
  langid = {english},
  title = {ΠΣ: Dependent Types without the Sugar},
  isbn = {978-3-642-12250-7 978-3-642-12251-4},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-12251-4_5},
  doi = {10.1007/978-3-642-12251-4_5},
  shorttitle = {ΠΣ},
  abstract = {The recent success of languages like Agda and Coq demonstrates the potential of using dependent types for programming. These systems rely on many high-level features like datatype definitions, pattern matching and implicit arguments to facilitate the use of the languages. However, these features complicate the metatheoretical study and are a potential source of bugs.To address these issues we introduce ΠΣ, a dependently typed core language. It is small enough for metatheoretical study and the type checker is small enough to be formally verified. In this language there is only one mechanism for recursion—used for types, functions and infinite objects—and an explicit mechanism to control unfolding, based on lifted types. Furthermore structural equality is used consistently for values and types; this is achieved by a new notion of α-equality for recursive definitions. We show, by translating several high-level constructions, that ΠΣ is suitable as a core language for dependently typed programming.},
  eventtitle = {International Symposium on Functional and Logic Programming},
  booktitle = {Functional and Logic Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-06-19},
  date = {2010-04-19},
  pages = {40-55},
  author = {Altenkirch, Thorsten and Danielsson, Nils Anders and Löh, Andres and Oury, Nicolas},
  file = {/Users/pgiarrusso/Zotero/storage/DPZ2QX8X/Altenkirch-Danielsson-Löh-Oury - 2010 - ΠΣ - Dependent Types without the Sugar.pdf;/Users/pgiarrusso/Zotero/storage/C93U6AT3/978-3-642-12251-4_5.html}
}
% == BibLateX quality report for Altenkirch2010PS:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@phdthesis{Sjobergdependently,
  langid = {english},
  title = {A dependently typed language with nontermination},
  url = {https://search.proquest.com/openview/330943cadf5b06dd54805248236e9ab1/1?pq-origsite=gscholar&cbl=18750&diss=y},
  abstract = {We propose a full-spectrum dependently typed programming language, Zombie, which supports general recursion natively. The Zombie implementation is an elaborating typechecker. We prove type safety for a large subset of the Zombie core language, including features such as computational irrelevance, CBV-reduction, and propositional equality with a heterogeneous, completely erased elimination form. Zombie does not automatically β-reduce expressions, but instead uses congruence closure for proof and type inference. We give a specification of a subset of the surface language via a bidirectional type system, which works “up-to-congruence,” and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system. Zombie also features an optional termination-checker, allowing nonterminating programs returning proofs as well as external proofs about programs.},
  urldate = {2018-06-19},
  author = {Sjöberg, Vilhelm},
  file = {/Users/pgiarrusso/Zotero/storage/5987JBTU/Sjöberg - A dependently typed language with nontermination.pdf;/Users/pgiarrusso/Zotero/storage/ED7QVTCT/1.html}
}
% == BibLateX quality report for Sjobergdependently:
% I don't know how to quality-check phdthesis references

@article{Coquand1992paradox,
  langid = {english},
  title = {The paradox of trees in type theory},
  volume = {32},
  issn = {0006-3835, 1572-9125},
  url = {https://link.springer.com/article/10.1007/BF01995104},
  doi = {10.1007/BF01995104},
  abstract = {We show how to represent a paradox similar to Russell's paradox in Type Theory withW-types and a type of all types, and how to use this in order to represent a fixed-point operator in such a theory. It is still open whether such a construction is possible without theW-type.},
  number = {1},
  journaltitle = {BIT Numerical Mathematics},
  shortjournal = {BIT},
  urldate = {2018-06-19},
  date = {1992-03-01},
  pages = {10-14},
  author = {Coquand, Thierry},
  file = {/Users/pgiarrusso/Zotero/storage/YGCZLCYV/Coquand - 1992 - The paradox of trees in type theory.pdf;/Users/pgiarrusso/Zotero/storage/8UYU9TI6/BF01995104.html}
}
% == BibLateX quality report for Coquand1992paradox:
% 'issn': not a valid ISSN

@article{Vazou2017Refinement,
  title = {Refinement Reflection: Complete Verification with SMT},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3158141},
  doi = {10.1145/3158141},
  shorttitle = {Refinement Reflection},
  abstract = {We introduce Refinement Reflection, a new framework for building SMT-based deductive verifiers. The key idea is to reflect the code implementing a user-defined function into the function’s (output) refinement type. As a consequence, at uses of the function, the function definition is instantiated in the SMT logic in a precise fashion that permits decidable verification. Reflection allows the user to write equational proofs of programs just by writing other programs using pattern-matching and recursion to perform case-splitting and induction. Thus, via the propositions-as-types principle, we show that reflection permits the specification of arbitrary functional correctness properties. Finally, we introduce a proof-search algorithm called Proof by Logical Evaluation that uses techniques from model checking and abstract interpretation, to completely automate equational reasoning. We have implemented reflection in Liquid Haskell and used it to verify that the widely used instances of the Monoid, Applicative, Functor, and Monad typeclasses actually satisfy key algebraic laws required to make the clients safe, and have used reflection to build the first library that actually verifies assumptions about associativity and ordering that are crucial for safe deterministic parallelism.},
  issue = {POPL},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-06-19},
  date = {2017-12},
  pages = {53:1--53:31},
  keywords = {Haskell,refinement types,theorem proving,verification},
  author = {Vazou, Niki and Tondwalkar, Anish and Choudhury, Vikraman and Scott, Ryan G. and Newton, Ryan R. and Wadler, Philip and Jhala, Ranjit},
  file = {/Users/pgiarrusso/Zotero/storage/6CIM7DPT/Vazou-Tondwalkar-Choudhury-Scott-Newton-Wadler-Jhala - 2017 - Refinement Reflection - Complete Verification with SMT.pdf}
}
% == BibLateX quality report for Vazou2017Refinement:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Yang2017Unifying,
  title = {Unifying Typing and Subtyping},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3133871},
  doi = {10.1145/3133871},
  abstract = {In recent years dependent types have become a hot topic in programming language research. A key reason why dependent types are interesting is that they allow unifying types and terms, which enables both additional expressiveness and economy of concepts. Unfortunately there has been much less work on dependently typed calculi for object-oriented programming. This is partly because it is widely acknowledged that the combination between dependent types and subtyping is particularly challenging. This paper presents λ I≤, which is a dependently typed generalization of System F≤. The resulting calculus follows the style of Pure Type Systems, and contains a single unified syntactic sort that accounts for expressions, types and kinds. To address the challenges posed by the combination of dependent types and subtyping, λ I≤ employs a novel technique that unifies typing and subtyping. In λ I≤ there is only a judgement that is akin to a typed version of subtyping. Both the typing relation, as well as type well-formedness are just special cases of the subtyping relation. The resulting calculus has a rich metatheory and enjoys of several standard and desirable properties, such as subject reduction, transitivity of subtyping, narrowing as well as standard substitution lemmas. All the metatheory of λ I≤ is mechanically proved in the Coq theorem prover. Furthermore, (and as far as we are aware) λ I≤ is the first dependently typed calculus that completely subsumes System F≤, while preserving various desirable properties.},
  issue = {OOPSLA},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-06-19},
  date = {2017-10},
  pages = {47:1--47:26},
  keywords = {dependent types,subtyping,object-oriented programming},
  author = {Yang, Yanpeng and Oliveira, Bruno C. d. S.},
  file = {/Users/pgiarrusso/Zotero/storage/R8GHV2CS/Yang-Oliveira - 2017 - Unifying Typing and Subtyping.pdf}
}
% == BibLateX quality report for Yang2017Unifying:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Aspinall1996Subtyping,
  title = {Subtyping dependent types},
  doi = {10.1109/LICS.1996.561307},
  abstract = {The need for subtyping in type-systems with dependent types has been realized for some years. But it is hard to prove that systems combining the two features have fundamental properties such as subject reduction. Here we investigate a subtyping extension of the system λP, which is an abstract version of the type system of the Edinburgh Logical Framework LF. By using an equivalent formulation, we establish some important properties of the new system λP⩽, including subject reduction. Our analysis culminates in a complete and terminating algorithm which establishes the decidability of type-checking},
  eventtitle = {Proceedings 11th Annual IEEE Symposium on Logic in Computer Science},
  booktitle = {Proceedings 11th Annual IEEE Symposium on Logic in Computer Science},
  date = {1996-07},
  pages = {86-97},
  keywords = {type theory,dependent types,subtyping,Computer science,Computer languages,Logic programming,Laboratories,decidability,Encoding,Algorithm design and analysis,Application software,Edinburgh Logical Framework,programming theory,subject reduction,terminating algorithm,type-checking,type-systems},
  author = {Aspinall, D. and Compagnoni, A.},
  file = {/Users/pgiarrusso/Zotero/storage/94ZBNIVM/Aspinall-Compagnoni - 1996 - Subtyping dependent types.pdf;/Users/pgiarrusso/Zotero/storage/MCFQ29H8/561307.html}
}

@article{Aspinall2001Subtyping,
  title = {Subtyping dependent types},
  volume = {266},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397500001754},
  doi = {10.1016/S0304-3975(00)00175-4},
  abstract = {The need for subtyping in type systems with dependent types has been realized for some years. But it is hard to prove that systems combining the two features have fundamental properties such as subject reduction. Here we investigate a subtyping extension of the system λP, which is an abstract version of the type system of the Edinburgh Logical Framework LF. By using an equivalent formulation, we establish some important properties of the new system λP⩽, including subject reduction. Our analysis culminates in a complete and terminating algorithm which establishes the decidability of type-checking.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2018-06-19},
  date = {2001-09-06},
  pages = {273-309},
  keywords = {Dependent types,Subtyping,Type theory},
  author = {Aspinall, David and Compagnoni, Adriana},
  file = {/Users/pgiarrusso/Zotero/storage/SE8K9X88/Aspinall-Compagnoni - 2001 - Subtyping dependent types.pdf;/Users/pgiarrusso/Zotero/storage/3CVRPD3U/S0304397500001754.html}
}

@article{Compagnoni2003Typed,
  title = {Typed operational semantics for higher-order subtyping},
  volume = {184},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540103000622},
  doi = {10.1016/S0890-5401(03)00062-2},
  abstract = {Bounded operator abstraction is a language construct relevant to object oriented programming languages and to ML2000, the successor to Standard ML. In this paper, we introduce Fω⩽, a variant of F$<$:ω with this feature and with Cardelli and Wegner’s kernel Fun rule for quantifiers. We define a typed-operational semantics with subtyping and prove that it is equivalent with Fω⩽, using logical relations to prove soundness. The typed-operational semantics provides a powerful and uniform technique to study metatheoretic properties of Fω⩽, such as Church–Rosser, subject reduction, the admissibility of structural rules, and the equivalence with the algorithmic presentation of the system that performs weak-head reductions. Furthermore, we can show decidability of subtyping using the typed-operational semantics and its equivalence with the usual presentation. Hence, this paper demonstrates for the first time that logical relations can be used to show decidability of subtyping.},
  number = {2},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2018-06-19},
  date = {2003-08-01},
  pages = {242-297},
  keywords = {Subtyping,Type theory,Dependent kinds,Lambda calculus,Typed-operational semantics},
  author = {Compagnoni, Adriana and Goguen, Healfdene},
  file = {/Users/pgiarrusso/Zotero/storage/NTK42FPK/Compagnoni-Goguen - 2003 - Typed operational semantics for higher-order subtyping.pdf;/Users/pgiarrusso/Zotero/storage/MCJ6DB34/S0890540103000622.html}
}

@article{Sjoberg2012Irrelevance,
  langid = {english},
  title = {Irrelevance, Heterogeneous Equality, and Call-by-value Dependent Type Systems},
  url = {https://arxiv.org/abs/1202.2923},
  doi = {10.4204/EPTCS.76.9},
  abstract = {We present a full-spectrum dependently typed core language which includes both nontermination and computational irrelevance (a.k.a. erasure), a combination which has not been studied before. The two features interact: to protect type safety we must be careful to only erase terminating expressions. Our language design is strongly influenced by the choice of CBV evaluation, and by our novel treatment of propositional equality which has a heterogeneous, completely erased elimination form.},
  urldate = {2018-06-20},
  date = {2012-02-14},
  author = {Sjöberg, Vilhelm and Casinghino, Chris and Ahn, Ki Yung and Collins, Nathan and Eades III, Harley D. and Fu, Peng and Kimmell, Garrin and Sheard, Tim and Stump, Aaron and Weirich, Stephanie},
  file = {/Users/pgiarrusso/Zotero/storage/HS6RNHLT/Sjöberg-Casinghino-Ahn-Collins-Iii-D-Fu-Kimmell-Sheard-Stump-Weirich - 2012 - Irrelevance, Heterogeneous Equality, and Call-by-value Dependent Type Systems.pdf;/Users/pgiarrusso/Zotero/storage/4UYXKQF5/1202.html}
}
% == BibLateX quality report for Sjoberg2012Irrelevance:
% Missing required field 'journaltitle'

@article{Coquand1990Proof,
  title = {A Proof of Strong Normalization for the Theory of Constructions Using a Kripke-Like Interpretation},
  url = {https://repository.upenn.edu/cis_reports/568},
  journaltitle = {Technical Reports (CIS)},
  date = {1990-07-01},
  author = {Coquand, Thierry and Gallier, Jean},
  file = {/Users/pgiarrusso/Zotero/storage/9JUFCCL2/Coquand-Gallier - 1990 - A Proof of Strong Normalization for the Theory of Constructions Using a Kripke-Like Interpretation.pdf;/Users/pgiarrusso/Zotero/storage/VLM5R69P/568.html}
}

@article{Vytiniotis2009Relating,
  langid = {american},
  title = {Relating step-indexed logical relations and bisimulations},
  url = {https://www.microsoft.com/en-us/research/publication/relating-step-indexed-logical-relations-and-bisimulations/},
  abstract = {Operational logical relations and bisimulations are two particularly successful syntactic techniques for reasoning about program equivalence. Although both techniques seem to have common intuitions, their basis is on different mathematical principles: induction for the former, and co-induction for the latter. The intuitive understanding of the two techniques seems more common, but their mathematical connection more …},
  journaltitle = {Microsoft Research},
  urldate = {2018-06-26},
  date = {2009-03-01},
  author = {Vytiniotis, Dimitrios and Koutavas, Vasileios},
  file = {/Users/pgiarrusso/Zotero/storage/86UF6RGR/Vytiniotis-Koutavas - 2009 - Relating step-indexed logical relations and bisimulations.pdf;/Users/pgiarrusso/Zotero/storage/N9LCWFAL/relating-step-indexed-logical-relations-and-bisimulations.html}
}

@article{Glew2013Formalisation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.5277},
  primaryClass = {cs},
  title = {Formalisation of the lambda aleph Runtime},
  url = {http://arxiv.org/abs/1307.5277},
  abstract = {In previous work we describe a novel approach to dependent typing based on a multivalued term language. In this technical report we formalise the runtime, a kind of operational semantics, for that language. We describe a fairly comprehensive core language, and then give a small-step operational semantics based on an abstract machine. Errors are explicit in the semantics. We also prove several simple properties: that every non-terminated machine state steps to something and that reduction is deterministic once input is fixed.},
  urldate = {2018-06-26},
  date = {2013-07-19},
  keywords = {Computer Science - Programming Languages},
  author = {Glew, Neal and Sweeney, Tim and Petersen, Leaf},
  file = {/Users/pgiarrusso/Zotero/storage/VN2ZAHT3/Glew-Sweeney-Petersen - 2013 - Formalisation of the lambda aleph Runtime.pdf;/Users/pgiarrusso/Zotero/storage/GFXXDFL2/1307.html}
}
% == BibLateX quality report for Glew2013Formalisation:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Yachi2016Sound,
  langid = {english},
  title = {A Sound and Complete Bisimulation for Contextual Equivalence in \$\$$\backslash$lambda \$\$-Calculus with Call/cc},
  isbn = {978-3-319-47957-6 978-3-319-47958-3},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-47958-3_10},
  doi = {10.1007/978-3-319-47958-3_10},
  abstract = {We develop a sound and complete proof method of contextual equivalence in λλ$\backslash$lambda -calculus with the abortive control operator call/cc (as opposed to delimited control operators like 𝚜𝚑𝚒𝚏𝚝shift$\backslash$mathtt \{shift\} and 𝚛𝚎𝚜𝚎𝚝reset$\backslash$mathtt \{reset\}), and prove the non-trivial equivalence between λf.f()λf.f()$\backslash$lambda f.$\backslash$,f() and λf.f();f()λf.f();f()$\backslash$lambda f.$\backslash$,f();f() for example, both for the first time to our knowledge. Although our method is based on environmental bisimulations (Sumii et al. 2004-), it makes an essential and general change to their metatheory, which is not only necessary for handling call/cc but is also applicable in other languages with no control operator.},
  eventtitle = {Asian Symposium on Programming Languages and Systems},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2018-06-26},
  date = {2016-11-21},
  pages = {171-186},
  author = {Yachi, Taichi and Sumii, Eijiro},
  file = {/Users/pgiarrusso/Zotero/storage/ZKT3QHWD/978-3-319-47958-3_10.html}
}
% == BibLateX quality report for Yachi2016Sound:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Amin2012Dependent,
  langid = {english},
  title = {Dependent Object Types},
  url = {https://infoscience.epfl.ch/record/183030},
  abstract = {We propose a new type-theoretic foundation of Scala and languages like it: the Dependent Object Types (DOT) calculus. DOT models Scala’s path-dependent types, abstract type members and its mixture of nominal and structural typing through the use of reﬁnement types. The core formalism makes no attempt to model inheritance and mixin composition. DOT normalizes Scala’s type system by unifying the constructs for type members and by providing classical intersection and union types which simplify greatest lower bound and least upper bound computations. In this paper, we present the DOT calculus, both formally and informally. We also discuss our work-in-progress to prove typesafety of the calculus. Amin, Nada; Moors, Adriaan; Odersky, Martin},
  urldate = {2018-06-28},
  date = {2012},
  author = {Amin, Nada and Moors, Adriaan and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/QN5TDH3I/Amin-Moors-Odersky - 2012 - Dependent Object Types.pdf}
}
% == BibLateX quality report for Amin2012Dependent:
% Missing required field 'journaltitle'

@article{Jones2007Practical,
  langid = {english},
  title = {Practical type inference for arbitrary-rank types},
  volume = {17},
  issn = {1469-7653, 0956-7968},
  url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/practical-type-inference-for-arbitraryrank-types/5339FB9DAB968768874D4C20FA6F8CB6},
  doi = {10.1017/S0956796806006034},
  abstract = {Haskell's popularity has driven the need for ever more expressive type system features, most of which threaten the decidability and practicality of Damas-Milner type inference. One such feature is the ability to write functions with higher-rank types – that is, functions that take polymorphic functions as their arguments. Complete type inference is known to be undecidable for higher-rank (impredicative) type systems, but in practice programmers are more than willing to add type annotations to guide the type inference engine, and to document their code. However, the choice of just what annotations are required, and what changes are required in the type system and its inference algorithm, has been an ongoing topic of research. We take as our starting point a λ-calculus proposed by Odersky and Läufer. Their system supports arbitrary-rank polymorphism through the exploitation of type annotations on λ-bound arguments and arbitrary sub-terms. Though elegant, and more convenient than some other proposals, Odersky and Läufer's system requires many annotations. We show how to use local type inference (invented by Pierce and Turner) to greatly reduce the annotation burden, to the point where higher-rank types become eminently usable. Higher-rank types have a very modest impact on type inference. We substantiate this claim in a very concrete way, by presenting a complete type-inference engine, written in Haskell, for a traditional Damas-Milner type system, and then showing how to extend it for higher-rank types. We write the type-inference engine using a monadic framework: it turns out to be a particularly compelling example of monads in action. The paper is long, but is strongly tutorial in style. Although we use Haskell as our example source language, and our implementation language, much of our work is directly applicable to any ML-like functional language.},
  number = {1},
  journaltitle = {Journal of Functional Programming},
  urldate = {2018-07-03},
  date = {2007-01},
  pages = {1-82},
  author = {Jones, Simon Peyton and Vytiniotis, Dimitrios and Weirich, Stephanie and Shields, Mark},
  file = {/Users/pgiarrusso/Zotero/storage/AAZ29R6K/Jones-Vytiniotis-Weirich-Shields - 2007 - Practical type inference for arbitrary-rank types.pdf;/Users/pgiarrusso/Zotero/storage/6FWR33GW/5339FB9DAB968768874D4C20FA6F8CB6.html}
}
% == BibLateX quality report for Jones2007Practical:
% 'issn': not a valid ISSN

@article{Shaikhha2018Efficient,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.02136},
  primaryClass = {cs, stat},
  title = {Efficient Differentiable Programming in a Functional Array-Processing Language},
  url = {http://arxiv.org/abs/1806.02136},
  abstract = {We present a system for the automatic differentiation of a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source automatic differentiation and global optimizations such as loop transformations. Thanks to this feature, we demonstrate how for some real-world machine learning and computer vision benchmarks, the system outperforms the state-of-the-art automatic differentiation tools.},
  urldate = {2018-07-03},
  date = {2018-06-06},
  keywords = {Computer Science - Programming Languages,Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation,Statistics - Machine Learning},
  author = {Shaikhha, Amir and Fitzgibbon, Andrew and Vytiniotis, Dimitrios and Jones, Simon Peyton and Koch, Christoph},
  file = {/Users/pgiarrusso/Zotero/storage/V8QHWECY/Shaikhha-Fitzgibbon-Vytiniotis-Jones-Koch - 2018 - Efficient Differentiable Programming in a Functional Array-Processing Language.pdf;/Users/pgiarrusso/Zotero/storage/2DICEE9D/1806.html}
}
% == BibLateX quality report for Shaikhha2018Efficient:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Schafer2015Completeness,
  location = {{New York, NY, USA}},
  title = {Completeness and Decidability of De Bruijn Substitution Algebra in Coq},
  isbn = {978-1-4503-3296-5},
  url = {http://doi.acm.org/10.1145/2676724.2693163},
  doi = {10.1145/2676724.2693163},
  abstract = {We consider a two-sorted algebra over de Bruijn terms and de Bruijn substitutions equipped with the constants and operations from Abadi et al.'s sigma-calculus. We consider expressions with term variables and substitution variables and show that the semantic equivalence obtained with the algebra coincides with the axiomatic equivalence obtained with finitely many axioms based on the sigma-calculus. We prove this result with an informative decision algorithm for axiomatic equivalence, which in the negative case returns a variable assignment separating the given expressions in the algebra. The entire development is formalized in Coq.},
  booktitle = {Proceedings of the 2015 Conference on Certified Programs and Proofs},
  series = {CPP '15},
  publisher = {{ACM}},
  urldate = {2018-07-04},
  date = {2015},
  pages = {67--73},
  keywords = {coq,explicit substitutions,algebra,completeness,de bruijn terms,decision procedures,finite axiomatization},
  author = {Schäfer, Steven and Smolka, Gert and Tebbi, Tobias},
  file = {/Users/pgiarrusso/Zotero/storage/WIEHCX36/Schäfer-Smolka-Tebbi - 2015 - Completeness and Decidability of De Bruijn Substitution Algebra in Coq.pdf}
}

@inproceedings{Aspinall1994Subtyping,
  langid = {english},
  title = {Subtyping with singleton types},
  isbn = {978-3-540-60017-6 978-3-540-49404-1},
  url = {https://link.springer.com/chapter/10.1007/BFb0022243},
  doi = {10.1007/BFb0022243},
  abstract = {We give syntax and a PER-model semantics for a typed λ-calculus with subtypes and singleton types. The calculus may be seen as a minimal calculus of subtyping with a simple form of dependent types. The aim is to study singleton types and to take a canny step towards more complex dependent subtyping systems. Singleton types have applications in the use of type systems for specification and program extraction: given a program P we can form the very tight specification \{P\} which is met uniquely by P. Singletons integrate abbreviational definitions into a type system: the hypothesis x: \{M\} asserts x=M. The addition of singleton types is a non-conservative extension of familiar subtyping theories. In our system, more terms are typable and previously typable terms have more (non-dependent) types.},
  eventtitle = {International Workshop on Computer Science Logic},
  booktitle = {Computer Science Logic},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-13},
  date = {1994-09-25},
  pages = {1-15},
  author = {Aspinall, David},
  file = {/Users/pgiarrusso/Zotero/storage/YGX3LLKL/Aspinall - 1994 - Subtyping with singleton types.pdf;/Users/pgiarrusso/Zotero/storage/3IZ79KL9/BFb0022243.html}
}
% == BibLateX quality report for Aspinall1994Subtyping:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Cardelli1988Structural,
  location = {{New York, NY, USA}},
  title = {Structural Subtyping and the Notion of Power Type},
  isbn = {978-0-89791-252-5},
  url = {http://doi.acm.org/10.1145/73560.73566},
  doi = {10.1145/73560.73566},
  booktitle = {Proceedings of the 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '88},
  publisher = {{ACM}},
  urldate = {2018-07-13},
  date = {1988},
  pages = {70--79},
  author = {Cardelli, L.},
  file = {/Users/pgiarrusso/Zotero/storage/Y7V9RGWH/Cardelli - 1988 - Structural Subtyping and the Notion of Power Type.pdf}
}

@inproceedings{Paulin-Mohring1993Inductive,
  langid = {english},
  title = {Inductive definitions in the system Coq rules and properties},
  isbn = {978-3-540-56517-8 978-3-540-47586-6},
  url = {https://link.springer.com/chapter/10.1007/BFb0037116},
  doi = {10.1007/BFb0037116},
  abstract = {In the pure Calculus of Constructions, it is possible to represent data structures and predicates using higher-order quantification. However, this representation is not satisfactory, from the point of view of both the efficiency of the underlying programs and the power of the logical system. For these reasons, the calculus was extended with a primitive notion of inductive definitions [8]. This paper describes the rules for inductive definitions in the system Coq. They are general enough to be seen as one formulation of adding inductive definitions to a typed lambda-calculus. We prove strong normalization for a subsystem of Coq corresponding to the pure Calculus of Constructions plus Inductive Definitions with only weak eliminations.},
  eventtitle = {International Conference on Typed Lambda Calculi and Applications},
  booktitle = {Typed Lambda Calculi and Applications},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-15},
  date = {1993-03-16},
  pages = {328-345},
  author = {Paulin-Mohring, Christine},
  file = {/Users/pgiarrusso/Zotero/storage/9A5GKQNU/Paulin-Mohring - 1993 - Inductive definitions in the system Coq rules and properties.pdf;/Users/pgiarrusso/Zotero/storage/PZAH2MX7/BFb0037116.html}
}
% == BibLateX quality report for Paulin-Mohring1993Inductive:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Coquand1988Inductively,
  langid = {english},
  title = {Inductively defined types},
  isbn = {978-3-540-52335-2 978-3-540-46963-6},
  url = {https://link.springer.com/chapter/10.1007/3-540-52335-9_47},
  doi = {10.1007/3-540-52335-9_47},
  eventtitle = {International Conference on Computer Logic},
  booktitle = {COLOG-88},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-15},
  date = {1988-12-12},
  pages = {50-66},
  author = {Coquand, Thierry and Paulin, Christine},
  file = {/Users/pgiarrusso/Zotero/storage/KM5IA4T4/Coquand-Paulin - 1988 - Inductively defined types.pdf;/Users/pgiarrusso/Zotero/storage/ZAEXKZ5P/10.html}
}
% == BibLateX quality report for Coquand1988Inductively:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Goguen1994Typed,
  langid = {english},
  title = {A Typed Operational Semantics for Type Theory},
  url = {https://www.era.lib.ed.ac.uk/handle/1842/405},
  abstract = {Untyped reduction provides a natural operational semantics for type theory. Normalization results say that such a semantics is sound. However, this reduction does not take type information into account and gives no information about canonical forms for terms. We introduce a new operational semantics, which we call typed operational semantics, which defines a reduction to normal form for terms which are well-typed in the type theory. 
 
The central result of the thesis is soundness of the typed operational semantics for the original system. Completeness of the semantics is straightforward. We demonstrate that this equivalence between the declarative and operational presentations of type theory has important metatheoretic consequences: results such as strengthening, subject reduction and strong normalization follow by straightforward induction on derivations in the new system. 
 
We introduce these ideas in the setting of the simply typed lambda calculus. We then extend the techniques to Luo's system UTT, which is Martin-Löf's Logical Framework extended by a general mechanism for inductive types, a predicative universe and an impredicative universe of propositions. We also give a proof-irrelevant set-theoretic semantics for UTT.},
  urldate = {2018-07-15},
  date = {1994-07},
  author = {Goguen, Healfdene},
  file = {/Users/pgiarrusso/Zotero/storage/D3YIDSNB/Goguen - 1994 - A Typed Operational Semantics for Type Theory.PDF;/Users/pgiarrusso/Zotero/storage/97JXJ5L2/405.html}
}
% == BibLateX quality report for Goguen1994Typed:
% Missing required field 'journaltitle'

@online{LuoProblem,
  title = {A Problem of Adequacy: conservativity of calculus of constructions over higher-order logic},
  url = {http://www.lfcs.inf.ed.ac.uk/reports/90/ECS-LFCS-90-121/},
  shorttitle = {A Problem of Adequacy},
  abstract = {This paper discusses a problem of adequacy in formalization of mathematical notions in type theories. Based on a point of view that there should be a clear distinction between logical formulae and data types, we believe that, in an impredicative type theory like the calculus of constructions, arbitrary sets should be formalized as non-propositional types rather than propositional types which may be formed impredicatively.
We show that the calculus of constructions with type constants is a conservative extension of the intuitionistic higher-order logic, provided that the object set of the higher-order logic is interpreted as a non-propositional type constant. This result is proved by giving a formulae-as-types formulation of higher-order logic and using a projection technique developed by Berardi and Mohring. Comparing with the non-conservativity result of the (pure) calculus of constructions over higher-order logic [Ber89][Geu89], this provides an evidence to support the above view of adequate formalization and gives a better understanding of the formulae-as-types principle for higher-order logic.
We briefly discuss how abstract mathematics (e.g., abstract algebras) can be adequately formalized in an impredicative type theoy with predicative universes to support abstract reasoning.},
  urldate = {2018-07-15},
  author = {Luo, Zhaohui},
  file = {/Users/pgiarrusso/Zotero/storage/KFK2H49T/Luo - A Problem of Adequacy - conservativity of calculus of constructions over higher-order logic.pdf;/Users/pgiarrusso/Zotero/storage/USUY2FDR/ECS-LFCS-90-121.html}
}
% == BibLateX quality report for LuoProblem:
% Exactly one of 'date' / 'year' must be present

@inproceedings{Luo1989ECC,
  title = {ECC, an extended calculus of constructions},
  doi = {10.1109/LICS.1989.39193},
  abstract = {A higher-order calculus ECC (extended calculus of constructions) is presented which can be seen as an extension of the calculus of constructions by adding strong sum types and a fully cumulative type hierarchy. ECC turns out to be rather expressive so that mathematical theories can be abstractly described and abstract mathematics may be adequately formalized. It is shown that ECC is strongly normalizing and has other nice proof-theoretic properties. An ω-set (realizability) model is described to show how the essential properties of the calculus can be captured set-theoretically},
  eventtitle = {[1989] Proceedings. Fourth Annual Symposium on Logic in Computer Science},
  booktitle = {[1989] Proceedings. Fourth Annual Symposium on Logic in Computer Science},
  date = {1989-06},
  pages = {386-395},
  keywords = {Buildings,Calculus,Computer science,formal logic,Mathematical model,Mathematics,set theory,Computer languages,formal languages,realizability,Abstract algebra,abstract mathematics,extended calculus of constructions,fully cumulative type hierarchy,Functional programming,higher-order calculus ECC,Inference algorithms,proof-theoretic properties,strong sum types,strongly normalizing,ω-set},
  author = {Luo, Z.},
  file = {/Users/pgiarrusso/Zotero/storage/QZDUZF6H/Luo - 1989 - ECC, an extended calculus of constructions.pdf;/Users/pgiarrusso/Zotero/storage/GXZQEHZD/39193.html}
}
% == BibLateX quality report for Luo1989ECC:
% ? Unsure about the formatting of the booktitle

@report{Barras1997Coq,
  title = {Coq in Coq},
  abstract = {. We formalize the definition and the metatheory of the Calculus of Constructions (CC) using the proof assistant Coq. In particular, we prove strong normalization and decidability of type inference. From the latter proof, we extract a certified Objective Caml program which performs type inference in CC and use this code to build a small-scale certified proof-checker.  Key words: Type Theory, proof-checker, Calculus of Constructions, metatheory, strong normalization proof, program extraction. 1. Introduction  1.1. Motivations  This work can be described as the formal certification in Coq of a proof-checker for the Calculus of Constructions (CC). We view it as a first experimental step towards a certified kernel for the whole Coq  system, of which CC is a significative fragment. In decidable type theories, a proof-checker is a program which verifies whether a given judgement (input) is valid or not (output). Valid meaning that there exists a derivation for that judgement following the in...},
  date = {1997},
  author = {Barras, Bruno and Werner, Benjamin},
  file = {/Users/pgiarrusso/Zotero/storage/PTFSSBCR/Barras-Werner - 1997 - Coq in Coq.pdf;/Users/pgiarrusso/Zotero/storage/EW3F652N/summary.html}
}
% == BibLateX quality report for Barras1997Coq:
% Missing required field 'type'
% Missing required field 'institution'

@inproceedings{Barras1996Verification,
  langid = {english},
  title = {Verification of the interface of a small proof system in coq},
  isbn = {978-3-540-65137-6 978-3-540-49562-8},
  url = {https://link.springer.com/chapter/10.1007/BFb0097785},
  doi = {10.1007/BFb0097785},
  abstract = {This article describes the formalization of the interface of a proof-checker. The latter is based on a kernel consisting of type-checking functions for the Calculus of Constructions, but it seems the ideas can generalize to other type systems, as far as they are based on the proofsas-terms principle. We suppose that the metatheory of the corresponding type system is proved (up to type decidability). We specify and certify the toplevel loop, the system invariant, and the error messages.},
  eventtitle = {International Workshop on Types for Proofs and Programs},
  booktitle = {Types for Proofs and Programs},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-15},
  date = {1996-12-15},
  pages = {28-45},
  author = {Barras, Bruno},
  file = {/Users/pgiarrusso/Zotero/storage/SBPIPXAE/Barras - 1996 - Verification of the interface of a small proof system in coq.pdf;/Users/pgiarrusso/Zotero/storage/E64TPLJB/BFb0097785.html}
}
% == BibLateX quality report for Barras1996Verification:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Kennedy2018Transposing,
  title = {Transposing G to C♯: Expressivity of generalized algebraic data types in an object-oriented language},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397518301270},
  doi = {10.1016/j.tcs.2018.02.030},
  shorttitle = {Transposing G to C♯},
  abstract = {Generalized algebraic datatypes (GADTs) are a hot topic in the functional programming community. Previously we showed that object-oriented languages such as C♯ and Java can express GADT declarations using Generics, but only some GADT programs. The addition of equational constraints on type parameters recovers expressivity. We now study this expressivity gap in more depth by extending an earlier translation from System F to C♯ to handle GADTs. Our efforts reveal some surprising limitations of Generics and provide further justification for equational constraints.},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2018-07-15},
  date = {2018-04-02},
  keywords = {C\#,Generalized algebraic data types,Generics,Polymorphism,System F},
  author = {Kennedy, Andrew J. and Russo, Claudio V.},
  file = {/Users/pgiarrusso/Zotero/storage/6TW5JHDQ/S0304397518301270.html}
}

@article{Lipton2018Troubling,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.03341},
  primaryClass = {cs, stat},
  title = {Troubling Trends in Machine Learning Scholarship},
  url = {http://arxiv.org/abs/1807.03341},
  abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
  urldate = {2018-07-16},
  date = {2018-07-09},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Lipton, Zachary C. and Steinhardt, Jacob},
  file = {/Users/pgiarrusso/Zotero/storage/2JXNHSYB/Lipton-Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship.pdf;/Users/pgiarrusso/Zotero/storage/BS8DPQQB/1807.html}
}
% == BibLateX quality report for Lipton2018Troubling:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Eisenberg2016Dependent,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.07978},
  primaryClass = {cs},
  title = {Dependent Types in Haskell: Theory and Practice},
  url = {http://arxiv.org/abs/1610.07978},
  shorttitle = {Dependent Types in Haskell},
  abstract = {Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been adding new type-level programming features for some time. Many of these features---chiefly: generalized algebraic datatypes (GADTs), type families, kind polymorphism, and promoted datatypes---have brought Haskell to the doorstep of dependent types. Many dependently typed programs can even currently be encoded, but often the constructions are painful. In this dissertation, I describe Dependent Haskell, which supports full dependent types via a backward-compatible extension to today's Haskell. An important contribution of this work is an implementation, in GHC, of a portion of Dependent Haskell, with the rest to follow. The features I have implemented are already released, in GHC 8.0. This dissertation contains several practical examples of Dependent Haskell code, a full description of the differences between Dependent Haskell and today's Haskell, a novel type-safe dependently typed lambda-calculus (called Pico) suitable for use as an intermediate language for compiling Dependent Haskell, and a type inference and elaboration algorithm, Bake, that translates Dependent Haskell to type-correct Pico.},
  urldate = {2018-07-17},
  date = {2016-10-25},
  keywords = {Computer Science - Programming Languages},
  author = {Eisenberg, Richard A.},
  file = {/Users/pgiarrusso/Zotero/storage/4YEN8QNT/Eisenberg - 2016 - Dependent Types in Haskell - Theory and Practice.pdf;/Users/pgiarrusso/Zotero/storage/9I7TTUHX/1610.html}
}
% == BibLateX quality report for Eisenberg2016Dependent:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Grigore2017Java,
  location = {{New York, NY, USA}},
  title = {Java Generics Are Turing Complete},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009871},
  doi = {10.1145/3009837.3009871},
  abstract = {This paper describes a reduction from the halting problem of Turing machines to subtype checking in Java. It follows that subtype checking in Java is undecidable, which answers a question posed by Kennedy and Pierce in 2007. It also follows that Java's type checker can recognize any recursive language, which improves a result of Gill and Levy from 2016. The latter point is illustrated by a parser generator for fluent interfaces.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2018-07-18},
  date = {2017},
  pages = {73--85},
  keywords = {decidability,Java,fluent interface,parser generator,subtype checking,Turing machine},
  author = {Grigore, Radu},
  file = {/Users/pgiarrusso/Zotero/storage/ZYY5RTLJ/Grigore - 2017 - Java Generics Are Turing Complete.pdf}
}

@inproceedings{Lindley2017Bea,
  location = {{New York, NY, USA}},
  title = {Do Be Do Be Do},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009897},
  doi = {10.1145/3009837.3009897},
  abstract = {We explore the design and implementation of Frank, a strict functional programming language with a bidirectional effect type system designed from the ground up around a novel variant of Plotkin and Pretnar's effect handler abstraction.   Effect handlers provide an abstraction for modular effectful programming: a handler acts as an interpreter for a collection of commands whose interfaces are statically tracked by the type system. However, Frank eliminates the need for an additional effect handling construct by generalising the basic mechanism of functional abstraction itself. A function is simply the special case of a Frank operator that interprets no commands. Moreover, Frank's operators can be multihandlers which simultaneously interpret commands from several sources at once, without disturbing the direct style of functional programming with values.   Effect typing in Frank employs a novel form of effect polymorphism which avoid mentioning effect variables in source code. This is achieved by propagating an ambient ability inwards, rather than accumulating unions of potential effects outwards.   We introduce Frank by example, and then give a formal account of the Frank type system and its semantics. We introduce Core Frank by elaborating Frank operators into functions, case expressions, and unary handlers, and then give a sound small-step operational semantics for Core Frank.   Programming with effects and handlers is in its infancy. We contribute an exploration of future possibilities, particularly in combination with other forms of rich type system.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2018-07-21},
  date = {2017},
  pages = {500--514},
  keywords = {pattern matching,continuations,call-by-push-value,algebraic effects,effect handlers,bidirectional typing,effect polymorphism},
  author = {Lindley, Sam and McBride, Conor and McLaughlin, Craig},
  file = {/Users/pgiarrusso/Zotero/storage/9SG59UMM/Lindley et al. - 2017 - Do Be Do Be Do.pdf}
}

@inproceedings{Christiansen2016Elaborator,
  location = {{New York, NY, USA}},
  title = {Elaborator Reflection: Extending Idris in Idris},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951932},
  doi = {10.1145/2951913.2951932},
  shorttitle = {Elaborator Reflection},
  abstract = {Many programming languages and proof assistants are defined by elaboration from a high-level language with a great deal of implicit information to a highly explicit core language. In many advanced languages, these elaboration facilities contain powerful tools for program construction, but these tools are rarely designed to be repurposed by users. We describe elaborator reflection, a paradigm for metaprogramming in which the elaboration machinery is made directly available to metaprograms, as well as a concrete realization of elaborator reflection in Idris, a functional language with full dependent types. We demonstrate the applicability of Idris’s reflected elaboration framework to a number of realistic problems, we discuss the motivation for the specific features of its design, and we explore the broader meaning of elaborator reflection as it can relate to other languages.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2018-07-23},
  date = {2016},
  pages = {284--297},
  keywords = {elaboration,dependent types,Metaprogramming},
  author = {Christiansen, David and Brady, Edwin},
  file = {/Users/pgiarrusso/Zotero/storage/2CGZRHZV/Christiansen-Brady - 2016 - Elaborator Reflection - Extending Idris in Idris.pdf}
}

@inproceedings{Malecha2016Extensible,
  langid = {english},
  title = {Extensible and Efficient Automation Through Reflective Tactics},
  isbn = {978-3-662-49497-4 978-3-662-49498-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-49498-1_21},
  doi = {10.1007/978-3-662-49498-1_21},
  abstract = {Foundational proof assistants simultaneously offer both expressive logics and strong guarantees. The price they pay for this flexibility is often the need to build and check explicit proof objects which can be expensive. In this work we develop a collection of techniques for building reflective automation, where proofs are witnessed by verified decision procedures rather than verbose proof objects. Our techniques center around a verified domain specific language for proving, tacRtac$\backslash$mathcal \{R\}\_\{tac\}, written in Gallina, Coq’s logic. The design of tactics makes it easy to combine them into higher-level automation that can be proved sound in a mostly automated way. Furthermore, unlike traditional uses of reflection, tacRtac$\backslash$mathcal \{R\}\_\{tac\} tactics are independent of the underlying problem domain, which allows them to be re-tasked to automate new problems with very little effort. We demonstrate the usability of tacRtac$\backslash$mathcal \{R\}\_\{tac\} through several case studies demonstrating orders of magnitude speedups for relatively little engineering work.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-27},
  date = {2016-04-03},
  pages = {532-559},
  author = {Malecha, Gregory and Bengtson, Jesper},
  file = {/Users/pgiarrusso/Zotero/storage/SYL5NCYZ/Malecha-Bengtson - 2016 - Extensible and Efficient Automation Through Reflective Tactics.pdf;/Users/pgiarrusso/Zotero/storage/4A85UKMA/978-3-662-49498-1_21.html}
}
% == BibLateX quality report for Malecha2016Extensible:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Jung2017RustBelt,
  title = {RustBelt: Securing the Foundations of the Rust Programming Language},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3158154},
  doi = {10.1145/3158154},
  shorttitle = {RustBelt},
  abstract = {Rust is a new systems programming language that promises to overcome the seemingly fundamental tradeoff between high-level safety guarantees and low-level control over resource management. Unfortunately, none of Rust's safety claims have been formally proven, and there is good reason to question whether they actually hold. Specifically, Rust employs a strong, ownership-based type system, but then extends the expressive power of this core type system through libraries that internally use unsafe features. In this paper, we give the first formal (and machine-checked) safety proof for a language representing a realistic subset of Rust. Our proof is extensible in the sense that, for each new Rust library that uses unsafe features, we can say what verification condition it must satisfy in order for it to be deemed a safe extension to the language. We have carried out this verification for some of the most important libraries that are used throughout the Rust ecosystem.},
  issue = {POPL},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-07-27},
  date = {2017-12},
  pages = {66:1--66:34},
  keywords = {type systems,logical relations,concurrency,separation logic,Rust},
  author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/YLDJ9L32/Jung-Jourdan-Krebbers-Dreyer - 2017 - RustBelt - Securing the Foundations of the Rust Programming Language.pdf}
}
% == BibLateX quality report for Jung2017RustBelt:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Swasey2017Robust,
  title = {Robust and Compositional Verification of Object Capability Patterns},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3133913},
  doi = {10.1145/3133913},
  abstract = {In scenarios such as web programming, where code is linked together from multiple sources, object capability patterns (OCPs) provide an essential safeguard, enabling programmers to protect the private state of their objects from corruption by unknown and untrusted code. However, the benefits of OCPs in terms of program verification have never been properly formalized. In this paper, building on the recently developed Iris framework for concurrent separation logic, we develop OCPL, the first program logic for compositionally specifying and verifying OCPs in a language with closures, mutable state, and concurrency. The key idea of OCPL is to account for the interface between verified and untrusted code by adopting a well-known idea from the literature on security protocol verification, namely robust safety. Programs that export only properly wrapped values to their environment can be proven robustly safe, meaning that their untrusted environment cannot violate their internal invariants. We use OCPL to give the first general, compositional, and machine-checked specs for several commonly-used OCPsâincluding the dynamic sealing, membrane, and caretaker patternsâwhich we then use to verify robust safety for representative client code. All our results are fully mechanized in the Coq proof assistant.},
  issue = {OOPSLA},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-07-27},
  date = {2017-10},
  pages = {89:1--89:26},
  keywords = {logical relations,object capabilities,compositional verification,separation logic,robust safety},
  author = {Swasey, David and Garg, Deepak and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/NWIHB2W7/Swasey-Garg-Dreyer - 2017 - Robust and Compositional Verification of Object Capability Patterns.pdf}
}
% == BibLateX quality report for Swasey2017Robust:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Krebbers2017Interactive,
  location = {{New York, NY, USA}},
  title = {Interactive Proofs in Higher-order Concurrent Separation Logic},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009855},
  doi = {10.1145/3009837.3009855},
  abstract = {When using a proof assistant to reason in an embedded logic -- like separation logic -- one cannot benefit from the proof contexts and basic tactics of the proof assistant. This results in proofs that are at a too low level of abstraction because they are cluttered with bookkeeping code related to manipulating the object logic.   In this paper, we introduce a so-called proof mode that extends the Coq proof assistant with (spatial and non-spatial) named proof contexts for the object logic. We show that thanks to these contexts we can implement high-level tactics for introduction and elimination of the connectives of the object logic, and thereby make reasoning in the embedded logic as seamless as reasoning in the meta logic of the proof assistant. We apply our method to Iris: a state of the art higher-order impredicative concurrent separation logic.   We show that our method is very general, and is not just limited to program verification. We demonstrate its generality by formalizing correctness proofs of fine-grained concurrent algorithms, derived constructs of the Iris logic, and a unary and binary logical relation for a language with concurrency, higher-order store, polymorphism, and recursive types. This is the first formalization of a binary logical relation for such an expressive language. We also show how to use the logical relation to prove contextual refinement of fine-grained concurrent algorithms.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2018-07-27},
  date = {2017},
  pages = {205--217},
  keywords = {Logical Relations,Coq,Fine-grained Concurrency,Interactive Theorem Proving,Separation Logic},
  author = {Krebbers, Robbert and Timany, Amin and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/5YLUBYAU/Krebbers-Timany-Birkedal - 2017 - Interactive Proofs in Higher-order Concurrent Separation Logic.pdf}
}

@inproceedings{Krebbers2017Essence,
  langid = {english},
  title = {The Essence of Higher-Order Concurrent Separation Logic},
  isbn = {978-3-662-54433-4 978-3-662-54434-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-54434-1_26},
  doi = {10.1007/978-3-662-54434-1_26},
  abstract = {Concurrent separation logics (CSLs) have come of age, and with age they have accumulated a great deal of complexity. Previous work on the Iris logic attempted to reduce the complex logical mechanisms of modern CSLs to two orthogonal concepts: partial commutative monoids (PCMs) and invariants. However, the realization of these concepts in Iris still bakes in several complex mechanisms—such as weakest preconditions and mask-changing view shifts—as primitive notions.In this paper, we take the Iris story to its (so to speak) logical conclusion, applying the reductionist methodology of Iris to Iris itself. Specifically, we define a small, resourceful base logic, which distills the essence of Iris: it comprises only the assertion layer of vanilla separation logic, plus a handful of simple modalities. We then show how the much fancier logical mechanisms of Iris—in particular, its entire program specification layer—can be understood as merely derived forms in our base logic. This approach helps to explain the meaning of Iris’s program specifications at a much higher level of abstraction than was previously possible. We also show that the step-indexed “later” modality of Iris is an essential source of complexity, in that removing it leads to a logical inconsistency. All our results are fully formalized in the Coq proof assistant.},
  eventtitle = {European Symposium on Programming},
  booktitle = {Programming Languages and Systems},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-27},
  date = {2017-04-25},
  pages = {696-723},
  author = {Krebbers, Robbert and Jung, Ralf and Bizjak, Aleš and Jourdan, Jacques-Henri and Dreyer, Derek and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/RNIA3H9H/Krebbers-Jung-Bizjak-Jourdan-Dreyer-Birkedal - 2017 - The Essence of Higher-Order Concurrent Separation Logic.pdf;/Users/pgiarrusso/Zotero/storage/BLYK4HDN/978-3-662-54434-1_26.html}
}
% == BibLateX quality report for Krebbers2017Essence:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@inproceedings{Jung2016Higherorder,
  location = {{New York, NY, USA}},
  title = {Higher-order Ghost State},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951943},
  doi = {10.1145/2951913.2951943},
  abstract = {The development of concurrent separation logic (CSL) has sparked a long line of work on modular verification of sophisticated concurrent programs. Two of the most important features supported by several existing extensions to CSL are higher-order quantification and custom ghost state. However, none of the logics that support both of these features reap the full potential of their combination. In particular, none of them provide general support for a feature we dub "higher-order ghost state": the ability to store arbitrary higher-order separation-logic predicates in ghost variables.   In this paper, we propose higher-order ghost state as a interesting and useful extension to CSL, which we formalize in the framework of Jung et al.'s recently developed Iris logic. To justify its soundness, we develop a novel algebraic structure called CMRAs ("cameras"), which can be thought of as "step-indexed partial commutative monoids". Finally, we show that Iris proofs utilizing higher-order ghost state can be effectively formalized in Coq, and discuss the challenges we faced in formalizing them.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2018-07-27},
  date = {2016},
  pages = {256--269},
  keywords = {interactive theorem proving,compositional verification,fine-grained concurrency,higher- order logic,Separation logic},
  author = {Jung, Ralf and Krebbers, Robbert and Birkedal, Lars and Dreyer, Derek},
  file = {/Users/pgiarrusso/Zotero/storage/S5DD9IRU/Jung-Krebbers-Birkedal-Dreyer - 2016 - Higher-order Ghost State.pdf}
}

@inproceedings{Krogh-Jespersen2017Relational,
  location = {{New York, NY, USA}},
  title = {A Relational Model of Types-and-effects in Higher-order Concurrent Separation Logic},
  isbn = {978-1-4503-4660-3},
  url = {http://doi.acm.org/10.1145/3009837.3009877},
  doi = {10.1145/3009837.3009877},
  abstract = {Recently we have seen a renewed interest in programming languages that tame the complexity of state and concurrency through refined type systems with more fine-grained control over effects. In addition to simplifying reasoning and eliminating whole classes of bugs, statically tracking effects opens the door to advanced compiler optimizations.     In this paper we present a relational model of a type-and-effect system for a higher-order, concurrent program- ming language. The model precisely captures the semantic invariants expressed by the effect annotations. We demonstrate that these invariants are strong enough to prove advanced program transformations, including automatic parallelization of expressions with suitably disjoint effects. The model also supports refinement proofs between abstract data types implementations with different internal data representations, including proofs that fine-grained concurrent algorithms refine their coarse-grained counterparts. This is the first model for such an expressive language that supports both effect-based optimizations and data abstraction.     The logical relation is defined in Iris, a state-of-the-art higher-order concurrent separation logic. This greatly simplifies proving well-definedness of the logical relation and also provides us with a powerful logic for reasoning in the model.},
  booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
  series = {POPL 2017},
  publisher = {{ACM}},
  urldate = {2018-07-27},
  date = {2017},
  pages = {218--231},
  keywords = {program transformation,Separation logic,automatic parallelisation,logical rela- tions,type-and-effect system},
  author = {Krogh-Jespersen, Morten and Svendsen, Kasper and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/D9TIHMSW/Krogh-Jespersen-Svendsen-Birkedal - 2017 - A Relational Model of Types-and-effects in Higher-order Concurrent Separation Logic.pdf}
}

@article{Bizjak2018Models,
  title = {On Models of Higher-Order Separation Logic},
  volume = {336},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066118300197},
  doi = {10.1016/j.entcs.2018.03.016},
  abstract = {We show how tools from categorical logic can be used to give a general account of models of higher-order separation logic with a sublogic of so-called persistent predicates satisfying the usual rules of higher-order logic. The models of separation logic are based on a notion of resource, a partial commutative monoid, and the persistent predicates can be defined using a modality. We classify well-behaved sublogics of persistent predicates in terms of interior operators on the partial commutative monoid of resources. We further show how the general constructions can be used to recover the model of Iris, a state-of-the-art higher-order separation logic with guarded recursive predicates.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {The Thirty-third Conference on the Mathematical Foundations of Programming Semantics (MFPS XXXIII)},
  urldate = {2018-07-27},
  date = {2018-04-16},
  pages = {57-78},
  keywords = {separation logic,modalities,model},
  author = {Bizjak, Aleš and Birkedal, Lars},
  file = {/Users/pgiarrusso/Zotero/storage/RRJDK44T/Bizjak-Birkedal - 2018 - On Models of Higher-Order Separation Logic.pdf;/Users/pgiarrusso/Zotero/storage/PRH5ZKDV/S1571066118300197.html}
}

@inproceedings{Krishnamurthi1998Synthesizing,
  langid = {english},
  title = {Synthesizing object-oriented and functional design to promote re-use},
  isbn = {978-3-540-64737-9 978-3-540-69064-1},
  url = {https://link.springer.com/chapter/10.1007/BFb0054088},
  doi = {10.1007/BFb0054088},
  abstract = {Many problems require recursively specified types of data and a collection of tools that operate on those data. Over time, these problems evolve so that the programmer must extend the toolkit or extend the types and adjust the existing tools accordingly. Ideally, this should be done without modifying existing code. Unfortunately, the prevailing program design strategies do not support both forms of extensibility: functional programming accommodates the addition of tools, while object-oriented programming supports either adding new tools or extending the data set, but not both. In this paper, we present a composite design pattern that synthesizes the best of both approaches and in the process resolves the tension between the two design strategies. We also show how this protocol suggests a new set of linguistic facilities for languages that support class systems.},
  eventtitle = {European Conference on Object-Oriented Programming},
  booktitle = {ECOOP’98 — Object-Oriented Programming},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-07-31},
  date = {1998-07-20},
  pages = {91-113},
  author = {Krishnamurthi, Shriram and Felleisen, Matthias and Friedman, Daniel P.},
  file = {/Users/pgiarrusso/Zotero/storage/CZ7MKML8/Krishnamurthi-Felleisen-Friedman - 1998 - Synthesizing object-oriented and functional design to promote re-use.pdf;/Users/pgiarrusso/Zotero/storage/827WAQNK/BFb0054088.html}
}
% == BibLateX quality report for Krishnamurthi1998Synthesizing:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Hermida2014Logical,
  title = {Logical Relations and Parametricity – A Reynolds Programme for Category Theory and Programming Languages},
  volume = {303},
  issn = {1571-0661},
  url = {http://www.sciencedirect.com/science/article/pii/S1571066114000346},
  doi = {10.1016/j.entcs.2014.02.008},
  abstract = {In his seminal paper on “Types, Abstraction and Parametric Polymorphism,” John Reynolds called for homomorphisms to be generalized from functions to relations. He reasoned that such a generalization would allow type-based “abstraction” (representation independence, information hiding, naturality or parametricity) to be captured in a mathematical theory, while accounting for higher-order types. However, after 30 years of research, we do not yet know fully how to do such a generalization. In this article, we explain the problems in doing so, summarize the work carried out so far, and call for a renewed attempt at addressing the problem.},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  series = {Proceedings of the Workshop on Algebra, Coalgebra and Topology (WACT 2013)},
  urldate = {2018-08-02},
  date = {2014-03-28},
  pages = {149-180},
  keywords = {Logical Relations,Category Theory,Data abstraction,Definability,Fibrations,Homomorphisms,Information hiding,Natural Transformations,Parametric polymorphism,Reflexive Graphs,Relation lifting,Relational Parametricity,Universal algebra},
  author = {Hermida, Claudio and Reddy, Uday S. and Robinson, Edmund P.},
  file = {/Users/pgiarrusso/Zotero/storage/PJJ5R4G5/Hermida-Reddy-Robinson - 2014 - Logical Relations and Parametricity – A Reynolds Programme for Category Theory and Programming Languages.pdf;/Users/pgiarrusso/Zotero/storage/QZLZUQU3/S1571066114000346.html}
}

@article{Bernardy2017Linear,
  title = {Linear Haskell: Practical Linearity in a Higher-order Polymorphic Language},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3158093},
  doi = {10.1145/3158093},
  shorttitle = {Linear Haskell},
  abstract = {Linear type systems have a long and storied history, but not a clear path forward to integrate with existing languages such as OCaml or Haskell. In this paper, we study a linear type system designed with two crucial properties in mind: backwards-compatibility and code reuse across linear and non-linear users of a library. Only then can the benefits of linear types permeate conventional functional programming. Rather than bifurcate types into linear and non-linear counterparts, we instead attach linearity to function arrows. Linear functions can receive inputs from linearly-bound values, but can also operate over unrestricted, regular values.  To demonstrate the efficacy of our linear type system~—~both how easy it can be integrated in an existing language implementation and how streamlined it makes it to write programs with linear types~—~we implemented our type system in ghc, the leading Haskell compiler, and demonstrate two kinds of applications of linear types: mutable data with pure interfaces; and enforcing protocols in I/O-performing functions.},
  issue = {POPL},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-08-02},
  date = {2017-12},
  pages = {5:1--5:29},
  keywords = {Haskell,polymorphism,linear logic,laziness,linear types,GHC,typestate},
  author = {Bernardy, Jean-Philippe and Boespflug, Mathieu and Newton, Ryan R. and Peyton Jones, Simon and Spiwack, Arnaud},
  file = {/Users/pgiarrusso/Zotero/storage/KQ6JU6NB/Bernardy-Boespflug-Newton-Peyton Jones-Spiwack - 2017 - Linear Haskell - Practical Linearity in a Higher-order Polymorphic Language.pdf}
}
% == BibLateX quality report for Bernardy2017Linear:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Diehl2018Generic,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08150},
  primaryClass = {cs},
  title = {Generic Zero-Cost Reuse for Dependent Types},
  url = {http://arxiv.org/abs/1803.08150},
  abstract = {Dependently typed languages are well known for having a problem with code reuse. Traditional non-indexed algebraic datatypes (e.g. lists) appear alongside a plethora of indexed variations (e.g. vectors). Functions are often rewritten for both non-indexed and indexed versions of essentially the same datatype, which is a source of code duplication. We work in a Curry-style dependent type theory, where the same untyped term may be classified as both the non-indexed and indexed versions of a datatype. Many solutions have been proposed for the problem of dependently typed reuse, but we exploit Curry-style type theory in our solution to not only reuse data and programs, but do so at zero-cost (without a runtime penalty). Our work is an exercise in dependently typed generic programming, and internalizes the process of zero-cost reuse as the identity function in a Curry-style theory.},
  urldate = {2018-08-04},
  date = {2018-03-21},
  keywords = {Computer Science - Programming Languages},
  author = {Diehl, Larry and Firsov, Denis and Stump, Aaron},
  file = {/Users/pgiarrusso/Zotero/storage/D2P44ULP/Diehl-Firsov-Stump - 2018 - Generic Zero-Cost Reuse for Dependent Types.pdf;/Users/pgiarrusso/Zotero/storage/AIRS2S5L/1803.html}
}
% == BibLateX quality report for Diehl2018Generic:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Wei2018Refunctionalization,
  title = {Refunctionalization of Abstract Abstract Machines: Bridging the Gap Between Abstract Abstract Machines and Abstract Definitional Interpreters (Functional Pearl)},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3236800},
  doi = {10.1145/3236800},
  shorttitle = {Refunctionalization of Abstract Abstract Machines},
  abstract = {Abstracting abstract machines is a systematic methodology for constructing sound static analyses for higher-order languages, by deriving small-step abstract abstract machines (AAMs) that perform abstract interpretation from abstract machines that perform concrete evaluation. Darais et al. apply the same underlying idea to monadic definitional interpreters, and obtain monadic abstract definitional interpreters (ADIs) that perform abstract interpretation in big-step style using monads. Yet, the relation between small-step abstract abstract machines and big-step abstract definitional interpreters is not well studied.   In this paper, we explain their functional correspondence and demonstrate how to systematically transform small-step abstract abstract machines into big-step abstract definitional interpreters. Building on known semantic interderivation techniques from the concrete evaluation setting, the transformations include linearization, lightweight fusion, disentanglement, refunctionalization, and the left inverse of the CPS transform. Linearization expresses nondeterministic choice through first-order data types, after which refunctionalization transforms the first-order data types that represent continuations into higher-order functions. The refunctionalized AAM is an abstract interpreter written in continuation-passing style (CPS) with two layers of continuations, which can be converted back to direct style with delimited control operators. Based on the known correspondence between delimited control and monads, we demonstrate that the explicit use of monads in abstract definitional interpreters is optional.   All transformations properly handle the collecting semantics and nondeterminism of abstract interpretation. Remarkably, we reveal how precise call/return matching in control-flow analysis can be obtained by refunctionalizing a small-step abstract abstract machine with proper caching.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-08-04},
  date = {2018-07},
  pages = {105:1--105:28},
  keywords = {Scala,abstract machines,control-flow analysis,refunctionalization},
  author = {Wei, Guannan and Decker, James and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/2XWESLDI/Wei-Decker-Rompf - 2018 - Refunctionalization of Abstract Abstract Machines - Bridging the Gap Between Abstract Abstract Machines and Abstract Definitional Interpreters (Functional Pearl).pdf}
}
% == BibLateX quality report for Wei2018Refunctionalization:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Harper1994Typetheoretic,
  location = {{New York, NY, USA}},
  title = {A Type-theoretic Approach to Higher-order Modules with Sharing},
  isbn = {978-0-89791-636-3},
  url = {http://doi.acm.org/10.1145/174675.176927},
  doi = {10.1145/174675.176927},
  abstract = {The design of a module system for constructing and maintaining large programs is a difficult task that raises a number of theoretical and practical issues. A fundamental issue is the management of the flow of information between program units at compile time via the notion of an interface. Experience has shown that fully opaque interfaces are awkward to use in practice since too much information is hidden, and that fully transparent interfaces lead to excessive interdependencies, creating problems for maintenance and separate compilation. The “sharing” specifications of Standard ML address this issue by allowing the programmer to specify equational relationships between types in separated modules, but are not expressive enough to allow the programmer complete control over the propagation of type information between modules.
These problems are addressed from a type-theoretic viewpoint by considering a calculus based on Girard's system F\&ohgr;. The calculus differs form those considered in previous studies by relying exclusively on a new form of weak sum type to propagate information at compile-time, in contrast to approaches based on strong sums which rely on substitution. The new form of sum type allows for the specification of equational, as well as type and kind, information in interfaces. This provides complete control over the propagation of compile-time information between program units and is sufficient to encode in a straightforward way most users of type sharing specifications in Standard ML.
Modules are treated as “first-class” citizens, and therefore the system supports higher-order modules and some object-oriented programming idioms; the language may be easily restricted to “second-class” modules found in ML-like languages.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '94},
  publisher = {{ACM}},
  urldate = {2018-08-04},
  date = {1994},
  pages = {123--137},
  author = {Harper, Robert and Lillibridge, Mark},
  file = {/Users/pgiarrusso/Zotero/storage/HNWNR9WX/Harper-Lillibridge - 1994 - A Type-theoretic Approach to Higher-order Modules with Sharing.pdf}
}

@article{Lane1981Mathematical,
  title = {Mathematical Models: A Sketch for the Philosophy of Mathematics},
  volume = {88},
  issn = {0002-9890},
  url = {https://www.jstor.org/stable/2321751},
  doi = {10.2307/2321751},
  shorttitle = {Mathematical Models},
  number = {7},
  journaltitle = {The American Mathematical Monthly},
  urldate = {2018-08-16},
  date = {1981},
  pages = {462-472},
  author = {Lane, Saunders Mac},
  file = {/Users/pgiarrusso/Zotero/storage/MP9WA3TU/Lane - 1981 - Mathematical Models - A Sketch for the Philosophy of Mathematics.pdf}
}

@article{Mogelberg2018Denotational,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00289},
  primaryClass = {cs},
  title = {Denotational semantics of recursive types in synthetic guarded domain theory},
  url = {http://arxiv.org/abs/1805.00289},
  abstract = {Just like any other branch of mathematics, denotational semantics of programming languages should be formalised in type theory, but adapting traditional domain theoretic semantics, as originally formulated in classical set theory to type theory has proven challenging. This paper is part of a project on formulating denotational semantics in type theories with guarded recursion. This should have the benefit of not only giving simpler semantics and proofs of properties such as adequacy, but also hopefully in the future to scale to languages with advanced features, such as general references, outside the reach of traditional domain theoretic techniques. Working in Guarded Dependent Type Theory (GDTT), we develop denotational semantics for FPC, the simply typed lambda calculus extended with recursive types, modelling the recursive types of FPC using the guarded recursive types of GDTT. We prove soundness and computational adequacy of the model in GDTT using a logical relation between syntax and semantics constructed also using guarded recursive types. The denotational semantics is intensional in the sense that it counts the number of unfold-fold reductions needed to compute the value of a term, but we construct a relation relating the denotations of extensionally equal terms, i.e., pairs of terms that compute the same value in a different number of steps. Finally we show how the denotational semantics of terms can be executed inside type theory and prove that executing the denotation of a boolean term computes the same value as the operational semantics of FPC.},
  urldate = {2018-09-04},
  date = {2018-05-01},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  author = {Møgelberg, Rasmus E. and Paviotti, Marco},
  file = {/Users/pgiarrusso/Zotero/storage/WUWCFLIK/Møgelberg-Paviotti - 2018 - Denotational semantics of recursive types in synthetic guarded domain theory.pdf;/Users/pgiarrusso/Zotero/storage/HPZ34HKR/1805.html}
}
% == BibLateX quality report for Mogelberg2018Denotational:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@inproceedings{Cohen2018Cubical,
  location = {{Dagstuhl, Germany}},
  title = {Cubical Type Theory: A Constructive Interpretation of the Univalence Axiom},
  volume = {69},
  isbn = {978-3-95977-030-9},
  url = {http://drops.dagstuhl.de/opus/volltexte/2018/8475},
  doi = {10.4230/LIPIcs.TYPES.2015.5},
  shorttitle = {Cubical Type Theory},
  booktitle = {21st International Conference on Types for Proofs and Programs (TYPES 2015)},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {{Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik}},
  urldate = {2018-09-18},
  date = {2018},
  pages = {5:1--5:34},
  keywords = {cubical sets,dependent type theory,univalence axiom},
  author = {Cohen, Cyril and Coquand, Thierry and Huber, Simon and Mörtberg, Anders},
  editor = {Uustalu, Tarmo},
  file = {/Users/pgiarrusso/Zotero/storage/NLLHNX2T/Cohen-Coquand-Huber-Mörtberg - 2018 - Cubical Type Theory - A Constructive Interpretation of the Univalence Axiom.pdf;/Users/pgiarrusso/Zotero/storage/DQZ6RGET/8475.html}
}
% == BibLateX quality report for Cohen2018Cubical:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Hammer2016Refinementa,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00097},
  primaryClass = {cs},
  title = {Refinement types for precisely named cache locations},
  url = {http://arxiv.org/abs/1610.00097v5},
  abstract = {Many programming language techniques for incremental computation employ programmer-specified names for cached information. At runtime, each name identifies a "cache location" for a dynamic data value or a sub-computation; in sum, these cache location choices guide change propagation and incremental (re)execution. We call a cache location name precise when it identifies at most one value or subcomputation; we call all other names imprecise, or ambiguous. At a minimum, cache location names must be precise to ensure that change propagation works correctly; yet, reasoning statically about names in incremental programs remains an open problem. As a first step, this paper defines and solves the precise name problem, where we verify that incremental programs with explicit names use them precisely. To do so, we give a refinement type and effect system, and prove it sound (every well-typed program uses names precisely). We also demonstrate that this type system is expressive by verifying example programs that compute over efficient representations of incremental sequences and sets. Beyond verifying these programs, our type system also describes their dynamic naming strategies, e.g., for library documentation purposes.},
  urldate = {2018-09-18},
  date = {2016-10-01},
  keywords = {Computer Science - Programming Languages,_tablet},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  file = {/Users/pgiarrusso/Zotero/storage/NB3T6P3H/Hammer-Dunfield-Economou-Narasimhamurthy - 2016 - Refinement types for precisely named cache locations.pdf;/Users/pgiarrusso/Zotero/storage/JNJW2WJJ/1610.html}
}
% == BibLateX quality report for Hammer2016Refinementa:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Hammer2018Fungi,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.07826},
  primaryClass = {cs},
  title = {Fungi: Typed incremental computation with names},
  url = {http://arxiv.org/abs/1808.07826},
  shorttitle = {Fungi},
  abstract = {Incremental computations attempt to exploit input similarities over time, reusing work that is unaffected by input changes. To maximize this reuse in a general-purpose programming setting, programmers need a mechanism to identify dynamic allocations (of data and subcomputations) that correspond over time. We present Fungi, a typed functional language for incremental computation with names. Unlike prior general-purpose languages for incremental computing, Fungi's notion of names is formal, general, and statically verifiable. Fungi's type-and-effect system permits the programmer to encode (program-specific) local invariants about names, and to use these invariants to establish global uniqueness for their composed programs, the property of using names correctly. We prove that well-typed Fungi programs respect global uniqueness. We derive a bidirectional version of the type and effect system, and we have implemented a prototype of Fungi in Rust. We apply Fungi to a library of incremental collections, showing that it is expressive in practice.},
  urldate = {2018-09-18},
  date = {2018-08-20},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,_tablet,Computer Science - Formal Languages and Automata Theory},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Narasimhamurthy, Monal and Economou, Dimitrios J.},
  file = {/Users/pgiarrusso/Zotero/storage/BRMEQBFR/Hammer-Dunfield-Headley-Narasimhamurthy-Economou - 2018 - Fungi - Typed incremental computation with names.pdf;/Users/pgiarrusso/Zotero/storage/VDRMSHVX/1808.html}
}
% == BibLateX quality report for Hammer2018Fungi:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@article{Rapoport2017Simplea,
  title = {A Simple Soundness Proof for Dependent Object Types},
  volume = {1},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3133870},
  doi = {10.1145/3133870},
  abstract = {Dependent Object Types (DOT) is intended to be a core calculus for modelling Scala. Its distinguishing feature is abstract type members, fields in objects that hold types rather than values. Proving soundness of DOT has been surprisingly challenging, and existing proofs are complicated, and reason about multiple concepts at the same time (e.g. types, values, evaluation). To serve as a core calculus for Scala, DOT should be easy to experiment with and extend, and therefore its soundness proof needs to be easy to modify.   This paper presents a simple and modular proof strategy for reasoning in DOT. The strategy separates reasoning about types from other concerns. It is centred around a theorem that connects the full DOT type system to a restricted variant in which the challenges and paradoxes caused by abstract type members are eliminated. Almost all reasoning in the proof is done in the intuitive world of this restricted type system. Once we have the necessary results about types, we observe that the other aspects of DOT are mostly standard and can be incorporated into a soundness proof using familiar techniques known from other calculi.},
  issue = {OOPSLA},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-09-19},
  date = {2017-10},
  pages = {46:1--46:27},
  keywords = {Computer Science - Programming Languages,type safety,_tablet,Scala,dependent object types,DOT calculus},
  author = {Rapoport, Marianna and Kabir, Ifaz and He, Paul and Lhoták, Ondřej},
  file = {/Users/pgiarrusso/Zotero/storage/UHEKZHVY/Rapoport-Kabir-He-Lhoták - 2017 - A Simple Soundness Proof for Dependent Object Types.pdf;/Users/pgiarrusso/Zotero/storage/WNAKE2N8/Rapoport-Kabir-He-Lhoták - 2017 - A Simple Soundness Proof for Dependent Object Types.pdf;/Users/pgiarrusso/Zotero/storage/B2NN7UZH/1706.html}
}
% == BibLateX quality report for Rapoport2017Simplea:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Hammer2008Memory,
  location = {{New York, NY, USA}},
  title = {Memory Management for Self-adjusting Computation},
  isbn = {978-1-60558-134-7},
  url = {http://doi.acm.org/10.1145/1375634.1375642},
  doi = {10.1145/1375634.1375642},
  abstract = {The cost of reclaiming space with traversal-based garbage collection is inversely proportional to the amount of free memory, i.e., O(1/(1-f)), where f is the fraction of memory that is live. Consequently, the cost of garbage collection can be very high when the size of the live data remains large relative to the available free space. Intuitively, this is because allocating a small amount of memory space will require the garbage collector to traverse a significant fraction of the memory only to discover little garbage. This is unfortunate because in some application domains the size of the memory-resident data can be generally high. This can cause high GC overheads, especially when generational assumptions do not hold. One such application domain is self-adjusting computation, where computations use memory-resident execution traces in order to respond to changes to their state (e.g., inputs) efficiently. This paper proposes memory-management techniques for self-adjusting computation that remain efficient even when the size of the live data is large. More precisely, the proposed techniques guarantee O(1) amortized cost for each reclaimed memory object. We propose a set of primitives for self-adjusting computation that support the proposed memory management techniques. The primitives provide an operation for allocating memory; we reclaim unused memory automatically. We implement a library for supporting the primitives in the C language and perform an experimental evaluation. Our experiments show that the approach can be implemented with reasonably small constant-factor overheads and that the programs written using the library behave optimally. Compared to previous implementations, we measure up to an order of magnitude improvement in performance and up to a 75\% reduction in space usage.},
  booktitle = {Proceedings of the 7th International Symposium on Memory Management},
  series = {ISMM '08},
  publisher = {{ACM}},
  urldate = {2018-09-20},
  date = {2008},
  pages = {51--60},
  keywords = {memoization,performance,self-adjusting computation,garbage collection,computational geometry,dynamic algorithms,dynamic dependency graphs,memory management},
  author = {Hammer, Matthew A. and Acar, Umut A.},
  file = {/Users/pgiarrusso/Zotero/storage/4C8WZT3N/Hammer-Acar - 2008 - Memory Management for Self-adjusting Computation.pdf}
}

@inproceedings{Acar2006Experimental,
  location = {{New York, NY, USA}},
  title = {An Experimental Analysis of Self-adjusting Computation},
  isbn = {978-1-59593-320-1},
  url = {http://doi.acm.org/10.1145/1133981.1133993},
  doi = {10.1145/1133981.1133993},
  abstract = {Dependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.},
  booktitle = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  series = {PLDI '06},
  publisher = {{ACM}},
  urldate = {2018-09-20},
  date = {2006},
  pages = {96--107},
  keywords = {performance,self-adjusting computation,computational geometry,dynamic algorithms,dynamic dependence graphs,memorization},
  author = {Acar, Umut A. and Blelloch, Guy E. and Blume, Matthias and Tangwongsan, Kanat},
  file = {/Users/pgiarrusso/Zotero/storage/I3D8H7YN/Acar-Blelloch-Blume-Tangwongsan - 2006 - An Experimental Analysis of Self-adjusting Computation.pdf}
}

@article{Acar2009Experimental,
  title = {An Experimental Analysis of Self-adjusting Computation},
  volume = {32},
  issn = {0164-0925},
  url = {http://doi.acm.org/10.1145/1596527.1596530},
  doi = {10.1145/1596527.1596530},
  abstract = {Recent work on adaptive functional programming (AFP) developed techniques for writing programs that can respond to modifications to their data by performing change propagation. To achieve this, executions of programs are represented with dynamic dependence graphs (DDGs) that record data dependences and control dependences in a way that a change-propagation algorithm can update the computation as if the program were from scratch, by re-executing only the parts of the computation affected by the changes. Since change-propagation only re-executes parts of the computation, it can respond to certain incremental modifications asymptotically faster than recomputing from scratch, potentially offering significant speedups. Such asymptotic speedups, however, are rare: for many computations and modifications, change propagation is no faster than recomputing from scratch. In this article, we realize a duality between dynamic dependence graphs and memoization, and combine them to give a change-propagation algorithm that can dramatically increase computation reuse. The key idea is to use DDGs to identify and re-execute the parts of the computation that are affected by modifications, while using memoization to identify the parts of the computation that remain unaffected by the changes. We refer to this approach as self-adjusting computation. Since DDGs are imperative, but (traditional) memoization requires purely functional computation, reusing computation correctly via memoization becomes a challenge. We overcome this challenge with a technique for remembering and reusing not just the results of function calls (as in conventional memoization), but their executions represented with DDGs. We show that the proposed approach is realistic by describing a library for self-adjusting computation, presenting efficient algorithms for realizing the library, and describing and evaluating an implementation. Our experimental evaluation with a variety of applications, ranging from simple list primitives to more sophisticated computational geometry algorithms, shows that the approach is effective in practice: compared to recomputing from-scratch; self-adjusting programs respond to small modifications to their data orders of magnitude faster.},
  number = {1},
  journaltitle = {ACM Trans. Program. Lang. Syst.},
  urldate = {2018-09-20},
  date = {2009-11},
  pages = {3:1--3:53},
  keywords = {memoization,performance,self-adjusting computation,dynamic algorithms,dynamic dependence graphs,Computational geometry},
  author = {Acar, Umut A. and Blelloch, Guy E. and Blume, Matthias and Harper, Robert and Tangwongsan, Kanat},
  file = {/Users/pgiarrusso/Zotero/storage/9MM6ID93/Acar-Blelloch-Blume-Harper-Tangwongsan - 2009 - An Experimental Analysis of Self-adjusting Computation.pdf}
}
% == BibLateX quality report for Acar2009Experimental:
% ? Possibly abbreviated journal title ACM Trans. Program. Lang. Syst.

@inproceedings{Arntzenius2016Datafun,
  location = {{New York, NY, USA}},
  title = {Datafun: A Functional Datalog},
  isbn = {978-1-4503-4219-3},
  url = {http://doi.acm.org/10.1145/2951913.2951948},
  doi = {10.1145/2951913.2951948},
  shorttitle = {Datafun},
  abstract = {Datalog may be considered either an unusually powerful query language or a carefully limited logic programming language. Datalog is declarative, expressive, and optimizable, and has been applied successfully in a wide variety of problem domains. However, most use-cases require extending Datalog in an application-specific manner. In this paper we define Datafun, an analogue of Datalog supporting higher-order functional programming. The key idea is to track monotonicity with types.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
  series = {ICFP 2016},
  publisher = {{ACM}},
  urldate = {2018-09-20},
  date = {2016},
  pages = {214--227},
  keywords = {domain-specific languages,functional programming,type theory,operational semantics,logic programming,adjoint logic,Datalog,denotational semantics,Prolog},
  author = {Arntzenius, Michael and Krishnaswami, Neelakantan R.},
  file = {/Users/pgiarrusso/Zotero/storage/EVX3J3GZ/Arntzenius-Krishnaswami - 2016 - Datafun - A Functional Datalog.pdf}
}

@article{Wright1994Syntactic,
  title = {A Syntactic Approach to Type Soundness},
  volume = {115},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540184710935},
  doi = {10.1006/inco.1994.1093},
  abstract = {We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2018-09-20},
  date = {1994-11-15},
  pages = {38-94},
  author = {Wright, A. K. and Felleisen, M.},
  file = {/Users/pgiarrusso/Zotero/storage/8SIXKFF4/Wright-Felleisen - 1994 - A Syntactic Approach to Type Soundness.pdf;/Users/pgiarrusso/Zotero/storage/VKHMC58H/S0890540184710935.html}
}

@inproceedings{Karlsson2018Extending,
  location = {{New York, NY, USA}},
  title = {Extending Scala with Records: Design, Implementation, and Evaluation},
  isbn = {978-1-4503-5836-1},
  url = {http://doi.acm.org/10.1145/3241653.3241661},
  doi = {10.1145/3241653.3241661},
  shorttitle = {Extending Scala with Records},
  abstract = {This paper presents a design for extensible records in Scala satisfying design goals such as structural subtyping, typesafe polymorphic operations, and separate compilation without runtime bytecode generation. Using new features of Scala 3, the design requires only minimal, local changes to the Scala 3 reference compiler Dotty as well as a small library component. Runtime performance is evaluated experimentally using a novel benchmarking suite generator, showing that the design is competitive with Scala 2's cached reflection for structural field access, and excels at immutable extension and update operations.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
  series = {Scala 2018},
  publisher = {{ACM}},
  urldate = {2018-09-21},
  date = {2018},
  pages = {72--82},
  keywords = {Scala,records,structural typing},
  author = {Karlsson, Olof and Haller, Philipp},
  file = {/Users/pgiarrusso/Zotero/storage/84LZ9RJW/Karlsson-Haller - 2018 - Extending Scala with Records - Design, Implementation, and Evaluation.pdf;/Users/pgiarrusso/Zotero/storage/HELT3I94/Karlsson-Haller - 2018 - Extending Scala with Records - Design, Implementation, and Evaluation.pdf}
}

@article{Goncharov2018Metalanguage,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.11256},
  primaryClass = {cs},
  title = {A Metalanguage for Guarded Iteration},
  url = {http://arxiv.org/abs/1807.11256},
  abstract = {Notions of guardedness serve to delineate admissible recursive definitions in various settings in a compositional manner. In recent work, we have introduced an axiomatic notion of guardedness in symmetric monoidal categories, which serves as a unifying framework for various examples from program semantics, process algebra, and beyond. In the present paper, we propose a generic metalanguage for guarded iteration based on combining this notion with the fine-grain call-by-value paradigm, which we intend as a unifying programming language for guarded and unguarded iteration in the presence of computational effects. We give a generic (categorical) semantics of this language over a suitable class of strong monads supporting guarded iteration, and show it to be in touch with the standard operational behaviour of iteration by giving a concrete big-step operational semantics for a certain specific instance of the metalanguage and establishing adequacy for this case.},
  urldate = {2018-09-23},
  date = {2018-07-30},
  keywords = {Computer Science - Logic in Computer Science},
  author = {Goncharov, Sergey and Rauch, Christoph and Schröder, Lutz},
  file = {/Users/pgiarrusso/Zotero/storage/QLKTHWJ7/Goncharov-Rauch-Schröder - 2018 - A Metalanguage for Guarded Iteration.pdf;/Users/pgiarrusso/Zotero/storage/IJI64CIX/1807.html}
}
% == BibLateX quality report for Goncharov2018Metalanguage:
% Unexpected field 'archivePrefix'
% Unexpected field 'primaryClass'
% Missing required field 'journaltitle'

@online{Guarded,
  title = {Guarded Traced Categories | SpringerLink},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-89366-2_17},
  urldate = {2018-09-23},
  file = {/Users/pgiarrusso/Zotero/storage/BRI4EVHD/978-3-319-89366-2_17.html}
}
% == BibLateX quality report for Guarded:
% Exactly one of 'date' / 'year' must be present

@article{Elsman2018Static,
  title = {Static Interpretation of Higher-order Modules in Futhark: Functional GPU Programming in the Large},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3236792},
  doi = {10.1145/3236792},
  shorttitle = {Static Interpretation of Higher-order Modules in Futhark},
  abstract = {We present a higher-order module system for the purely functional data-parallel array language Futhark. The module language has the property that it is completely eliminated at compile time, yet it serves as a powerful tool for organizing libraries and complete programs. The presentation includes a static and a dynamic semantics for the language in terms of, respectively, a static type system and a provably terminating elaboration of terms into terms of an underlying target language. The development is formalised in Coq using a novel encoding of semantic objects based on products, sets, and finite maps. The module language features a unified treatment of module type abstraction and core language polymorphism and is rich enough for expressing practical forms of module composition.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-09-25},
  date = {2018-07},
  pages = {97:1--97:30},
  keywords = {functional languages,modules,compilers,GPGPU},
  author = {Elsman, Martin and Henriksen, Troels and Annenkov, Danil and Oancea, Cosmin E.},
  file = {/Users/pgiarrusso/Zotero/storage/RC6WYQCS/Elsman-Henriksen-Annenkov-Oancea - 2018 - Static Interpretation of Higher-order Modules in Futhark - Functional GPU Programming in the Large.pdf}
}
% == BibLateX quality report for Elsman2018Static:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Ohori2018Finitary,
  title = {Finitary Polymorphism for Optimizing Type-directed Compilation},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3236776},
  doi = {10.1145/3236776},
  abstract = {We develop a type-theoretical method for optimizing type directed compilation of polymorphic languages, implement the method in SML\#, which is a full-scale compiler of Standard ML extended with several advanced features that require type-passing operational semantics, and report its effectiveness through performance evaluation. For this purpose, we first define a predicative second-order lambda calculus with finitary polymorphism, where each type abstraction is explicitly constrained to a finite type universe, and establishes the type soundness with respect to a type-passing operational semantics. Different from a calculus with stratified type universes, type universes of the calculus are terms that represent a finite set of instance types. We then develop a universe reconstruction algorithm that takes a term of the standard second-order lambda calculus, checks if the term is typable with finitary polymorphism, and, if typable, constructs a term in the calculus of finitary polymorphism. Based on these results, we present a type-based optimization method for polymorphic functions. Since our formalism is based on the second-order lambda calculus, it can be used to optimize various polymorphic languages. We implement the optimization method for native (tag-free) data representation and record polymorphism, and evaluate its effectiveness through benchmarks. The evaluation shows that 83.79\% of type passing abstractions are eliminated, and achieves the average of 15.28\% speed-up of compiled code.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-09-25},
  date = {2018-07},
  pages = {81:1--81:29},
  keywords = {Compiler Optimization,Finitary Polymorphism,Second-Order Lambda Calculus,SML\#,Type-Directed Compilation},
  author = {Ohori, Atsushi and Ueno, Katsuhiro and Mima, Hisayuki},
  file = {/Users/pgiarrusso/Zotero/storage/MY3DIGRT/Ohori-Ueno-Mima - 2018 - Finitary Polymorphism for Optimizing Type-directed Compilation.pdf}
}
% == BibLateX quality report for Ohori2018Finitary:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@article{Horn2018Incremental,
  title = {Incremental Relational Lenses},
  volume = {2},
  issn = {2475-1421},
  url = {http://doi.acm.org/10.1145/3236769},
  doi = {10.1145/3236769},
  abstract = {Lenses are a popular approach to bidirectional transformations, a generalisation of the view update problem in databases, in which we wish to make changes to source tables to effect a desired change on a view. However, perhaps surprisingly, lenses have seldom actually been used to implement updatable views in databases. Bohannon, Pierce and Vaughan proposed an approach to updatable views called relational lenses, but to the best of our knowledge this proposal has not been implemented or evaluated to date. We propose incremental relational lenses, that equip relational lenses with change-propagating semantics that map small changes to the view to (potentially) small changes to the source tables. We also present a language-integrated implementation of relational lenses and a detailed experimental evaluation, showing orders of magnitude improvement over the non-incremental approach. Our work shows that relational lenses can be used to support expressive and efficient view updates at the language level, without relying on updatable view support from the underlying database.},
  issue = {ICFP},
  journaltitle = {Proc. ACM Program. Lang.},
  urldate = {2018-09-25},
  date = {2018-07},
  pages = {74:1--74:30},
  keywords = {lenses,incremental computation,bidirectional transformations,relational calculus},
  author = {Horn, Rudi and Perera, Roly and Cheney, James},
  file = {/Users/pgiarrusso/Zotero/storage/SK46346R/Horn-Perera-Cheney - 2018 - Incremental Relational Lenses.pdf}
}
% == BibLateX quality report for Horn2018Incremental:
% ? Possibly abbreviated journal title Proc. ACM Program. Lang.

@inproceedings{Kabir2018kDOT,
  location = {{New York, NY, USA}},
  title = {κDOT: Scaling DOT with Mutation and Constructors},
  isbn = {978-1-4503-5836-1},
  url = {http://doi.acm.org/10.1145/3241653.3241659},
  doi = {10.1145/3241653.3241659},
  shorttitle = {κDOT},
  abstract = {Scala unifies concepts from object and module systems by allowing for objects with type members which are referenced via path-dependent types. The Dependent Object Types (DOT) calculus of Amin et al. models only this core part of Scala, but does not have many fundamental features of Scala such as strict and mutable fields. Since the most commonly used field types in Scala are strict,the correspondence between DOT and Scala is too weak for us to meaningfully prove static analyses safe for Scala by proving them safe for DOT.  A DOT calculus that can support strict and mutable fields together with constructors that do field initialization would be more suitable for analysis of Scala. Toward this goal, we present κDOT, an extension of DOT that supports constructors and field mutation and can emulate the different types of fields in Scala. We have proven κDOT sound through a mechanized proof in Coq. We present the key features of κDOT and its operational semantics and discuss work-in-progress toward making κDOT fully strict.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
  series = {Scala 2018},
  publisher = {{ACM}},
  urldate = {2018-09-28},
  date = {2018},
  pages = {40--50},
  keywords = {type safety,dependent object types,mutation},
  author = {Kabir, Ifaz and Lhoták, Ondřej},
  file = {/Users/pgiarrusso/Zotero/storage/27Z8HSXC/Kabir-Lhoták - 2018 - κDOT - Scaling DOT with Mutation and Constructors.pdf}
}

@inproceedings{Hong2018Path,
  location = {{New York, NY, USA}},
  title = {Path Dependent Types with Path-equality},
  isbn = {978-1-4503-5836-1},
  url = {http://doi.acm.org/10.1145/3241653.3241657},
  doi = {10.1145/3241653.3241657},
  abstract = {While the Scala type system provides expressive features like objects with type members, the lack of equality checking between path-dependent types prohibits some programming idioms. One such an example is abstract domain combinators in implementing static analyzers. In this paper, we propose to extend the Scala type system with path-equality, and formalize it as a DOT variant, π DOT, which supports records with type members and elds. We show that π DOT has the normalization property and prove its type soundness.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
  series = {Scala 2018},
  publisher = {{ACM}},
  urldate = {2018-09-28},
  date = {2018},
  pages = {35--39},
  keywords = {Scala,DOT,path equality},
  author = {Hong, Jaemin and Park, Jihyeok and Ryu, Sukyoung},
  file = {/Users/pgiarrusso/Zotero/storage/GX5PS5FR/Hong-Park-Ryu - 2018 - Path Dependent Types with Path-equality.pdf}
}

@inproceedings{Liu2018Initialization,
  location = {{New York, NY, USA}},
  title = {Initialization Patterns in Dotty},
  isbn = {978-1-4503-5836-1},
  url = {http://doi.acm.org/10.1145/3241653.3241662},
  doi = {10.1145/3241653.3241662},
  abstract = {Safe object initialization is important to avoid a category of runtime errors in programming languages. In this paper, we provide a case study of the initialization patterns on the Dotty compiler. In particular, we find that calling dynamic-dispatching methods, the usage of closures and instantiating nested classes are important for initialization of Scala objects. Based on the study, we conclude that existing proposals for safe initialization are inadequate for Scala.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
  series = {Scala 2018},
  publisher = {{ACM}},
  urldate = {2018-09-28},
  date = {2018},
  pages = {51--55},
  keywords = {Scala,Object initilization},
  author = {Liu, Fengyun and Biboudis, Aggelos and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/FERH7QGS/Liu-Biboudis-Odersky - 2018 - Initialization Patterns in Dotty.pdf}
}

@article{Leivant1991Finitely,
  title = {Finitely stratified polymorphism},
  volume = {93},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/0890540191900535},
  doi = {10.1016/0890-5401(91)90053-5},
  abstract = {We consider predicative type-abstraction disciplines based on type quantification with finitely stratified levels. These lie in the vast middle ground between quantifier-free parametric abstraction and full impredicative abstraction. Stratified polymorphism has an unproblematic set-theoretic semantics, and may lend itself to new approaches to type inference, without sacrificing useful expressive power. Our main technical result is that the functions representable in the finitely stratified polymorphic λ-calculus are precisely the super-elementary functions, i.e., the class ε4 in Grzegorczyk's subrecursive hierarchy. This implies that there is no super-elementary bound on the length of optimal normalization sequences, and that the equality problem for finitely stratified polymorphic λ-expressions is not super-elementary. We also observe that finitely stratified polymorphism augmented with type recursion admits functional algorithms that are not typable in the full second order λ-calculus.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  series = {Selections from 1989 IEEE Symposium on Logic in Computer Science},
  urldate = {2018-09-30},
  date = {1991-07-01},
  pages = {93-113},
  author = {Leivant, Daniel},
  file = {/Users/pgiarrusso/Zotero/storage/VBACA4TB/Leivant - 1991 - Finitely stratified polymorphism.pdf;/Users/pgiarrusso/Zotero/storage/9YXJ2QHC/0890540191900535.html}
}

@inproceedings{Person2008Differential,
  location = {{New York, NY, USA}},
  title = {Differential Symbolic Execution},
  isbn = {978-1-59593-995-1},
  url = {http://doi.acm.org/10.1145/1453101.1453131},
  doi = {10.1145/1453101.1453131},
  abstract = {Detecting and characterizing the effects of software changes is a fundamental component of software maintenance. Version differencing information can be used to perform version merging, infer change characteristics, produce program documentation, and guide program re-validation. Existing techniques for characterizing code changes, however, are imprecise leading to unnecessary maintenance efforts. In this paper, we introduce a novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change. This technique, which we call differential symbolic execution (DSE), exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results. We define the foundational concepts of DSE, describe cost-effective tool support for DSE, and illustrate its potential benefit through an exploratory study that considers version histories of two Java code bases.},
  booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  series = {SIGSOFT '08/FSE-16},
  publisher = {{ACM}},
  urldate = {2018-09-30},
  date = {2008},
  pages = {226--237},
  keywords = {program differencing,software evolution,symbolic execution},
  author = {Person, Suzette and Dwyer, Matthew B. and Elbaum, Sebastian and Pǎsǎreanu, Corina S.},
  file = {/Users/pgiarrusso/Zotero/storage/X2CS2MPX/Person-Dwyer-Elbaum-Pǎsǎreanu - 2008 - Differential Symbolic Execution.pdf}
}

@article{Wang2018Demystifying,
  langid = {english},
  title = {Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator},
  url = {https://arxiv.org/abs/1803.10228},
  shorttitle = {Demystifying Differentiable Programming},
  urldate = {2018-09-30},
  date = {2018-03-27},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Wang, Fei and Wu, Xilun and Essertel, Gregory and Decker, James and Rompf, Tiark},
  file = {/Users/pgiarrusso/Zotero/storage/GFGJVN6Z/Wang-Wu-Essertel-Decker-Rompf - 2018 - Demystifying Differentiable Programming - Shift-Reset the Penultimate Backpropagator.pdf;/Users/pgiarrusso/Zotero/storage/RACZVCUP/Wang-Wu-Essertel-Decker-Rompf - 2018 - Demystifying Differentiable Programming - Shift-Reset the Penultimate Backpropagator.pdf;/Users/pgiarrusso/Zotero/storage/GPIBTZ59/1803.html;/Users/pgiarrusso/Zotero/storage/YXSB34ND/1803.html}
}
% == BibLateX quality report for Wang2018Demystifying:
% Missing required field 'journaltitle'

@inproceedings{Dunfield2003Type,
  langid = {english},
  title = {Type Assignment for Intersections and Unions in Call-by-Value Languages},
  isbn = {978-3-540-36576-1},
  abstract = {We develop a system of type assignment with intersection types, union types, indexed types, and universal and existential dependent types that is sound in a call-by-value functional language. The combination of logical and computational principles underlying our formulation naturally leads to the central idea of type-checking subterms in evaluation order. We thereby provide a uniform generalization and explanation of several earlier isolated systems. The proof of progress and type preservation, usually formulated for closed terms only, relies on a notion of definite substitution.},
  booktitle = {Foundations of Software Science and Computation Structures},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2003},
  pages = {250-266},
  keywords = {Dependent Type,Intersection Type,Property Type,Typing Rule,Union Type},
  author = {Dunfield, Joshua and Pfenning, Frank},
  editor = {Gordon, Andrew D.},
  file = {/Users/pgiarrusso/Zotero/storage/F6WTZLZ4/Dunfield-Pfenning - 2003 - Type Assignment for Intersections and Unions in Call-by-Value Languages.pdf}
}
% == BibLateX quality report for Dunfield2003Type:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Danvy2005There,
  title = {There and Back Again},
  volume = {66},
  issn = {0169-2968},
  url = {http://dl.acm.org/citation.cfm?id=1227189.1227194},
  abstract = {We present a programming pattern where a recursive function defined over a data structure traverses another data structure at return time. The idea is that the recursive calls get us 'there' by traversing the first data structure and the returns get us 'back again' while traversing the second data structure. We name this programming pattern of traversing a data structure at call time and another data structure at return time "There And Back Again" (TABA). The TABA pattern directly applies to computing symbolic convolutions and to multiplying polynomials. It also blends well with other programming patterns such as dynamic programming and traversing a list at double speed. We illustrate TABA and dynamic programming with Catalan numbers. We illustrate TABA and traversing a list at double speed with palindromes and we obtain a novel solution to this traditional exercise. Finally, through a variety of tree traversals, we show how to apply TABA to other data structures than lists. A TABA-based function written in direct style makes full use of an ALGOL-like control stack and needs no heap allocation. Conversely, in a TABA-based function written in continuation-passing style and recursively defined over a data structure (traversed at call time), the continuation acts as an iterator over a second data structure (traversed at return time). In general, the TABA pattern saves one from accumulating intermediate data structures at call time.},
  number = {4},
  journaltitle = {Fundam. Inf.},
  urldate = {2018-10-02},
  date = {2005-01},
  pages = {397--413},
  keywords = {continuations,defunctionalization,Recursive programming,TABA},
  author = {Danvy, Olivier and Goldberg, Mayer},
  file = {/Users/pgiarrusso/Zotero/storage/I5R6N6GI/Danvy-Goldberg - 2005 - There and Back Again.pdf}
}
% == BibLateX quality report for Danvy2005There:
% ? Possibly abbreviated journal title Fundam. Inf.

@inproceedings{Fu2014Self,
  langid = {english},
  title = {Self Types for Dependently Typed Lambda Encodings},
  isbn = {978-3-319-08918-8},
  abstract = {We revisit lambda encodings of data, proposing new solutions to several old problems, in particular dependent elimination with lambda encodings. We start with a type-assignment form of the Calculus of Constructions, restricted recursive definitions and Miquel’s implicit product. We add a type construct ιx.T, called a self type, which allows T to refer to the subject of typing. We show how the resulting System S with this novel form of dependency supports dependent elimination with lambda encodings, including induction principles. Strong normalization of S is established by defining an erasure from S to a version of F ω with positive recursive type definitions, which we analyze. We also prove type preservation for S.},
  booktitle = {Rewriting and Typed Lambda Calculi},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer International Publishing}},
  date = {2014},
  pages = {224-239},
  keywords = {Strong Normalization,Induction Principle,Lambda Calculus,Type Construct,Type Theory},
  author = {Fu, Peng and Stump, Aaron},
  editor = {Dowek, Gilles},
  file = {/Users/pgiarrusso/Zotero/storage/S38UWIXX/Fu-Stump - 2014 - Self Types for Dependently Typed Lambda Encodings.pdf}
}
% == BibLateX quality report for Fu2014Self:
% 'isbn': not a valid ISBN
% ? Unsure about the formatting of the booktitle

@article{Stump2018realizability,
  title = {From realizability to induction via dependent intersection},
  volume = {169},
  issn = {0168-0072},
  url = {http://www.sciencedirect.com/science/article/pii/S0168007218300277},
  doi = {10.1016/j.apal.2018.03.002},
  abstract = {In this paper, it is shown that induction is derivable in a type-assignment formulation of the second-order dependent type theory λP2, extended with the implicit product type of Miquel, dependent intersection type of Kopylov, and a built-in equality type. The crucial idea is to use dependent intersections to internalize a result of Leivant's showing that Church-encoded data may be seen as realizing their own type correctness statements, under the Curry–Howard isomorphism.},
  number = {7},
  journaltitle = {Annals of Pure and Applied Logic},
  shortjournal = {Annals of Pure and Applied Logic},
  urldate = {2018-10-03},
  date = {2018-07-01},
  pages = {637-655},
  keywords = {Derivable induction,Extrinsic typing,Internalized realizability,Lambda encodings},
  author = {Stump, Aaron},
  file = {/Users/pgiarrusso/Zotero/storage/U7THQPC8/Stump - 2018 - From realizability to induction via dependent intersection.pdf;/Users/pgiarrusso/Zotero/storage/RYAMFREB/S0168007218300277.html}
}

@inproceedings{McBride2004Functional,
  location = {{New York, NY, USA}},
  title = {Functional Pearl: I Am Not a Number–i Am a Free Variable},
  isbn = {978-1-58113-850-4},
  url = {http://doi.acm.org/10.1145/1017472.1017477},
  doi = {10.1145/1017472.1017477},
  shorttitle = {Functional Pearl},
  abstract = {In this paper, we show how to manipulate syntax with binding using a mixed representation of names for free variables (with respect to the task in hand) and de Bruijn indices [5] for bound variables. By doing so, we retain the advantages of both representations: naming supports easy, arithmetic-free manipulation of terms; de Bruijn indices eliminate the need for α-conversion. Further, we have ensured that not only the user but also the implementation need never deal with de Bruijn indices, except within key basic operations.Moreover, we give a hierarchical representation for names which naturally reflects the structure of the operations we implement. Name choice is safe and straightforward. Our technology combines easily with an approach to syntax manipulation inspired by Huet's 'zippers'[10].Without the ideas in this paper, we would have struggled to implement EPIGRAM [19]. Our example-constructing inductive elimination operators for datatype families-is but one of many where it proves invaluable.},
  booktitle = {Proceedings of the 2004 ACM SIGPLAN Workshop on Haskell},
  series = {Haskell '04},
  publisher = {{ACM}},
  urldate = {2018-10-04},
  date = {2004},
  pages = {1--9},
  keywords = {abstract syntax,haskell,bound variables,de Bruijn representation,free variables,fresh names,implementing epigram,induction principles},
  author = {McBride, Conor and McKinna, James},
  file = {/Users/pgiarrusso/Zotero/storage/EQRXWQAA/McBride-McKinna - 2004 - Functional Pearl - I Am Not a Number–i Am a Free Variable.pdf}
}

@article{Lafont1997Interaction,
  title = {Interaction Combinators},
  volume = {137},
  issn = {0890-5401},
  url = {http://www.sciencedirect.com/science/article/pii/S0890540197926432},
  doi = {10.1006/inco.1997.2643},
  abstract = {It is shown that a very simple system ofinteraction combinators, with only three symbols and six rules, is a universal model of distributed computation, in a sense that will be made precise. This paper is the continuation of the author's work oninteraction nets, inspired by Girard's proof nets forlinear logic, but no preliminary knowledge of these topics is required for its reading.},
  number = {1},
  journaltitle = {Information and Computation},
  shortjournal = {Information and Computation},
  urldate = {2018-10-06},
  date = {1997-08-25},
  pages = {69-101},
  author = {Lafont, Yves},
  file = {/Users/pgiarrusso/Zotero/storage/XZ8MZX7J/Lafont - 1997 - Interaction Combinators.pdf;/Users/pgiarrusso/Zotero/storage/QMHN4F23/S0890540197926432.html}
}

@article{Putnam1980Models,
  langid = {english},
  title = {Models and reality},
  volume = {45},
  issn = {0022-4812, 1943-5886},
  url = {https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/models-and-reality1/658467661C4423D118E9D5130322B2D3},
  doi = {10.2307/2273415},
  abstract = {In 1922 Skolem delivered an address before the Fifth Congress of Scandinavian Mathematicians in which he pointed out what he called a “relativity of set-theoretic notions”. This “relativity” has frequently been regarded as paradoxical; but today, although one hears the expression “the Löwenheim-Skolem Paradox”, it seems to be thought of as only an apparent paradox, something the cognoscenti enjoy but are not seriously troubled by. Thus van Heijenoort writes, “The existence of such a ‘relativity’ is sometimes referred to as the Löwenheim-Skolem Paradox. But, of course, it is not a paradox in the sense of an antinomy; it is a novel and unexpected feature of formal systems.” In this address I want to take up Skolem's arguments, not with the aim of refuting them but with the aim of extending them in somewhat the direction he seemed to be indicating. It is not my claim that the “Löwenheim-Skolem Paradox” is an antinomy in formal logic; but I shall argue that it is an antinomy, or something close to it, in philosophy of language. Moreover, I shall argue that the resolution of the antinomy—the only resolution that I myself can see as making sense—has profound implications for the great metaphysical dispute about realism which has always been the central dispute in the philosophy of language.The structure of my argument will be as follows: I shall point out that in many different areas there are three main positions on reference and truth: there is the extreme Platonist position, which posits nonnatural mental powers of directly “grasping” forms (it is characteristic of this position that “understanding” or “grasping” is itself an irreducible and unexplicated notion); there is the verificationist position which replaces the classical notion of truth with the notion of verification or proof, at least when it comes to describing how the language is understood; and there is the moderate realist position which seeks to preserve the centrality of the classical notions of truth and reference without postulating nonnatural mental powers.},
  number = {3},
  journaltitle = {The Journal of Symbolic Logic},
  urldate = {2018-10-28},
  date = {1980-09},
  pages = {464-482},
  author = {Putnam, Hilary},
  file = {/Users/pgiarrusso/Zotero/storage/VEIRQGIA/Putnam - 1980 - Models and reality.pdf;/Users/pgiarrusso/Zotero/storage/659SGEB7/658467661C4423D118E9D5130322B2D3.html}
}
% == BibLateX quality report for Putnam1980Models:
% 'issn': not a valid ISSN

@inproceedings{Krishnaswami2012Higherorder,
  location = {{New York, NY, USA}},
  title = {Higher-order Functional Reactive Programming in Bounded Space},
  isbn = {978-1-4503-1083-3},
  url = {http://doi.acm.org/10.1145/2103656.2103665},
  doi = {10.1145/2103656.2103665},
  abstract = {Functional reactive programming (FRP) is an elegant and successful approach to programming reactive systems declaratively. The high levels of abstraction and expressivity that make FRP attractive as a programming model do, however, often lead to programs whose resource usage is excessive and hard to predict. In this paper, we address the problem of space leaks in discrete-time functional reactive programs. We present a functional reactive programming language that statically bounds the size of the dataflow graph a reactive program creates, while still permitting use of higher-order functions and higher-type streams such as streams of streams. We achieve this with a novel linear type theory that both controls allocation and ensures that all recursive definitions are well-founded. We also give a denotational semantics for our language by combining recent work on metric spaces for the interpretation of higher-order causal functions with length-space models of space-bounded computation. The resulting category is doubly closed and hence forms a model of the logic of bunched implications.},
  booktitle = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '12},
  publisher = {{ACM}},
  urldate = {2018-10-28},
  date = {2012},
  pages = {45--58},
  keywords = {functional reactive programming,_tablet,linear logic,dataflow,bunched implications,space-bounded computation},
  author = {Krishnaswami, Neelakantan R. and Benton, Nick and Hoffmann, Jan},
  file = {/Users/pgiarrusso/Zotero/storage/W2EWNLPZ/Krishnaswami-Benton-Hoffmann - 2012 - Higher-order Functional Reactive Programming in Bounded Space.pdf}
}

@report{Gonthier2015Small,
  langid = {english},
  title = {A Small Scale Reflection Extension for the Coq system},
  url = {https://hal.inria.fr/inria-00258384/document},
  abstract = {This is the user manual of Ssreflect, a set of extensions to the proof scripting language of the Coq proof assistant. While these extensions were developed to support a particular proof methodology - small-scale reflection - most of them actually are of a quite general nature, improving the functionality of Coq in basic areas such as script layout and structuring, proof context management, and rewriting. Consequently, and in spite of the title of this document, most of the extensions described here should be of interest for all Coq users, whether they embrace small-scale reflection or not.},
  institution = {{Inria Saclay Ile de France}},
  type = {report},
  urldate = {2018-11-12},
  date = {2015},
  author = {Gonthier, Georges and Mahboubi, Assia and Tassi, Enrico},
  file = {/Users/pgiarrusso/Zotero/storage/RCJKZI4U/Gonthier-Mahboubi-Tassi - 2015 - A Small Scale Reflection Extension for the Coq system.pdf;/Users/pgiarrusso/Zotero/storage/8VFFJGLW/en.html}
}

@inproceedings{Montagu2009Modeling,
  location = {{New York, NY, USA}},
  title = {Modeling Abstract Types in Modules with Open Existential Types},
  isbn = {978-1-60558-379-2},
  url = {http://doi.acm.org/10.1145/1480881.1480926},
  doi = {10.1145/1480881.1480926},
  abstract = {We propose F-zip, a calculus of open existential types that is an extension of System F obtained by decomposing the introduction and elimination of existential types into more atomic constructs. Open existential types model modular type abstraction as done in module systems. The static semantics of F-zip adapts standard techniques to deal with linearity of typing contexts, its dynamic semantics is a small-step reduction semantics that performs extrusion of type abstraction as needed during reduction, and the two are related by subject reduction and progress lemmas. Applying the Curry-Howard isomorphism, F-zip can be also read back as a logic with the same expressive power as second-order logic but with more modular ways of assembling partial proofs. We also extend the core calculus to handle the double vision problem as well as type-level and term-level recursion. The resulting language turns out to be a new formalization of (a minor variant of) Dreyer's internal language for recursive and mixin modules.},
  booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '09},
  publisher = {{ACM}},
  urldate = {2018-11-14},
  date = {2009},
  pages = {354--365},
  keywords = {abstract types,existential types,generativity,lambda-calculus,linear type systems,modularity,modules,type systems},
  author = {Montagu, Benoît and Rémy, Didier},
  file = {/Users/pgiarrusso/Zotero/storage/ZZCMZH8A/Montagu-Rémy - 2009 - Modeling Abstract Types in Modules with Open Existential Types.pdf}
}

@article{Drossopoulou1999Java,
  langid = {english},
  title = {Is the Java type system sound?},
  volume = {5},
  issn = {1096-9942},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9942%28199901/03%295%3A1%3C3%3A%3AAID-TAPO2%3E3.0.CO%3B2-T},
  doi = {10.1002/(SICI)1096-9942(199901/03)5:1<3::AID-TAPO2>3.0.CO;2-T},
  abstract = {A proof of the soundness of the Java type system is a first, necessary step towards demonstrating which Java programs won't compromise computer security. We consider a subset of Java describing primitive types, classes, inheritance, instance variables and methods, interfaces, shadowing, dynamic method binding, object creation, null, arrays, and exception throwing and handling. We argue that for this subset the type system is sound, by proving that program execution preserves the types, up to subclasses/subinterfaces. © 1999 John Wiley \& Sons, Inc.},
  number = {1},
  journaltitle = {Theory and Practice of Object Systems},
  urldate = {2018-11-21},
  date = {1999-01-01},
  pages = {3-24},
  author = {Drossopoulou, Sophia and Eisenbach, Susan and Khurshid, Sarfraz},
  file = {/Users/pgiarrusso/Zotero/storage/Y3N8KSWS/Drossopoulou-Eisenbach-Khurshid - 1999 - Is the Java type system sound.pdf;/Users/pgiarrusso/Zotero/storage/FNKCYA49/03)513AID-TAPO23.0.html}
}

@inproceedings{Nipkow1998Javalight,
  location = {{New York, NY, USA}},
  title = {Javalight is Type-safe—Definitely},
  isbn = {978-0-89791-979-1},
  url = {http://doi.acm.org/10.1145/268946.268960},
  doi = {10.1145/268946.268960},
  abstract = {Javalight is a large sequential sublanguage of Java. We formalize its abstract syntax, type system, well-formedness conditions, and an operational evaluation semantics. Based on this formalization, we can express and prove type soundness. All definitions and proofs have been done formally in the theorem prover Isabelle/HOL. Thus this paper demonstrates that machine-checking the design of non-trivial programming languages has become a reality.},
  booktitle = {Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '98},
  publisher = {{ACM}},
  urldate = {2018-11-22},
  date = {1998},
  pages = {161--170},
  author = {Nipkow, Tobias and von Oheimb, David},
  options = {useprefix=true},
  file = {/Users/pgiarrusso/Zotero/storage/B77CR62E/Nipkow-von Oheimb - 1998 - Javalight is Type-safe—Definitely.pdf}
}

@inproceedings{Stucki2018Truly,
  location = {{New York, NY, USA}},
  title = {Truly Abstract Interfaces for Algebraic Data Types: The Extractor Typing Problem},
  isbn = {978-1-4503-5836-1},
  url = {http://doi.acm.org/10.1145/3241653.3241658},
  doi = {10.1145/3241653.3241658},
  shorttitle = {Truly Abstract Interfaces for Algebraic Data Types},
  abstract = {Pattern matching enables inspecting algebraic data types, but typically prevents hiding the implementation of the matched algebraic data type. In Scala, instead, extractors also allow pattern matching on non-algebraic data types and invoking methods on the obtained objects, while partially decoupling API consumers from the API implementation.  But as we show in this paper, pattern matching using extractors is restricted compared to matching against case classes. We argue this violates the appropriate variant of the uniform access principle. To address this problem, we propose a small language extension, which enables defining truly abstract interfaces and freely evolve their implementation.},
  booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
  series = {Scala 2018},
  publisher = {{ACM}},
  urldate = {2018-11-26},
  date = {2018},
  pages = {56--60},
  keywords = {abstract types,extractors,interfaces,pattern matching,Scala},
  author = {Stucki, Nicolas and Giarrusso, Paolo G. and Odersky, Martin},
  file = {/Users/pgiarrusso/Zotero/storage/G3BRBK86/Stucki-Giarrusso-Odersky - 2018 - Truly Abstract Interfaces for Algebraic Data Types - The Extractor Typing Problem.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

