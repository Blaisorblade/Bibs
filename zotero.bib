
@book{1994pepm94,
  title = {{{PEPM}}'94 - {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Semantics}}-{{Based Program Manipulation}}, {{Walt Disney World Vilage}}, {{Orlando}}, {{Florida}}, {{USA}}, 25 {{June}} 1994, {{Proceedings}}. {{Technical Report}} 94/9},
  year = {1994},
  publisher = {{University of Melbourne, Australia, Department of Computer Science}},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{Abadi1990model,
  title = {A {{PER}} Model of Polymorphism and Recursive Types},
  booktitle = {[1990] {{Proceedings}}. {{Fifth Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Abadi, M. and Plotkin, G. D.},
  year = {1990},
  month = jun,
  pages = {355--365},
  doi = {10.1109/LICS.1990.113761},
  abstract = {A model of Reynold's polymorphic lambda calculus is provided, which also allows the recursive definition of elements and types. The techniques uses a good class of partial equivalence relations (PERs) over a certain CPO. This allows the combination of inverse-limits for recursion and intersection for polymorphism},
  keywords = {Buildings,Computer science,CPO,Equations,equivalence classes,formal languages,intersection,inverse-limits,Logic,O-category,partial equivalence relations,partial orders,PER model,Polymorphic lambda calculus,polymorphism,recursion,recursive definition,recursive functions,recursive types}
}

@article{Abadi1991Explicit,
  title = {Explicit Substitutions},
  author = {Abadi, M. and Cardelli, L. and Curien, P.-L. and L{\'e}vy, J.-J.},
  year = {1991},
  month = oct,
  volume = {1},
  pages = {375--416},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796800000186},
  abstract = {AbstractThe {$\lambda\sigma$}-calculus is a refinement of the {$\lambda$}-calculus where substitutions are manipulated explicitly. The {$\lambda\sigma$}-calculus provides a setting for studying the theory of substitutions, with pleasant mathematical properties. It is also a useful bridge between the classical {$\lambda$}-calculus and concrete implementations.},
  journal = {Journal of Functional Programming},
  number = {4}
}

@inproceedings{Abadi1996syntactic,
  title = {Syntactic {{Considerations}} on {{Recursive Types}}},
  booktitle = {Proceedings of the 11th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Abadi, Martin and Fiore, Marcelo P.},
  year = {1996},
  pages = {242--252},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  abstract = {We study recursive types from a syntactic perspective. In particular, we compare the formulations of recursive types that are used in programming languages and formal systems. Our main tool is a new syntactic explanation of type expressions as functors. We also introduce a simple logic for programs with recursive types in which we carry out our proofs.},
  isbn = {0-8186-7463-6},
  series = {{{LICS}} '96}
}

@techreport{Abal40,
  title = {40 {{Variability Bugs}} in the {{Linux Kernel}} \textemdash{} {{A Qualitative Study}}},
  author = {Abal, Iago and Braband, Claus and W{\k{a}}sowski, Andrzej}
}

@incollection{Abbott2003categories,
  title = {Categories of {{Containers}}},
  booktitle = {Foundations of {{Software Science}} and {{Computation Structures}}},
  author = {Abbott, Michael and Altenkirch, Thorsten and Ghani, Neil},
  editor = {Gordon, Andrew D.},
  year = {2003},
  month = jan,
  pages = {23--38},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We introduce the notion of containers as a mathematical formalisation of the idea that many important datatypes consist of templates where data is stored. We show that containers have good closure properties under a variety of constructions including the formation of initial algebras and final coalgebras. We also show that containers include strictly positive types and shapely types but that there are containers which do not correspond to either of these. Further, we derive a representation result classifying the nature of polymorphic functions between containers. We finish this paper with an application to the theory of shapely types and refer to a forthcoming paper which applies this theory to differentiable types.},
  copyright = {\textcopyright{}2003 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-00897-2 978-3-540-36576-1},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {2620},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Abbott2003derivatives,
  title = {Derivatives of {{Containers}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Abbott, Michael and Altenkirch, Thorsten and Ghani, Neil and McBride, Conor},
  editor = {Hofmann, Martin},
  year = {2003},
  month = jan,
  pages = {16--30},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We are investigating McBride's idea that the type of one-hole contexts are the formal derivative of a functor from a categorical perspective. Exploiting our recent work on containers we are able to characterise derivatives by a universal property and show that the laws of calculus including a rule for initial algebras as presented by McBride hold \textemdash{} hence the differentiable containers include those generated by polynomials and least fixpoints. Finally, we discuss abstract containers (i.e. quotients of containers) \textemdash{} this includes a container which plays the role of e x in calculus by being its own derivative.},
  copyright = {\textcopyright{}2003 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-40332-6 978-3-540-44904-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {2701},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Abbott2005data,
  title = {{$\partial$} for {{Data}}: {{Differentiating Data Structures}}},
  shorttitle = {{$\partial$} for {{Data}}},
  author = {Abbott, Michael and Altenkirch, Thorsten and McBride, Conor and Ghani, Neil},
  year = {2005},
  month = jan,
  volume = {65},
  pages = {1--28},
  abstract = {This paper and our conference paper (Abbott, Altenkirch, Ghani, and McBride, 2003b) explain and analyse the notion of the derivative of a data structure as the type of its one-hole contexts based on the central observation made by McBride (2001). To make the idea precise we need a generic notion of a data type, which leads to the notion of a container, introduced in (Abbott, Altenkirch, and Ghani, 2003a) and investigated extensively in (Abbott, 2003). Using containers we can provide a notion of linear map which is the concept missing from McBride's first analysis. We verify the usual laws of differential calculus including the chain rule and establish laws for initial algebras and terminal coalgebras.},
  journal = {Fundamenta Informaticae},
  number = {1}
}

@book{Abel1998foetus,
  title = {Foetus \textendash{} {{Termination Checker}} for {{Simple Functional Programs}}. Www.Tcs.Informatik. Uni-Muenchen.de/ Abel/Foetus},
  author = {Abel, Andreas},
  year = {1998},
  abstract = {We introduce a simple functional language foetus (lambda calculus with tuples, constructors and pattern matching) supplied with a termination checker. This checker tries to find a well-founded structural order on the parameters on the given function to prove termination. The components of the check algorithm are: function call extraction out of the program text, call graph completion and finding a lexical order for the function parameters. The HTML version of this paper contains many ready-to-run Web-based examples. 1}
}

@inproceedings{Abel2007Normalization,
  title = {Normalization by {{Evaluation}} for {{Martin}}-{{L{\"o}f Type Theory}} with {{Typed Equality Judgements}}},
  booktitle = {22nd {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}} ({{LICS}} 2007)},
  author = {Abel, A. and Coquand, T. and Dybjer, P.},
  year = {2007},
  month = jul,
  pages = {3--12},
  doi = {10.1109/LICS.2007.33},
  abstract = {The decidability of equality is proved for Martin-L{\"o}f type theory with a universe {\'a} la Russell and typed beta-eta- equality judgements. A corollary of this result is that the constructor for dependent function types is injective, a property which is crucial for establishing the correctness of the type-checking algorithm. The decision procedure uses normalization by evaluation, an algorithm which first interprets terms in a domain with untyped semantic elements and then extracts normal forms. The correctness of this algorithm is established using a PER-model and a logical relation between syntax and semantics.},
  keywords = {Application software,beta-eta-equality judgements,Buildings,Calculus,Computer science,Decision feedback equalizers,dependent function types,Logic,logical relation,Martin-Lof type theory,Mathematical model,PER-model,semantic elements,semantic networks,type theory,type-checking algorithm}
}

@article{Abel2007Normalizationa,
  title = {Normalization by {{Evaluation}} for {{Martin}}-{{L{\"o}f Type Theory}} with {{One Universe}}},
  author = {Abel, Andreas and Aehlig, Klaus and Dybjer, Peter},
  year = {2007},
  month = apr,
  volume = {173},
  pages = {17--39},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2007.02.025},
  abstract = {We present an algorithm for computing normal terms and types in Martin-L{\"o}f type theory with one universe and eta-conversion. We prove that two terms or types are equal in the theory iff the normal forms are identical (as de Bruijn terms). It thus follows that our algorithm can be used for deciding equality in Martin-L{\"o}f type theory. The algorithm uses the technique of normalization by evaluation; normal forms are computed by first evaluating terms and types in a suitable model. The normal forms are then extracted from the semantic elements. We prove its completeness by a PER model and its soundness by a Kripke logical relation.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Dependent Types,Domain Semantics,Normalization by Evaluation,Type Theory,Universe},
  series = {Proceedings of the 23rd {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXIII}})}
}

@inproceedings{Abel2009Typed,
  title = {Typed {{Applicative Structures}} and {{Normalization}} by {{Evaluation}} for {{System~F$\omega$}}},
  booktitle = {Computer {{Science Logic}}},
  author = {Abel, Andreas},
  editor = {Gr{\"a}del, Erich and Kahle, Reinhard},
  year = {2009},
  pages = {40--54},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present a normalization-by-evaluation (NbE) algorithm for System F {$\omega$} with {$\beta\eta$}-equality, the simplest impredicative type theory with computation on the type level. Values are kept abstract and requirements on values are kept to a minimum, allowing many different implementations of the algorithm. The algorithm is verified through a general model construction using typed applicative structures, called type and object structures. Both soundness and completeness of NbE are conceived as an instance of a single fundamental theorem.},
  isbn = {978-3-642-04027-6},
  keywords = {Fundamental Theorem,Object Structure,Semantic Type,Type Constructor,Type Structure},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Abel2014Formalized,
  title = {A {{Formalized Proof}} of {{Strong Normalization}} for {{Guarded Recursive Types}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Abel, Andreas and Vezzosi, Andrea},
  year = {2014},
  month = nov,
  pages = {140--158},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-12736-1_8},
  abstract = {We consider a simplified version of Nakano's guarded fixed-point types in a representation by infinite type expressions, defined coinductively. Smallstep reduction is parametrized by a natural number ``depth'' that expresses under how many guards we may step during evaluation. We prove that reduction is strongly normalizing for any depth. The proof involves a typed inductive notion of strong normalization and a Kripke model of types in two dimensions: depth and typing context. Our results have been formalized in Agda and serve as a case study of reasoning about a language with coinductive type expressions.},
  isbn = {978-3-319-12735-4 978-3-319-12736-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Abel2014Normalization,
  title = {Normalization by {{Evaluation}} in the {{Delay Monad}}: {{A Case Study}} for {{Coinduction}} via {{Copatterns}} and {{Sized Types}}},
  shorttitle = {Normalization by {{Evaluation}} in the {{Delay Monad}}},
  author = {Abel, Andreas and Chapman, James},
  year = {2014},
  month = jun,
  volume = {153},
  pages = {51--67},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.153.4},
  abstract = {In this paper, we present an Agda formalization of a normalizer for simply-typed lambda terms. The normalizer consists of two coinductively defined functions in the delay monad: One is a standard evaluator of lambda terms to closures, the other a type-directed reifier from values to eta-long beta-normal forms. Their composition, normalization-by-evaluation, is shown to be a total function a posteriori, using a standard logical-relations argument. The successful formalization serves as a proof-of-concept for coinductive programming and reasoning using sized types and copatterns, a new and presently experimental feature of Agda.},
  archivePrefix = {arXiv},
  eprint = {1406.2059},
  eprinttype = {arxiv},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,D.3.3,F.3.2,F.3.3,F.4.1}
}

@article{Abel2017Decidability,
  title = {Decidability of {{Conversion}} for {{Type Theory}} in {{Type Theory}}},
  author = {Abel, Andreas and {\"O}hman, Joakim and Vezzosi, Andrea},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {23:1--23:29},
  issn = {2475-1421},
  doi = {10.1145/3158111},
  abstract = {Type theory should be able to handle its own meta-theory, both to justify its foundational claims and to obtain a verified implementation. At the core of a type checker for intensional type theory lies an algorithm to check equality of types, or in other words, to check whether two types are convertible. We have formalized in Agda a practical conversion checking algorithm for a dependent type theory with one universe {\`a} la Russell, natural numbers, and {$\eta$}-equality for {$\Pi$} types. We prove the algorithm correct via a Kripke logical relation parameterized by a suitable notion of equivalence of terms. We then instantiate the parameterized fundamental lemma twice: once to obtain canonicity and injectivity of type formers, and once again to prove the completeness of the algorithm. Our proof relies on inductive-recursive definitions, but not on the uniqueness of identity proofs. Thus, it is valid in variants of intensional Martin-L{\"o}f Type Theory as long as they support induction-recursion, for instance, Extensional, Observational, or Homotopy Type Theory.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Agda,Dependent types,Formalization,Logical relations},
  number = {POPL}
}

@article{Abel2017Normalization,
  title = {Normalization by {{Evaluation}} for {{Sized Dependent Types}}},
  author = {Abel, Andreas and Vezzosi, Andrea and Winterhalter, Theo},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {33:1--33:30},
  issn = {2475-1421},
  doi = {10.1145/3110277},
  abstract = {Sized types have been developed to make termination checking more perspicuous, more powerful, and more modular by integrating termination into type checking. In dependently-typed proof assistants where proofs by induction are just recursive functional programs, the termination checker is an integral component of the trusted core, as validity of proofs depend on termination. However, a rigorous integration of full-fledged sized types into dependent type theory is lacking so far. Such an integration is non-trivial, as explicit sizes in proof terms might get in the way of equality checking, making terms appear distinct that should have the same semantics. In this article, we integrate dependent types and sized types with higher-rank size polymorphism, which is essential for generic programming and abstraction. We introduce a size quantifier {\^a} which lets us ignore sizes in terms for equality checking, alongside with a second quantifier {\^I} for abstracting over sizes that do affect the semantics of types and terms. Judgmental equality is decided by an adaptation of normalization-by-evaluation for our new type theory, which features type shape-directed reflection and reification. It follows that subtyping and type checking of normal forms are decidable as well, the latter by a bidirectional algorithm.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {dependent types,eta-equality,normalization-by-evaluation,proof irrelevance,sized types,subtyping,universes},
  number = {ICFP}
}

@article{Abel2019Normalization,
  title = {Normalization by Evaluation for Call-by-Push-Value and Polarized Lambda-Calculus},
  author = {Abel, Andreas and Sattler, Christian},
  year = {2019},
  month = feb,
  abstract = {We observe that normalization by evaluation for simply-typed lambda-calculus with weak coproducts can be carried out in a weak bi-cartesian closed category of presheaves equipped with a monad that allows us to perform case distinction on neutral terms of sum type. The placement of the monad influences the normal forms we obtain: for instance, placing the monad on coproducts gives us eta-long beta-pi normal forms where pi refers to permutation of case distinctions out of elimination positions. We further observe that placing the monad on every coproduct is rather wasteful, and an optimal placement of the monad can be determined by considering polarized simple types inspired by focalization. Polarization classifies types into positive and negative, and it is sufficient to place the monad at the embedding of positive types into negative ones. We consider two calculi based on polarized types: pure call-by-push-value (CBPV) and polarized lambda-calculus, the natural deduction calculus corresponding to focalized sequent calculus. For these two calculi, we present algorithms for normalization by evaluation. We further discuss different implementations of the monad and their relation to existing normalization proofs for lambda-calculus with sums. Our developments have been partially formalized in the Agda proof assistant.},
  archivePrefix = {arXiv},
  eprint = {1902.06097},
  eprinttype = {arxiv},
  journal = {arXiv:1902.06097 [cs, math]},
  keywords = {03F05,Computer Science - Logic in Computer Science,Computer Science - Programming Languages,F.3.2,F.4.1,Mathematics - Logic},
  primaryClass = {cs, math}
}

@article{Abramsky2000Handbook,
  title = {The {{Handbook}} of {{Logic}} in {{Computer Science}}},
  author = {Abramsky, Samson},
  year = {2000}
}

@article{Abramsky2010Introduction,
  title = {Introduction to Categories and Categorical Logic},
  author = {Abramsky, Samson and Tzevelekos, Nikos},
  year = {2010},
  volume = {813},
  pages = {3--94},
  doi = {10.1007/978-3-642-12821-9_1},
  abstract = {The aim of these notes is to provide a succinct, accessible introduction to some of the basic ideas of category theory and categorical logic. The notes are based on a lecture course given at Oxford over the past few years. They contain numerous exercises, and hopefully will prove useful for self-study by those seeking a first introduction to the subject, with fairly minimal prerequisites. The coverage is by no means comprehensive, but should provide a good basis for further study; a guide to further reading is included. The main prerequisite is a basic familiarity with the elements of discrete mathematics: sets, relations and functions. An Appendix contains a summary of what we will need, and it may be useful to review this first. In addition, some prior exposure to abstract algebra - vector spaces and linear maps, or groups and group homomorphisms - would be helpful.},
  archivePrefix = {arXiv},
  eprint = {1102.1313},
  eprinttype = {arxiv},
  journal = {arXiv:1102.1313 [cs, math]},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  primaryClass = {cs, math}
}

@inproceedings{Acar2003selective,
  title = {Selective {{Memoization}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Acar, Umut A. and Blelloch, Guy E. and Harper, Robert},
  year = {2003},
  pages = {14--25},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/604131.604133},
  abstract = {We present a framework for applying memoization selectively. The framework provides programmer control over equality, space usage, and identification of precise dependences so that memoization can be applied according to the needs of an application. Two key properties of the framework are that it is efficient and yields programs whose performance can be analyzed using standard techniques.We describe the framework in the context of a functional language and an implementation as an SML library. The language is based on a modal type system and allows the programmer to express programs that reveal their true data dependences when executed. The SML implementation cannot support this modal type system statically, but instead employs run-time checks to ensure correct usage of primitives.},
  isbn = {1-58113-628-5},
  keywords = {memoization,performance,programmer controlled,selective},
  series = {{{POPL}} '03}
}

@inproceedings{Acar2006experimental,
  title = {An Experimental Analysis of Self-Adjusting Computation},
  booktitle = {Proceedings of the 27th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Acar, Umut A. and Blelloch, Guy E. and Blume, Matthias and Tangwongsan, Kanat},
  year = {2006},
  pages = {96--107},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1133981.1133993},
  abstract = {Dependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.},
  isbn = {978-1-59593-320-1},
  keywords = {computational geometry,dynamic algorithms,dynamic dependence graphs,memorization,performance,self-adjusting computation},
  series = {{{PLDI}} '06}
}

@article{Acar2009experimental,
  title = {An Experimental Analysis of Self-Adjusting Computation},
  author = {Acar, Umut A. and Blelloch, Guy E. and Blume, Matthias and Harper, Robert and Tangwongsan, Kanat},
  year = {2009},
  month = nov,
  volume = {32},
  pages = {3:1--3:53},
  issn = {0164-0925},
  doi = {10.1145/1596527.1596530},
  abstract = {Recent work on adaptive functional programming (AFP) developed techniques for writing programs that can respond to modifications to their data by performing change propagation. To achieve this, executions of programs are represented with dynamic dependence graphs (DDGs) that record data dependences and control dependences in a way that a change-propagation algorithm can update the computation as if the program were from scratch, by re-executing only the parts of the computation affected by the changes. Since change-propagation only re-executes parts of the computation, it can respond to certain incremental modifications asymptotically faster than recomputing from scratch, potentially offering significant speedups. Such asymptotic speedups, however, are rare: for many computations and modifications, change propagation is no faster than recomputing from scratch. In this article, we realize a duality between dynamic dependence graphs and memoization, and combine them to give a change-propagation algorithm that can dramatically increase computation reuse. The key idea is to use DDGs to identify and re-execute the parts of the computation that are affected by modifications, while using memoization to identify the parts of the computation that remain unaffected by the changes. We refer to this approach as self-adjusting computation. Since DDGs are imperative, but (traditional) memoization requires purely functional computation, reusing computation correctly via memoization becomes a challenge. We overcome this challenge with a technique for remembering and reusing not just the results of function calls (as in conventional memoization), but their executions represented with DDGs. We show that the proposed approach is realistic by describing a library for self-adjusting computation, presenting efficient algorithms for realizing the library, and describing and evaluating an implementation. Our experimental evaluation with a variety of applications, ranging from simple list primitives to more sophisticated computational geometry algorithms, shows that the approach is effective in practice: compared to recomputing from-scratch; self-adjusting programs respond to small modifications to their data orders of magnitude faster.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {Computational geometry,dynamic algorithms,dynamic dependence graphs,memoization,performance,self-adjusting computation},
  number = {1}
}

@article{Acar2011selective,
  title = {Selective {{Memoization}}},
  author = {Acar, Umut A. and Blelloch, Guy E. and Harper, Robert},
  year = {2011},
  month = jun,
  abstract = {This paper presents language techniques for applying memoization selectively. The techniques provide programmer control over equality, space usage, and identification of precise dependences so that memoization can be applied according to the needs of an application. Two key properties of the approach are that it accepts and efficient implementation and yields programs whose performance can be analyzed using standard analysis techniques. We describe our approach in the context of a functional language called MFL and an implementation as a Standard ML library. The MFL language employs a modal type system to enable the programmer to express programs that reveal their true data dependences when executed. We prove that the MFL language is sound by showing that that MFL programs yield the same result as they would with respect to a standard, non-memoizing semantics. The SML implementation cannot support the modal type system of MFL statically but instead employs run-time checks to ensure correct usage of primitives.},
  archivePrefix = {arXiv},
  eprint = {1106.0447},
  eprinttype = {arxiv},
  journal = {arXiv:1106.0447 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Acar2013core,
  title = {A Core Calculus for Provenance},
  author = {Acar, Umut A. and Ahmed, Amal and Cheney, James and Perera, Roly},
  year = {2013},
  month = jan,
  volume = {21},
  pages = {919--969},
  doi = {10.3233/JCS-130487},
  abstract = {Provenance is an increasing concern due to the ongoing revolution in sharing and processing scientific data on the Web and in other computer systems. It is proposed that many computer systems will need to become provenance-aware in order to provide satisfactory accountability, reproducibility, and trust for scientific or other high-value data. To date, there is not a consensus concerning appropriate formal models or security properties for provenance. In previous work, we introduced a formal framework for provenance security and proposed formal definitions of properties called disclosure and obfuscation. In this article, we study refined notions of positive and negative disclosure and obfuscation in a concrete setting, that of a general-purpose programing language. Previous models of provenance have focused on special-purpose languages such as workflows and database queries. We consider a higher-order, functional language with sums, products, and recursive types and functions, and equip it with a tracing semantics in which traces themselves can be replayed as computations. We present an annotation-propagation framework that supports many provenance views over traces, including standard forms of provenance studied previously. We investigate some relationships among provenance views and develop some partial solutions to the disclosure and obfuscation problems, including correct algorithms for disclosure and positive obfuscation based on trace slicing.},
  journal = {Journal of Computer Security},
  number = {6}
}

@inproceedings{Acar2015coupling,
  title = {Coupling {{Memory}} and {{Computation}} for {{Locality Management}}},
  booktitle = {1st {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2015)},
  author = {Acar, Umut A. and Blelloch, Guy and Fluet, Matthew and Muller, Stefan K. and Raghunathan, Ram},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  year = {2015},
  volume = {32},
  pages = {1--14},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.1},
  isbn = {978-3-939897-80-4},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-50121}
}

@inproceedings{Acar2017Brief,
  title = {Brief {{Announcement}}: {{Parallel Dynamic Tree Contraction}} via {{Self}}-{{Adjusting Computation}}},
  shorttitle = {Brief {{Announcement}}},
  booktitle = {Proceedings of the 29th {{ACM Symposium}} on {{Parallelism}} in {{Algorithms}} and {{Architectures}}},
  author = {Acar, Umut A. and Aksenov, Vitaly and Westrick, Sam},
  year = {2017},
  pages = {275--277},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3087556.3087595},
  abstract = {Dynamic algorithms are used to compute a property of some data while the data undergoes changes over time. Many dynamic algorithms have been proposed but nearly all are sequential. In this paper, we present our ongoing work on designing a parallel algorithm for the dynamic trees problem, which requires computing a property of a forest as the forest undergoes changes. Our algorithm allows insertion and/or deletion of both vertices and edges anywhere in the input and performs updates in parallel. We obtain our algorithm by applying a dynamization technique called self-adjusting computation to the classic algorithm of Miller and Reif for tree contraction.},
  isbn = {978-1-4503-4593-4},
  keywords = {change propagation,dynamic,parallel,self-adjusting computation,tree contraction},
  series = {{{SPAA}} '17}
}

@inproceedings{Accattoli2014Beta,
  title = {Beta {{Reduction}} Is {{Invariant}}, {{Indeed}}},
  booktitle = {Proceedings of the {{Joint Meeting}} of the {{Twenty}}-{{Third EACSL Annual Conference}} on {{Computer Science Logic}} ({{CSL}}) and the {{Twenty}}-{{Ninth Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}} ({{LICS}})},
  author = {Accattoli, Beniamino and Dal Lago, Ugo},
  year = {2014},
  pages = {8:1--8:10},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2603088.2603105},
  abstract = {Slot and van Emde Boas' weak invariance thesis states that reasonable machines can simulate each other within a polynomially overhead in time. Is {$\lambda$}-calculus a reasonable machine? Is there a way to measure the computational complexity of a {$\lambda$}-term? This paper presents the first complete positive answer to this long-standing problem. Moreover, our answer is completely machine-independent and based over a standard notion in the theory of {$\lambda$}-calculus: the length of a leftmost-outermost derivation to normal form is an invariant cost model. Such a theorem cannot be proved by directly relating {$\lambda$}-calculus with Turing machines or random access machines, because of the size explosion problem: there are terms that in a linear number of steps produce an exponentially long output. The first step towards the solution is to shift to a notion of evaluation for which the length and the size of the output are linearly related. This is done by adopting the linear substitution calculus (LSC), a calculus of explicit substitutions modelled after linear logic proof nets and admitting a decomposition of leftmost-outermost derivations with the desired property. Thus, the LSC is invariant with respect to, say, random access machines. The second step is to show that LSC is invariant with respect to the {$\lambda$}-calculus. The size explosion problem seems to imply that this is not possible: having the same notions of normal form, evaluation in the LSC is exponentially longer than in the {$\lambda$}-calculus. We solve such an impasse by introducing a new form of shared normal form and shared reduction, deemed useful. Useful evaluation avoids those steps that only unshare the output without contributing to {$\beta$}-redexes, i.e. the steps that cause the blow-up in size. The main technical contribution of the paper is indeed the definition of useful reductions and the thorough analysis of their properties.},
  isbn = {978-1-4503-2886-9},
  keywords = {$\\lambda$-calculus,computational complexity,cost models,explicit substitutions,sharing},
  series = {{{CSL}}-{{LICS}} '14}
}

@article{Acharya2014rise,
  title = {Rise of the {{Rest}}: {{The Growing Impact}} of {{Non}}-{{Elite Journals}}},
  shorttitle = {Rise of the {{Rest}}},
  author = {Acharya, Anurag and Verstak, Alex and Suzuki, Helder and Henderson, Sean and Iakhiaev, Mikhail and Lin, Cliff Chiung Yu and Shetty, Namit},
  year = {2014},
  month = oct,
  abstract = {In this paper, we examine the evolution of the impact of non-elite journals. We attempt to answer two questions. First, what fraction of the top-cited articles are published in non-elite journals and how has this changed over time. Second, what fraction of the total citations are to non-elite journals and how has this changed over time. We studied citations to articles published in 1995-2013. We computed the 10 most-cited journals and the 1000 most-cited articles each year for all 261 subject categories in Scholar Metrics. We marked the 10 most-cited journals in a category as the elite journals for the category and the rest as non-elite. There are two conclusions from our study. First, the fraction of top-cited articles published in non-elite journals increased steadily over 1995-2013. While the elite journals still publish a substantial fraction of high-impact articles, many more authors of well-regarded papers in diverse research fields are choosing other venues. The number of top-1000 papers published in non-elite journals for the representative subject category went from 149 in 1995 to 245 in 2013, a growth of 64\%. Looking at broad research areas, 4 out of 9 areas saw at least one-third of the top-cited articles published in non-elite journals in 2013. For 6 out of 9 areas, the fraction of top-cited papers published in non-elite journals for the representative subject category grew by 45\% or more. Second, now that finding and reading relevant articles in non-elite journals is about as easy as finding and reading articles in elite journals, researchers are increasingly building on and citing work published everywhere. Considering citations to all articles, the percentage of citations to articles in non-elite journals went from 27\% in 1995 to 47\% in 2013. Six out of nine broad areas had at least 50\% of citations going to articles published in non-elite journals in 2013.},
  archivePrefix = {arXiv},
  eprint = {1410.2217},
  eprinttype = {arxiv},
  journal = {arXiv:1410.2217 [cs]},
  keywords = {Computer Science - Digital Libraries},
  primaryClass = {cs}
}

@article{Adams2006pure,
  title = {Pure Type Systems with Judgemental Equality},
  author = {Adams, Robin},
  year = {2006},
  volume = {16},
  pages = {219--246},
  doi = {10.1017/S0956796805005770},
  journal = {Journal of Functional Programming},
  number = {02}
}

@inproceedings{Adams2010scrap,
  title = {Scrap {{Your Zippers}}: {{A Generic Zipper}} for {{Heterogeneous Types}}},
  shorttitle = {Scrap {{Your Zippers}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Adams, Michael D.},
  year = {2010},
  pages = {13--24},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863495.1863499},
  abstract = {The zipper type provides the ability to efficiently edit tree-shaped data in a purely functional setting by providing constant time edits at a focal point in an immutable structure. It is used in a number of applications and is widely applicable for manipulating tree-shaped data structures. The traditional zipper suffers from two major limitations, however. First, it operates only on homogeneous types. That is to say, every node the zipper visits must have the same type. In practice, many tree-shaped types do not satisfy this condition, and thus cannot be handled by the traditional zipper. Second, the traditional zipper involves a significant amount of boilerplate code. A custom implementation must be written for each type the zipper traverses. This is error prone and must be updated whenever the type being traversed changes. The generic zipper presented in this paper overcomes these limitations. It operates over any type and requires no boilerplate code to be written by the user. The only restriction is that the types traversed must be instances of the Data class from the Scrap your Boilerplate framework},
  isbn = {978-1-4503-0251-7},
  keywords = {generic programming,heterogeneous types,scrap your boilerplate,zippers},
  series = {{{WGP}} '10}
}

@inproceedings{Adams2014optimizing,
  title = {Optimizing {{SYB}} Is {{Easy}}!},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2014 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Adams, Michael D. and Farmer, Andrew and Magalh{\~a}es, Jos{\'e} Pedro},
  year = {2014},
  pages = {71--82},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2543728.2543730},
  abstract = {The most widely used generic-programming system in the Haskell community, Scrap Your Boilerplate (SYB), also happens to be one of the slowest. Generic traversals in SYB are often an order of magnitude slower than equivalent handwritten, non-generic traversals. Thus while SYB allows the concise expression of many traversals, its use incurs a significant runtime cost. Existing techniques for optimizing other generic-programming systems are not able to eliminate this overhead. This paper presents an optimization that completely eliminates this cost. Essentially, it is a partial evaluation that takes advantage of domain-specific knowledge about the structure of SYB. It optimizes SYB-style traversals to be as fast as handwritten, non-generic code, and benchmarks show that this optimization improves the speed of SYB-style code by an order of magnitude or more.},
  isbn = {978-1-4503-2619-3},
  keywords = {Datatype-generic programming,Haskell,keywords: optimization,partial evaluation,performance,scrap your boilerplate (syb)},
  series = {{{PEPM}} '14}
}

@inproceedings{Afroozeh2015one,
  title = {One {{Parser}} to {{Rule Them All}}},
  booktitle = {2015 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}} ({{Onward}}!)},
  author = {Afroozeh, Ali and Izmaylova, Anastasia},
  year = {2015},
  pages = {151--170},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814228.2814242},
  abstract = {Despite the long history of research in parsing, constructing parsers for real programming languages remains a difficult and painful task. In the last decades, different parser generators emerged to allow the construction of parsers from a BNF-like specification. However, still today, many parsers are handwritten, or are only partly generated, and include various hacks to deal with different peculiarities in programming languages. The main problem is that current declarative syntax definition techniques are based on pure context-free grammars, while many constructs found in programming languages require context information. In this paper we propose a parsing framework that embraces context information in its core. Our framework is based on data-dependent grammars, which extend context-free grammars with arbitrary computation, variable binding and constraints. We present an implementation of our framework on top of the Generalized LL (GLL) parsing algorithm, and show how common idioms in syntax of programming languages such as (1) lexical disambiguation filters, (2) operator precedence, (3) indentation-sensitive rules, and (4) conditional preprocessor directives can be mapped to data-dependent grammars. We demonstrate the initial experience with our framework, by parsing more than 20000 Java, C\#, Haskell, and OCaml source files.},
  isbn = {978-1-4503-3688-8},
  keywords = {context-aware scanning,data-dependent grammars,disambiguation,GLL,offside rule,operator precedence,parsing,preprocessor directives,scannerless parsing},
  series = {Onward! 2015}
}

@inproceedings{Ager2003functional,
  title = {A {{Functional Correspondence Between Evaluators}} and {{Abstract Machines}}},
  booktitle = {Proceedings of the 5th {{ACM SIGPLAN International Conference}} on {{Principles}} and {{Practice}} of {{Declaritive Programming}}},
  author = {Ager, Mads Sig and Biernacki, Dariusz and Danvy, Olivier and Midtgaard, Jan},
  year = {2003},
  pages = {8--19},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/888251.888254},
  abstract = {We bridge the gap between functional evaluators and abstract machines for the {$\lambda$}-calculus, using closure conversion, transformation into continuation-passing style, and defunctionalization.We illustrate this approach by deriving Krivine's abstract machine from an ordinary call-by-name evaluator and by deriving an ordinary call-by-value evaluator from Felleisen et al.'s CEK machine. The first derivation is strikingly simpler than what can be found in the literature. The second one is new. Together, they show that Krivine's abstract machine and the CEK machine correspond to the call-by-name and call-by-value facets of an ordinary evaluator for the {$\lambda$}-calculus.We then reveal the denotational content of Hannan and Miller's CLS machine and of Landin's SECD machine. We formally compare the corresponding evaluators and we illustrate some degrees of freedom in the design spaces of evaluators and of abstract machines for the {$\lambda$}-calculus with computational effects.Finally, we consider the Categorical Abstract Machine and the extent to which it is more of a virtual machine than an abstract machine.},
  isbn = {1-58113-705-2},
  keywords = {abstract machines,closure conversion,defunctionalization,interpreters,transformation into continuation-passing style (CPS)},
  series = {{{PPDP}} '03}
}

@article{Aguirre2017Relational,
  title = {A {{Relational Logic}} for {{Higher}}-Order {{Programs}}},
  author = {Aguirre, Alejandro and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Strub, Pierre-Yves},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {21:1--21:29},
  issn = {2475-1421},
  doi = {10.1145/3110265},
  abstract = {Relational program verification is a variant of program verification where one can reason about two programs and as a special case about two executions of a single program on different inputs. Relational program verification can be used for reasoning about a broad range of properties, including equivalence and refinement, and specialized notions such as continuity, information flow security or relative cost. In a higher-order setting, relational program verification can be achieved using relational refinement type systems, a form of refinement types where assertions have a relational interpretation. Relational refinement type systems excel at relating structurally equivalent terms but provide limited support for relating terms with very different structures. We present a logic, called Relational Higher Order Logic (RHOL), for proving relational properties of a simply typed {\^I}\guillemotright{}-calculus with inductive types and recursive definitions. RHOL retains the type-directed flavour of relational refinement type systems but achieves greater expressivity through rules which simultaneously reason about the two terms as well as rules which only contemplate one of the two terms. We show that RHOL has strong foundations, by proving an equivalence with higher-order logic (HOL), and leverage this equivalence to derive key meta-theoretical properties: subject reduction, admissibility of a transitivity rule and set-theoretical soundness. Moreover, we define sound embeddings for several existing relational type systems such as relational refinement types and type systems for dependency analysis and relative cost, and we verify examples that were out of reach of prior work.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Formal Verification,refinement types,Relational Logic},
  number = {ICFP}
}

@inproceedings{Ahman2017Dijkstra,
  title = {Dijkstra {{Monads}} for {{Free}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ahman, Danel and Hri{\c t}cu, C{\u a}t{\u a}lin and Maillard, Kenji and Mart{\'i}nez, Guido and Plotkin, Gordon and Protzenko, Jonathan and Rastogi, Aseem and Swamy, Nikhil},
  year = {2017},
  pages = {515--529},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009878},
  abstract = {Dijkstra monads enable a dependent type theory to be enhanced with support for specifying and verifying effectful code via weakest preconditions. Together with their closely related counterparts, Hoare monads, they provide the basis on which verification tools like F*, Hoare Type Theory (HTT), and Ynot are built. We show that Dijkstra monads can be derived {\^a}for free{\^a} by applying a continuation-passing style (CPS) translation to the standard monadic definitions of the underlying computational effects. Automatically deriving Dijkstra monads in this way provides a correct-by-construction and efficient way of reasoning about user-defined effects in dependent type theories. We demonstrate these ideas in EMF*, a new dependently typed calculus, validating it via both formal proof and a prototype implementation within F*. Besides equipping F* with a more uniform and extensible effect system, EMF* enables a novel mixture of intrinsic and extrinsic proofs within F*.},
  isbn = {978-1-4503-4660-3},
  keywords = {dependent types,effectful programming,proof assistants,Verification},
  series = {{{POPL}} 2017}
}

@article{Ahmed2003Indexed,
  title = {An {{Indexed Model}} of {{Impredicative Polymorphism}} and {{Mutable References}}},
  author = {Ahmed, Amal and Appel, Andrew W. and Virga, Roberto},
  year = {2003},
  month = dec,
  abstract = {We present a semantic model of the polymorphic lambda calculus augmented with a higher-order store, allowing the storage of values of any type, including impredicative quantified types, mutable references, recursive types, and functions. Our model provides the first denotational semantics for a type system with updatable references to values of impredicative quantified types. The central idea behind our semantics is that instead of tracking the exact type of a mutable reference in a possible world our model keeps track of the approximate type. While high-level languages like ML and Java do not themselves support storage of impredicative existential packages in mutable cells, this feature is essential when representing ML function closures, that is, in a target language for typed closure conversion of ML programs. 1}
}

@phdthesis{Ahmed2004Semantics,
  title = {Semantics of {{Types}} for {{Mutable State}} ({{Thesis}}) | {{Computer Science Department}} at {{Princeton University}}},
  author = {Ahmed, Amal Jamil},
  year = {2004},
  month = jun,
  abstract = {Proof-carrying code (PCC) is a framework for mechanically verifying the safety of machine language programs. A program that is successfully verified by a PCC system is guaranteed to be safe to execute, but this safety guarantee is contingent upon the correctness of various trusted components. For instance, in traditional PCC systems the trusted computing base includes a large set of low-level typing rules. Foundational PCC systems seek to minimize the size of the trusted computing base. In particular, they eliminate the need to trust complex, low-level type systems by providing machine-checkable proofs of type soundness for real machine languages.

In this thesis, I demonstrate the use of logical relations for proving
the soundness of type systems for mutable state. Specifically, I
focus on type systems that ensure the safe allocation, update, and
reuse of memory. For each type in the language, I define logical
relations that explain the meaning of the type in terms of the
operational semantics of the language. Using this model of types, I
prove each typing rule as a lemma.

The major contribution is a model of System F with general references
-- that is, mutable cells that can hold values of any closed
type including other references, functions, recursive types, and
impredicative quantified types. The model is based on ideas from both
possible worlds and the indexed model of Appel and McAllester.

I show how the model of mutable references is encoded in higher-order
logic. I also show how to construct an indexed possible-worlds model for a von Neumann machine. The latter is used in the Princeton Foundational PCC system to prove type safety for a full-fledged low-level typed assembly language. Finally, I present a semantic model for a region calculus that supports type-invariant references as well as memory reuse.},
  school = {Princeton}
}

@inproceedings{Ahmed2006StepIndexed,
  title = {Step-{{Indexed Syntactic Logical Relations}} for {{Recursive}} and {{Quantified Types}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Ahmed, Amal},
  year = {2006},
  month = mar,
  pages = {69--83},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/11693024_6},
  abstract = {We present a sound and complete proof technique, based on syntactic logical relations, for showing contextual equivalence of expressions in a {$\lambda$}-calculus with recursive types and impredicative universal and existential types. Our development builds on the step-indexed PER model of recursive types presented by Appel and McAllester. We have discovered that a direct proof of transitivity of that model does not go through, leaving the ``PER'' status of the model in question. We show how to extend the Appel-McAllester model to obtain a logical relation that we can prove is transitive, as well as sound and complete with respect to contextual equivalence. We then augment this model to support relational reasoning in the presence of quantified types.Step-indexed relations are indexed not just by types, but also by the number of steps available for future evaluation. This stratification is essential for handling various circularities, from recursive functions, to recursive types, to impredicative polymorphism. The resulting construction is more elementary than existing logical relations which require complex machinery such as domain theory, admissibility, syntactic minimal invariance, and {$\top$} {$\top$}-closure.},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{Ahmed2008Typed,
  title = {Typed {{Closure Conversion Preserves Observational Equivalence}}},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Ahmed, Amal and Blume, Matthias},
  year = {2008},
  pages = {157--168},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1411204.1411227},
  abstract = {Language-based security relies on the assumption that all potential attacks are bound by the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence. We investigate the problem of fully abstract compilation, i.e., compilation that both preserves and reflects observational equivalence. In particular, we prove that typed closure conversion for the polymorphic \guillemotright{}-calculus with existential and recursive types is fully abstract. Our proof uses operational techniques in the form of a step-indexed logical relation and construction of certain wrapper terms that "back-translate" from target values to source values. Although typed closure conversion has been assumed to be fully abstract, we are not aware of any previous result that actually proves this.},
  isbn = {978-1-59593-919-7},
  keywords = {equivalence-preserving compilation,full abstraction,step-indexed logical relations,typed closure conversion},
  series = {{{ICFP}} '08}
}

@inproceedings{Ahmed2009Statedependent,
  title = {State-Dependent {{Representation Independence}}},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ahmed, Amal and Dreyer, Derek and Rossberg, Andreas},
  year = {2009},
  pages = {340--353},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1480881.1480925},
  abstract = {Mitchell's notion of representation independence is a particularly useful application of Reynolds' relational parametricity -- two different implementations of an abstract data type can be shown contextually equivalent so long as there exists a relation between their type representations that is preserved by their operations. There have been a number of methods proposed for proving representation independence in various pure extensions of System F (where data abstraction is achieved through existential typing), as well as in Algol- or Java-like languages (where data abstraction is achieved through the use of local mutable state). However, none of these approaches addresses the interaction of existential type abstraction and local state. In particular, none allows one to prove representation independence results for generative ADTs -- i.e. ADTs that both maintain some local state and define abstract types whose internal representations are dependent on that local state. In this paper, we present a syntactic, logical-relations-based method for proving representation independence of generative ADTs in a language supporting polymorphic types, existential types, general recursive types, and unrestricted ML-style mutable references. We demonstrate the effectiveness of our method by using it to prove several interesting contextual equivalences that involve a close interaction between existential typing and local state, as well as some well-known equivalences from the literature (such as Pitts and Stark's "awkward" example) that have caused trouble for previous logical-relations-based methods. The success of our method relies on two key technical innovations. First, in order to handle generative ADTs, we develop a possible-worlds model in which relational interpretations of types are allowed to grow over time in a manner that is tightly coupled with changes to some local state. Second, we employ a step-indexed stratification of possible worlds, which facilitates a simplified account of mutable references of higher type.},
  isbn = {978-1-60558-379-2},
  keywords = {_tablet_modified,abstract data types,existential types,local state,representation independence,step-indexed logical relations},
  series = {{{POPL}} '09}
}

@article{Ahmed2017Theorems,
  title = {Theorems for {{Free}} for {{Free}}: {{Parametricity}}, with and {{Without Types}}},
  shorttitle = {Theorems for {{Free}} for {{Free}}},
  author = {Ahmed, Amal and Jamner, Dustin and Siek, Jeremy G. and Wadler, Philip},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {39:1--39:28},
  issn = {2475-1421},
  doi = {10.1145/3110283},
  abstract = {The polymorphic blame calculus integrates static typing, including universal types, with dynamic typing. The primary challenge with this integration is preserving parametricity: even dynamically-typed code should satisfy it once it has been cast to a universal type. Ahmed et al. (2011) employ runtime type generation in the polymorphic blame calculus to preserve parametricity, but a proof that it does so has been elusive. Matthews and Ahmed (2008) gave a proof of parametricity for a closely related system that combines ML and Scheme, but later found a flaw in their proof. In this paper we present an improved version of the polymorphic blame calculus and we prove that it satisfies relational parametricity. The proof relies on a step-indexed Kripke logical relation. The step-indexing is required to make the logical relation well-defined in the case for the dynamic type. The possible worlds include the mapping of generated type names to their types and the mapping of type names to relations. We prove the Fundamental Property of this logical relation and that it is sound with respect to contextual equivalence. To demonstrate the utility of parametricity in the polymorphic blame calculus, we derive two free theorems.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {dynamic typing,gradual typing,logical relation,parametricity},
  number = {ICFP}
}

@inproceedings{Allais2013new,
  title = {New {{Equations}} for {{Neutral Terms}}: {{A Sound}} and {{Complete Decision Procedure}}, {{Formalized}}},
  shorttitle = {New {{Equations}} for {{Neutral Terms}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN Workshop}} on {{Dependently}}-Typed {{Programming}}},
  author = {Allais, Guillaume and McBride, Conor and Boutillier, Pierre},
  year = {2013},
  pages = {13--24},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2502409.2502411},
  abstract = {The definitional equality of an intensional type theory is its test of type compatibility. Today's systems rely on ordinary evaluation semantics to compare expressions in types, frustrating users with type errors arising when evaluation fails to identify two `obviously' equal terms. If only the machine could decide a richer theory! We propose a way to decide theories which supplement evaluation with `{$\nu$}-rules', rearranging the neutral parts of normal forms, and report a successful initial experiment. We study a simple {$\lambda$}-calculus with primitive fold, map and append operations on lists and develop in Agda a sound and complete decision procedure for an equational theory enriched with monoid, functor and fusion laws.},
  isbn = {978-1-4503-2384-0},
  keywords = {logical relations,map fusion,normalization by evaluation,simply typed lambda calculus},
  series = {{{DTP}} '13}
}

@incollection{Allais2017Typeandscope,
  title = {Type-and-Scope Safe Programs and Their Proofs},
  booktitle = {{{CPP}} 2017},
  author = {Allais, Guillaume and Chapman, James and McBride, Conor and McKinna, James},
  editor = {Bertot, Yves and Vafeiadis, Viktor},
  year = {2017},
  month = jan,
  publisher = {{ACM, New York, NY}},
  address = {{New York}},
  abstract = {We abstract the common type-and-scope safe structure fromcomputations on lambda-terms that deliver, e.g., renaming, substitution, evaluation, CPS-transformation, and printing witha name supply. By exposing this structure, we can prove generic simulation and fusion lemmas relating operations built this way. This work has been fully formalised in Agda.},
  isbn = {978-1-4503-4705-1},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{Allais2018Typing,
  title = {Typing with {{Leftovers}} - {{A}} Mechanization of {{Intuitionistic Multiplicative}}-{{Additive Linear Logic}}},
  booktitle = {23rd {{International Conference}} on {{Types}} for {{Proofs}} and {{Programs}} ({{TYPES}} 2017)},
  author = {Allais, Guillaume},
  editor = {Abel, Andreas and Forsberg, Fredrik Nordvall and Kaposi, Ambrus},
  year = {2018},
  volume = {104},
  pages = {1:1--1:22},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.TYPES.2017.1},
  isbn = {978-3-95977-071-2},
  keywords = {Agda,Bidirectional,Linear Logic,Type System},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@article{Allen2006innovations,
  title = {Innovations in Computational Type Theory Using {{Nuprl}}},
  author = {Allen, S. F. and Bickford, M. and Constable, R. L. and Eaton, R. and Kreitz, C. and Lorigo, L. and Moran, E.},
  year = {2006},
  month = dec,
  volume = {4},
  pages = {428--469},
  issn = {1570-8683},
  doi = {10.1016/j.jal.2005.10.005},
  abstract = {For twenty years the Nuprl (``new pearl'') system has been used to develop software systems and formal theories of computational mathematics. It has also been used to explore and implement computational type theory (CTT)\textemdash{}a formal theory of computation closely related to Martin-L{\"o}f's intuitionistic type theory (ITT) and to the calculus of inductive constructions (CIC) implemented in the Coq prover.

This article focuses on the theory and practice underpinning our use of Nuprl for much of the last decade. We discuss innovative elements of type theory, including new type constructors such as unions and dependent intersections, our theory of classes, and our theory of event structures.

We also discuss the innovative architecture of Nuprl as a distributed system and as a transactional database of formal mathematics using the notion of abstract object identifiers. The database has led to an independent project called the Formal Digital Library, FDL, now used as a repository for Nuprl results as well as selected results from HOL, MetaPRL, and PVS. We discuss Howe's set theoretic semantics that is used to relate such disparate theories and systems as those represented by these provers.},
  journal = {Journal of Applied Logic},
  keywords = {Computational type theory,Dependent intersection types,Formal digital libraries,Logic of events,Martin-Löf type theory,Polymorphic subtyping,Program extraction,Proofs as programs,tactics,Union types},
  number = {4},
  series = {Towards {{Computer Aided Mathematics}}}
}

@inproceedings{Altenkirch1999Monadic,
  title = {Monadic {{Presentations}} of {{Lambda Terms Using Generalized Inductive Types}}},
  booktitle = {Computer {{Science Logic}}},
  author = {Altenkirch, Thorsten and Reus, Bernhard},
  editor = {Flum, J{\"o}rg and {Rodriguez-Artalejo}, Mario},
  year = {1999},
  month = sep,
  pages = {453--468},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/3-540-48168-0_32},
  abstract = {We present a definition of untyped {$\lambda$}-terms using a heterogeneous datatype, i.e. an inductively defined operator. This operator can be extended to a Kleisli triple, which is a concise way to verify the substitution laws for {$\lambda$}-calculus. We also observe that repetitions in the definition of the monad as well as in the proofs can be avoided by using well-founded recursion and induction instead of structural induction. We extend the construction to the simply typed {$\lambda$}-calculus using dependent types, and show that this is an instance of a generalization of Kleisli triples. The proofs for the untyped case have been checked using the LEGO system.},
  copyright = {\textcopyright{}1999 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-66536-6 978-3-540-48168-3},
  keywords = {Artificial Intelligence (incl. Robotics),category theory,inductive types,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Theory of Computation,type theory,λ-Calculus},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Altenkirch2003generic,
  title = {Generic {{Programming}} within {{Dependently Typed Programming}}},
  booktitle = {Generic {{Programming}}},
  author = {Altenkirch, Thorsten and Mcbride, Conor},
  editor = {Gibbons, Jeremy and Jeuring, Johan},
  year = {2003},
  month = jan,
  pages = {1--20},
  publisher = {{Springer US}},
  abstract = {We show how higher kinded generic programming can be represented faithfully within a dependently typed programming system. This development has been implemented using the Oleg system. The present work can be seen as evidence for our thesis that extensions of type systems can be done by programming within a dependently typed language, using data as codes for types.},
  copyright = {\textcopyright{}2003 Springer-Verlag US},
  isbn = {978-1-4757-5320-2 978-0-387-35672-3},
  keywords = {Data Structures,Mathematics of Computing,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {115},
  series = {{{IFIP}} \textemdash{} {{The International Federation}} for {{Information Processing}}}
}

@article{Altenkirch2009Bigstep,
  title = {Big-Step Normalisation},
  author = {Altenkirch, Thorsten and Chapman, James},
  year = {2009},
  month = jul,
  volume = {19},
  pages = {311--333},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796809007278},
  abstract = {AbstractTraditionally, decidability of conversion for typed {$\lambda$}-calculi is established by showing that small-step reduction is confluent and strongly normalising. Here we investigate an alternative approach employing a recursively defined normalisation function which we show to be terminating and which reflects and preserves conversion. We apply our approach to the simply typed {$\lambda$}-calculus with explicit substitutions and {$\beta\eta$}-equality, a system which is not strongly normalising. We also show how the construction can be extended to system T with the usual {$\beta$}-rules for the recursion combinator. Our approach is practical, since it does verify an actual implementation of normalisation which, unlike normalisation by evaluation, is first order. An important feature of our approach is that we are using logical relations to establish equational soundness (identity of normal forms reflects the equational theory), instead of the usual syntactic reasoning using the Church\textendash{}Rosser property of a term rewriting system.},
  journal = {Journal of Functional Programming},
  number = {3-4}
}

@inproceedings{Altenkirch2010PS,
  title = {{{$\Pi\Sigma$}}: {{Dependent Types}} without the {{Sugar}}},
  shorttitle = {{{$\Pi\Sigma$}}},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Altenkirch, Thorsten and Danielsson, Nils Anders and L{\"o}h, Andres and Oury, Nicolas},
  year = {2010},
  month = apr,
  pages = {40--55},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12251-4_5},
  abstract = {The recent success of languages like Agda and Coq demonstrates the potential of using dependent types for programming. These systems rely on many high-level features like datatype definitions, pattern matching and implicit arguments to facilitate the use of the languages. However, these features complicate the metatheoretical study and are a potential source of bugs.To address these issues we introduce {$\Pi\Sigma$}, a dependently typed core language. It is small enough for metatheoretical study and the type checker is small enough to be formally verified. In this language there is only one mechanism for recursion\textemdash{}used for types, functions and infinite objects\textemdash{}and an explicit mechanism to control unfolding, based on lifted types. Furthermore structural equality is used consistently for values and types; this is achieved by a new notion of {$\alpha$}-equality for recursive definitions. We show, by translating several high-level constructions, that {$\Pi\Sigma$} is suitable as a core language for dependently typed programming.},
  isbn = {978-3-642-12250-7 978-3-642-12251-4},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Altenkirch2016Partiality,
  title = {Partiality, {{Revisited}}: {{The Partiality Monad}} as a {{Quotient Inductive}}-{{Inductive Type}}},
  shorttitle = {Partiality, {{Revisited}}},
  author = {Altenkirch, Thorsten and Danielsson, Nils Anders and Kraus, Nicolai},
  year = {2016},
  month = oct,
  abstract = {Capretta's delay monad can be used to model partial computations, but it has the "wrong" notion of built-in equality, strong bisimilarity. An alternative is to quotient the delay monad by the "right" notion of equality, weak bisimilarity. However, recent work by Chapman et al. suggests that it is impossible to define a monad structure on the resulting construction in common forms of type theory without assuming the axiom of countable choice. Using an idea from homotopy type theory -a higher inductive-inductive type- we construct a partiality monad without relying on countable choice. We prove that, in the presence of countable choice, our partiality monad is equivalent to the delay monad quotiented by weak bisimilarity. Finally, we outline several applications.},
  archivePrefix = {arXiv},
  eprint = {1610.09254},
  eprinttype = {arxiv},
  journal = {arXiv:1610.09254 [cs]},
  keywords = {03B15,Computer Science - Logic in Computer Science,F.4.1},
  primaryClass = {cs}
}

@inproceedings{Alvarez-Picallo2019Change,
  title = {Change Actions: {{Models}} of Generalised Differentiation},
  shorttitle = {Change Actions},
  booktitle = {Foundations of {{Software Science}} and {{Computation Structures}}},
  author = {{Alvarez-Picallo}, Mario and Ong, C.-H. Luke},
  editor = {Boja{\'n}czyk, Miko{\l}aj and Simpson, Alex},
  year = {2019},
  pages = {45--61},
  publisher = {{Springer International Publishing}},
  abstract = {Change structures, introduced by Cai et al., have recently been proposed as a semantic framework for incremental computation. We generalise change actions, an alternative to change structures, to arbitrary cartesian categories and propose the notion of change action model as a categorical model for (higher-order) generalised differentiation. Change action models naturally arise from many geometric and computational settings, such as (generalised) cartesian differential categories, group models of discrete calculus, and Kleene algebra of regular expressions. We show how to build canonical change action models on arbitrary cartesian categories, reminiscent of the F{\`a}a di Bruno construction.},
  isbn = {978-3-030-17127-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Alvarez-Picallo2019Changea,
  title = {Change Actions: {{Models}} of Generalised Differentiation},
  shorttitle = {Change {{Actions}}},
  author = {{Alvarez-Picallo}, Mario and Ong, C.-H. Luke},
  year = {2019},
  month = feb,
  abstract = {Cai et al. have recently proposed change structures as a semantic framework for incremental computation. We generalise change structures to arbitrary cartesian categories and propose the notion of change action model as a categorical model for (higher-order) generalised differentiation. Change action models naturally arise from many geometric and computational settings, such as (generalised) cartesian differential categories, group models of discrete calculus, and Kleene algebra of regular expressions. We show how to build canonical change action models on arbitrary cartesian categories, reminiscent of the F\textbackslash{}`aa di Bruno construction.},
  archivePrefix = {arXiv},
  eprint = {1902.05465},
  eprinttype = {arxiv},
  journal = {arXiv:1902.05465 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@article{Amadio1991recursion,
  title = {Recursion over Realizability Structures},
  author = {Amadio, Roberto M.},
  year = {1991},
  month = mar,
  volume = {91},
  pages = {55--85},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90074-C},
  abstract = {Realizability structures play a major role in the metamathematics of intuitionistic systems and they are a basic tool in the extraction of the computational content of constructive proofs. Besides their rich categorical structure and effectiveness properties provide a privileged mathematical setting for the semantics of data types of programming languages. In this paper we emphasize the modelling of recursive definitions of programs and types. A realizability model for a language including Girard's system F and an operator of recursion on types is given and some of its local properties are studied.},
  journal = {Information and Computation},
  number = {1}
}

@article{Amadio1993subtyping,
  title = {Subtyping {{Recursive Types}}},
  author = {Amadio, Roberto M. and Cardelli, Luca},
  year = {1993},
  month = sep,
  volume = {15},
  pages = {575--631},
  issn = {0164-0925},
  doi = {10.1145/155183.155231},
  abstract = {We investigate the interactions of subtyping and recursive types, in a simply typed \&lgr;-calculus. The two fundamental questions here are whether two (recursive)types are in the subtype relation and whether a term has a type. To address the first question, we relate various definitions of type equivalence and subtyping that are induced by a model, an ordering on infinite trees, an algorithm, and a set of type rules. We show soundness and completeness among the rules, the algorithm, and the tree semantics. We also prove soundness and a restricted form of completeness for the model. To address the second question, we show that to every pair of types in the subtype relation we can associate a term whose denotation is the uniquely determined coercion map between the two types. Moreover, we derive an algorithm that, when given a term with implicit coercions, can infer its least type whenever possible.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {coercions,Lambda-calculus,partial-equivalence relations,recursive types,regular trees,subtyping,tree orderings,type equivalence,typechecking algorithm},
  number = {4}
}

@article{Amadio2013certifying,
  title = {Certifying and Reasoning about Cost Annotations of Functional Programs},
  author = {Amadio, Roberto M. and {R{\'e}gis-Gianas}, Yann},
  year = {2013},
  month = jan,
  abstract = {We present a so-called labelling method to insert cost annotations in a higher-order functional program, to certify their correctness with respect to a standard compilation chain to assembly code including safe memory management, and to reason on them in a higher-order Hoare logic.},
  journal = {Higher-Order and Symbolic Computation},
  language = {en}
}

@article{Amin2012Dependent,
  title = {Dependent Object Types},
  author = {Amin, Nada and Moors, Adriaan and Odersky, Martin},
  year = {2012},
  abstract = {We propose a new type-theoretic foundation of Scala and languages like it: the Dependent Object Types (DOT) calculus. DOT models Scala's path-dependent types, abstract type members and its mixture of nominal and structural typing through the use of refinement types. The core formalism makes no attempt to model inheritance and mixin composition. DOT normalizes Scala's type system by unifying the constructs for type members and by providing classical intersection and union types which simplify greatest lower bound and least upper bound computations. In this paper, we present the DOT calculus, both formally and informally. We also discuss our work-in-progress to prove typesafety of the calculus. Amin, Nada; Moors, Adriaan; Odersky, Martin},
  language = {en}
}

@inproceedings{Amin2014foundations,
  title = {Foundations of {{Path}}-Dependent {{Types}}},
  booktitle = {Proceedings of the 2014 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} \& {{Applications}}},
  author = {Amin, Nada and Rompf, Tiark and Odersky, Martin},
  year = {2014},
  pages = {233--249},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2660193.2660216},
  abstract = {A scalable programming language is one in which the same concepts can describe small as well as large parts. Towards this goal, Scala unifies concepts from object and module systems. An essential ingredient of this unification is the concept of objects with type members, which can be referenced through path-dependent types. Unfortunately, path-dependent types are not well-understood, and have been a roadblock in grounding the Scala type system on firm theory. We study several calculi for path-dependent types. We present DOT which captures the essence - DOT stands for Dependent Object Types. We explore the design space bottom-up, teasing apart inherent from accidental complexities, while fully mechanizing our models at each step. Even in this simple setting, many interesting patterns arise from the interaction of structural and nominal features. Whereas our simple calculus enjoys many desirable and intuitive properties, we demonstrate that the theory gets much more complicated once we add another Scala feature, type refinement, or extend the subtyping relation to a lattice. We discuss possible remedies and trade-offs in modeling type systems for Scala-like languages.},
  isbn = {978-1-4503-2585-1},
  keywords = {Calculus,dependent types,Objects},
  series = {{{OOPSLA}} '14}
}

@phdthesis{Amin2016Dependent,
  title = {Dependent Object Types},
  author = {Amin, Nada},
  year = {2016},
  address = {{Lausanne, Switzerland}},
  doi = {10.5075/epfl-thesis-7156, urn:nbn:ch:bel-epfl-thesis7156-6},
  keywords = {_tablet_modified},
  school = {EPFL}
}

@article{Amin2016Essence,
  title = {The Essence of Dependent Object Types},
  author = {Amin, Nada and Gr{\"u}tter, Karl Samuel and Odersky, Martin and Rompf, Tiark and Stucki, Sandro},
  year = {2016},
  journal = {WadlerFest 2016},
  keywords = {_tablet}
}

@inproceedings{Amin2017LMSVerify,
  title = {{{LMS}}-{{Verify}}: {{Abstraction Without Regret}} for {{Verified Systems Programming}}},
  shorttitle = {{{LMS}}-{{Verify}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Amin, Nada and Rompf, Tiark},
  year = {2017},
  pages = {859--873},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009867},
  abstract = {Performance critical software is almost always developed in C, as programmers do not trust high-level languages to deliver the same reliable performance. This is bad because low-level code in unsafe languages attracts security vulnerabilities and because development is far less productive, with PL advances mostly lost on programmers operating under tight performance constraints. High-level languages provide memory safety out of the box, but they are deemed too slow and unpredictable for serious system software.   Recent years have seen a surge in staging and generative programming: the key idea is to use high-level languages and their abstraction power as glorified macro systems to compose code fragments in first-order, potentially domain-specific, intermediate languages, from which fast C can be emitted. But what about security? Since the end result is still C code, the safety guarantees of the high-level host language are lost.   In this paper, we extend this generative approach to emit ACSL specifications along with C code. We demonstrate that staging achieves ``abstraction without regret'' for verification: we show how high-level programming models, in particular higher-order composable contracts from dynamic languages, can be used at generation time to compose and generate first-order specifications that can be statically checked by existing tools. We also show how type classes can automatically attach invariants to data types, reducing the need for repetitive manual annotations.   We evaluate our system on several case studies that varyingly exercise verification of memory safety, overflow safety, and functional correctness. We feature an HTTP parser that is (1) fast (2) high-level: implemented using staged parser combinators (3) secure: with verified memory safety. This result is significant, as input parsing is a key attack vector, and vulnerabilities related to HTTP parsing have been documented in all widely-used web servers.},
  isbn = {978-1-4503-4660-3},
  keywords = {blame,Contracts,dsls,Frama-C,LMS,memory safety,security,Verification},
  series = {{{POPL}} 2017}
}

@inproceedings{Amin2017Type,
  title = {Type Soundness Proofs with Definitional Interpreters},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Amin, Nada and Rompf, Tiark},
  year = {2017},
  pages = {666--679},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009866},
  abstract = {While type soundness proofs are taught in every graduate PL class, the gap between realistic languages and what is accessible to formal proofs is large. In the case of Scala, it has been shown that its formal model, the Dependent Object Types (DOT) calculus, cannot simultaneously support key metatheoretic properties such as environment narrowing and subtyping transitivity, which are usually required for a type soundness proof. Moreover, Scala and many other realistic languages lack a general substitution property. The first contribution of this paper is to demonstrate how type soundness proofs for advanced, polymorphic, type systems can be carried out with an operational semantics based on high-level, definitional interpreters, implemented in Coq. We present the first mechanized soundness proofs in this style for System F and several extensions, including mutable references. Our proofs use only straightforward induction, which is significant, as the combination of big-step semantics, mutable references, and polymorphism is commonly believed to require coinductive proof techniques. The second main contribution of this paper is to show how DOT-like calculi emerge from straightforward generalizations of the operational aspects of F, exposing a rich design space of calculi with path-dependent types inbetween System F and DOT, which we dub the System D Square. By working directly on the target language, definitional interpreters can focus the design space and expose the invariants that actually matter at runtime. Looking at such runtime invariants is an exciting new avenue for type system design.},
  isbn = {978-1-4503-4660-3},
  keywords = {_tablet_modified,Definitional interpreters,dependent object types,DOT,Scala,Type soundness},
  series = {{{POPL}} 2017}
}

@article{Anand2017Revisiting,
  title = {Revisiting {{Parametricity}}: {{Inductives}} and {{Uniformity}} of {{Propositions}}},
  shorttitle = {Revisiting {{Parametricity}}},
  author = {Anand, Abhishek and Morrisett, Greg},
  year = {2017},
  month = may,
  abstract = {Reynold's parametricity theory captures the property that parametrically polymorphic functions behave uniformly: they produce related results on related instantiations. In dependently-typed programming languages, such relations and uniformity proofs can be expressed internally, and generated as a program translation. We present a new parametricity translation for a significant fragment of Coq. Previous translations of parametrically polymorphic propositions allowed non-uniformity. For example, on related instantiations, a function may return propositions that are logically inequivalent (e.g. True and False). We show that uniformity of polymorphic propositions is not achievable in general. Nevertheless, our translation produces proofs that the two propositions are logically equivalent and also that any two proofs of those propositions are related. This is achieved at the cost of potentially requiring more assumptions on the instantiations, requiring them to be isomorphic in the worst case. Our translation augments the previous one for Coq by carrying and compositionally building extra proofs about parametricity relations. It is made easier by a new method for translating inductive types and pattern matching. The new method builds upon and generalizes previous such translations for dependently-typed programming languages. Using reification and reflection, we have implemented our translation as Coq programs. We obtain several stronger free theorems applicable to an ongoing compiler-correctness project. Previously, proofs of some of these theorems took several hours to finish.},
  archivePrefix = {arXiv},
  eprint = {1705.01163},
  eprinttype = {arxiv},
  journal = {arXiv:1705.01163 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@inproceedings{Ancona2017Generalizing,
  title = {Generalizing {{Inference Systems}} by {{Coaxioms}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Ancona, Davide and Dagnino, Francesco and Zucca, Elena},
  year = {2017},
  month = apr,
  pages = {29--55},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-54434-1_2},
  abstract = {We introduce a generalized notion of inference system to support structural recursion on non well-founded datatypes. Besides axioms and inference rules with the usual meaning, a generalized inference system allows coaxioms, which are, intuitively, axioms which can only be applied ``at infinite depth'' in a proof tree. This notion nicely subsumes standard inference systems and their inductive and coinductive interpretation, while providing more flexibility. Indeed, the classical results on the existence and constructive characterization of least and greatest fixed points can be extended to our generalized framework, interpreting recursive definitions as fixed points which are not necessarily the least, nor the greatest one. This allows formal reasoning in cases where the inductive and coinductive interpretation do not provide the intended meaning, or are mixed together.},
  language = {en}
}

@inproceedings{Angiuli2014homotopical,
  title = {Homotopical {{Patch Theory}}},
  booktitle = {{{ICFP}}},
  author = {Angiuli, Carlo and Morehouse, Ed and Licata, Daniel R. and Harper, Robert},
  year = {2014},
  abstract = {Homotopy type theory is an extension of Martin-L{\"o}f type theory, based on a correspondence with homotopy theory and higher category theory. The propositional equality type becomes proof-relevant, and acts like paths in a space. Higher inductive types are a new class of datatypes which are specified by constructors not only for points but also for paths.  In this paper, we show how patch theory in the style of the Darcs version control system can be developed in homotopy type theory. We reformulate patch theory using the tools of homotopy type theory, and clearly separate formal theories of patches from their interpretation in terms of basic revision control mechanisms.  A patch theory is presented by a higher inductive type. Models of a patch theory are functions from that type, which, because function are functors, automatically preserve the structure of patches. Several standard tools of homotopy theory come into play, demonstrating the use of these methods in a practical programming context.}
}

@inproceedings{Apel2011semistructured,
  title = {Semistructured {{Merge}}: {{Rethinking Merge}} in {{Revision Control Systems}}},
  shorttitle = {Semistructured {{Merge}}},
  booktitle = {Proceedings of the 19th {{ACM SIGSOFT Symposium}} and the 13th {{European Conference}} on {{Foundations}} of {{Software Engineering}}},
  author = {Apel, Sven and Liebig, J{\"o}rg and Brandl, Benjamin and Lengauer, Christian and K{\"a}stner, Christian},
  year = {2011},
  pages = {190--200},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2025113.2025141},
  abstract = {An ongoing problem in revision control systems is how to resolve conflicts in a merge of independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems that inherit the strengths of both: the generality of unstructured systems and the expressiveness of structured systems. The idea is to provide structural information of the underlying software artifacts --- declaratively, in the form of annotated grammars. This way, a wide variety of languages can be supported and the information provided can assist in the automatic resolution of two classes of conflicts: ordering conflicts and semantic conflicts. The former can be resolved independently of the language and the latter using specific conflict handlers. We have been developing a tool that supports semistructured merge and conducted an empirical study on 24 software projects developed in Java, C\#, and Python comprising 180 merge scenarios. We found that semistructured merge reduces the number of conflicts in 60\% of the sample merge scenarios by, on average, 34\%, compared to unstructured merge. We found also that renaming is challenging in that it can increase the number of conflicts during semistructured merge, and that a combination of unstructured and semistructured merge is a pragmatic way to go.},
  isbn = {978-1-4503-0443-6},
  keywords = {featurehouse,fstmerge,revision control,semistructured merge,software merging,version control},
  series = {{{ESEC}}/{{FSE}} '11}
}

@inproceedings{Apel2013exploring,
  title = {Exploring {{Feature Interactions}} in the {{Wild}}: {{The New Feature}}-Interaction {{Challenge}}},
  shorttitle = {Exploring {{Feature Interactions}} in the {{Wild}}},
  booktitle = {Proceedings of the 5th {{International Workshop}} on {{Feature}}-{{Oriented Software Development}}},
  author = {Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert and K{\"a}stner, Christian and Garvin, Brady},
  year = {2013},
  pages = {1--8},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2528265.2528267},
  abstract = {The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.},
  isbn = {978-1-4503-2168-6},
  keywords = {feature interactions,feature modularity,feature-interaction problem,feature-oriented software development},
  series = {{{FOSD}} '13}
}

@inproceedings{Appel1989continuationpassing,
  title = {Continuation-Passing, {{Closure}}-Passing {{Style}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Appel, A. W. and Jim, T.},
  year = {1989},
  pages = {293--302},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/75277.75303},
  abstract = {We implemented a continuation-passing style (CPS) code generator for ML. Our CPS language is represented as an ML datatype in which all functions are named and most kinds of ill-formed expressions are impossible. We separate the code generation into phases that rewrite this representation into ever-simpler forms. Closures are represented explicitly as records, so that closure strategies can be communicated from one phase to another. No stack is used. Our benchmark data shows that the new method is an improvement over our previous, abstract-machine based code generator.},
  isbn = {0-89791-294-2},
  series = {{{POPL}} '89}
}

@article{Appel1998ssa,
  title = {{{SSA}} Is {{Functional Programming}}},
  author = {Appel, Andrew W.},
  year = {1998},
  month = apr,
  volume = {33},
  pages = {17--20},
  issn = {0362-1340},
  doi = {10.1145/278283.278285},
  journal = {SIGPLAN Not.},
  number = {4}
}

@inproceedings{Appel2007Very,
  title = {A {{Very Modal Model}} of a {{Modern}}, {{Major}}, {{General Type System}}},
  booktitle = {Proceedings of the 34th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Appel, Andrew W. and Melli{\`e}s, Paul-Andr{\'e} and Richards, Christopher D. and Vouillon, J{\'e}r{\^o}me},
  year = {2007},
  pages = {109--122},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1190216.1190235},
  abstract = {We present a model of recursive and impredicatively quantified types with mutable references. We interpret in this model all of the type constructors needed for typed intermediate languages and typed assembly languages used for object-oriented and functional languages. We establish in this purely semantic fashion a soundness proof of the typing systems underlying these TILs and TALs---ensuring that every well-typed program is safe. The technique is generic, and applies to any small-step semantics including {$\lambda$}-calculus, labeled transition systems, and von Neumann machines. It is also simple, and reduces mainly to defining a Kripke semantics of the G{\"o}del-L{\"o}b logic of provability. We have mechanically verified in Coq the soundness of our type system as applied to a von Neumann machine.},
  isbn = {978-1-59593-575-5},
  keywords = {impredicative polymorphism,Kripke models,mutable references,recursive types},
  series = {{{POPL}} '07}
}

@inproceedings{Ariola1995callbyneed,
  title = {A {{Call}}-by-Need {{Lambda Calculus}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ariola, Zena M. and Maraist, John and Odersky, Martin and Felleisen, Matthias and Wadler, Philip},
  year = {1995},
  pages = {233--246},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/199448.199507},
  abstract = {The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the call-by-need continuation passing transformation and the realization of sharing via assignments.},
  isbn = {0-89791-692-1},
  series = {{{POPL}} '95}
}

@article{Ariola1997callbyneed,
  title = {The Call-by-Need Lambda Calculus},
  author = {Ariola, Zena M. and Felleisen, Matthias},
  year = {1997},
  month = may,
  volume = {7},
  pages = {265--301},
  issn = {1469-7653},
  doi = {null},
  abstract = {Plotkin (1975) showed that the lambda calculus is a good model of the evaluation process for call-by-name functional programs. Reducing programs to constants or lambda abstractions according to the leftmost-outermost strategy exactly mirrors execution on an abstract machine like Landin\&apos;s SECD machine. The machine-based evaluator returns a constant or the token closure if and only if the standard reduction sequence starting at the same program will end in the same constant or in some lambda abstraction. However, the calculus does not capture the sharing of the evaluation of arguments that lazy implementations use to speed up the execution. More precisely, a lazy implementation evaluates procedure arguments only when needed and then only once. All other references to the formal procedure parameter re-use the value of the first argument evaluation. The mismatch between the operational semantics of the lambda calculus and the actual behavior of the prototypical implementation is a major obstacle for compiler writers. Unlike implementors of the leftmost-outermost strategy or of a call-by-value language, implementors of lazy systems cannot easily explain the behavior of their evaluator in terms of source level syntax. Hence, they often cannot explain why a certain syntactic transformation `works' and why another doesn\&apos;t. In this paper we develop an equational characterization of the most popular lazy implementation technique \textendash{} traditionally called `call-by-need' \textendash{} and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than Plotkin\&apos;s call-by-name lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g. the call-by-need continuation passing transformation and the realization of sharing via assignments. Some of this material first appeared in a paper presented at the 1995 ACM Conference on the Principles of Programming Languages. The paper was a joint effort with Maraist, Odersky and Wadler, who had independently developed a different equational characterization of call-by-need. We contrast our work with that of Maraist et al. in the body of this paper where appropriate.},
  journal = {Journal of Functional Programming},
  number = {03}
}

@article{Ariola2009typetheoretic,
  title = {A Type-Theoretic Foundation of Delimited Continuations},
  author = {Ariola, Zena M. and Herbelin, Hugo and Sabry, Amr},
  year = {2009},
  month = sep,
  volume = {22},
  pages = {233--273},
  issn = {1388-3690, 1573-0557},
  doi = {10.1007/s10990-007-9006-0},
  abstract = {There is a correspondence between classical logic and programming language calculi with first-class continuations. With the addition of control delimiters, the continuations become composable and the calculi become more expressive. We present a fine-grained analysis of control delimiters and formalise that their addition corresponds to the addition of a single dynamically-scoped variable modelling the special top-level continuation. From a type perspective, the dynamically-scoped variable requires effect annotations. In the presence of control, the dynamically-scoped variable can be interpreted in a purely functional way by applying a store-passing style. At the type level, the effect annotations are mapped within standard classical logic extended with the dual of implication, namely subtraction. A continuation-passing-style transformation of lambda-calculus with control and subtraction is defined. Combining the translations provides a decomposition of standard CPS transformations for delimited continuations. Incidentally, we also give a direct normalisation proof of the simply-typed lambda-calculus with control and subtraction.},
  journal = {Higher-Order and Symbolic Computation},
  language = {en},
  number = {3}
}

@inproceedings{Armbrust2015Spark,
  title = {Spark {{SQL}}: {{Relational Data Processing}} in {{Spark}}},
  shorttitle = {Spark {{SQL}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
  year = {2015},
  pages = {1383--1394},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2723372.2742797},
  abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
  isbn = {978-1-4503-2758-9},
  keywords = {data warehouse,databases,hadoop,machine learning,spark},
  note = {00000},
  series = {{{SIGMOD}} '15}
}

@inproceedings{Arntzenius2016Datafun,
  title = {Datafun: {{A}} Functional Datalog},
  shorttitle = {Datafun},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Arntzenius, Michael and Krishnaswami, Neelakantan R.},
  year = {2016},
  pages = {214--227},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951948},
  abstract = {Datalog may be considered either an unusually powerful query language or a carefully limited logic programming language. Datalog is declarative, expressive, and optimizable, and has been applied successfully in a wide variety of problem domains. However, most use-cases require extending Datalog in an application-specific manner. In this paper we define Datafun, an analogue of Datalog supporting higher-order functional programming. The key idea is to track monotonicity with types.},
  isbn = {978-1-4503-4219-3},
  keywords = {adjoint logic,Datalog,denotational semantics,domain-specific languages,functional programming,logic programming,operational semantics,Prolog,type theory},
  series = {{{ICFP}} 2016}
}

@inproceedings{Aspinall1994Subtyping,
  title = {Subtyping with Singleton Types},
  booktitle = {Computer {{Science Logic}}},
  author = {Aspinall, David},
  year = {1994},
  month = sep,
  pages = {1--15},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0022243},
  abstract = {We give syntax and a PER-model semantics for a typed {$\lambda$}-calculus with subtypes and singleton types. The calculus may be seen as a minimal calculus of subtyping with a simple form of dependent types. The aim is to study singleton types and to take a canny step towards more complex dependent subtyping systems. Singleton types have applications in the use of type systems for specification and program extraction: given a program P we can form the very tight specification \{P\} which is met uniquely by P. Singletons integrate abbreviational definitions into a type system: the hypothesis x: \{M\} asserts x=M. The addition of singleton types is a non-conservative extension of familiar subtyping theories. In our system, more terms are typable and previously typable terms have more (non-dependent) types.},
  isbn = {978-3-540-60017-6 978-3-540-49404-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Aspinall1996Subtyping,
  title = {Subtyping Dependent Types},
  booktitle = {Proceedings 11th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Aspinall, D. and Compagnoni, A.},
  year = {1996},
  month = jul,
  pages = {86--97},
  doi = {10.1109/LICS.1996.561307},
  abstract = {The need for subtyping in type-systems with dependent types has been realized for some years. But it is hard to prove that systems combining the two features have fundamental properties such as subject reduction. Here we investigate a subtyping extension of the system {$\lambda$}P, which is an abstract version of the type system of the Edinburgh Logical Framework LF. By using an equivalent formulation, we establish some important properties of the new system {$\lambda$}P{$\leqslant$}, including subject reduction. Our analysis culminates in a complete and terminating algorithm which establishes the decidability of type-checking},
  keywords = {Algorithm design and analysis,Application software,Computer languages,Computer science,decidability,dependent types,Edinburgh Logical Framework,Encoding,Laboratories,Logic programming,programming theory,subject reduction,subtyping,terminating algorithm,type theory,type-checking,type-systems}
}

@article{Aspinall2001Subtyping,
  title = {Subtyping Dependent Types},
  author = {Aspinall, David and Compagnoni, Adriana},
  year = {2001},
  month = sep,
  volume = {266},
  pages = {273--309},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(00)00175-4},
  abstract = {The need for subtyping in type systems with dependent types has been realized for some years. But it is hard to prove that systems combining the two features have fundamental properties such as subject reduction. Here we investigate a subtyping extension of the system {$\lambda$}P, which is an abstract version of the type system of the Edinburgh Logical Framework LF. By using an equivalent formulation, we establish some important properties of the new system {$\lambda$}P{$\leqslant$}, including subject reduction. Our analysis culminates in a complete and terminating algorithm which establishes the decidability of type-checking.},
  journal = {Theoretical Computer Science},
  keywords = {Dependent types,Subtyping,Type theory},
  number = {1}
}

@article{Atiyah1994Responses,
  title = {Responses to: {{A}}. {{Jaffe}} and {{F}}. {{Quinn}}, ``{{Theoretical}} Mathematics: Toward a Cultural Synthesis of Mathematics and Theoretical Physics'' [{{Bull}}. {{Amer}}. {{Math}}. {{Soc}}. ({{N}}.{{S}}.) 29 (1993), No. 1, 1\textendash{}13; {{MR1202292}} (94h:00007)]},
  shorttitle = {Responses To},
  author = {Atiyah, Michael and Borel, Armand and Chaitin, G. J. and Friedan, Daniel and Glimm, James and Gray, Jeremy J. and Hirsch, Morris W. and Mac Lane, Saunders and Mandelbrot, Benoit B. and Ruelle, David and Schwarz, Albert and Uhlenbeck, Karen and Thom, Ren{\'e} and Witten, Edward and Zeeman, Sir Christopher},
  year = {1994},
  volume = {30},
  pages = {178--207},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0273-0979-1994-00503-8},
  journal = {Bulletin of the American Mathematical Society},
  number = {2}
}

@incollection{Atkey2009syntax,
  title = {Syntax for {{Free}}: {{Representing Syntax}} with {{Binding Using Parametricity}}},
  shorttitle = {Syntax for {{Free}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Atkey, Robert},
  editor = {Curien, Pierre-Louis},
  year = {2009},
  pages = {35--49},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We show that, in a parametric model of polymorphism, the type {$\forall$} {$\alpha$}. (({$\alpha\rightarrow\alpha$}) \textrightarrow{$\alpha$}) \textrightarrow{}({$\alpha\rightarrow\alpha\rightarrow\alpha$}) \textrightarrow{$\alpha$} is isomorphic to closed de Bruijn terms. That is, the type of closed higher-order abstract syntax terms is isomorphic to a concrete representation. To demonstrate the proof we have constructed a model of parametric polymorphism inside the Coq proof assistant. The proof of the theorem requires parametricity over Kripke relations. We also investigate some variants of this representation.},
  copyright = {\textcopyright{}2009 Springer Berlin Heidelberg},
  isbn = {978-3-642-02272-2 978-3-642-02273-9},
  keywords = {Computing Methodologies,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Mathematics of Computing,Symbolic and Algebraic Manipulation},
  language = {en},
  number = {5608},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Atkey2009unembedding,
  title = {Unembedding {{Domain}}-Specific {{Languages}}},
  booktitle = {Proceedings of the {{2Nd ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Atkey, Robert and Lindley, Sam and Yallop, Jeremy},
  year = {2009},
  pages = {37--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596638.1596644},
  abstract = {Higher-order abstract syntax provides a convenient way of embedding domain-specific languages, but is awkward to analyse and manipulate directly. We explore the boundaries of higher-order abstract syntax. Our key tool is the unembedding of embedded terms as de Bruijn terms, enabling intensional analysis. As part of our solution we present techniques for separating the definition of an embedded program from its interpretation, giving modular extensions of the embedded language, and different ways to encode the types of the embedded language.},
  isbn = {978-1-60558-508-6},
  keywords = {domain-specific languages,higher-order abstract syntax,type classes,unembedding},
  series = {Haskell '09}
}

@inproceedings{Atkey2012relational,
  title = {Relational {{Parametricity}} for {{Higher Kinds}}},
  booktitle = {Computer {{Science Logic}} ({{CSL}}'12) - 26th {{International Workshop}}/21st {{Annual Conference}} of the {{EACSL}}},
  author = {Atkey, Robert},
  editor = {C{\'e}gielski, Patrick and Durand, Arnaud},
  year = {2012},
  volume = {16},
  pages = {46--61},
  doi = {10.4230/LIPIcs.CSL.2012.46},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Atkey2014relationally,
  title = {A {{Relationally Parametric Model}} of {{Dependent Type Theory}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Atkey, Robert and Ghani, Neil and Johann, Patricia},
  year = {2014},
  pages = {503--515},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2535838.2535852},
  abstract = {Reynolds' theory of relational parametricity captures the invariance of polymorphically typed programs under change of data representation. Reynolds' original work exploited the typing discipline of the polymorphically typed lambda-calculus System F, but there is now considerable interest in extending relational parametricity to type systems that are richer and more expressive than that of System F. This paper constructs parametric models of predicative and impredicative dependent type theory. The significance of our models is twofold. Firstly, in the impredicative variant we are able to deduce the existence of initial algebras for all indexed=functors. To our knowledge, ours is the first account of parametricity for dependent types that is able to lift the useful deduction of the existence of initial algebras in parametric models of System F to the dependently typed setting. Secondly, our models offer conceptual clarity by uniformly expressing relational parametricity for dependent types in terms of reflexive graphs, which allows us to unify the interpretations of types and kinds, instead of taking the relational interpretation of types as a primitive notion. Expressing our model in terms of reflexive graphs ensures that it has canonical choices for the interpretations of the standard type constructors of dependent type theory, except for the interpretation of the universe of small types, where we formulate a refined interpretation tailored for relational parametricity. Moreover, our reflexive graph model opens the door to generalisations of relational parametricity, for example to higher-dimensional relational parametricity.},
  isbn = {978-1-4503-2544-8},
  keywords = {Dependent type theory,relational parametricity},
  series = {{{POPL}} '14}
}

@inproceedings{Augusteijn1998sorting,
  title = {Sorting {{Morphisms}} (2)},
  booktitle = {3rd {{International Summer School}} on {{Advanced Functional Programming}}, Volume 1608 of {{LNCS}}},
  author = {Augusteijn, Lex},
  year = {1998},
  pages = {1--27},
  publisher = {{Springer-Verlag}},
  abstract = {Sorting algorithms can be classified in many different ways. The way presented here is by expressing the algorithms as functional programs and to classify them by means of their recursion patterns. These patterns on their turn can be classified as the natural recursion patterns that destruct or construct a given data-type, the so called cata- and anamorphisms respectively. We show that the selection of the recursion pattern can be seen as the major design decision, in most cases leaving no more room for more decisions in the design of the sorting algorithm. It is also shown that the use of alternative data structures may lead to new sorting algorithms. This presentation also serves as a gentle, light-weight, introduction into the various morphisms.}
}

@incollection{Augusteijn1999sorting,
  title = {Sorting {{Morphisms}} (1)},
  booktitle = {Advanced {{Functional Programming}}},
  author = {Augusteijn, Lex},
  editor = {Swierstra, S. Doaitse and Oliveira, Jos{\'e} N. and Henriques, Pedro R.},
  year = {1999},
  month = jan,
  pages = {1--27},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Sorting algorithms can be classified in many different ways. The way presented here is by expressing the algorithms as functional programs and to classify them by means of their recursion patterns. These patterns on their turn can be classified as the natural recursion patterns that destruct or construct a given data-type, the so called cata- and anamorphisms respectively. We show that the selection of the recursion pattern can be seen as the major design decision, in most cases leaving no more room for more decisions in the design of the sorting algorithm. It is also shown that the use of alternative data structures may lead to new sorting algorithms. This presentation also serves as a gentle, light-weight, introduction into the various morphisms.},
  copyright = {\textcopyright{}1999 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-66241-9 978-3-540-48506-3},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  number = {1608},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Augustsson1984compiler,
  title = {A {{Compiler}} for {{Lazy ML}}},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  author = {Augustsson, Lennart},
  year = {1984},
  pages = {218--227},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800055.802038},
  abstract = {LML is a strongly typed, statically scoped functional Language with Lazy evaluation. It is compiled trough a number of program transformations which makes the code generation easier. Code is generated in two steps, first code for an abstract graph manipulation machine, the G-machine. From this code machine code is generated. Some benchmark tests are also presented.},
  isbn = {0-89791-142-3},
  series = {{{LFP}} '84}
}

@inproceedings{Augustsson1999exercise,
  title = {An Exercise in Dependent Types: {{A}} Well-Typed Interpreter},
  shorttitle = {An Exercise in Dependent Types},
  booktitle = {In {{Workshop}} on {{Dependent Types}} in {{Programming}}, {{Gothenburg}}},
  author = {Augustsson, Lennart and Carlsson, Magnus},
  year = {1999},
  abstract = {The result type of an interpreter written in a typed language is normally a tagged union. By using depent types, we can be more precise about the type of values that the intepreter returns. There is no need for tagging these values with their type, something which opens the door to more efficient interpreters.}
}

@inproceedings{Avanzini2015analysing,
  title = {Analysing the {{Complexity}} of {{Functional Programs}}: {{Higher}}-Order {{Meets First}}-Order},
  shorttitle = {Analysing the {{Complexity}} of {{Functional Programs}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Avanzini, Martin and Dal Lago, Ugo and Moser, Georg},
  year = {2015},
  pages = {152--164},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784753},
  abstract = {We show how the complexity of higher-order functional programs can be analysed automatically by applying program transformations to a defunctionalised versions of them, and feeding the result to existing tools for the complexity analysis of first-order term rewrite systems. This is done while carefully analysing complexity preservation and reflection of the employed transformations such that the complexity of the obtained term rewrite system reflects on the complexity of the initial program. Further, we describe suitable strategies for the application of the studied transformations and provide ample experimental data for assessing the viability of our method.},
  isbn = {978-1-4503-3669-7},
  keywords = {defunctionalisation,term rewriting,termination and resource analysis},
  series = {{{ICFP}} 2015}
}

@article{Avigad2004forcing,
  title = {Forcing in {{Proof Theory}}},
  author = {Avigad, Jeremy},
  year = {2004},
  month = sep,
  volume = {10},
  pages = {305--333},
  issn = {1943-5894},
  doi = {10.2178/bsl/1102022660},
  abstract = {Paul Cohen's method of forcing, together with Saul Kripke's related semantics for modal and intuitionistic logic, has had profound effects on a number of branches of mathematical logic, from set theory and model theory to constructive and categorical logic. Here, I argue that forcing also has a place in traditional Hilbert-style proof theory, where the goal is to formalize portions of ordinary mathematics in restricted axiomatic theories, and study those theories in constructive or syntactic terms. I will discuss the aspects of forcing that are useful in this respect, and some sample applications. The latter include ways of obtaining conservation results for classical and intuitionistic theories, interpreting classical theories in constructive ones, and constructivizing model-theoretic arguments.},
  journal = {Bulletin of Symbolic Logic},
  number = {03}
}

@inproceedings{Awodey2009Kripke,
  title = {Kripke {{Semantics}} for {{Martin}}-{{L{\"o}f}}'s {{Extensional Type Theory}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Awodey, Steve and Rabe, Florian},
  year = {2009},
  month = jul,
  pages = {249--263},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02273-9_19},
  abstract = {It is well-known that simple type theory is complete with respect to non-standard models. Completeness for standard models only holds when increasing the class of models, e.g., to cartesian closed categories. Similarly, dependent type theory is complete for locally cartesian closed categories. However, it is usually difficult to establish the coherence of interpretations of dependent type theory, i.e., to show that the interpretations of equal expressions are indeed equal. Several classes of models have been used to remedy this problem.We contribute to this investigation by giving a semantics that is both coherent and sufficiently general for completeness while remaining relatively easy to compute with. Our models interpret types of Martin-L{\"o}f's extensional dependent type theory as sets indexed over posets or, equivalently, as fibrations over posets. This semantics can be seen as a generalization to dependent type theory of the interpretation of intuitionistic first-order logic in Kripke models. This yields a simple coherent model theory with respect to which simple and dependent type theory are sound and complete.},
  isbn = {978-3-642-02272-2 978-3-642-02273-9},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Axelsson2012generic,
  title = {A {{Generic Abstract Syntax Model}} for {{Embedded Languages}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Axelsson, Emil},
  year = {2012},
  pages = {323--334},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364527.2364573},
  abstract = {Representing a syntax tree using a data type often involves having many similar-looking constructors. Functions operating on such types often end up having many similar-looking cases. Different languages often make use of similar-looking constructions. We propose a generic model of abstract syntax trees capable of representing a wide range of typed languages. Syntactic constructs can be composed in a modular fashion enabling reuse of abstract syntax and syntactic processing within and across languages. Building on previous methods of encoding extensible data types in Haskell, our model is a pragmatic solution to Wadler's "expression problem". Its practicality has been confirmed by its use in the implementation of the embedded language Feldspar.},
  isbn = {978-1-4503-1054-3},
  keywords = {embedded domain-specific languages,generic programming,the expression problem},
  series = {{{ICFP}} '12}
}

@inproceedings{Aydemir2008Engineering,
  title = {Engineering {{Formal Metatheory}}},
  booktitle = {Proceedings of the 35th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Aydemir, Brian and Chargu{\'e}raud, Arthur and Pierce, Benjamin C. and Pollack, Randy and Weirich, Stephanie},
  year = {2008},
  pages = {3--15},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1328438.1328443},
  abstract = {Machine-checked proofs of properties of programming languages have become acritical need, both for increased confidence in large and complex designsand as a foundation for technologies such as proof-carrying code. However, constructing these proofs remains a black art, involving many choices in the formulation of definitions and theorems that make a huge cumulative difference in the difficulty of carrying out large formal developments. There presentation and manipulation of terms with variable binding is a key issue. We propose a novel style for formalizing metatheory, combining locally nameless representation of terms and cofinite quantification of free variable names in inductivedefinitions of relations on terms (typing, reduction, ...). The key technical insight is that our use of cofinite quantification obviates the need for reasoning about equivariance (the fact that free names can be renamed in derivations); in particular, the structural induction principles of relations defined using cofinite quantification are strong enough for metatheoretic reasoning, and need not be explicitly strengthened. Strong inversion principles follow (automatically, in Coq) from the induction principles. Although many of the underlying ingredients of our technique have been used before, their combination here yields a significant improvement over other methodologies using first-order representations, leading to developments that are faithful to informal practice, yet require noexternal tool support and little infrastructure within the proof assistant. We have carried out several large developments in this style using the Coq proof assistant and have made them publicly available. Our developments include type soundness for System F sub; and core ML (with references, exceptions, datatypes, recursion, and patterns) and subject reduction for the Calculus of Constructions. Not only do these developments demonstrate the comprehensiveness of our approach; they have also been optimized for clarity and robustness, making them good templates for future extension.},
  isbn = {978-1-59593-689-9},
  keywords = {binding,coq,locally nameless},
  note = {00000},
  series = {{{POPL}} '08}
}

@article{BachPoulsen2017Intrinsicallytyped,
  title = {Intrinsically-Typed {{Definitional Interpreters}} for {{Imperative Languages}}},
  author = {Bach Poulsen, Casper and Rouvoet, Arjen and Tolmach, Andrew and Krebbers, Robbert and Visser, Eelco},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {16:1--16:34},
  issn = {2475-1421},
  doi = {10.1145/3158104},
  abstract = {A definitional interpreter defines the semantics of an object language in terms of the (well-known) semantics of a host language, enabling understanding and validation of the semantics through execution. Combining a definitional interpreter with a separate type system requires a separate type safety proof. An alternative approach, at least for pure object languages, is to use a dependently-typed language to encode the object language type system in the definition of the abstract syntax. Using such intrinsically-typed abstract syntax definitions allows the host language type checker to verify automatically that the interpreter satisfies type safety. Does this approach scale to larger and more realistic object languages, and in particular to languages with mutable state and objects?  In this paper, we describe and demonstrate techniques and libraries in Agda that successfully scale up intrinsically-typed definitional interpreters to handle rich object languages with non-trivial binding structures and mutable state. While the resulting interpreters are certainly more complex than the simply-typed {$\lambda$}-calculus interpreter we start with, we claim that they still meet the goals of being concise, comprehensible, and executable, while guaranteeing type safety for more elaborate object languages. We make the following contributions: (1) A dependent-passing style technique for hiding the weakening of indexed values as they propagate through monadic code. (2) An Agda library for programming with scope graphs and frames, which provides a uniform approach to dealing with name binding in intrinsically-typed interpreters. (3) Case studies of intrinsically-typed definitional interpreters for the simply-typed {$\lambda$}-calculus with references (STLC+Ref) and for a large subset of Middleweight Java (MJ).},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Agda,definitional interpreters,dependent types,Java,mechanized semantics,scope graphs,type safety},
  number = {POPL}
}

@article{Back1988calculus,
  title = {A Calculus of Refinements for Program Derivations},
  author = {Back, R. J. R.},
  year = {1988},
  month = aug,
  volume = {25},
  pages = {593--624},
  issn = {0001-5903, 1432-0525},
  doi = {10.1007/BF00291051},
  abstract = {A calculus of program refinements is described, to be used as a tool for the step-by-step derivation of correct programs. A derivation step is considered correct if the new program preserves the total correctness of the old program. This requirement is expressed as a relation of (correct) refinement between nondeterministic program statements. The properties of this relation are studied in detail. The usual sequential statement constructors are shown to be monotone with respect to this relation and it is shown how refinement between statements can be reduced to a proof of total correctness of the refining statement. A special emphasis is put on the correctness of replacement steps, where some component of a program is replaced by another component. A method by which assertions can be added to statements to justify replacements in specific contexts is developed. The paper extends the weakest precondition technique of Dijkstra to proving correctness of larger program derivation steps, thus providing a unified framework for the axiomatic, the stepwise refinement and the transformational approach to program construction and verification.},
  journal = {Acta Informatica},
  keywords = {Computational Mathematics and Numerical Analysis,Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Information Systems and Communication Service,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {6}
}

@article{Backus1978Can,
  title = {Can {{Programming Be Liberated}} from the {{Von Neumann Style}}?: {{A Functional Style}} and {{Its Algebra}} of {{Programs}}},
  shorttitle = {Can {{Programming Be Liberated}} from the {{Von Neumann Style}}?},
  author = {Backus, John},
  year = {1978},
  month = aug,
  volume = {21},
  pages = {613--641},
  issn = {0001-0782},
  doi = {10.1145/359576.359579},
  abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor\textemdash{}the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.
An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.
Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose ``unknowns'' are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.
 A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states\textemdash{}only one state transition occurs per major computation.},
  journal = {Commun. ACM},
  keywords = {algebra of programs,applicative computing systems,applicative state transition systems,combining forms,functional forms,functional programming,metacomposition,models of computing systems,program correctness,program termination,program transformation,programming languages,von Neumann computers,von Neumann languages},
  number = {8}
}

@article{Baez2009physics,
  title = {Physics, {{Topology}}, {{Logic}} and {{Computation}}: {{A Rosetta Stone}}},
  shorttitle = {Physics, {{Topology}}, {{Logic}} and {{Computation}}},
  author = {Baez, John C. and Stay, Mike},
  year = {2009},
  month = mar,
  abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology: namely, a linear operator behaves very much like a "cobordism". Similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of "closed symmetric monoidal category". We assume no prior knowledge of category theory, proof theory or computer science.},
  archivePrefix = {arXiv},
  eprint = {0903.0340},
  eprinttype = {arxiv},
  journal = {arXiv:0903.0340 [quant-ph]},
  keywords = {Mathematics - Category Theory,Quantum Physics},
  primaryClass = {quant-ph}
}

@inproceedings{Bahr2015generalising,
  title = {Generalising {{Tree Traversals}} to {{DAGs}}: {{Exploiting Sharing Without}} the {{Pain}}},
  shorttitle = {Generalising {{Tree Traversals}} to {{DAGs}}},
  booktitle = {Proceedings of the 2015 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Bahr, Patrick and Axelsson, Emil},
  year = {2015},
  pages = {27--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2678015.2682539},
  abstract = {We present a recursion scheme based on attribute grammars that can be transparently applied to trees and acyclic graphs. Our recursion scheme allows the programmer to implement a tree traversal and then apply it to compact graph representations of trees instead. The resulting graph traversals avoid recomputation of intermediate results for shared nodes -- even if intermediate results are used in different contexts. Consequently, this approach leads to asymptotic speedup proportional to the compression provided by the graph representation. In general, however, this sharing of intermediate results is not sound. Therefore, we complement our implementation of the recursion scheme with a number of correspondence theorems that ensure soundness for various classes of traversals. We illustrate the practical applicability of the implementation as well as the complementing theory with a number of examples.},
  isbn = {978-1-4503-3297-2},
  keywords = {attribute grammars,graph traversal,Haskell,sharing},
  series = {{{PEPM}} '15}
}

@article{Bainbridge1990functorial,
  title = {Functorial Polymorphism},
  author = {Bainbridge, E. S. and Freyd, P. J. and Scedrov, A. and Scott, P. J.},
  year = {1990},
  month = jan,
  volume = {70},
  pages = {35--64},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(90)90151-7},
  journal = {Theoretical Computer Science},
  number = {1},
  series = {Special {{Issue Fourth Workshop}} on {{Mathematical Foundations}} of {{Programming Semantics}}, {{Boulder}}, {{CO}}, {{May}} 1988}
}

@article{Bakouny2017Coqbased,
  title = {A {{Coq}}-Based Synthesis of {{Scala}} Programs Which Are Correct-by-Construction},
  author = {Bakouny, Youssef El and Crolard, Tristan and Mezher, Dani},
  year = {2017},
  month = jun,
  doi = {10.1145/3103111.3104041},
  abstract = {The present paper introduces Scala-of-Coq, a new compiler that allows a Coq-based synthesis of Scala programs which are "correct-by-construction". A typical workflow features a user implementing a Coq functional program, proving this program's correctness with regards to its specification and making use of Scala-of-Coq to synthesize a Scala program that can seamlessly be integrated into an existing industrial Scala or Java application.},
  archivePrefix = {arXiv},
  eprint = {1706.05271},
  eprinttype = {arxiv},
  journal = {arXiv:1706.05271 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Balabonski2017Foundations,
  title = {Foundations of {{Strong Call}} by {{Need}}},
  author = {Balabonski, Thibaut and Barenbaum, Pablo and Bonelli, Eduardo and Kesner, Delia},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {20:1--20:29},
  issn = {2475-1421},
  doi = {10.1145/3110264},
  abstract = {We present a call-by-need strategy for computing strong normal forms of open terms (reduction is admitted inside the body of abstractions and substitutions, and the terms may contain free variables), which guarantees that arguments are only evaluated when needed and at most once. The strategy is shown to be complete with respect to {\^I}{$^2$}-reduction to strong normal form. The proof of completeness relies on two key tools: (1) the definition of a strong call-by-need calculus where reduction may be performed inside any context, and (2) the use of non-idempotent intersection types. More precisely, terms admitting a {\^I}{$^2$}-normal form in pure lambda calculus are typable, typability implies (weak) normalisation in the strong call-by-need calculus, and weak normalisation in the strong call-by-need calculus implies normalisation in the strong call-by-need strategy. Our (strong) call-by-need strategy is also shown to be conservative over the standard (weak) call-by-need.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Call-by-need,Completeness,Evaluation Strategies},
  number = {ICFP}
}

@article{Baldan1999Basic,
  title = {Basic {{Theory}} of {{F}}-{{Bounded Quantification}}},
  author = {Baldan, Paolo and Ghelli, Giorgio and Raffaet{\`a}, Alessandra},
  year = {1999},
  month = sep,
  volume = {153},
  pages = {173--237},
  issn = {0890-5401},
  doi = {10.1006/inco.1999.2802},
  abstract = {System F-bounded is a second-order typed lambda calculus, where the basic features of object-oriented languages can be naturally modelled. F-bounded extends the better known system F{$\leqslant$}, in a way that provides an immediate solution for the treatment of the so-called ``binary methods.'' Although more powerful than F{$\leqslant$} and also quite natural, system F-bounded has only been superficially studied from a foundational perspective and many of its essential properties have been conjectured but never proved in the literature. The aim of this paper is to give a solid foundation to F-bounded, by addressing and proving the key properties of the system. In particular, transitivity elimination, completeness of the type checking semi-algorithm, the subject reduction property for {$\beta\eta$} reduction, conservativity with respect to system F{$\leqslant$}, and antisymmetry of a ``full'' subsystem are considered, and various possible formulations for system F-bounded are compared. Finally, a semantic interpretation of system F-bounded is presented, based on partial equivalence relations.},
  journal = {Information and Computation},
  keywords = {_tablet},
  number = {2}
}

@article{Barbanera1994prooffunctional,
  title = {Proof-Functional Connectives and Realizability},
  author = {Barbanera, Franco and Martini, Simone},
  year = {1994},
  month = may,
  volume = {33},
  pages = {189--211},
  issn = {0933-5846, 1432-0665},
  doi = {10.1007/BF01203032},
  abstract = {The meaning of a formula built out of proof-functional connectives depends in an essential way upon the intensional aspect of the proofs of the component subformulas. We study three such connectives, strong equivalence (where the two directions of the equivalence are established by mutually inverse maps), strong conjunction (where the two components of the conjunction are established by the same proof) and relevant implication (where the implication is established by an identity map). For each of these connectives we give a type assignment system, a realizability semantics, and a completeness theorem. This form of completeness implies the semantic completeness of the type assignment system.},
  journal = {Archive for Mathematical Logic},
  keywords = {03F07,03F55,Algebra,Mathematical Logic and Foundations,Mathematics; general},
  language = {en},
  number = {3}
}

@article{Barbanera1996Proofirrelevance,
  title = {Proof-Irrelevance out of Excluded-Middle and Choice in the Calculus of Constructions},
  author = {Barbanera, Franco and Berardi, Stefano},
  year = {1996},
  month = may,
  volume = {6},
  pages = {519--526},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796800001829},
  abstract = {We present a short and direct syntactic proof of the fact that adding the axiom of choice and the principle of excluded-middle to Coquand\textendash{}Huet's Calculus of Constructions gives proof-irrelevance.},
  journal = {Journal of Functional Programming},
  keywords = {_tablet},
  language = {en},
  number = {3}
}

@article{Barbanera1996symmetric,
  title = {A {{Symmetric Lambda Calculus}} for {{Classical Program Extraction}}},
  author = {Barbanera, Franco and Berardi, Stefano},
  year = {1996},
  month = mar,
  volume = {125},
  pages = {103--117},
  issn = {0890-5401},
  doi = {10.1006/inco.1996.0025},
  abstract = {We introduce a{$\lambda$}-calculus with symmetric reduction rules and ``classical'' types, i.e., types corresponding to formulas of classical propositional logic. The strong normalization property is proved to hold for such a calculus, as well as for its extension to a system equivalent to Peano arithmetic. A theorem on the shape of terms in normal form is also proved, making it possible to get recursive functions out of proofs of{$\Pi$}02formulas, i.e., those corresponding to program specifications.},
  journal = {Information and Computation},
  number = {2}
}

@book{Barendregt1984Lambda,
  title = {The {{Lambda Calculus}}: {{Its Syntax}} and {{Semantics}}},
  shorttitle = {The {{Lambda Calculus}}},
  author = {Barendregt, H. P.},
  year = {1984},
  publisher = {{Elsevier}}
}

@article{Barendregt2009applications,
  title = {Applications of Infinitary Lambda Calculus},
  author = {Barendregt, Henk and Klop, Jan Willem},
  year = {2009},
  month = may,
  volume = {207},
  pages = {559--582},
  issn = {0890-5401},
  doi = {10.1016/j.ic.2008.09.003},
  abstract = {We present an introduction to infinitary lambda calculus, highlighting its main properties. Subsequently we give three applications of infinitary lambda calculus. The first addresses the non-definability of Surjective Pairing, which was shown by the first author not to be definable in lambda calculus. We show how this result follows easily as an application of Berry's Sequentiality Theorem, which itself can be proved in the setting of infinitary lambda calculus. The second pertains to the notion of relative recursiveness of number-theoretic functions. The third application concerns an explanation of counterexamples to confluence of lambda calculus extended with non-left-linear reduction rules: Adding non-left-linear reduction rules such as {$\delta$} xx \textrightarrow{} x or the reduction rules for Surjective Pairing to the lambda calculus yields non-confluence, as proved by the second author. We discuss how an extension to the infinitary lambda calculus, where B{\"o}hm trees can be directly manipulated as infinite terms, yields a more simple and intuitive explanation of the correctness of these Church-Rosser counterexamples.},
  journal = {Information and Computation},
  number = {5},
  series = {From {{Type Theory}} to {{Morphological Complexity}}: {{Special Issue}} Dedicated to the 60th {{Birthday Anniversary}} of {{Giuseppe Longo}}}
}

@inproceedings{Barras1996Verification,
  title = {Verification of the Interface of a Small Proof System in Coq},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Barras, Bruno},
  year = {1996},
  month = dec,
  pages = {28--45},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0097785},
  abstract = {This article describes the formalization of the interface of a proof-checker. The latter is based on a kernel consisting of type-checking functions for the Calculus of Constructions, but it seems the ideas can generalize to other type systems, as far as they are based on the proofsas-terms principle. We suppose that the metatheory of the corresponding type system is proved (up to type decidability). We specify and certify the toplevel loop, the system invariant, and the error messages.},
  isbn = {978-3-540-65137-6 978-3-540-49562-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@techreport{Barras1997Coq,
  title = {Coq in {{Coq}}},
  author = {Barras, Bruno and Werner, Benjamin},
  year = {1997},
  abstract = {. We formalize the definition and the metatheory of the Calculus of Constructions (CC) using the proof assistant Coq. In particular, we prove strong normalization and decidability of type inference. From the latter proof, we extract a certified Objective Caml program which performs type inference in CC and use this code to build a small-scale certified proof-checker.  Key words: Type Theory, proof-checker, Calculus of Constructions, metatheory, strong normalization proof, program extraction. 1. Introduction  1.1. Motivations  This work can be described as the formal certification in Coq of a proof-checker for the Calculus of Constructions (CC). We view it as a first experimental step towards a certified kernel for the whole Coq  system, of which CC is a significative fragment. In decidable type theories, a proof-checker is a program which verifies whether a given judgement (input) is valid or not (output). Valid meaning that there exists a derivation for that judgement following the in...}
}

@article{Barrett2016virtual,
  title = {Virtual {{Machine Warmup Blows Hot}} and {{Cold}}},
  author = {Barrett, Edd and Bolz, Carl Friedrich and Killick, Rebecca and Knight, Vincent and Mount, Sarah and Tratt, Laurence},
  year = {2016},
  month = feb,
  abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: first the warmup phase determines which parts of a program would most benefit from dynamic compilation; after compilation has occurred the program is said to be at peak performance. When measuring the performance of JIT compiling VMs, data collected during the warmup phase is generally discarded, placing the focus on peak performance. In this paper we run a number of small, deterministic benchmarks on a variety of well known VMs. In our experiment, less than one quarter of the benchmark/VM pairs conform to the traditional notion of warmup, and none of the VMs we tested consistently warms up in the traditional notion. This raises a number of questions about VM benchmarking, which are of interest to both VM authors and end users.},
  archivePrefix = {arXiv},
  eprint = {1602.00602},
  eprinttype = {arxiv},
  journal = {arXiv:1602.00602 [cs]},
  keywords = {Computer Science - Programming Languages,D.3},
  primaryClass = {cs}
}

@article{Barthe1998monadic,
  title = {Monadic {{Type Systems}}: {{Pure Type Systems}} for {{Impure Settings}} ({{Preliminary Report}})},
  shorttitle = {Monadic {{Type Systems}}},
  author = {Barthe, Gilles and Hatcliff, John and Thiemann, Peter},
  year = {1998},
  volume = {10},
  pages = {54--120},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(05)80691-7},
  abstract = {Pure type systems and computational monads are two parameterized frameworks that have proved to be quite useful in both theoretical and practical applications. We join the foundational concepts of both of these to obtain monadic type systems. Essentially, monadic type systems inherit the parameterized higher-order type structure of pure type systems and the monadic term and type structure used to capture computational effects in the theory of computational monads. We demonstrate that monadic type systems nicely characterize previous work and suggest how they can support several new theoretical and practical applications.

A technical foundation for monadic type systems is laid by recasting and scaling up the main results from pure type systems (confluence, subject reduction, strong normalisation for particular classes of systems, etc.) and from operational presentations of computational monads (notions of operational equivalence based on applicative similarity, co-induction proof techniques).

We demonstrate the use of monadic type systems with case studies of several call-by-value and call-by-name systems. Our framework allows to capture the restriction to value polymorphism in the type structure and is flexible enough to accommodate extensions of the type system, e.g., with higher-order polymorphism. The theoretical foundations make monadic type systems well-suited as a typed intermediate language for compilation and specialization of higher-order, strict and non-strict functional programs. The monadic structure guarantees sound compile-time optimizations and the parameterized type structure guarantees sufficient expressiveness.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {functional programming,Monads,operational semantics,polymorphism,pure type systems},
  series = {{{HOOTS II}}, {{Second Workshop}} on {{Higher}}-{{Order Operational Techniques}} in {{Semantics}}}
}

@article{Barthe1999typechecking,
  title = {Type-Checking Injective Pure Type Systems},
  author = {Barthe, Gilles},
  year = {1999},
  volume = {9},
  pages = {675--698},
  abstract = {Injective pure type systems form a large class of pure type systems for which one can compute by purely syntactic means two sorts elmt({$\Gamma$}|M) and sort({$\Gamma$}|M), where {$\Gamma$} is a pseudo-context and M is a pseudo-term, and such that for every sort s,
{$\Gamma\vdash$}M:A {$\wedge$} {$\Gamma\vdash$}A:s {$\Rightarrow$} elmt({$\Gamma$}|M)=s
{$\Gamma$} {$\vdash$} M:s {$\Rightarrow$} sort({$\Gamma$}|M)=s.
By eliminating the problematic clause in the (abstraction) rule in favor of constraints over elmt(.|.) and sort(.|.), we provide a sound and complete type-checking algorithm for injective pure type systems. In addition, we prove expansion postponement for a variant of injective pure type systems where the problematic clause in the (abstraction) rule is replaced in favor of constraints over elmt(.|.) and sort(.|.).},
  journal = {Journal of Functional Programming},
  number = {06}
}

@inproceedings{Barthe2002CPS,
  title = {{{CPS Translating Inductive}} and {{Coinductive Types}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Semantics}}-Based {{Program Manipulation}}},
  author = {Barthe, Gilles and Uustalu, Tarmo},
  year = {2002},
  pages = {131--142},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/503032.503043},
  abstract = {We investigate CPS translatability of typed {$\lambda$}-calculi with inductive and coinductive types. We show that tenable Plotkin-style call-by-name CPS translations exist for simply typed {$\lambda$}-calculi with a natural number type and stream types and, more generally, with arbitrary positive inductive and coinductive types. These translations also work in the presence of control operators and generalize for dependently typed calculi where case-like eliminations are only allowed in non-dependent forms. No translation is possible along the same lines for small {$\Sigma$}-types and sum types with dependent case.},
  isbn = {978-1-58113-455-1},
  keywords = {classical logic and control,CPS translations,dependent types,inductive and coinductive types,typed $\\lambda$-calculi},
  series = {{{PEPM}} '02}
}

@inproceedings{Barthe2003pure,
  title = {Pure {{Patterns Type Systems}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Barthe, Gilles and Cirstea, Horatiu and Kirchner, Claude and Liquori, Luigi},
  year = {2003},
  pages = {250--261},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/604131.604152},
  abstract = {We introduce a new framework of algebraic pure type systems in which we consider rewrite rules as lambda terms with patterns and rewrite rule application as abstraction application with built-in matching facilities. This framework, that we call "Pure Pattern Type Systems", is particularly well-suited for the foundations of programming (meta)languages and proof assistants since it provides in a fully unified setting higher-order capabilities and pattern matching ability together with powerful type systems. We prove some standard properties like confluence and subject reduction for the case of a syntactic theory and under a syntactical restriction over the shape of patterns. We also conjecture the strong normalization of typable terms. This work should be seen as a contribution to a formal connection between logics and rewriting, and a step towards new proof engines based on the Curry-Howard isomorphism.},
  isbn = {1-58113-628-5},
  keywords = {Curry-Howard,Lambda-calculus,logics,matching,patterns,pure type systems,rewriting},
  series = {{{POPL}} '03}
}

@inproceedings{Bastin2014scaladyno,
  title = {{{ScalaDyno}}: {{Making Name Resolution}} and {{Type Checking Fault}}-Tolerant},
  shorttitle = {{{ScalaDyno}}},
  booktitle = {Proceedings of the {{Fifth Annual Scala Workshop}}},
  author = {Bastin, C{\'e}dric and Ureche, Vlad and Odersky, Martin},
  year = {2014},
  pages = {1--5},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2637647.2637649},
  abstract = {The ScalaDyno compiler plugin allows fast prototyping with the Scala programming language, in a way that combines the benefits of both statically and dynamically typed languages. Static name resolution and type checking prevent partially-correct code from being compiled and executed. Yet, allowing programmers to test critical paths in a program without worrying about the consistency of the entire code base is crucial to fast prototyping and agile development. This is where ScalaDyno comes in: it allows partially-correct programs to be compiled and executed, while shifting compile-time errors to program runtime. The key insight in ScalaDyno is that name and type errors affect limited areas of the code, which can be replaced by instructions to output the respective errors at runtime. This allows byte code generation and execution for partially correct programs, thus allowing Python or JavaScript-like fast prototyping in Scala. This is all done without sacrificing name resolution, full type checking and optimizations for the correct parts of the code -- they are still performed, but without getting in the way of agile development. Finally, for release code or sensitive refactoring, runtime errors can be disabled, thus allowing full static name resolution and type checking typical of the Scala compiler.},
  isbn = {978-1-4503-2868-5},
  keywords = {deferred type errors,dynamic typing,Scala},
  series = {{{SCALA}} '14}
}

@article{Bauer2009rz,
  title = {{{RZ}}: A {{Tool}} for {{Bringing Constructive}} and {{Computable Mathematics Closer}} to {{Programming Practice}}},
  shorttitle = {{{RZ}}},
  author = {Bauer, Andrej and Stone, Christopher A.},
  year = {2009},
  month = jan,
  volume = {19},
  pages = {17--43},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/exn026},
  abstract = {Realizability theory is not just a fundamental tool in logic and computability. It also has direct application to the design and implementation of programs, since it can produce code interfaces for the data structure corresponding to a mathematical theory. Our tool, called RZ, serves as a bridge between the worlds of constructive mathematics and programming. By using the realizability interpretation of constructive mathematics, RZ translates specifications in constructive logic into annotated interface code in Objective Caml. The system supports a rich input language allowing descriptions of complex mathematical structures. RZ does not extract code from proofs, but allows any implementation method, from handwritten code to code extracted from proofs by other tools.},
  journal = {Journal of Logic and Computation},
  language = {en},
  number = {1}
}

@article{Bauerprogramming,
  title = {Programming with Algebraic Effects and Handlers},
  author = {Bauer, Andrej and Pretnar, Matija},
  issn = {2352-2208},
  doi = {10.1016/j.jlamp.2014.02.001},
  abstract = {Eff is a programming language based on the algebraic approach to computational effects, in which effects are viewed as algebraic operations and effect handlers as homomorphisms from free algebras. Eff supports first-class effects and handlers through which we may easily define new computational effects, seamlessly combine existing ones, and handle them in novel ways. We give a denotational semantics of Eff and discuss a prototype implementation based on it. Through examples we demonstrate how the standard effects are treated in Eff, and how Eff supports programming techniques that use various forms of delimited continuations, such as backtracking, breadth-first search, selection functionals, cooperative multi-threading, and others.},
  journal = {Journal of Logical and Algebraic Methods in Programming}
}

@inproceedings{Bawden1999quasiquotation,
  title = {Quasiquotation in {{Lisp}} (2)},
  booktitle = {Partial {{Evaluation}} and {{Semantic}}-{{Based Program Manipulation}}},
  author = {Bawden, Alan},
  year = {1999},
  pages = {4--12},
  abstract = {Quasiquotation is the technology commonly used in Lisp to write program-generating
programs. This paper explains how quasiquotation works, why it works well, and what its limitations
are. A brief history of quasiquotation is included.},
  keywords = {lisp,metaprogramming,quasiquote,quote}
}

@inproceedings{Bawden1999quasiquotationa,
  title = {Quasiquotation in {{Lisp}} (1)},
  booktitle = {O. {{Danvy}}, {{Ed}}., {{University}} of {{Aarhus}}, {{Dept}}. of {{Computer Science}}},
  author = {Bawden, Alan},
  year = {1999},
  pages = {88--99},
  abstract = {Quasiquotation is the technology commonly used in Lisp to write program-generating programs. In this paper I will review the history and development of this technology, and explain why it works so well in practice.}
}

@inproceedings{Beguet2014accelerating,
  title = {Accelerating {{Parser Combinators}} with {{Macros}}},
  booktitle = {Proceedings of the {{Fifth Annual Scala Workshop}}},
  author = {B{\'e}guet, Eric and Jonnalagedda, Manohar},
  year = {2014},
  pages = {7--17},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2637647.2637653},
  abstract = {Parser combinators provide an elegant way of writing parsers: parser implementations closely follow the structure of the underlying grammar, while accommodating interleaved host language code for data processing. However, the host language features used for composition introduce substantial overhead, which leads to poor performance. In this paper, we present a technique to systematically eliminate this overhead. We use Scala macros to analyse the grammar specification at compile-time and remove composition, leaving behind an efficient top-down, recursive-descent parser. We compare our macro-based approach to a staging-based approach using the LMS framework, and provide an experience report in which we discuss the advantages and drawbacks of both methods. Our library outperforms Scala's standard parser combinators on a set of benchmarks by an order of magnitude, and is 2x faster than code generated by LMS.},
  isbn = {978-1-4503-2868-5},
  keywords = {macros,optimization,parser combinators,Scala},
  series = {{{SCALA}} '14}
}

@inproceedings{Bell1997typedriven,
  title = {Type-Driven {{Defunctionalization}}},
  booktitle = {Proceedings of the {{Second ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bell, Jeffrey M. and Bellegarde, Fran{\c c}oise and Hook, James},
  year = {1997},
  pages = {25--37},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/258948.258953},
  abstract = {In 1972, Reynolds outlined a general method for eliminating functional arguments known as defunctionalization. The idea underlying defunctionalization is encoding a functional value as first-order data, and then realizing the applications of the encoded function via an apply function. Although this process is simple enough, problems arise when defunctionalization is used in a polymorphic language. In such a language, a functional argument of a higher-order function can take different type instances in different applications. As a consequence, its associated apply function can be untypable in the source language. In the paper we present a defunctionalization transformation which preserves typability. Moreover, the transformation imposes no restriction on functional arguments of recursive functions, and it handles functions as results as well as functions encapsulated in constructors. The key to this success is the use of type information in the defunctionalization transformation. Run-time characteristics are preserved by defunctionalization; hence, there is no performance improvement coming from the transformation itself. However closures need not be implemented to compile the transformed program.},
  isbn = {0-89791-918-1},
  series = {{{ICFP}} '97}
}

@inproceedings{Benton1996linear,
  title = {Linear Logic, Monads and the Lambda Calculus},
  booktitle = {, {{Eleventh Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}, 1996. {{LICS}} '96. {{Proceedings}}},
  author = {Benton, N. and Wadler, P.},
  year = {1996},
  month = jul,
  pages = {420--431},
  doi = {10.1109/LICS.1996.561458},
  abstract = {Models of intuitionistic linear logic also provide models of Moggi's computational metalanguage. We use the adjoint presentation of these models and the associated adjoint calculus to show that three translations, due mainly to Moggi, of the lambda calculus into the computational metalanguage (direct, call-by-name and call-by-value) correspond exactly to three translations, due mainly to Girard, of intuitionistic logic into intuitionistic linear logic. We also consider extending these results to languages with recursion},
  keywords = {adjoint calculus,adjoint presentation,Calculus,call-by-name,call-by-value,Computational modeling,Ear,formal logic,intuitionistic linear logic,Laboratories,lambda calculus,linear logic,Logic,Moggi's computational metalanguage,Monads,Proposals}
}

@article{Benton1998computational,
  title = {Computational Types from a Logical Perspective},
  author = {Benton, P. N. and Bierman, G. M. and De Paiva, V. C. V.},
  year = {1998},
  month = mar,
  volume = {8},
  pages = {177--193},
  issn = {1469-7653},
  doi = {null},
  abstract = {Moggi\&apos;s computational lambda calculus is a metalanguage for denotational semantics which arose from the observation that many different notions of computation have the categorical structure of a strong monad on a cartesian closed category. In this paper we show that the computational lambda calculus also arises naturally as the term calculus corresponding (by the Curry\textendash{}Howard correspondence) to a novel intuitionistic modal propositional logic. We give natural deduction, sequent calculus and Hilbert-style presentations of this logic and prove strong normalisation and confluence results.},
  journal = {Journal of Functional Programming},
  number = {02}
}

@inproceedings{Benton2009Biorthogonality,
  title = {Biorthogonality, {{Step}}-Indexing and {{Compiler Correctness}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Benton, Nick and Hur, Chung-Kil},
  year = {2009},
  pages = {97--108},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596550.1596567},
  abstract = {We define logical relations between the denotational semantics of a simply typed functional language with recursion and the operational behaviour of low-level programs in a variant SECD machine. The relations, which are defined using biorthogonality and stepindexing, capture what it means for a piece of low-level code to implement a mathematical, domain-theoretic function and are used to prove correctness of a simple compiler. The results have been formalized in the Coq proof assistant.},
  isbn = {978-1-60558-332-7},
  keywords = {biorthogonality,compiler verification,Denotational semantics,proof assistants,step-indexing},
  series = {{{ICFP}} '09}
}

@article{Benton2012strongly,
  title = {Strongly {{Typed Term Representations}} in {{Coq}}},
  author = {Benton, Nick and Hur, Chung-Kil and Kennedy, Andrew J. and McBride, Conor},
  year = {2012},
  month = aug,
  volume = {49},
  pages = {141--159},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-011-9219-0},
  abstract = {There are two approaches to formalizing the syntax of typed object languages in a proof assistant or programming language. The extrinsic approach is to first define a type that encodes untyped object expressions and then make a separate definition of typing judgements over the untyped terms. The intrinsic approach is to make a single definition that captures well-typed object expressions, so ill-typed expressions cannot even be expressed. Intrinsic encodings are attractive and naturally enforce the requirement that metalanguage operations on object expressions, such as substitution, respect object types. The price is that the metalanguage types of intrinsic encodings and operations involve non-trivial dependency, adding significant complexity. This paper describes intrinsic-style formalizations of both simply-typed and polymorphic languages, and basic syntactic operations thereon, in the Coq proof assistant. The Coq types encoding object-level variables (de Bruijn indices) and terms are indexed by both type and typing environment. One key construction is the boot-strapping of definitions and lemmas about the action of substitutions in terms of similar ones for a simpler notion of renamings. In the simply-typed case, this yields definitions that are free of any use of type equality coercions. In the polymorphic case, some substitution operations do still require type coercions, which we at least partially tame by uniform use of heterogeneous equality.},
  journal = {Journal of Automated Reasoning},
  keywords = {_tablet_modified,Artificial Intelligence (incl. Robotics),de Bruijn indices,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Symbolic and Algebraic Manipulation,The Coq proof assistant,Typed object languages},
  language = {en},
  number = {2}
}

@article{Berardi2008typed,
  title = {A Typed Lambda Calculus with Intersection Types},
  shorttitle = {Calculi, {{Types}} and {{Applications}}},
  author = {Berardi, S. and {de'Liguoro}, U. and Bono, Viviana and Venneri, Betti and Bettini, Lorenzo},
  year = {2008},
  month = may,
  volume = {398},
  pages = {95--113},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2008.01.046},
  abstract = {Intersection types are well known to type theorists mainly for two reasons. Firstly, they type all and only the strongly normalizable lambda terms. Secondly, the intersection type operator is a meta-level operator, that is, there is no direct logical counterpart in the Curry\textendash{}Howard isomorphism sense. In particular, its meta-level nature implies that it does not correspond to the intuitionistic conjunction. The intersection type system is naturally a type inference system (system {\`a} la Curry), but the meta-level nature of the intersection operator does not allow to easily design an equivalent typed system (system {\`a} la Church). There are many proposals in the literature to design such systems, but none of them gives an entirely satisfactory answer to the problem. In this paper, we will review the main results in the literature both on the logical interpretation of intersection types and on proposed typed lambda calculi. The core of this paper is a new proposal for a true intersection typed lambda calculus, without any meta-level notion. Namely, any typable term (in the intersection type inference) has a corresponding typed term (which is the same as the untyped term by erasing the type decorations and the typed term constructors) with the same type, and vice versa. The main idea is to introduce a relevant parallel term constructor which corresponds to the intersection type constructor, in such a way that terms in parallel share the same resources, that is, the same context of free typed variables. Three rules allow us to generate all typed terms. The first two rules, Application and Lambda-abstraction, are performed on all the components of a parallel term in a synchronized way. Finally, via the third rule of Local Renaming, once a free typed variable is bounded by lambda-abstraction, each of the terms in parallel can do its local renaming, with type refinement, of that particular resource.},
  journal = {Theoretical Computer Science},
  keywords = {Church style,Curry style,intersection types,lambda calculus,Parallelism,Shared resources,type inference},
  number = {1}
}

@article{Berger2012typed,
  title = {Typed vs. {{Untyped Realizability}}},
  author = {Berger, Ulrich and Hou, Tie},
  year = {2012},
  month = sep,
  volume = {286},
  pages = {57--71},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2012.08.005},
  abstract = {We study the domain-theoretic semantics of a Church-style typed {$\lambda$}-calculus with constructors, pattern matching and recursion, and show that it is closely related to the semantics of its untyped counterpart. The motivation for this study comes from program extraction from proofs via realizability where one has the choice of extracting typed or untyped terms from proofs. Our result shows that under a certain regularity condition, the choice is irrelevant. The regularity condition is that in every use of a fixed point type fix {$\alpha$}.{$\rho$}, {$\alpha$} occurs only positively in {$\rho$}.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {logical relation,Program extraction,realizability,Scott domain,typed and untyped λ-calculus},
  series = {Proceedings of the 28th {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXVIII}})}
}

@misc{Beringerabstract,
  title = {An {{Abstract View}} of {{Programming Languages}}},
  author = {Beringer, Lennart},
  howpublished = {http://www.lfcs.inf.ed.ac.uk/reports/90/ECS-LFCS-90-113/}
}

@misc{BeringerCandidates,
  title = {Candidates for {{Substitution}}},
  author = {Beringer, Lennart},
  howpublished = {http://www.lfcs.inf.ed.ac.uk/reports/97/ECS-LFCS-97-358/}
}

@misc{Beringerintroduction,
  title = {An Introduction to Fibrations, Topos Theory, the Effective Topos and Modest Sets},
  author = {Beringer, Lennart},
  howpublished = {http://www.lfcs.inf.ed.ac.uk/reports/92/ECS-LFCS-92-208/}
}

@article{Bernadet2013simple,
  title = {A Simple Presentation of the Effective Topos},
  author = {Bernadet, Alexis and {Graham-Lengrand}, St{\'e}phane},
  year = {2013},
  month = jul,
  abstract = {We propose for the Effective Topos an alternative construction: a realisability framework composed of two levels of abstraction. This construction simplifies the proof that the Effective Topos is a topos (equipped with natural numbers), which is the main issue that this paper addresses. In this our work can be compared to Frey's monadic tripos-to-topos construction. However, no topos theory or even category theory is here required for the construction of the framework itself, which provides a semantics for higher-order type theories, supporting extensional equalities and the axiom of unique choice.},
  archivePrefix = {arXiv},
  eprint = {1307.3832},
  eprinttype = {arxiv},
  journal = {arXiv:1307.3832 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@inproceedings{Bernardy2010Parametricity,
  title = {Parametricity and {{Dependent Types}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Bernardy, Jean-Philippe and Jansson, Patrik and Paterson, Ross},
  year = {2010},
  pages = {345--356},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863543.1863592},
  abstract = {Reynolds' abstraction theorem shows how a typing judgement in System F can be translated into a relational statement (in second order predicate logic) about inhabitants of the type. We (in second order predicate logic) about inhabitants of the type. We obtain a similar result for a single lambda calculus (a pure type system), in which terms, types and their relations are expressed. Working within a single system dispenses with the need for an interpretation layer, allowing for an unusually simple presentation. While the unification puts some constraints on the type system (which we spell out), the result applies to many interesting cases, including dependently-typed ones.},
  isbn = {978-1-60558-794-3},
  keywords = {abstraction theorem,free theorems,pure type system},
  series = {{{ICFP}} '10}
}

@inproceedings{Bernardy2011realizability,
  title = {Realizability and {{Parametricity}} in {{Pure Type Systems}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Foundations}} of {{Software Science}} and {{Computational Structures}}: {{Part}} of the {{Joint European Conferences}} on {{Theory}} and {{Practice}} of {{Software}}},
  author = {Bernardy, Jean-Philippe and Lasson, Marc},
  year = {2011},
  pages = {108--122},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  abstract = {We describe a systematic method to build a logic from any programming language described as a Pure Type System (PTS). The formulas of this logic express properties about programs. We define a parametricity theory about programs and a realizability theory for the logic. The logic is expressive enough to internalize both theories. Thanks to the PTS setting, we abstract most idiosyncrasies specific to particular type theories. This confers generality to the results, and reveals parallels between parametricity and realizability.},
  isbn = {978-3-642-19804-5},
  series = {{{FOSSACS}}'11/{{ETAPS}}'11}
}

@incollection{Bernardy2011realizabilitya,
  title = {Realizability and {{Parametricity}} in {{Pure Type Systems}}},
  booktitle = {Foundations of {{Software Science}} and {{Computational Structures}}},
  author = {Bernardy, Jean-Philippe and Lasson, Marc},
  editor = {Hofmann, Martin},
  year = {2011},
  month = jan,
  pages = {108--122},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We describe a systematic method to build a logic from any programming language described as a Pure Type System (PTS). The formulas of this logic express properties about programs. We define a parametricity theory about programs and a realizability theory for the logic. The logic is expressive enough to internalize both theories. Thanks to the PTS setting, we abstract most idiosyncrasies specific to particular type theories. This confers generality to the results, and reveals parallels between parametricity and realizability.},
  copyright = {\textcopyright{}2011 Springer Berlin Heidelberg},
  isbn = {978-3-642-19804-5 978-3-642-19805-2},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {6604},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Bernardy2012computational,
  title = {A {{Computational Interpretation}} of {{Parametricity}}},
  booktitle = {Proceedings of the 2012 27th {{Annual IEEE}}/{{ACM Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Bernardy, Jean-Philippe and Moulin, Guilhem},
  year = {2012},
  pages = {135--144},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/LICS.2012.25},
  abstract = {Reynolds' abstraction theorem has recently been extended to lambda-calculi with dependent types. In this paper, we show how this theorem can be internalized. More precisely, we describe an extension of the Pure Type Systems with a special parametricity rule (with computational content), and prove fundamental properties such as Church-Rosser's and strong normalization. All instances of the abstraction theorem can be both expressed and proved in the calculus itself. Moreover, one can apply parametricity to the parametricity rule: parametricity is itself parametric.},
  isbn = {978-0-7695-4769-5},
  keywords = {lambda calculus,Type structure},
  series = {{{LICS}} '12}
}

@article{Bernardy2012proofs,
  title = {Proofs for Free},
  author = {Bernardy, Jean-Philippe and Jansson, Patrik and Paterson, Ross},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {107--152},
  issn = {1469-7653},
  doi = {10.1017/S0956796812000056},
  abstract = {Reynolds' abstraction theorem (Reynolds, J. C. (1983) Types, abstraction and parametric polymorphism, Inf. Process. 83(1), 513\textendash{}523) shows how a typing judgement in System F can be translated into a relational statement (in second-order predicate logic) about inhabitants of the type. We obtain a similar result for pure type systems (PTSs): for any PTS used as a programming language, there is a PTS that can be used as a logic for parametricity. Types in the source PTS are translated to relations (expressed as types) in the target. Similarly, values of a given type are translated to proofs that the values satisfy the relational interpretation. We extend the result to inductive families. We also show that the assumption that every term satisfies the parametricity condition generated by its type is consistent with the generated logic.},
  journal = {Journal of Functional Programming},
  number = {02}
}

@article{Bernardy2017Linear,
  title = {Linear {{Haskell}}: Practical Linearity in a Higher-Order Polymorphic Language},
  shorttitle = {Linear Haskell},
  author = {Bernardy, Jean-Philippe and Boespflug, Mathieu and Newton, Ryan R. and Peyton Jones, Simon and Spiwack, Arnaud},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {5:1--5:29},
  issn = {2475-1421},
  doi = {10.1145/3158093},
  abstract = {Linear type systems have a long and storied history, but not a clear path forward to integrate with existing languages such as OCaml or Haskell. In this paper, we study a linear type system designed with two crucial properties in mind: backwards-compatibility and code reuse across linear and non-linear users of a library. Only then can the benefits of linear types permeate conventional functional programming. Rather than bifurcate types into linear and non-linear counterparts, we instead attach linearity to function arrows. Linear functions can receive inputs from linearly-bound values, but can also operate over unrestricted, regular values.  To demonstrate the efficacy of our linear type system~\textemdash{}~both how easy it can be integrated in an existing language implementation and how streamlined it makes it to write programs with linear types~\textemdash{}~we implemented our type system in ghc, the leading Haskell compiler, and demonstrate two kinds of applications of linear types: mutable data with pure interfaces; and enforcing protocols in I/O-performing functions.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {GHC,Haskell,laziness,linear logic,linear types,polymorphism,typestate},
  number = {POPL}
}

@incollection{Bertino1990optimization,
  title = {Optimization of Queries Using Nested Indices},
  booktitle = {Advances in {{Database Technology}} \textemdash{} {{EDBT}} '90},
  author = {Bertino, Elisa},
  editor = {Bancilhon, Fran{\c c}ois and Thanos, Constantino and Tsichritzis, Dennis},
  year = {1990},
  month = jan,
  pages = {44--59},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The notion of nested object is a basic concept of the object-oriented paradigm. It allows the value of an object attribute to be another object or a set of other objects. This means that a class consists of a set of attributes, and the values of the attributes are objects that belong to other classes; that is, the definition of a class forms a hierarchy of classes. All attributes of the nested classes are nested attributes of the root of the hierarchy. In a previous paper [Bert 89], we have introduced the notion of nested index that associates the values of a nested attribute with the objects instances of the root of the hierarchy. In that paper, we have evaluated the performance of this indexing mechanism in the case of queries containing a single predicate. In the present paper, we consider the usage of nested indices in the framework of more general queries containing several predicates.},
  copyright = {\textcopyright{}1990 Springer-Verlag},
  isbn = {978-3-540-52291-1 978-3-540-46948-3},
  keywords = {Artificial Intelligence (incl. Robotics),Data Storage Representation,Database Management,Mathematical Logic and Formal Languages,Models and Principles,Programming Languages; Compilers; Interpreters},
  number = {416},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Bertino1991indexing,
  title = {An Indexing Techniques for Object-Oriented Databases},
  booktitle = {Seventh {{International Conference}} on {{Data Engineering}}, 1991. {{Proceedings}}},
  author = {Bertino, E.},
  year = {1991},
  month = apr,
  pages = {160--170},
  doi = {10.1109/ICDE.1991.131463},
  abstract = {The basic characteristics of object-oriented data models and query languages are summarized. An indexing technique is presented that supports an efficient evaluation of nested predicates for queries involving class hierarchies. A preliminary comparison of the proposed indexing technique with other techniques is presented. A first extension of this work is to define cost formulas for the proposed index organization and to compare it with other organizations, such as the multi-index and inherited-multi-index organizations. A second extension is to define indexing techniques to support alternative predicates on properties that are semantically equivalent. In addition to these, an important issue concerns indexing support for the use of methods in queries},
  keywords = {class hierarchies,cost formulas,Database languages,Indexing,indexing techniques,nested predicates,Object oriented databases,Object oriented modeling,Object oriented programming,object-oriented databases,query languages,Spatial databases,Vehicles}
}

@inproceedings{Bertino1992optimization,
  title = {Optimization of Object-Oriented Queries Using Path Indices},
  booktitle = {Second {{International Workshop}} on {{Research Issues}} on {{Data Engineering}}, 1992: {{Transaction}} and {{Query Processing}}},
  author = {Bertino, E. and Guglielmina, C.},
  year = {1992},
  month = feb,
  pages = {140--149},
  doi = {10.1109/RIDE.1992.227413},
  abstract = {This paper addresses the problem of efficiently evaluating nested predicates in object-oriented databases. The authors have previously introduced the notion of path index (E. Bertino et al., 1989) that associates the values of a nested attribute with the instances of the class root of a given aggregation hierarchy. They evaluated the performance of the path index in the case of queries containing a single predicate. Here they consider the usage of the path index in the framework of more general queries containing several predicates},
  keywords = {aggregation hierarchy,Database languages,database theory,Indexing,Navigation,nested predicates,Object oriented modeling,object-oriented databases,object-oriented queries,optimisation,path index,query optimization,query processing,Vehicles}
}

@article{Bertino1993pathindex,
  title = {Path-Index: {{An}} Approach to the Efficient Execution of Object-Oriented Queries},
  shorttitle = {Path-Index},
  author = {Bertino, E. and Guglielmina, C.},
  year = {1993},
  month = feb,
  volume = {10},
  pages = {1--27},
  issn = {0169-023X},
  doi = {10.1016/0169-023X(93)90017-J},
  abstract = {This paper addresses the prolem of efficiently evaluating nested predicates in object-oriented databases. In a previous paper [4] we have introduced the notion of path index that associates the values of a nested attribute with the instances of the class root of a given aggregation hierarchy. In that paper we have evaluated the performance of the path index in the case of queries containing a single predicate. In the present paper we consider the use of the path index in the framework of more general queries containing several predicates.},
  journal = {Data \& Knowledge Engineering},
  keywords = {complex objects,indexing techniques,Object-oriented query languages,query optimization},
  number = {1}
}

@article{Bertino1994index,
  title = {Index {{Configuration}} in {{Object}}-Oriented {{Databases}}},
  author = {Bertino, Elisa},
  year = {1994},
  month = jul,
  volume = {3},
  pages = {355--399},
  issn = {1066-8888},
  doi = {10.1007/BF01232644},
  abstract = {In relational databases, an attribute of a relation can have only a single primitive value, making it cumbersome to model complex objects. The object-oriented paradigm removes this difficulty by introducing the notion of nested objects, which allows the value of an object attribute to be another object or a set of other objects. This means that a class consists of a set of attributes, and the values of the attributes are objects that belong to other classes; that is, the definition of a class forms a hierarchy of classes. All attributes of the nested classes are nested attributes of the root of the hierarchy. A branch of such hierarchy is called a path. In this article, we address the problem of index configuration for a given path. We first summarize some basic concepts, and introduce the concept of index configuration for a path. Then we present cost formulas to evaluate the costs of the various configurations. Finally, we present the algorithm that determines the optimal configuration, and show its correctness.},
  journal = {The VLDB Journal},
  keywords = {index selection,physical database design,query optimization},
  number = {3}
}

@incollection{Bertino1997objectoriented,
  title = {Object-{{Oriented Databases}}},
  booktitle = {Indexing {{Techniques}} for {{Advanced Database Systems}}},
  author = {Bertino, Elisa and Ooi, Beng Chin and {Sacks-Davis}, Ron and Tan, Kian-Lee and Zobel, Justin and Shidlovsky, Boris and Catania, Barbara},
  year = {1997},
  month = jan,
  pages = {1--38},
  publisher = {{Springer US}},
  abstract = {There has been a growing acceptance of the object-oriented data model as the basis of next generation database management systems (DBMSs). Both pure object-oriented DBMS (OODBMSs) and object-relational DBMS (ORDBMSs) have been developed based on object-oriented concepts. Object-relational DBMS, in particular, extend the SQL language by incorporating all the concepts of the object-oriented data model. A large number of products for both categories of DBMS is today available. In particular, all major vendors of relational DBMSs are turning their products into ORDBMSs [Nori, 1996].},
  copyright = {\textcopyright{}1997 Kluwer Academic Publishers},
  isbn = {978-1-4613-7856-3 978-1-4615-6227-6},
  keywords = {Data Structures; Cryptology and Information Theory,Geographical Information Systems/Cartography,Information Storage and Retrieval},
  language = {en},
  number = {8},
  series = {The {{Springer International Series}} on {{Advances}} in {{Database Systems}}}
}

@article{Betts2015Design,
  title = {The {{Design}} and {{Implementation}} of a {{Verification Technique}} for {{GPU Kernels}}},
  author = {Betts, Adam and Chong, Nathan and Donaldson, Alastair F. and Ketema, Jeroen and Qadeer, Shaz and Thomson, Paul and Wickerson, John},
  year = {2015},
  month = may,
  volume = {37},
  pages = {10:1--10:49},
  issn = {0164-0925},
  doi = {10.1145/2743017},
  abstract = {We present a technique for the formal verification of GPU kernels, addressing two classes of correctness properties: data races and barrier divergence. Our approach is founded on a novel formal operational semantics for GPU kernels termed \emph{synchronous, delayed visibility (SDV)} semantics, which captures the execution of a GPU kernel by multiple groups of threads. The SDV semantics provides operational definitions for barrier divergence and for both inter- and intra-group data races. We build on the semantics to develop a method for reducing the task of verifying a massively parallel GPU kernel to that of verifying a sequential program. This completely avoids the need to reason about thread interleavings, and allows existing techniques for sequential program verification to be leveraged. We describe an efficient encoding of data race detection and propose a method for automatically inferring the loop invariants that are required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, that can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 162 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {barrier synchronization,concurrency,data races,GPUs,Verification},
  number = {3}
}

@inproceedings{Bhatotia2015ithreads,
  title = {{{iThreads}}: {{A Threading Library}} for {{Parallel Incremental Computation}}},
  shorttitle = {{{iThreads}}},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj{\"o}rn B. and Rodrigues, Rodrigo},
  year = {2015},
  pages = {645--659},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2694344.2694371},
  abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
  isbn = {978-1-4503-2835-7},
  keywords = {concurrent dynamic dependence graph (CDDG),incremental computation,memoization,release consistency (RC) memory model,self-adjusting computation,shared-memory multithreading},
  series = {{{ASPLOS}} '15}
}

@article{Biering2007BIhyperdoctrines,
  title = {{{BI}}-Hyperdoctrines, {{Higher}}-Order {{Separation Logic}}, and {{Abstraction}}},
  author = {Biering, Bodil and Birkedal, Lars and {Torp-Smith}, Noah},
  year = {2007},
  month = aug,
  volume = {29},
  issn = {0164-0925},
  doi = {10.1145/1275497.1275499},
  abstract = {We present a precise correspondence between separation logic and a simple notion of predicate BI, extending the earlier correspondence given between part of separation logic and propositional BI. Moreover, we introduce the notion of a BI hyperdoctrine, show that it soundly models classical and intuitionistic first- and higher-order predicate BI, and use it to show that we may easily extend separation logic to higher-order. We also demonstrate that this extension is important for program proving, since it provides sound reasoning principles for data abstraction in the presence of aliasing.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {abstraction,hyperdoctrines,Separation logic},
  number = {5}
}

@article{Bille2005survey,
  title = {A Survey on Tree Edit Distance and Related Problems},
  author = {Bille, Philip},
  year = {2005},
  month = jun,
  volume = {337},
  pages = {217--239},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2004.12.030},
  abstract = {We survey the problem of comparing labeled trees based on simple local operations of deleting, inserting, and relabeling nodes. These operations lead to the tree edit distance, alignment distance, and inclusion problem. For each problem we review the results available and present, in detail, one or more of the central algorithms for solving the problem.},
  journal = {Theoretical Computer Science},
  keywords = {Tree alignment,Tree edit distance,Tree inclusion,Tree matching},
  number = {1\textendash{}3}
}

@article{Bird1980tabulation,
  title = {Tabulation {{Techniques}} for {{Recursive Programs}}},
  author = {Bird, R. S.},
  year = {1980},
  month = dec,
  volume = {12},
  pages = {403--417},
  issn = {0360-0300},
  doi = {10.1145/356827.356831},
  journal = {ACM Comput. Surv.},
  number = {4}
}

@article{Bird1989algebraic,
  title = {Algebraic {{Identities}} for {{Program Calculation}}},
  author = {Bird, R. S.},
  year = {1989},
  month = jan,
  volume = {32},
  pages = {122--126},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/32.2.122},
  abstract = {To calculate a program means to derive it from a suitable specification by a process of equational reasoning. We describe a number of basic algebraic identities that turn out to be extremely useful in this task. These identities express relationship between the higher-order functions commonly encountered in functional programming. The idea of program calculation is illustrated with two non-trivial examples.},
  journal = {The Computer Journal},
  language = {en},
  number = {2}
}

@incollection{Bird1989lectures,
  title = {Lectures on {{Constructive Functional Programming}}},
  booktitle = {Constructive {{Methods}} in {{Computing Science}}},
  author = {Bird, Richard S.},
  editor = {Broy, Manfred},
  year = {1989},
  month = jan,
  pages = {151--217},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The subject of these lectures is a calculus of functions for deriving programs from their specifications. This calculus consists of a range of concepts and notations for defining functions over various data types \textemdash{} including lists, trees, and arrays \textemdash{} together with their algebraic and other properties. Each lecture begins with a specific problem, and the theory necessary to solve it is then developed. In this way we hope to show that a functional approach to the problem of systematically calculating programs from their specifications can take its place alongside other methodologies.},
  copyright = {\textcopyright{}1989 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-74886-8 978-3-642-74884-4},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Techniques,Software Engineering},
  language = {en},
  number = {55},
  series = {{{NATO ASI Series}}}
}

@incollection{Bird1990calculus,
  title = {A Calculus of Functions for Program Derivation},
  booktitle = {Research {{Topics}} in {{Functional Programming}}},
  author = {Bird, R. S.},
  editor = {Turner, David A.},
  year = {1990},
  pages = {287--307},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  address = {{Boston, MA, USA}},
  isbn = {0-201-17236-4}
}

@inproceedings{Bird1996algebra,
  title = {The {{Algebra}} of {{Programming}}},
  booktitle = {Proceedings of the {{NATO Advanced Study Institute}} on {{Deductive Program Design}}},
  author = {Bird, Richard and {de Moor}, Oege},
  year = {1996},
  pages = {167--203},
  publisher = {{Springer-Verlag New York, Inc.}},
  address = {{Secaucus, NJ, USA}},
  isbn = {3-540-60947-4}
}

@incollection{Bird1998nested,
  title = {Nested Datatypes},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Bird, Richard and Meertens, Lambert},
  editor = {Jeuring, Johan},
  year = {1998},
  pages = {52--67},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A nested datatype, also known as a non-regular datatype, is a parametrised datatype whose declaration involves different instances of the accompanying type parameters. Nested datatypes have been mostly ignored in functional programming until recently, but they are turning out to be both theoretically important and useful in practice. The aim of this paper is to suggest a functorial semantics for such datatypes, with an associated calculational theory that mirrors and extends the standard theory for regular datatypes. Though elegant and generic, the proposed approach appears more limited than one would like, and some of the limitations are discussed.},
  copyright = {\textcopyright{}1998 Springer-Verlag},
  isbn = {978-3-540-64591-7 978-3-540-69345-1},
  keywords = {Algorithm Analysis and Problem Complexity,Logics and Meanings of Programs,Mathematics of Computing,Programming Techniques,Software Engineering},
  language = {en},
  number = {1422},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Bird2008zippy,
  title = {Zippy {{Tabulations}} of {{Recursive Functions}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Bird, Richard S.},
  editor = {Audebaud, Philippe and {Paulin-Mohring}, Christine},
  year = {2008},
  pages = {92--109},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper is devoted to the statement and proof of a theorem showing how recursive definitions whose associated call graphs satisfy certain shape conditions can be converted systematically into efficient bottom-up tabulation schemes. The increase in efficiency can be dramatic, typically transforming an exponential time algorithm into one that takes only quadratic time. The proof of the theorem relies heavily on the theory of zips developed by Roland Backhouse and Paul Hoogendijk.},
  copyright = {\textcopyright{}2008 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-70593-2 978-3-540-70594-9},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {5133},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Birkedal2010categorytheoretic,
  title = {The Category-Theoretic Solution of Recursive Metric-Space Equations},
  author = {Birkedal, Lars and St{\o}vring, Kristian and Thamsborg, Jacob},
  year = {2010},
  month = oct,
  volume = {411},
  pages = {4102--4122},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2010.07.010},
  abstract = {It is well known that one can use an adaptation of the inverse-limit construction to solve recursive equations in the category of complete ultrametric spaces. We show that this construction generalizes to a large class of categories with metric-space structure on each set of morphisms: the exact nature of the objects is less important. In particular, the construction immediately applies to categories where the objects are ultrametric spaces with `extra structure', and where the morphisms preserve this extra structure. The generalization is inspired by classical domain-theoretic work by Smyth and Plotkin. For many of the categories we consider, there is a natural subcategory in which each set of morphisms is required to be a compact metric space. Our setting allows for a proof that such a subcategory always inherits solutions of recursive equations from the full category. As another application, we present a construction that relates solutions of generalized domain equations in the sense of Smyth and Plotkin to solutions of equations in our class of categories. Our primary motivation for solving generalized recursive metric-space equations comes from recent and ongoing work on Kripke-style models in which the sets of worlds must be recursively defined. We show a series of examples motivated by this line of work.},
  journal = {Theoretical Computer Science},
  keywords = {Fixed point,Metric space,Recursive equation},
  number = {47}
}

@article{Birkedal2012relational,
  title = {A Relational Realizability Model for Higher-Order Stateful {{ADTs}}},
  author = {Birkedal, Lars and St{\o}vring, Kristian and Thamsborg, Jacob},
  year = {2012},
  month = may,
  volume = {81},
  pages = {491--521},
  issn = {1567-8326},
  doi = {10.1016/j.jlap.2012.03.004},
  abstract = {We present a realizability model for reasoning about contextual equivalence of higher-order programs with impredicative polymorphism, recursive types, and higher-order mutable state. The model combines the virtues of two recent earlier models: (1) Ahmed, Dreyer, and Rossberg's step-indexed logical relations model, which was designed to facilitate proofs of representation independence for ``state-dependent'' ADTs and (2) Birkedal, St{\o}vring, and Thamsborg's realizability logical relations model, which was designed to facilitate abstract proofs without tedious step-index arithmetic. The resulting model can be used to give abstract proofs of representation independence for ``state-dependent'' ADTs.},
  journal = {The Journal of Logic and Algebraic Programming},
  keywords = {Abstract data types,Local state,Logical relations,Parametricity},
  number = {4},
  series = {Special {{Issue}}: {{NWPT}} 2009}
}

@article{Birkedal2016Guarded,
  title = {Guarded Cubical Type Theory: Path Equality for Guarded Recursion},
  shorttitle = {Guarded Cubical Type Theory},
  author = {Birkedal, Lars and Bizjak, Ale{\v s} and Clouston, Ranald and Grathwohl, Hans Bugge and Spitters, Bas and Vezzosi, Andrea},
  year = {2016},
  month = jun,
  abstract = {This paper improves the treatment of equality in guarded dependent type theory (GDTT), by combining it with cubical type theory (CTT). GDTT is an extensional type theory with guarded recursive types, which are useful for building models of program logics, and for programming and reasoning with coinductive types. We wish to implement GDTT with decidable type-checking, while still supporting non-trivial equality proofs that reason about the extensions of guarded recursive constructions. CTT is a variation of Martin-L\textbackslash{}"of type theory in which the identity type is replaced by abstract paths between terms. CTT provides a computational interpretation of functional extensionality, is conjectured to have decidable type checking, and has an implemented type-checker. Our new type theory, called guarded cubical type theory, provides a computational interpretation of extensionality for guarded recursive types. This further expands the foundations of CTT as a basis for formalisation in mathematics and computer science. We present examples to demonstrate the expressivity of our type theory, all of which have been checked using a prototype type-checker implementation, and present semantics in a presheaf category.},
  archivePrefix = {arXiv},
  eprint = {1606.05223},
  eprinttype = {arxiv},
  journal = {arXiv:1606.05223 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages,F.3.2,F.3.3},
  primaryClass = {cs}
}

@inproceedings{Bizjak2016Guarded,
  title = {Guarded {{Dependent Type Theory}} with {{Coinductive Types}}},
  booktitle = {Foundations of {{Software Science}} and {{Computation Structures}}},
  author = {Bizjak, Ale{\v s} and Grathwohl, Hans Bugge and Clouston, Ranald and M{\o}gelberg, Rasmus E. and Birkedal, Lars},
  editor = {Jacobs, Bart and L{\"o}ding, Christof},
  year = {2016},
  pages = {20--35},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-49630-5_2},
  abstract = {We present guarded dependent type theory, {$\mathsf{g}\mathsf{D}\mathsf{T}\mathsf{T}$}gDTT\textbackslash{}mathsf \{gDTT\}, an extensional dependent type theory with a `later' modality and clock quantifiers for programming and proving with guarded recursive and coinductive types. The later modality is used to ensure the productivity of recursive definitions in a modular, type based, way. Clock quantifiers are used for controlled elimination of the later modality and for encoding coinductive types using guarded recursive types. Key to the development of {$\mathsf{g}\mathsf{D}\mathsf{T}\mathsf{T}$}gDTT\textbackslash{}mathsf \{gDTT\} are novel type and term formers involving what we call `delayed substitutions'. These generalise the applicative functor rules for the later modality considered in earlier work, and are crucial for programming and proving with dependent types. We show soundness of the type theory with respect to a denotational model.},
  isbn = {978-3-662-49630-5},
  keywords = {Applicative Functor,Clock Variable,Elimination Rule,Type Isomorphism,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Bizjak2018Denotational,
  title = {Denotational Semantics for Guarded Dependent Type Theory},
  author = {Bizjak, Ale{\v s} and M{\o}gelberg, Rasmus Ejlers},
  year = {2018},
  month = feb,
  abstract = {We present a new model of Guarded Dependent Type Theory (GDTT), a type theory with guarded recursion and multiple clocks in which one can program with, and reason about coinductive types. Productivity of recursively defined coinductive programs and proofs is encoded in types using guarded recursion, and can therefore be checked modularly, unlike the syntactic checks implemented in modern proof assistants. The model is based on a category of covariant presheaves over a category of time objects, and quantification over clocks is modelled using a presheaf of clocks. To model the clock irrelevance axiom, crucial for programming with coinductive types, types must be interpreted as presheaves orthogonal to the object of clocks. In the case of dependent types, this translates to a unique lifting condition similar to the one found in homotopy theoretic models of type theory. Since the universes defined by the standard Hofmann-Streicher construction in this model do not satisfy this property, the universes in GDTT must be indexed by contexts of clock variables. A large and technical part of the paper is devoted to showing that these can be constructed in such a way that inclusions between universes induced by inclusions of clock variable contexts commute on the nose with type operations on the universes.},
  archivePrefix = {arXiv},
  eprint = {1802.03744},
  eprinttype = {arxiv},
  journal = {arXiv:1802.03744 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@article{Bizjak2018models,
  title = {On Models of Higher-Order Separation Logic},
  author = {Bizjak, Ale{\v s} and Birkedal, Lars},
  year = {2018},
  month = apr,
  volume = {336},
  pages = {57--78},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2018.03.016},
  abstract = {We show how tools from categorical logic can be used to give a general account of models of higher-order separation logic with a sublogic of so-called persistent predicates satisfying the usual rules of higher-order logic. The models of separation logic are based on a notion of resource, a partial commutative monoid, and the persistent predicates can be defined using a modality. We classify well-behaved sublogics of persistent predicates in terms of interior operators on the partial commutative monoid of resources. We further show how the general constructions can be used to recover the model of Iris, a state-of-the-art higher-order separation logic with guarded recursive predicates.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {modalities,model,separation logic},
  series = {The {{Thirty}}-Third {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXXIII}})}
}

@inproceedings{Black2003applying,
  title = {Applying {{Traits}} to the {{Smalltalk Collection Classes}}},
  booktitle = {Proceedings of the 18th {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programing}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Black, Andrew P. and Sch{\"a}rli, Nathanael and Ducasse, St{\'e}phane},
  year = {2003},
  pages = {47--64},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/949305.949311},
  abstract = {Traits are a programming language technology that promote the reuse of methods between unrelated classes. This paper reports on a refactoring of the Smalltalk collections classes using traits. The original collection classes contained much duplication of code; traits let us remove all of it. We also found places where the protocols of the collections lacked uniformity; traits allowed us to correct these non-uniformities without code duplication.Traits also make it possible to reuse fragments of collection code outside of the existing hierarchy; for example, they make it easy to convert other collection-like things into true collections. Our refactoring reduced the number of methods in the collection classes by approximately 10 per cent. More importantly, understandability maintainability and reusability of the code were significantly improved.},
  isbn = {1-58113-712-5},
  keywords = {collection hierarchy,inheritance,mixins,multiple Inheritance,refactoring,reuse,Smalltalk,stream classes,traits},
  series = {{{OOPSLA}} '03}
}

@incollection{Blanchette2014truly,
  title = {Truly {{Modular}} ({{Co}})Datatypes for {{Isabelle}}/{{HOL}}},
  booktitle = {Interactive {{Theorem Proving}}},
  author = {Blanchette, Jasmin Christian and H{\"o}lzl, Johannes and Lochbihler, Andreas and Panny, Lorenz and Popescu, Andrei and Traytel, Dmitriy},
  editor = {Klein, Gerwin and Gamboa, Ruben},
  year = {2014},
  month = jan,
  pages = {93--110},
  publisher = {{Springer International Publishing}},
  abstract = {We extended Isabelle/HOL with a pair of definitional commands for datatypes and codatatypes. They support mutual and nested (co)recursion through well-behaved type constructors, including mixed recursion\textendash{}corecursion, and are complemented by syntaxes for introducing primitively (co)recursive functions and by a general proof method for reasoning coinductively. As a case study, we ported Isabelle's Coinductive library to use the new commands, eliminating the need for tedious ad hoc constructions.},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-08969-0 978-3-319-08970-6},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering,Systems and Data Security},
  language = {en},
  number = {8558},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Blanchette2015foundational,
  title = {Foundational {{Extensible Corecursion}}: {{A Proof Assistant Perspective}}},
  shorttitle = {Foundational {{Extensible Corecursion}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Blanchette, Jasmin Christian and Popescu, Andrei and Traytel, Dmitriy},
  year = {2015},
  pages = {192--204},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784732},
  abstract = {This paper presents a formalized framework for defining corecursive functions safely in a total setting, based on corecursion up-to and relational parametricity. The end product is a general corecursor that allows corecursive (and even recursive) calls under "friendly" operations, including constructors. Friendly corecursive functions can be registered as such, thereby increasing the corecursor's expressiveness. The metatheory is formalized in the Isabelle proof assistant and forms the core of a prototype tool. The corecursor is derived from first principles, without requiring new axioms or extensions of the logic.},
  isbn = {978-1-4503-3669-7},
  keywords = {(Co)recursion,higher-order logic,Isabelle,parametricity,proof assistants},
  series = {{{ICFP}} 2015}
}

@article{Blaszczyk2013Ten,
  title = {Ten {{Misconceptions}} from the {{History}} of {{Analysis}} and {{Their Debunking}}},
  author = {Blaszczyk, Piotr and Katz, Mikhail G. and Sherry, David},
  year = {2013},
  month = mar,
  volume = {18},
  pages = {43--74},
  issn = {1233-1821, 1572-8471},
  doi = {10.1007/s10699-012-9285-8},
  abstract = {The widespread idea that infinitesimals were "eliminated" by the "great triumvirate" of Cantor, Dedekind, and Weierstrass is refuted by an uninterrupted chain of work on infinitesimal-enriched number systems. The elimination claim is an oversimplification created by triumvirate followers, who tend to view the history of analysis as a pre-ordained march toward the radiant future of Weierstrassian epsilontics. In the present text, we document distortions of the history of analysis stemming from the triumvirate ideology of ontological minimalism, which identified the continuum with a single number system. Such anachronistic distortions characterize the received interpretation of Stevin, Leibniz, d'Alembert, Cauchy, and others.},
  archivePrefix = {arXiv},
  eprint = {1202.4153},
  eprinttype = {arxiv},
  journal = {Foundations of Science},
  keywords = {01A85; 26E35; 03A05; 97A20; 97C30,Mathematics - Classical Analysis and ODEs,Mathematics - History and Overview,Mathematics - Logic},
  number = {1}
}

@inproceedings{Blelloch1995parallelism,
  title = {Parallelism in {{Sequential Functional Languages}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Functional Programming Languages}} and {{Computer Architecture}}},
  author = {Blelloch, Guy and Greiner, John},
  year = {1995},
  pages = {226--237},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/224164.224210},
  isbn = {0-89791-719-7},
  series = {{{FPCA}} '95}
}

@inproceedings{Blelloch2013cache,
  title = {Cache and {{I}}/{{O Efficent Functional Algorithms}}},
  booktitle = {Proceedings of the 40th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Blelloch, Guy E. and Harper, Robert},
  year = {2013},
  pages = {39--50},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2429069.2429077},
  abstract = {The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size primary memory(cache) of size M, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard RAM model. The models, however, require specifying algorithms at a very low level requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation. In this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an O(n\_B logM/Bn\_B)cost sorting algorithm, which is optimal in the ideal cache and I/O models.},
  isbn = {978-1-4503-1832-7},
  keywords = {cost semantics,I/O efficient algorithms},
  series = {{{POPL}} '13}
}

@article{Blelloch2015cache,
  title = {Cache {{Efficient Functional Algorithms}}},
  author = {Blelloch, Guy E. and Harper, Robert},
  year = {2015},
  month = jun,
  volume = {58},
  pages = {101--108},
  issn = {0001-0782},
  doi = {10.1145/2776825},
  abstract = {The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size fast memory (cache) of size M, and an unbounded slow memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard Random Access Machine (RAM) model. The models, however, require specifying algorithms at a very low level, requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation. We present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bounds imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an o(n/BlogM/Bn/B) cost sorting algorithm, which is optimal in the ideal cache and I/O models.},
  journal = {Commun. ACM},
  number = {7}
}

@article{Bloo1996barendregt,
  title = {The {{Barendregt Cube}} with {{Definitions}} and {{Generalised Reduction}}},
  author = {Bloo, Roel and Kamareddine, Fairouz and Nederpelt, Rob},
  year = {1996},
  month = may,
  volume = {126},
  pages = {123--143},
  issn = {0890-5401},
  doi = {10.1006/inco.1996.0041},
  abstract = {In this paper, we propose to extend the Barendregt Cube by generalising{$\beta$}-reduction and by adding definition mechanisms. Generalised reduction allows contracting more visible redexes than usual, and definitions are an important tool to allow for a more flexible typing system. We show that this extension satisfies most of the original properties of the Cube including Church-Rosser, Subject Reduction and Strong Normalisation.},
  journal = {Information and Computation},
  number = {2}
}

@inproceedings{Bloom2009Thorn,
  title = {Thorn: {{Robust}}, Concurrent, Extensible Scripting on the {{JVM}}},
  shorttitle = {Thorn},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Bloom, Bard and Field, John and Nystrom, Nathaniel and {\"O}stlund, Johan and Richards, Gregor and Strni{\v s}a, Rok and Vitek, Jan and Wrigstad, Tobias},
  year = {2009},
  pages = {117--136},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1640089.1640098},
  abstract = {Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.},
  isbn = {978-1-60558-766-0},
  keywords = {actors,pattern matching,scripting},
  series = {{{OOPSLA}} '09}
}

@inproceedings{Blume2006Extensible,
  title = {Extensible Programming with First-Class Cases},
  booktitle = {Proceedings of the {{Eleventh ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Blume, Matthias and Acar, Umut A. and Chae, Wonseok},
  year = {2006},
  pages = {239--250},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1159803.1159836},
  abstract = {We present language mechanisms for polymorphic, extensible records and their exact dual, polymorphic sums with extensible first-class cases. These features make it possible to easily extend existing code with new cases. In fact, such extensions do not require any changes to code that adheres to a particular programming style. Using that style, individual extensions can be written independently and later be composed to form larger components. These language mechanisms provide a solution to the expression problem.We study the proposed mechanisms in the context of an implicitly typed, purely functional language PolyR. We give a type system for the language and provide rules for a 2-phase transformation: first into an explicitly typed {$\lambda$}-calculus with record polymorphism, and finally to efficient index-passing code. The first phase eliminates sums and cases by taking advantage of the duality with records.We implement a version of PolyR extended with imperative features and pattern matching - we call this language MLPolyR. Programs in MLPolyR require no type annotations - the implementation employs a reconstruction algorithm to infer all types. The compiler generates machine code (currently for PowerPC) and optimizes the representation of sums by eliminating closures generated by the dual construction.},
  isbn = {978-1-59593-309-6},
  keywords = {duality,first-class cases,records,sums},
  series = {{{ICFP}} '06}
}

@inproceedings{Boehm2008foundations,
  title = {Foundations of the {{C}}++ {{Concurrency Memory Model}}},
  booktitle = {Proceedings of the 2008 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Boehm, Hans-J. and Adve, Sarita V.},
  year = {2008},
  pages = {68--78},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1375581.1375591},
  abstract = {Currently multi-threaded C or C++ programs combine a single-threaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a well-defined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:We (mostly) insist on sequential consistency for race-free programs, in spite of implementation issues that came to light after the Java work. We give no semantics to programs with data races. There are no benign C++ data races. We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock. This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often under-appreciated implementation constraints, drives us towards the above decisions.},
  isbn = {978-1-59593-860-2},
  keywords = {C++,data race,memory consistency,memory model,sequential consistency,trylock},
  series = {{{PLDI}} '08}
}

@inproceedings{Bogle1994Reducing,
  title = {Reducing Cross Domain Call Overhead Using Batched Futures},
  booktitle = {Proceedings of the {{Ninth Annual Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Language}}, and {{Applications}}},
  author = {Bogle, Phillip and Liskov, Barbara},
  year = {1994},
  pages = {341--354},
  publisher = {{ACM}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/191080.191133},
  abstract = {In many systems such as operating systems and databases it is important to run client code in a separate protection domain so that it cannot interfere with correct operation of the system. Clients communicate with the server by making cross domain calls, but these are expensive, often costing substantially more than running the call itself. This paper describes a new mechanism called batched futures that transparently batches possibly interrelated client calls. Batching makes domain crossings happen less often, thus substantially reducing the cost. We describe how the mechanism is implemented for the Thor object-oriented database system, and presents performance results showing the benefit of the mechanism on various benchmarks.},
  isbn = {978-0-89791-688-2},
  series = {{{OOPSLA}} '94}
}

@article{Bohm1985automatic,
  title = {Automatic Synthesis of Typed {{$\Lambda$}}-Programs on Term Algebras},
  author = {B{\"o}hm, Corrado and Berarducci, Alessandro},
  year = {1985},
  volume = {39},
  pages = {135--154},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(85)90135-5},
  abstract = {The notion of iteratively defined functions from and to heterogeneous term algebras is introduced as the solution of a finite set of equations of a special shape.
Such a notion has remarkable consequences: (1) Choosing the second-order typed lamdda-calculus ({$\Lambda$} for short) as a programming language enables one to represent algebra elements and iterative functions by automatic uniform synthesis paradigms, using neither conditional nor recursive constructs. (2) A completeness theorem for {$\Lambda$}-terms with type of degree at most two and a companion corollary for {$\Lambda$}-programs have been proved. (3) A new congruence relation for the last-mentioned {$\Lambda$}-terms which is stronger than {$\Lambda$}-convertibility is introduced and proved to have the meaning of a {$\Lambda$}-program equivalence. Moreover, an extension of the paradigms to the synthesis of functions of higher complexity is considered and exemplified. All the concepts are explained and motivated by examples over integers, list- and tree-structures.},
  journal = {Theoretical Computer Science},
  series = {Third {{Conference}} on {{Foundations}} of {{Software Technology}} and {{Theoretical Computer Science}}}
}

@inproceedings{Bolingbroke2009types,
  title = {Types {{Are Calling Conventions}}},
  booktitle = {Proceedings of the 2nd {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Bolingbroke, Maximilian C. and Peyton Jones, Simon L.},
  year = {2009},
  pages = {1--12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596638.1596640},
  abstract = {It is common for compilers to derive the calling convention of a function from its type. Doing so is simple and modular but misses many optimisation opportunities, particularly in lazy, higher-order functional languages with extensive use of currying. We restore the lost opportunities by defining Strict Core, a new intermediate language whose type system makes the missing distinctions: laziness is explicit, and functions take multiple arguments and return multiple results.},
  isbn = {978-1-60558-508-6},
  keywords = {arity,calling conventions,intermediate language,strictness,unboxing,uncurrying},
  series = {Haskell '09}
}

@inproceedings{Boyland2003Checking,
  title = {Checking {{Interference}} with {{Fractional Permissions}}},
  booktitle = {Static {{Analysis}}},
  author = {Boyland, John},
  editor = {Cousot, Radhia},
  year = {2003},
  pages = {55--72},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We describe a type system for checking interference using the concept of linear capabilities (which we call ``permissions''). Our innovations include the concept of ``fractional'' permissions: reads can be permitted with fractional permissions whereas writes require complete permissions. This distinction expresses the fact that reads on the same state do not conflict with each other. One may give shared read access at one point while still retaining write permission afterwards. We give an operational semantics of a simple imperative language with structured parallelism and prove that the permission system enables parallelism to proceed with deterministic results.},
  isbn = {978-3-540-44898-3},
  keywords = {Alias Analysis,Operational Semantic,Parallel Composition,Permission System,Separation Logic},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Boylandvariable,
  title = {Variable {{Arity}} for {{LF}}},
  author = {Boyland, John Tang and Zhao, Tian}
}

@inproceedings{Bracker2016Supermonads,
  title = {Supermonads: {{One Notion}} to {{Bind Them All}}},
  shorttitle = {Supermonads},
  booktitle = {Proceedings of the 9th {{International Symposium}} on {{Haskell}}},
  author = {Bracker, Jan and Nilsson, Henrik},
  year = {2016},
  pages = {158--169},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976002.2976012},
  abstract = {Several popular generalizations of monads have been implemented in Haskell. Unfortunately, because the shape of the associated type constructors do not match the standard Haskell monad interface, each such implementation provides its own type class and versions of associated library functions. Furthermore, simultaneous use of different monadic notions can be cumbersome as it in general is necessary to be explicit about which notion is used where. In this paper we introduce supermonads: an encoding of monadic notions that captures several different generalizations along with a version of the standard library of monadic functions that work uniformly with all of them. As standard Haskell type inference does not work for supermonads due to their generality, our supermonad implementation is accompanied with a language extension, in the form of a plugin for the Glasgow Haskell Compiler (GHC), that allows type inference for supermonads, obviating the need for manual annotations.},
  isbn = {978-1-4503-4434-0},
  keywords = {functional programming,Glasgow Haskell Compiler,Haskell,Monads,syntactic support,type checker plugin},
  series = {Haskell 2016}
}

@inproceedings{Brady2003Inductive,
  title = {Inductive Families Need Not Store Their Indices},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Brady, Edwin and McBride, Conor and McKinna, James},
  editor = {Berardi, Stefano and Coppo, Mario and Damiani, Ferruccio},
  year = {2003},
  pages = {115--129},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We consider the problem of efficient representation of dependently typed data. In particular, we consider a language TT based on Dybjer's notion of inductive families [10] and reanalyse their general form with a view to optimising the storage associated with their use. We introduce an execution language, ExTT, which allows the commenting out of computationally irrelevant subterms and show how to use properties of elimination rules to elide constructor arguments and tags in ExTT. We further show how some types can be collapsed entirely at run-time. Several examples are given, including a representation of the simply typed {$\lambda$}-calculus for which our analysis yields an 80\% reduction in run-time storage requirements.},
  isbn = {978-3-540-24849-1},
  keywords = {Alternative Implementation,Dependent Type,Elimination Rule,Functional Language,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Brady2013idris,
  title = {Idris, a General-Purpose Dependently Typed Programming Language: {{Design}} and Implementation},
  shorttitle = {Idris, a General-Purpose Dependently Typed Programming Language},
  author = {Brady, Edwin},
  year = {2013},
  volume = {23},
  pages = {552--593},
  issn = {1469-7653},
  doi = {10.1017/S095679681300018X},
  abstract = {Many components of a dependently typed programming language are by now well understood, for example, the underlying type theory, type checking, unification and evaluation. How to combine these components into a realistic and usable high-level language is, however, folklore, discovered anew by successive language implementors. In this paper, I describe the implementation of Idris, a new dependently typed functional programming language. Idris is intended to be a general-purpose programming language and as such provides high-level concepts such as implicit syntax, type classes and do notation. I describe the high-level language and the underlying type theory, and present a tactic-based method for elaborating concrete high-level syntax with implicit arguments and type classes into a fully explicit type theory. Furthermore, I show how this method facilitates the implementation of new high-level language constructs.},
  journal = {Journal of Functional Programming},
  number = {05}
}

@incollection{Brandt1997coinductive,
  title = {Coinductive Axiomatization of Recursive Type Equality and Subtyping},
  booktitle = {Typed Lambda Calculi and Applications},
  author = {Brandt, Michael and Henglein, Fritz},
  editor = {de Groote, Philippe and Hindley, J. Roger},
  year = {1997},
  pages = {63--81},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present new sound and complete axiomatizations of type equality and subtype inequality for a first-order type language with regular recursive types. The rules are motivated by coinductive characterizations of type containment and type equality via simulation and bisimulation, respectively. The main novelty of the axiomatization is the fixpoint rule (or coinduction principle), which has the form where P is either a type equality {$\tau$}={$\tau{'}$} or type containment {$\tau\leq\tau{'}$}. We define what it means for a proof (formal derivation) to be formally contractive and show that the fixpoint rule is sound if the proof of the premise A, P {$\vdash$} P is contractive. (A proof of A, P {$\vdash$} P using the assumption axiom is, of course, not contractive.) The fixpoint rule thus allows us to capture a coinductive relation in the fundamentally inductive framework of inference systems. The new axiomatizations are ``leaner'' than previous axiomatizations, particularly so for type containment since no separate axiomatization of type equality is required, as in Amadio and Cardelli's axiomatization. They give rise to a natural operational interpretation of proofs as coercions. In particular, the fixpoint rule corresponds to definition by recursion. Finally, the axiomatization is closely related to (known) efficient algorithms for deciding type equality and type containment. These can be modified to not only decide type equality and type containment, but also construct proofs in our axiomatizations efficiently. In connection with the operational interpretation of proofs as coercions this gives efficient (O(n 2) time) algorithms for constructing efficient coercions from a type to any of its supertypes or isomorphic types.},
  copyright = {\textcopyright{}1997 Springer-Verlag},
  isbn = {978-3-540-62688-6 978-3-540-68438-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Techniques},
  language = {en},
  number = {1210},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Brandt1998coinductive,
  title = {Coinductive Axiomatization of Recursive Type Equality and Subtyping},
  author = {Brandt, Michael and Henglein, Fritz},
  year = {1998},
  month = dec,
  volume = {33},
  pages = {309--338},
  issn = {0169-2968},
  abstract = {We present new sound and complete axiomatizations of type equality and subtype inequality for a first-order type language with regular recursive types. The rules are motivated by coinductive characterizations of type containment and type equality via simulation and bisimulation, respectively. The main novelty of the axiomatization is the fixpoint rule (or coinduction principle). It states that from A,P \$\$\textbackslash{}vdash\$\$ P one may deduce A \$\$\textbackslash{}vdash\$\$ P, where P is either a type equality {$\tau$} = {$\tau$}' or type containment {$\tau$} {$\leq$} {$\tau$}' and the proof of the premise must be contractive in a sense we define in this paper. In particular, a proof of A, P \$\$\textbackslash{}vdash\$\$ P using the assumption axiom is not contractive. The fixpoint rule embodies a finitary coinduction principle and thus allows us to capture a coinductive relation in the fundamentally inductive framework of inference systems. The new axiomatizations are more concise than previous axiomatizations, particularly so for type containment since no separate axiomatization of type equality is required, as in Amadio and Cardelli's axiomatization. They give rise to a natural operational interpretation of proofs as coercions. In particular, the fixpoint rule corresponds to definition by recursion. Finally, the axiomatization is closely related to (known) efficient algorithms for deciding type equality and type containment. These can be modified to not only decide type equality and type containment, but also construct proofs in our axiomatizations efficiently. In connection with the operational interpretation of proofs as coercions this gives efficient (O(n 2) time) algorithms for constructing efficient coercions from a type to any of its supertypes or isomorphic types. More generally, we show how adding the fixpoint rule makes it possible to characterize inductively a set that is coinductively defined as the kernel (greatest fixed point) of an inference system.},
  journal = {Fundam. Inf.},
  keywords = {axiomatization,coercion,coinduction,fixpoint,inference rule,inference system,operational interpretation,recursive type,subtyping,type equality},
  number = {4}
}

@inproceedings{Bransen2015incremental,
  title = {Incremental {{Evaluation}} of {{Higher Order Attributes}}},
  booktitle = {Proceedings of the 2015 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Bransen, Jeroen and Dijkstra, Atze and Swierstra, S. Doaitse},
  year = {2015},
  pages = {39--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2678015.2682541},
  abstract = {Compilers, amongst other programs, often work with data that (slowly) changes over time. When the changes between subsequent runs of the compiler are small, one would hope the compiler to incrementally update its results, resulting in much lower running times. However, the manual construction of an incremental compiler is very hard and error prone and therefore usually not an option. Attribute grammars provide an attractive way of constructing compilers, as they are compositional in nature and allow for aspect oriented programming. In this work we extend previous work on the automatic generation of incremental attribute grammar evaluators, with the purpose of (semi-)automatically generating an incremental compiler from the regular attribute grammar definition, by adding support for incremental evaluation of higher order attributes, a well known extension to the classical attribute grammars that is used in many ways in compiler construction, for example to model different compiler phases.},
  isbn = {978-1-4503-3297-2},
  keywords = {attribute grammars,change propagation,incremental evaluation,program transformation,type inference},
  series = {{{PEPM}} '15}
}

@inproceedings{Brassel2010transforming,
  title = {Transforming Functional Logic Programs into Monadic Functional Programs},
  booktitle = {In {{Workshop}} on {{Functional}} and ({{Constraint}}) {{Logic Programming}}, {{Draft Proceedings}}},
  author = {Bra{\ss}el, Bernd and Fischer, Sebastian and Hanus, Michael and Reck, Fabian},
  year = {2010},
  abstract = {Abstract. We present a high-level transformation scheme to translate lazy functional logic programs into pure Haskell programs. This transformation is based on a recent proposal to efficiently implement lazy non-deterministic computations in Haskell into monadic style. We build on this work and define a systematic method to transform lazy functional logic programs into monadic programs with explicit sharing. This results in a transformation scheme which produces high-level and flexible target code. For instance, the target code is parametric w.r.t. the concrete evaluation monad. Thus, different monad instances could, for example, define different search strategies (e.g., depth-first, breadth-first, parallel). We formally describe the basic compilation scheme and some useful extensions. 1}
}

@article{Breazu-Tannen1988Extensional,
  title = {Extensional Models for Polymorphism},
  author = {{Breazu-Tannen}, Val and Coquand, Thierry},
  year = {1988},
  month = jul,
  volume = {59},
  pages = {85--114},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(88)90097-7},
  abstract = {We present a general method for constructing extensional models for the Girard-Reynolds polymorphic lambda calculus\textemdash{}the polymorphic extensional collapse. The method yields models that satisfy additional, computationally motivated constraints like having only two polymorphic booleans and having only the numerals as polymorphic integers. Moreover, the method can be used to show that any simply typed lambda model can be fully and faithfully embedded into a model of the polymorphic lambda calculus.},
  journal = {Theoretical Computer Science},
  number = {1}
}

@inproceedings{Breitner2014safe,
  title = {Safe {{Zero}}-Cost {{Coercions}} for {{Haskell}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Breitner, Joachim and Eisenberg, Richard A. and Peyton Jones, Simon and Weirich, Stephanie},
  year = {2014},
  pages = {189--202},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628141},
  abstract = {Generative type abstractions -- present in Haskell, OCaml, and other languages -- are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.},
  isbn = {978-1-4503-2873-9},
  keywords = {coercion,Haskell,newtype deriving,type class},
  series = {{{ICFP}} '14}
}

@inproceedings{Brookes2004Semantics,
  title = {A {{Semantics}} for {{Concurrent Separation Logic}}},
  booktitle = {{{CONCUR}} 2004 - {{Concurrency Theory}}},
  author = {Brookes, Stephen},
  editor = {Gardner, Philippa and Yoshida, Nobuko},
  year = {2004},
  pages = {16--34},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present a denotational semantics based on action traces, for parallel programs which share mutable data and synchronize using resources and conditional critical regions. We introduce a resource-sensitive logic for partial correctness, adapting separation logic to the concurrent setting, as proposed by O'Hearn. The logic allows program proofs in which ``ownership'' of a piece of state is deemed to transfer dynamically between processes and resources. We prove soundness of this logic, using a novel ``local'' interpretation of traces, and we show that every provable program is race-free.},
  isbn = {978-3-540-28644-8},
  keywords = {Denotational Semantic,Inference Rule,Parallel Composition,Parallel Program,Resource Invariant},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Brookes2007semantics,
  title = {A Semantics for Concurrent Separation Logic},
  author = {Brookes, Stephen},
  year = {2007},
  month = may,
  volume = {375},
  pages = {227--270},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.12.034},
  abstract = {We present a trace semantics for a language of parallel programs which share access to mutable data. We introduce a resource-sensitive logic for partial correctness, based on a recent proposal of O'Hearn, adapting separation logic to the concurrent setting. The logic allows proofs of parallel programs in which ``ownership'' of critical data, such as the right to access, update or deallocate a pointer, is transferred dynamically between concurrent processes. We prove soundness of the logic, using a novel ``local'' interpretation of traces which allows accurate reasoning about ownership. We show that every provable program is race-free.},
  journal = {Theoretical Computer Science},
  keywords = {Concurrency,Logic,Pointers,Race condition,Semantics},
  number = {1},
  series = {Festschrift for {{John C}}. {{Reynolds}}'s 70th Birthday}
}

@inproceedings{Brotherston2017Granullar,
  title = {Granullar: {{Gradual}} Nullable Types for {{Java}}},
  shorttitle = {Granullar},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Compiler Construction}}},
  author = {Brotherston, Dan and Dietl, Werner and Lhot{\'a}k, Ond{\v r}ej},
  year = {2017},
  pages = {87--97},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3033019.3033032},
  abstract = {Object-oriented languages like Java and C\# allow the null value for all references. This supports many flexible patterns, but has led to many errors, security vulnerabilities, and system crashes. \% Static type systems can prevent null-pointer exceptions at compile time, but require annotations, in particular for used libraries. Conservative defaults choose the most restrictive typing, preventing many errors, but requiring a large annotation effort. Liberal defaults choose the most flexible typing, requiring less annotations, but giving weaker guarantees. Trusted annotations can be provided, but are not checked and require a large manual effort. None of these approaches provide a strong guarantee that the checked part of the program is isolated from the unchecked part: even with conservative defaults, null-pointer exceptions can occur in the checked part.   This paper presents Granullar, a gradual type system for null-safety. Developers start out verifying null-safety for the most important components of their applications. At the boundary to unchecked components, runtime checks are inserted by Granullar to guard the verified system from being polluted by unexpected null values. This ensures that null-pointer exceptions can only occur within the unchecked code or at the boundary to checked code; the checked code is free of null-pointer exceptions.   We present Granullar for Java, define the checked-unchecked boundary, and how runtime checks are generated. We evaluate our approach on real world software annotated for null-safety. We demonstrate the runtime checks, and acceptable compile-time and run-time performance impacts. Granullar enables combining a checked core with untrusted libraries in a safe manner, improving on the practicality of such a system.},
  isbn = {978-1-4503-5233-8},
  keywords = {gradual type systems,nullness,pluggable type systems,runtime checks},
  series = {{{CC}} 2017}
}

@inproceedings{Brown2017Typed,
  title = {Typed {{Self}}-Evaluation via {{Intensional Type Functions}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Brown, Matt and Palsberg, Jens},
  year = {2017},
  pages = {415--428},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009853},
  abstract = {Many popular languages have a self-interpreter, that is, an interpreter for the language written in itself. So far, work on polymorphically-typed self-interpreters has concentrated on self-recognizers that merely recover a program from its representation. A larger and until now unsolved challenge is to implement a polymorphically-typed self-evaluator that evaluates the represented program and produces a representation of the result. We present F{\"I}{\^A}{$\mathrm{\mu}$}i, the first {\^I}\guillemotright{}-calculus that supports a polymorphically-typed self-evaluator. Our calculus extends F{\"I} with recursive types and intensional type functions and has decidable type checking. Our key innovation is a novel implementation of type equality proofs that enables us to define a versatile representation of programs. Our results establish a new category of languages that can support polymorphically-typed self-evaluators.},
  isbn = {978-1-4503-4660-3},
  keywords = {lambda calculus,meta programming,Self Evaluation,Self Interpretation,Self Representation,type equality},
  series = {{{POPL}} 2017}
}

@incollection{Bruce1984semantics,
  title = {The Semantics of Second Order Polymorphic Lambda Calculus},
  booktitle = {Semantics of {{Data Types}}},
  author = {Bruce, Kim B. and Meyer, Albert R.},
  year = {1984},
  pages = {131--144},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-13346-1_6},
  language = {en}
}

@incollection{Bruce1997comparing,
  title = {Comparing Object Encodings},
  booktitle = {Theoretical {{Aspects}} of {{Computer Software}}},
  author = {Bruce, Kim B. and Cardelli, Luca and Pierce, Benjamin C.},
  editor = {Abadi, Mart{\'i}n and Ito, Takayasu},
  year = {1997},
  pages = {415--438},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Recent years have seen the development of several foundational models for statically typed object-oriented programming. But despite their intuitive similarity, differences in the technical machinery used to formulate the various proposals have made them difficult to compare. Using the typed lambda-calculus F {$<$} {$\omega$} : as a common basis, we now offer a detailed comparison of four models: (1) a recursive-record encoding similar to the ones used by Cardelli [Car84], Reddy [Red88, KR94], Cook [Coo89, CHC90], and others; (2) Hofmann, Pierce, and Turner's existential encoding [PT94, HP95]; (3) Bruce's model based on existential and recursive types [Bru94]; and (4) Abadi, Cardelli, and Viswanathan's type-theoretic encoding [ACV96] of a calculus of primitive objects.},
  copyright = {\textcopyright{}1997 Springer-Verlag},
  isbn = {978-3-540-63388-4 978-3-540-69530-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {1281},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Bruce1998statically,
  title = {A Statically Safe Alternative to Virtual Types},
  booktitle = {{{ECOOP}}'98 \textemdash{} {{Object}}-{{Oriented Programming}}},
  author = {Bruce, Kim B. and Odersky, Martin and Wadler, Philip},
  year = {1998},
  month = jul,
  pages = {523--549},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0054106},
  abstract = {Parametric types and virtual types have recently been proposed as extensions to Java to support genericity. In this paper we investigate the strengths and weaknesses of each. We suggest a variant of virtual types which has similar expressiveness, but supports safe static type checking. This results in a language in which both parametric types and virtual types are well-integrated, and which is statically type-safe.},
  isbn = {978-3-540-64737-9 978-3-540-69064-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Bruijn1991telescopic,
  title = {Telescopic Mappings in Typed Lambda Calculus},
  author = {Bruijn, N. G. De},
  year = {1991},
  volume = {91},
  pages = {189--204},
  abstract = {The paper develops notation for strings of abstracters in typed lambda calculus, and shows how to treat them more or less as single abstracters. 0 1991 Academic Press. Inc. 1.},
  journal = {Information and Computation}
}

@article{Brusilovsky1997Minilanguages,
  title = {Mini-Languages: A Way to Learn Programming Principles},
  shorttitle = {Mini-Languages},
  author = {Brusilovsky, Peter and Calabrese, Eduardo and Hvorecky, Jozef and Kouchnirenko, Anatoly and Miller, Philip},
  year = {1997},
  month = mar,
  volume = {2},
  pages = {65--83},
  issn = {1360-2357, 1573-7608},
  doi = {10.1023/A:1018636507883},
  abstract = {Mini-languages are a visually intuitive, simple and powerful way to introduce students to programming. They are a good foundation for general computer science instruction, provide insight into programming for the general population, and teach algorithmic thinking. The goal of the paper is to provide an extensive review of the mini-language approach to teaching programming. For different audiences and in different countries, the authors have extensive experience in design and application of mini-languages. We outline the problems that motivate the application of this approach, present a brief history, review several existing mini-languages, and provide discussion of lessons learned. In particular, we discuss how to choose a mini-language for a particular group of students and list some requirements for a successful application of a mini- language. We conclude with a discussion of possible future directions of the mini-language approach development},
  journal = {Education and Information Technologies},
  keywords = {Computer Science; general,Data Structures; Cryptology and Information Theory,Education (general),highereducation,informatics,languages,logo,programming,secondaryeducation,Technology Education},
  language = {en},
  number = {1}
}

@article{Buchlovsky2006typetheoretic,
  title = {A {{Type}}-Theoretic {{Reconstruction}} of the {{Visitor Pattern}}},
  author = {Buchlovsky, Peter and Thielecke, Hayo},
  year = {2006},
  month = may,
  volume = {155},
  pages = {309--329},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2005.11.061},
  abstract = {In object-oriented languages, the Visitor pattern can be used to traverse tree-like data structures: a visitor object contains some operations, and the data structure objects allow themselves to be traversed by accepting visitors. In the polymorphic lambda calculus (System F), tree-like data structures can be encoded as polymorphic higher-order functions. In this paper, we reconstruct the Visitor pattern from the polymorphic encoding by way of generics in Java. We sketch how the quantified types in the polymorphic encoding can guide reasoning about visitors in general.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Generic Java,object-oriented programming,polymorphic types,visitor pattern},
  series = {Proceedings of the 21st {{Annual Conference}} on {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXI}}) {{Mathematical Foundations}} of {{Programming Semantics XXI}}}
}

@incollection{Buffenbarger1995syntactic,
  title = {Syntactic Software Merging},
  booktitle = {Software {{Configuration Management}}},
  author = {Buffenbarger, Jim},
  editor = {Estublier, Jacky},
  year = {1995},
  pages = {153--172},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/3-540-60578-9_14},
  abstract = {Software merging is the process of combining multiple existing versions of a source file, to produce a new version. Typically, the goal is for the new version to implement some kind of union of the features implemented by the existing versions. A variety of merge tools are available, but software merging is still a tedious process, and mistakes are easy to make. This paper describes the fundamentals of merging, surveys the known methods of software merging, including a method based on programming-language syntax, and discusses a set of tools that perform syntactic merging.},
  copyright = {\textcopyright{}1995 Springer-Verlag},
  isbn = {978-3-540-60578-2 978-3-540-47768-6},
  keywords = {IT in Business,Software Engineering,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {1005},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Buiras2015hlio,
  title = {{{HLIO}}: {{Mixing Static}} and {{Dynamic Typing}} for {{Information}}-Flow {{Control}} in {{Haskell}}},
  shorttitle = {{{HLIO}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Buiras, Pablo and Vytiniotis, Dimitrios and Russo, Alejandro},
  year = {2015},
  pages = {289--301},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784758},
  abstract = {Information-Flow Control (IFC) is a well-established approach for allowing untrusted code to manipulate sensitive data without disclosing it. IFC is typically enforced via type systems and static analyses or via dynamic execution monitors. The LIO Haskell library, originating in operating systems research, implements a purely dynamic monitor of the sensitivity level of a computation, particularly suitable when data sensitivity levels are only known at runtime. In this paper, we show how to give programmers the flexibility of deferring IFC checks to runtime (as in LIO), while also providing static guarantees---and the absence of runtime checks---for parts of their programs that can be statically verified (unlike LIO). We present the design and implementation of our approach, HLIO (Hybrid LIO), as an embedding in Haskell that uses a novel technique for deferring IFC checks based on singleton types and constraint polymorphism. We formalize HLIO, prove non-interference, and show how interesting IFC examples can be programmed. Although our motivation is IFC, our technique for deferring constraints goes well beyond and offers a methodology for programmer-controlled hybrid type checking in Haskell.},
  isbn = {978-1-4503-3669-7},
  keywords = {constraint kinds,data kinds,dynamic typing,gradual typing,hybrid typing,Information-flow control,singleton types},
  series = {{{ICFP}} 2015}
}

@article{Burstall1977transformation,
  title = {A {{Transformation System}} for {{Developing Recursive Programs}}},
  author = {Burstall, R. M. and Darlington, John},
  year = {1977},
  month = jan,
  volume = {24},
  pages = {44--67},
  issn = {0004-5411},
  doi = {10.1145/321992.321996},
  abstract = {A system of rules for transforming programs is described, with the programs in the form of recursion equations. An initially very simple, lucid, and hopefully correct program is transformed into a more efficient one by altering the recursion structure. Illustrative examples of program transformations are given, and a tentative implementation is described. Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program-manipulation system is indicated.},
  journal = {J. ACM},
  number = {1}
}

@inproceedings{Burstall1980hope,
  title = {{{HOPE}}: {{An Experimental Applicative Language}}},
  shorttitle = {{{HOPE}}},
  booktitle = {Proceedings of the 1980 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Burstall, R. M. and MacQueen, D. B. and Sannella, D. T.},
  year = {1980},
  pages = {136--143},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800087.802799},
  abstract = {An applicative language called HOPE is described and discussed. The underlying goal of the design and implementation effort was to produce a very simple programming language which encourages the construction of clear and manipulable programs. HOPE does not include an assignment statement; this is felt to be an important simplification. The user may freely define his own data types, without the need to devise a complicated encoding in terms of low-level types. The language is very strongly typed, and as implemented it incorporates a typechecker which handles polymorphic types and overloaded operators. Functions are defined by a set of recursion equations; the left-hand side of each equation includes a pattern used to determine which equation to use for a given argument. The availability of arbitrary higher-order types allows functions to be defined which 'package' recursion. Lazily-evaluated lists are provided, allowing the use of infinite lists which could be used to provide interactive input/output and concurrency. HOPE also includes a simple modularisation facility which may be used to protect the implementation of an abstract data type.},
  series = {{{LFP}} '80}
}

@article{Buss2002prospects,
  title = {The Prospects for Mathematical Logic in the Twenty-First Century},
  author = {Buss, Samuel R. and Kechris, Alexander S. and Pillay, Anand and Shore, Richard A.},
  year = {2002},
  month = may,
  abstract = {The four authors present their speculations about the future developments of mathematical logic in the twenty-first century. The areas of recursion theory, proof theory and logic for computer science, model theory, and set theory are discussed independently.},
  archivePrefix = {arXiv},
  eprint = {cs/0205003},
  eprinttype = {arxiv},
  journal = {arXiv:cs/0205003},
  keywords = {A.1,Computer Science - Logic in Computer Science,F.0,I.2.0}
}

@incollection{Caccamo2001higherorder,
  title = {A {{Higher}}-{{Order Calculus}} for {{Categories}}},
  booktitle = {Theorem {{Proving}} in {{Higher Order Logics}}},
  author = {C{\'a}ccamo, Mario and Winskel, Glynn},
  editor = {Boulton, Richard J. and Jackson, Paul B.},
  year = {2001},
  month = jan,
  pages = {136--153},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A calculus for a fragment of category theory is presented. The types in the language denote categories and the expressions functors. The judgements of the calculus systematise categorical arguments such as: an expression is functorial in its free variables; two expressions are naturally isomorphic in their free variables. There are special binders for limits and more general ends. The rules for limits and ends support an algebraic manipulation of universal constructions as opposed to a more traditional diagrammatic approach. Duality within the calculus and applications in proving continuity are discussed with examples. The calculus gives a basis for mechanising a theory of categories in a generic theorem prover like Isabelle.},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-42525-0 978-3-540-44755-9},
  keywords = {Artificial Intelligence (incl. Robotics),Logic Design,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering},
  language = {en},
  number = {2152},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Cai2016System,
  title = {System {{F}}-Omega with {{Equirecursive Types}} for {{Datatype}}-Generic {{Programming}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cai, Yufei and Giarrusso, Paolo G. and Ostermann, Klaus},
  year = {2016},
  volume = {2016},
  publisher = {{ACM}},
  keywords = {Datatype-generic programming,equirecursive types,functors}
}

@inproceedings{Cardelli1984Compiling,
  title = {Compiling a {{Functional Language}}},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  author = {Cardelli, Luca},
  year = {1984},
  pages = {208--217},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800055.802037},
  abstract = {This paper summarizes my experience in implementing a compiler for a functional language. The language is ML(1) [Milner 84] and the compiler was first implemented in 1980 as a personal project when I was a postgraduate student at the University of Edinburgh(2). At the time, I was familiar with programming language semantics but knew very little about compiler technology; interpreters had been my main programming concern. Major influences in the design of this compiler have been [Steele 77] [Steele 78] and the implementation folklore for statically and dynamically scoped dialects of Lisp [Allen 78]. As a result, the internal structure of the compiler is fairly unorthodox, if compared for example with [Aho 78]. Anyway, a compiler for a language like ML has to be different. ML is interactive, statically scoped, strongly typed, polymorphic, and has first class higher-order functions, type inference and dynamic allocation. These features preclude many well-known implementation styles, particularly the ones used for Lisp (because of static scoping), the Algol family (because of functional values) and C (because of nested scoping and strong typing). The interaction of these features is what gives ML its ``character'', and makes compilation challenging. The compiler has been recently partially converted to the new ML standard. The major points of interest which are discussed in this paper are: (a) the interactive interpreter-like usage; (b) the polymorphic type inference algorithm; (c) the compilation of pattern matching; (d) the optimization of the representation of user defined data types; (e) the compilation of functional closures, function application and variable access; (f) the intermediate abstract machine and its formal operational semantics; (g) modules and type-safe separate compilation.},
  isbn = {978-0-89791-142-9},
  series = {{{LFP}} '84}
}

@inproceedings{Cardelli1988Structural,
  title = {Structural {{Subtyping}} and the {{Notion}} of {{Power Type}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cardelli, L.},
  year = {1988},
  pages = {70--79},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/73560.73566},
  isbn = {978-0-89791-252-5},
  series = {{{POPL}} '88}
}

@inproceedings{Cardelli1997program,
  title = {Program {{Fragments}}, {{Linking}}, and {{Modularization}}},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cardelli, Luca},
  year = {1997},
  pages = {266--277},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/263699.263735},
  isbn = {0-89791-853-3},
  series = {{{POPL}} '97}
}

@article{Cardone1991type,
  title = {Type Inference with Recursive Types: {{Syntax}} and Semantics},
  shorttitle = {Type Inference with Recursive Types},
  author = {Cardone, Felice and Coppo, Mario},
  year = {1991},
  month = may,
  volume = {92},
  pages = {48--80},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90020-3},
  abstract = {In this paper we study type inference systems for {$\lambda$}-calculus with a recursion operator over types. The main syntactical properties, notably the existence of principal type schemes, are proved to hold when recursive types are viewed as finite notations for infinite (regular) type expressions representing their infinite unfoldings. Exploiting the approximation structure of a model for the untyped language of terms, types are interpreted as limits of sequences of their approximations. We show that the interpretation is essentially unique and that two types have equal interpretation if and only if their infinite unfoldings are identical. Finally, a completeness theorem is proved to hold w.r.t. the specific model we consider for a natural (infinitary) extension of the type inference system.},
  journal = {Information and Computation},
  number = {1}
}

@book{Carettefinally,
  title = {Finally {{Tagless}}, {{Partially Evaluated}} \textendash{} {{Tagless Staged Interpreters}} for {{Simpler Typed Languages}}},
  author = {Carette, Jacques and Kiselyov, Oleg and Shan, Chung-chieh},
  abstract = {We have built the first family of tagless interpretations for a higher-order typed object language in a typed metalanguage (Haskell or ML) that require no dependent types, generalized algebraic data types, or postprocessing to eliminate tags. The statically type-preserving interpretations include an evaluator, a compiler (or staged evaluator), a partial evaluator, and call-by-name and call-by-value CPS transformers. Our main idea is to encode HOAS using cogen functions rather than data constructors. In other words, we represent object terms not in an initial algebra but using the coalgebraic structure of the {$\lambda$}-calculus. Our representation also simulates inductive maps from types to types, which are required for typed partial evaluation and CPS transformations. Our encoding of an object term abstracts over the various ways to interpret it, yet statically assures that the interpreters never get stuck. To achieve self-interpretation and show Jones-optimality, we relate this exemplar of higher-rank and higher-kind polymorphism to plugging a term into a context of let-polymorphic bindings.}
}

@inproceedings{Cartwright1985Types,
  title = {Types {{As Intervals}}},
  booktitle = {Proceedings of the 12th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cartwright, Robert},
  year = {1985},
  pages = {22--36},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/318593.318604},
  abstract = {To accommodate polymorphic data types and operations, several computer scientists--most notably MacQueen, Plotkin, and Sethi--have proposed formalizing types as ideals. Although this approach is intuitively appealing, the resulting type system is both complex and restrictive because the type constructor that creates function types is not monotonic, and hence not computable. As a result, types cannot be treated as data values, precluding the formalization of type constructors and polymorphic program modules (where types are values) as higher order computable functions. Moreover, recursive definitions of new types do not necessarily have solutions.
This paper proposes a new formulation of types--called intervals--that subsumes the theory of types as ideals, yet avoids the pathologies caused by non-monotonic type constructors. In particular, the set of interval types contains the set of ideal types as a proper subset and all of the primitive type operations on intervals are extensions of the corresponding operations on ideals. Nevertheless, all of the primitive interval type constructors including the function type constructor and type quantifiers are computable operations. Consequently, types are higher order data values that can be freely manipulated within programs.
The key idea underlying the formalization of types as intervals is that negative information should be included in the description of a type. Negative information identifies the finite elements that do not belong to a type, just as conventional, positive information identifies the elements that do. Unless the negative information in a type description is the exact complement of the positive information, the description is partial in the sense that it approximates many different types--an interval of ideals between the positive information and the complement of the negative information. Although programmers typically deal with total (maximal) types, partial types appear to be an essential feature of a comprehensive polymorphic type system that accommodates types as data, just as partial functions are essential in any universal programming language.},
  isbn = {978-0-89791-147-4},
  series = {{{POPL}} '85}
}

@article{Casinghino2012StepIndexed,
  title = {Step-{{Indexed Normalization}} for a {{Language}} with {{General Recursion}}},
  author = {Casinghino, Chris and Sj{\"o}berg, Vilhelm and Weirich, Stephanie},
  year = {2012},
  month = feb,
  volume = {76},
  pages = {25--39},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.76.4},
  abstract = {The Trellys project has produced several designs for practical dependently typed languages. These languages are broken into two fragments-a\_logical\_fragment where every term normalizes and which is consistent when interpreted as a logic, and a\_programmatic\_fragment with general recursion and other convenient but unsound features. In this paper, we present a small example language in this style. Our design allows the programmer to explicitly mention and pass information between the two fragments. We show that this feature substantially complicates the metatheory and present a new technique, combining the traditional Girard-Tait method with step-indexed logical relations, which we use to show normalization for the logical fragment.},
  archivePrefix = {arXiv},
  eprint = {1202.2918},
  eprinttype = {arxiv},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages}
}

@inproceedings{Castagna2011Settheoretic,
  title = {Set-Theoretic {{Foundation}} of {{Parametric Polymorphism}} and {{Subtyping}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Castagna, Giuseppe and Xu, Zhiwu},
  year = {2011},
  pages = {94--106},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2034773.2034788},
  abstract = {We define and study parametric polymorphism for a type system with recursive, product, union, intersection, negation, and function types. We first recall why the definition of such a system was considered hard "when not impossible" and then present the main ideas at the basis of our solution. In particular, we introduce the notion of "convexity" on which our solution is built up and discuss its connections with parametricity as defined by Reynolds to whose study our work sheds new light.},
  isbn = {978-1-4503-0865-6},
  keywords = {parametricity,polymorphism,subtyping,types,XML},
  series = {{{ICFP}} '11}
}

@inproceedings{Castagna2014polymorphic,
  title = {Polymorphic {{Functions}} with {{Set}}-Theoretic {{Types}}: {{Part}} 1: {{Syntax}}, {{Semantics}}, and {{Evaluation}}},
  shorttitle = {Polymorphic {{Functions}} with {{Set}}-Theoretic {{Types}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Castagna, Giuseppe and Nguyen, Kim and Xu, Zhiwu and Im, Hyeonseung and Lenglet, Sergue{\"i} and Padovani, Luca},
  year = {2014},
  pages = {5--17},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2535838.2535840},
  abstract = {This article is the first part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed lambda-calculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data.},
  isbn = {978-1-4503-2544-8},
  keywords = {intersection types,polymorphism,types,XML},
  series = {{{POPL}} '14}
}

@inproceedings{Castagna2015polymorphic,
  title = {Polymorphic {{Functions}} with {{Set}}-{{Theoretic Types}}: {{Part}} 2: {{Local Type Inference}} and {{Type Reconstruction}}},
  shorttitle = {Polymorphic {{Functions}} with {{Set}}-{{Theoretic Types}}},
  booktitle = {Proceedings of the {{42Nd Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Castagna, Giuseppe and Nguyen, Kim and Xu, Zhiwu and Abate, Pietro},
  year = {2015},
  pages = {289--302},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2676726.2676991},
  abstract = {This article is the second part of a two articles series about the definition of higher order polymorphic functions in a type system with recursive types and set-theoretic type connectives (unions, intersections, and negations). In the first part, presented in a companion paper, we defined and studied the syntax, semantics, and evaluation of the explicitly-typed version of a calculus, in which type instantiation is driven by explicit instantiation annotations. In this second part we present a local type inference system that allows the programmer to omit explicit instantiation annotations for function applications, and a type reconstruction system that allows the programmer to omit explicit type annotations for function definitions. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages with union and intersection types and/or for semi-structured data processing.},
  isbn = {978-1-4503-3300-9},
  keywords = {intersection types,polymorphism,semantic subtyping,type constraints,types,XML},
  series = {{{POPL}} '15}
}

@inproceedings{Castro2016Farms,
  title = {Farms, {{Pipes}}, {{Streams}} and {{Reforestation}}: {{Reasoning About Structured Parallel Processes Using Types}} and {{Hylomorphisms}}},
  shorttitle = {Farms, {{Pipes}}, {{Streams}} and {{Reforestation}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Castro, David and Hammond, Kevin and Sarkar, Susmit},
  year = {2016},
  pages = {4--17},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951920},
  abstract = {The increasing importance of parallelism has motivated the creation of better abstractions for writing parallel software, including structured parallelism using nested algorithmic skeletons. Such approaches provide high-level abstractions that avoid common problems, such as race conditions, and often allow strong cost models to be defined. However, choosing a combination of algorithmic skeletons that yields good parallel speedups for a program on some specific parallel architecture remains a difficult task. In order to achieve this, it is necessary to simultaneously reason both about the costs of different parallel structures and about the semantic equivalences between them. This paper presents a new type-based mechanism that enables strong static reasoning about these properties. We exploit well-known properties of a very general recursion pattern, hylomorphisms, and give a denotational semantics for structured parallel processes in terms of these hylomorphisms. Using our approach, it is possible to determine formally whether it is possible to introduce a desired parallel structure into a program without altering its functional behaviour, and also to choose a version of that parallel structure that minimises some given cost model.},
  isbn = {978-1-4503-4219-3},
  keywords = {hylomorphisms,Parallelism,term rewriting systems,type systems},
  series = {{{ICFP}} 2016}
}

@article{Cazzola2016Language,
  title = {Language Components for Modular {{DSLs}} Using Traits},
  author = {Cazzola, Walter and Vacchi, Edoardo},
  year = {2016},
  month = apr,
  volume = {45},
  pages = {16--34},
  issn = {1477-8424},
  doi = {10.1016/j.cl.2015.12.001},
  abstract = {Recent advances in tooling and modern programming languages have progressively brought back the practice of developing domain-specific languages as a means to improve software development. Consequently, the problem of making composition between languages easier by emphasizing code reuse and componentized programming is a topic of increasing interest in research. In fact, it is not uncommon for different languages to share common features, and, because in the same project different DSLs may coexist to model concepts from different problem areas, it is interesting to study ways to develop modular, extensible languages. Earlier work has shown that traits can be used to modularize the semantics of a language implementation; a lot of attention is often spent on embedded DSLs; even when external DSLs are discussed, the main focus is on modularizing the semantics. In this paper we will show a complete trait-based approach to modularize not only the semantics but also the syntax of external DSLs, thereby simplifying extension and therefore evolution of a language implementation. We show the benefits of implementing these techniques using the Scala programming language.},
  journal = {Computer Languages, Systems \& Structures}
}

@misc{CenterforHistoryandNewMediaZotero,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{Cervesato2003linear,
  title = {A {{Linear Spine Calculus}}},
  author = {Cervesato, Iliano and Pfenning, Frank},
  year = {2003},
  month = jan,
  volume = {13},
  pages = {639--688},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/13.5.639},
  abstract = {We present the spine calculus S\textrightarrow{$\multimap\&\top$} as an efficient representation for the linear {$\lambda$}-calculus {$\lambda\rightarrow\multimap\&\top$} which includes unrestricted functions (\textrightarrow{}) linear functions ({$\multimap$}) additive pairing (\&) and additive unit ({$\top$}). S\textrightarrow{$\multimap\&\top$} enhances the representation of Church's simply typed {$\lambda$}-calculus by enforcing extensionality and by incorporating linear constructs. This approach permits procedures such as unification to retain the efficient head access that characterizes first-order term languages without the overhead of performing {$\eta$}-conversions at run time. Applications lie in proof search, logic programming, and logical frameworks based on linear type theories. It is also related to foundational work on term assignment calculi for presentations of the sequent calculus. We define the spine calculus, give translations of {$\lambda\rightarrow\multimap\&\top$} into S\textrightarrow{$\multimap\&\top$} and vice versa, prove their soundness and completeness with respect to typing and reductions, and show that the typable fragment of the spine calculus is strongly normalizing and admits unique canonical, i.e. {$\beta\eta$}-normal, forms.},
  journal = {Journal of Logic and Computation},
  keywords = {Linear lambda calculus; term assignment systems; uniform provability.},
  language = {en},
  number = {5}
}

@inproceedings{Chafi2011domainspecific,
  title = {A {{Domain}}-Specific {{Approach}} to {{Heterogeneous Parallelism}}},
  booktitle = {Proceedings of the 16th {{ACM Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Chafi, Hassan and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Atreya, Anand R. and Olukotun, Kunle},
  year = {2011},
  pages = {35--46},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1941553.1941561},
  abstract = {Exploiting heterogeneous parallel hardware currently requires mapping application code to multiple disparate programming models. Unfortunately, general-purpose programming models available today can yield high performance but are too low-level to be accessible to the average programmer. We propose leveraging domain-specific languages (DSLs) to map high-level application code to heterogeneous devices. To demonstrate the potential of this approach we present OptiML, a DSL for machine learning. OptiML programs are implicitly parallel and can achieve high performance on heterogeneous hardware with no modification required to the source code. For such a DSL-based approach to be tractable at large scales, better tools are required for DSL authors to simplify language creation and parallelization. To address this concern, we introduce Delite, a system designed specifically for DSLs that is both a framework for creating an implicitly parallel DSL as well as a dynamic runtime providing automated targeting to heterogeneous parallel hardware. We show that OptiML running on Delite achieves single-threaded, parallel, and GPU performance superior to explicitly parallelized MATLAB code in nearly all cases.},
  isbn = {978-1-4503-0119-0},
  keywords = {domain-specific languages,dynamic optimizations,parallel programming,runtimes},
  series = {{{PPoPP}} '11}
}

@inproceedings{Chakravarty2007data,
  title = {Data {{Parallel Haskell}}: {{A Status Report}}},
  shorttitle = {Data {{Parallel Haskell}}},
  booktitle = {Proceedings of the 2007 {{Workshop}} on {{Declarative Aspects}} of {{Multicore Programming}}},
  author = {Chakravarty, Manuel M. T. and Leshchinskiy, Roman and Peyton Jones, Simon and Keller, Gabriele and Marlow, Simon},
  year = {2007},
  pages = {10--18},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1248648.1248652},
  abstract = {We describe the design and current status of our effort to implement the programming model of nested data parallelism into the Glasgow Haskell Compiler. We extended the original programming model and its implementation, both of which were first popularised by the NESL language, in terms of expressiveness as well as efficiency. Our current aim is to provide a convenient programming environment for SMP parallelism, and especially multicore architectures. Preliminary benchmarks show that we are, at least for some programs, able to achieve good absolute performance and excellent speedups.},
  isbn = {978-1-59593-690-5},
  series = {{{DAMP}} '07}
}

@incollection{Chang2012callbyneed,
  title = {The {{Call}}-by-{{Need Lambda Calculus}}, {{Revisited}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Chang, Stephen and Felleisen, Matthias},
  editor = {Seidl, Helmut},
  year = {2012},
  month = jan,
  pages = {128--147},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The existing call-by-need {$\lambda$} describe lazy evaluation via equational logics. A programmer can use these logics to safely ascertain whether one term is behaviorally equivalent to another or to determine the value of a lazy program. However, neither of the existing calculi models evaluation in a way that matches lazy implementations. Both calculi suffer from the same two problems. First, the calculi never discard function calls, even after they are completely resolved. Second, the calculi include re-association axioms even though these axioms are merely administrative steps with no counterpart in any implementation. In this paper, we present an alternative axiomatization of lazy evaluation using a single axiom. It eliminates both the function call retention problem and the extraneous re-association axioms. Our axiom uses a grammar of contexts to describe the exact notion of a needed computation. Like its predecessors, our new calculus satisfies consistency and standardization properties and is thus suitable for reasoning about behavioral equivalence. In addition, we establish a correspondence between our semantics and Launchbury's natural semantics.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-28868-5 978-3-642-28869-2},
  keywords = {Call-by-need,Computer Communication Networks,lambda calculus,laziness,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {7211},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Chang2017Type,
  title = {Type {{Systems As Macros}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Chang, Stephen and Knauth, Alex and Greenman, Ben},
  year = {2017},
  pages = {694--705},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009886},
  abstract = {We present Turnstile, a metalanguage for creating typed embedded languages. To implement the type system, programmers write type checking rules resembling traditional judgment syntax. To implement the semantics, they incorporate elaborations into these rules. Turnstile critically depends on the idea of linguistic reuse. It exploits a macro system in a novel way to simultaneously type check and rewrite a surface program into a target language. Reusing a macro system also yields modular implementations whose rules may be mixed and matched to create other languages. Combined with typical compiler and runtime reuse, Turnstile produces performant typed embedded languages with little effort.},
  isbn = {978-1-4503-4660-3},
  keywords = {macros,type systems,typed embedded DSLs},
  series = {{{POPL}} 2017}
}

@inproceedings{Chapman2010gentle,
  title = {The {{Gentle Art}} of {{Levitation}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Chapman, James and Dagand, Pierre-{\'E}variste and McBride, Conor and Morris, Peter},
  year = {2010},
  pages = {3--14},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863543.1863547},
  abstract = {We present a closed dependent type theory whose inductive types are given not by a scheme for generative declarations, but by encoding in a universe. Each inductive datatype arises by interpreting its description - a first-class value in a datatype of descriptions. Moreover, the latter itself has a description. Datatype-generic programming thus becomes ordinary programming. We show some of the resulting generic operations and deploy them in particular, useful ways on the datatype of datatype descriptions itself. Simulations in existing systems suggest that this apparently self-supporting setup is achievable without paradox or infinite regress.},
  isbn = {978-1-60558-794-3},
  keywords = {data structure,metaprogramming,Monads,proof assistants,type systems},
  series = {{{ICFP}} '10}
}

@inproceedings{Chen2014Functional,
  title = {Functional {{Programming}} for {{Dynamic}} and {{Large Data}} with {{Self}}-Adjusting {{Computation}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Chen, Yan and Acar, Umut A. and Tangwongsan, Kanat},
  year = {2014},
  pages = {227--240},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628150},
  abstract = {Combining type theory, language design, and empirical work, we present techniques for computing with large and dynamically changing datasets. Based on lambda calculus, our techniques are suitable for expressing a diverse set of algorithms on large datasets and, via self-adjusting computation, enable computations to respond automatically to changes in their data. To improve the scalability of self-adjusting computation, we present a type system for precise dependency tracking that minimizes the time and space for storing dependency metadata. The type system eliminates an important assumption of prior work that can lead to recording spurious dependencies. We present a type-directed translation algorithm that generates correct self-adjusting programs without relying on this assumption. We then show a probabilistic-chunking technique to further decrease space usage by controlling the fundamental space-time tradeoff in self-adjusting computation. We implement and evaluate these techniques, showing promising results on challenging benchmarks involving large graphs.},
  isbn = {978-1-4503-2873-9},
  keywords = {granularity control,incremental graph algorithms,information-flow type system,performance,self-adjusting computation},
  series = {{{ICFP}} '14}
}

@inproceedings{Chen2016Principal,
  title = {Principal Type Inference for {{GADTs}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Chen, Sheng and Erwig, Martin},
  year = {2016},
  pages = {416--428},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2837614.2837665},
  abstract = {We present a new method for GADT type inference that improves the precision of previous approaches. In particular, our approach accepts more type-correct programs than previous approaches when they do not employ type annotations. A side benefit of our approach is that it can detect a wide range of runtime errors that are missed by previous approaches. Our method is based on the idea to represent type refinements in pattern-matching branches by choice types, which facilitate a separation of the typing and reconciliation phases and thus support case expressions. This idea is formalized in a type system, which is both sound and a conservative extension of the classical Hindley-Milner system. We present the results of an empirical evaluation that compares our algorithm with previous approaches.},
  isbn = {978-1-4503-3549-2},
  keywords = {_tablet,Choice Type,GADT,type inference,Type Reconciliation,Variational Unification},
  series = {{{POPL}} 2016}
}

@article{Cheney2012Formalizing,
  title = {Formalizing {{Adequacy}}: {{A Case Study}} for {{Higher}}-Order {{Abstract Syntax}}},
  shorttitle = {Formalizing {{Adequacy}}},
  author = {Cheney, James and Norrish, Michael and Vestergaard, Ren{\'e}},
  year = {2012},
  month = aug,
  volume = {49},
  pages = {209--239},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-011-9221-6},
  abstract = {Adequacy is an important criterion for judging whether a formalization is suitable for reasoning about the actual object of study. The issue is particularly subtle in the expansive case of approaches to languages with name-binding. In prior work, adequacy has been formalized only with respect to specific representation techniques. In this article, we give a general formal definition based on model-theoretic isomorphisms or interpretations. We investigate and formalize an adequate interpretation of untyped lambda-calculus within a higher-order metalanguage in Isabelle/HOL using the Nominal Datatype Package. Formalization elucidates some subtle issues that have been neglected in informal arguments concerning adequacy.},
  journal = {Journal of Automated Reasoning},
  language = {en},
  number = {2}
}

@inproceedings{Cheney2013Practical,
  title = {A {{Practical Theory}} of {{Language}}-Integrated {{Query}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Cheney, James and Lindley, Sam and Wadler, Philip},
  year = {2013},
  pages = {403--416},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500586},
  abstract = {Language-integrated query is receiving renewed attention, in part because of its support through Microsoft's LINQ framework. We present a practical theory of language-integrated query based on quotation and normalisation of quoted terms. Our technique supports join queries, abstraction over values and predicates, composition of queries, dynamic generation of queries, and queries with nested intermediate data. Higher-order features prove useful even for constructing first-order queries. We prove a theorem characterising when a host query is guaranteed to generate a single SQL query. We present experimental results confirming our technique works, even in situations where Microsoft's LINQ framework either fails to produce an SQL query or, in one case, produces an avalanche of SQL queries.},
  isbn = {978-1-4503-2326-0},
  keywords = {antiquotation,f\#,lambda calculus,LINQ,quotation,sql},
  series = {{{ICFP}} '13}
}

@incollection{Cheney2013theory,
  title = {Toward a {{Theory}} of {{Self}}-Explaining {{Computation}}},
  booktitle = {In {{Search}} of {{Elegance}} in the {{Theory}} and {{Practice}} of {{Computation}}},
  author = {Cheney, James and Acar, Umut A. and Perera, Roly},
  editor = {Tannen, Val and Wong, Limsoon and Libkin, Leonid and Fan, Wenfei and Tan, Wang-Chiew and Fourman, Michael},
  year = {2013},
  month = jan,
  pages = {193--216},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Provenance techniques aim to increase the reliability of human judgments about data by making its origin and derivation process explicit. Originally motivated by the needs of scientific databases and scientific computation, provenance has also become a major issue for business and government data on the Web. However, so far provenance has been studied only in relatively restrictive settings: typically, for data stored in databases or scientific workflow systems, and processed by query or workflow languages of limited expressiveness. Long-term provenance solutions require an understanding of provenance in other settings, particularly the general-purpose programming or scripting languages that are used to glue different components such as databases, Web services and workflows together. Moreover, what is required is not only an account of mechanisms for recording provenance, but also a theory of what it means for provenance information to explain or justify a computation. In this paper, we begin to outline a such a theory of self-explaining computation. We introduce a model of provenance for a simple imperative language based on operational derivations and explore its properties.},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-41659-0 978-3-642-41660-6},
  keywords = {Database Management,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters},
  number = {8000},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Cheney2014database,
  title = {Database {{Queries}} That {{Explain}} Their {{Work}}},
  author = {Cheney, James and Ahmed, Amal and Acar, Umut A.},
  year = {2014},
  month = aug,
  doi = {10.1145/2643135.2643143},
  abstract = {Provenance for database queries or scientific workflows is often motivated as providing explanation, increasing understanding of the underlying data sources and processes used to compute the query, and reproducibility, the capability to recompute the results on different inputs, possibly specialized to a part of the output. Many provenance systems claim to provide such capabilities; however, most lack formal definitions or guarantees of these properties, while others provide formal guarantees only for relatively limited classes of changes. Building on recent work on provenance traces and slicing for functional programming languages, we introduce a detailed tracing model of provenance for multiset-valued Nested Relational Calculus, define trace slicing algorithms that extract subtraces needed to explain or recompute specific parts of the output, and define query slicing and differencing techniques that support explanation. We state and prove correctness properties for these techniques and present a proof-of-concept implementation in Haskell.},
  archivePrefix = {arXiv},
  eprint = {1408.1675},
  eprinttype = {arxiv},
  journal = {arXiv:1408.1675 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Programming Languages,D.3.0,D.3.3},
  primaryClass = {cs}
}

@inproceedings{Cheney2014query,
  title = {Query {{Shredding}}: {{Efficient Relational Evaluation}} of {{Queries}} over {{Nested Multisets}}},
  shorttitle = {Query {{Shredding}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Cheney, James and Lindley, Sam and Wadler, Philip},
  year = {2014},
  pages = {1027--1038},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2588555.2612186},
  abstract = {Nested relational query languages have been explored extensively, and underlie industrial language-integrated query systems such as Microsoft's LINQ. However, relational databases do not natively support nested collections in query results. This can lead to major performance problems: if programmers write queries that yield nested results, then such systems typically either fail or generate a large number of queries. We present a new approach to query shredding, which converts a query returning nested data to a fixed number of SQL queries. Our approach, in contrast to prior work, handles multiset semantics, and generates an idiomatic SQL:1999 query directly from a normal form for nested queries. We provide a detailed description of our translation and present experiments showing that it offers comparable or better performance than a recent alternative approach on a range of examples.},
  isbn = {978-1-4503-2376-5},
  keywords = {language-integrated query,querying nested collections},
  series = {{{SIGMOD}} '14}
}

@article{Chin1995transformation,
  title = {A Transformation Method for Dynamic-Sized Tabulation},
  author = {Chin, Wei-Ngan and Hagiya, Masami},
  year = {1995},
  month = feb,
  volume = {32},
  pages = {93--115},
  issn = {0001-5903, 1432-0525},
  doi = {10.1007/BF01177742},
  abstract = {Tupling is a transformation tactic to obtain new functions, without redundant calls and/or multiple traversals of common inputs. It achieves this feat by allowing each set (tuple) of function calls to be computed recursively from its previous set. In previous works by Chin and Khoo [8,9], a safe (terminating) fold/unfold transformation algorithm was developed for some classes of functions which are guaranteed to be successfully tupled. However, these classes of functions currently usestatic-sized tables for eliminating the redundant calls. As shown by Richard Bird in [3], there are also other classes of programs whose redundant calls could only be eliminated by usingdynamic-sized tabulation. This paper proposes a new solution to dynamic-sized tabulation by an extension to the tupling tactic. Our extension useslambda abstractions which can be viewed as either dynamic-sized tables or applications of the higher-order generalisation technique to facilitate tupling. Significant speedups could be obtained after the transformed programs were vectorised, as confirmed by experiment.},
  journal = {Acta Informatica},
  keywords = {Computational Mathematics and Numerical Analysis,Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Information Systems and Communication Service,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {2}
}

@incollection{Chitil1998common,
  title = {Common Subexpressions Are Uncommon in Lazy Functional Languages},
  booktitle = {Implementation of {{Functional Languages}}},
  author = {Chitil, Olaf},
  editor = {Clack, Chris and Hammond, Kevin and Davie, Tony},
  year = {1998},
  month = jan,
  pages = {53--71},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Common subexpression elimination is a well-known compiler optimisation that saves time by avoiding the repetition of the same computation. In lazy functional languages, referential transparency renders the identification of common subexpressions very simple. More common subexpressions can be recognised because they can be of arbitrary type whereas standard common subexpression elimination only shares primitive values. However, because lazy functional languages decouple program structure from data space allocation and control flow, analysing its effects and deciding under which conditions the elimination of a common subexpression is beneficial proves to be quite difficult. We developed and implemented the transformation for the language Haskell by extending the Glasgow Haskell compiler. On real-world programs the transformation showed nearly no effect. The reason is that common subexpressions whose elimination could speed up programs are uncommon in lazy functional languages.},
  copyright = {\textcopyright{}1998 Springer-Verlag},
  isbn = {978-3-540-64849-9 978-3-540-68528-9},
  keywords = {Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {1467},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Chlipala2015optimizing,
  title = {An {{Optimizing Compiler}} for a {{Purely Functional Web}}-Application {{Language}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Chlipala, Adam},
  year = {2015},
  pages = {10--21},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784741},
  abstract = {High-level scripting languages have become tremendously popular for development of dynamic Web applications. Many programmers appreciate the productivity benefits of automatic storage management, freedom from verbose type annotations, and so on. While it is often possible to improve performance substantially by rewriting an application in C or a similar language, very few programmers bother to do so, because of the consequences for human development effort. This paper describes a compiler that makes it possible to have most of the best of both worlds, coding Web applications in a high-level language but compiling to native code with performance comparable to handwritten C code. The source language is Ur/Web, a domain-specific, purely functional, statically typed language for the Web. Through a coordinated suite of relatively straightforward program analyses and algebraic optimizations, we transform Ur/Web programs into almost-idiomatic C code, with no garbage collection, little unnecessary memory allocation for intermediate values, etc. Our compiler is in production use for commercial Web sites supporting thousands of users, and microbenchmarks demonstrate very competitive performance versus mainstream tools.},
  isbn = {978-1-4503-3669-7},
  keywords = {pure functional programming,Web programming languages,whole-program optimization},
  series = {{{ICFP}} 2015}
}

@inproceedings{Choi2001Compiling,
  title = {Compiling {{Lazy Functional Programs Based}} on the {{Spineless Tagless G}}-Machine for the {{Java Virtual Machine}}},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Choi, Kwanghoon and Lim, Hyun-il and Han, Taisook},
  year = {2001},
  month = mar,
  pages = {92--107},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-44716-4_6},
  abstract = {A systematic method of compiling lazy functional programs based on the Spineless Tagless G-machine (STGM) is presented for the Java Virtual Machine (JVM). A new specification of the STGM, which consists of a compiler and a reduction machine, is presented; the compiler translates a program in the STG language, which is the source language for the STGM, into a program in an intermediate language called L-code, and our reduction machine reduces the L-code program into an answer. With our representation for the reduction machine by the Java language, an L-code program is translated into a Java program simulating the reduction machine.The translated Java programs also run at a reasonable execution speed. Our experiment shows that execution times of translated benchmarks are competitive compared with those in a traditional Haskell interpreter, Hugs, particularly when Glasgow Haskell compiler's STG-level optimizations are applied.},
  language = {en}
}

@inproceedings{Christiansen2016Elaborator,
  title = {Elaborator {{Reflection}}: {{Extending Idris}} in {{Idris}}},
  shorttitle = {Elaborator {{Reflection}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Christiansen, David and Brady, Edwin},
  year = {2016},
  pages = {284--297},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951932},
  abstract = {Many programming languages and proof assistants are defined by elaboration from a high-level language with a great deal of implicit information to a highly explicit core language. In many advanced languages, these elaboration facilities contain powerful tools for program construction, but these tools are rarely designed to be repurposed by users. We describe elaborator reflection, a paradigm for metaprogramming in which the elaboration machinery is made directly available to metaprograms, as well as a concrete realization of elaborator reflection in Idris, a functional language with full dependent types. We demonstrate the applicability of Idris's reflected elaboration framework to a number of realistic problems, we discuss the motivation for the specific features of its design, and we explore the broader meaning of elaborator reflection as it can relate to other languages.},
  isbn = {978-1-4503-4219-3},
  keywords = {dependent types,elaboration,Metaprogramming},
  series = {{{ICFP}} 2016}
}

@article{Church1940Formulation,
  title = {A {{Formulation}} of the {{Simple Theory}} of {{Types}}},
  author = {Church, Alonzo},
  year = {1940},
  volume = {5},
  pages = {56--68},
  issn = {0022-4812},
  doi = {10.2307/2266170},
  journal = {The Journal of Symbolic Logic},
  number = {2}
}

@incollection{Cicek2015refinement,
  title = {Refinement {{Types}} for {{Incremental Computational Complexity}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {{\c C}i{\c c}ek, Ezgi and Garg, Deepak and Acar, Umut},
  editor = {Vitek, Jan},
  year = {2015},
  month = apr,
  pages = {406--431},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {With recent advances, programs can be compiled to efficiently respond to incremental input changes. However, there is no language-level support for reasoning about the time complexity of incremental updates. Motivated by this gap, we present CostIt, a higher-order functional language with a lightweight refinement type system for proving asymptotic bounds on incremental computation time. Type refinements specify which parts of inputs and outputs may change, as well as dynamic stability, a measure of time required to propagate changes to a program's execution trace, given modified inputs. We prove our type system sound using a new step-indexed cost semantics for change propagation and demonstrate the precision and generality of our technique through examples.},
  copyright = {\textcopyright{}2015 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-662-46668-1 978-3-662-46669-8},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Data Mining and Knowledge Discovery,Database Management,Information Storage and Retrieval,Information Systems Applications (incl. Internet)},
  language = {en},
  number = {9032},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Cicek2016Type,
  title = {A {{Type Theory}} for {{Incremental Computational Complexity}} with {{Control Flow Changes}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {{\c C}i{\c c}ek, Ezgi and Paraskevopoulou, Zoe and Garg, Deepak},
  year = {2016},
  pages = {132--145},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951950},
  abstract = {Incremental computation aims to speed up re-runs of a program after its inputs have been modified slightly. It works by recording a trace of the program's first run and propagating changes through the trace in incremental runs, trying to re-use as much of the original trace as possible. The recent work CostIt is a type and effect system to establish the time complexity of incremental runs of a program, as a function of input changes. However, CostIt is limited in two ways. First, it prohibits input changes that influence control flow. This makes it impossible to type programs that, for instance, branch on inputs that may change. Second, the soundness of CostIt is proved relative to an abstract cost semantics, but it is unclear how the semantics can be realized.   In this paper, we address both these limitations. We present DuCostIt, a re-design of CostIt, that combines reasoning about costs of change propagation and costs of from-scratch evaluation. The latter lifts the restriction on control flow changes. To obtain the type system, we refine Flow Caml, a type system for information flow analysis, with cost effects. Additionally, we inherit from CostIt index refinements to track data structure sizes and a co-monadic type. Using a combination of binary and unary step-indexed logical relations, we prove DuCostIt's cost analysis sound relative to not only an abstract cost semantics, but also a concrete semantics, which is obtained by translation to an ML-like language.},
  isbn = {978-1-4503-4219-3},
  keywords = {complexity analysis,incremental computation,type and effect systems},
  series = {{{ICFP}} 2016}
}

@inproceedings{Cicek2017Relational,
  title = {Relational {{Cost Analysis}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {{\c C}i{\c c}ek, Ezgi and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Hoffmann, Jan},
  year = {2017},
  pages = {316--329},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009858},
  abstract = {Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis, compiler optimizations, security and privacy. Techniques based on program analysis, type systems and abstract interpretation are well-studied, but methods for analyzing how the execution costs of two programs compare to each other have not received attention. Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs. In this work, we propose a relational cost analysis technique that is capable of establishing precise bounds on the difference in the execution cost of two programs by making use of relational properties of programs and inputs. We develop , a refinement type and effect system for a higher-order functional language with recursion and subtyping. The key novelty of our technique is the combination of relational refinements with two modes of typing{\^a}relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs. This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach. We prove our type system sound using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs. We demonstrate the precision and generality of our technique through examples.},
  isbn = {978-1-4503-4660-3},
  keywords = {complexity analysis,Relational reasoning,type and effect systems},
  series = {{{POPL}} 2017}
}

@inproceedings{Clement1986simple,
  title = {A {{Simple Applicative Language}}: {{Mini}}-{{ML}}},
  shorttitle = {A {{Simple Applicative Language}}},
  booktitle = {Proceedings of the 1986 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Cl{\'e}ment, Dominique and Despeyroux, Thierry and Kahn, Gilles and Despeyroux, Jo{\"e}lle},
  year = {1986},
  pages = {13--27},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/319838.319847},
  isbn = {0-89791-200-4},
  series = {{{LFP}} '86}
}

@inproceedings{Clifford2014allocation,
  title = {Allocation {{Folding Based}} on {{Dominance}}},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Memory Management}}},
  author = {Clifford, Daniel and Payer, Hannes and Starzinger, Michael and Titzer, Ben L.},
  year = {2014},
  pages = {15--24},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2602988.2602994},
  abstract = {Memory management system performance is of increasing importance in today's managed languages. Two lingering sources of overhead are the direct costs of memory allocations and write barriers. This paper introduces it allocation folding, an optimization technique where the virtual machine automatically folds multiple memory allocation operations in optimized code together into a single, larger it allocation group. An allocation group comprises multiple objects and requires just a single bounds check in a bump-pointer style allocation, rather than a check for each individual object. More importantly, all objects allocated in a single allocation group are guaranteed to be contiguous after allocation and thus exist in the same generation, which makes it possible to statically remove write barriers for reference stores involving objects in the same allocation group. Unlike object inlining, object fusing, and object colocation, allocation folding requires no special connectivity or ownership relation between the objects in an allocation group. We present our analysis algorithm to determine when it is safe to fold allocations together and discuss our implementation in V8, an open-source, production JavaScript virtual machine. We present performance results for the Octane and Kraken benchmark suites and show that allocation folding is a strong performance improvement, even in the presence of some heap fragmentation. Additionally, we use four hand-selected benchmarks JPEGEncoder, NBody, Soft3D, and Textwriter where allocation folding has a large impact.},
  isbn = {978-1-4503-2921-7},
  keywords = {dynamic optimization,garbage collection,javascript,memory managment,write barriers},
  series = {{{ISMM}} '14}
}

@inproceedings{Clifton2000multijava,
  title = {{{MultiJava}}: {{Modular Open Classes}} and {{Symmetric Multiple Dispatch}} for {{Java}}},
  shorttitle = {{{MultiJava}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Clifton, Curtis and Leavens, Gary T. and Chambers, Craig and Millstein, Todd},
  year = {2000},
  pages = {130--145},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/353171.353181},
  abstract = {We present MultiJava, a backward-compatible extension to Java supporting open classes and symmetric multiple dispatch. Open classes allow one to add to the set of methods that an existing class supports without creating distinct subclasses or editing existing code. Unlike the "Visitor" design pattern, open classes do not require advance planning, and open classes preserve the ability to add new subclasses modularly and safely. Multiple dispatch offers several well-known advantages over the single dispatching of conventional object-oriented languages, including a simple solution to some kinds of "binary method" problems. MultiJava's multiple dispatch retains Java's existing class-based encapsulation properties. We adapt previous theoretical work to allow compilation units to be statically typechecked modularly and safely, ruling out any link-time or run-time type errors. We also present a n compilation scheme that operates modularly and incurs performance overhead only where open classes or multiple dispatching are actually used.},
  isbn = {1-58113-200-X},
  series = {{{OOPSLA}} '00}
}

@inproceedings{Clinger1988implementation,
  title = {Implementation {{Strategies}} for {{Continuations}}},
  booktitle = {Proceedings of the 1988 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Clinger, Will and Hartheimer, Anne and Ost, Eric},
  year = {1988},
  pages = {124--131},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/62678.62692},
  abstract = {Scheme and Smalltalk continuations may have unlimited extent. This means that a purely stack-based implementation of continuations, as suffices for most languages, is inadequate. Several implementation strategies have been described in the literature. Determining which is best requires knowledge of the kinds of programs that will commonly be run.
Danvy, for example, has conjectured that continuation captures occur in clusters. That is, the same continuation, once captured, is likely to be captured again. As evidence, Danvy cited the use of continuations in a research setting. We report that Danvy's conjecture is somewhat true in the commercial setting of MacScheme+Toolsmith\texttrademark, which provides tools for developing Macintosh user interfaces in Scheme. These include an interrupt-driven event system and multitasking, both implemented by liberal use of continuations.
We describe several implementation strategies for continuations and compare four of them using benchmarks. We conclude that the most popular strategy may have a slight edge when continuations are not used at all, but that other strategies perform better when continuations are used and Danvy's conjecture holds.},
  isbn = {0-89791-273-X},
  series = {{{LFP}} '88}
}

@inproceedings{Clinger1991macros,
  title = {Macros {{That Work}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Clinger, William and Rees, Jonathan},
  year = {1991},
  pages = {155--162},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/99583.99607},
  isbn = {0-89791-419-8},
  series = {{{POPL}} '91}
}

@article{Clinger1999implementation,
  title = {Implementation {{Strategies}} for {{First}}-{{Class Continuations}}},
  author = {Clinger, William D. and Hartheimer, Anne H. and Ost, Eric M.},
  year = {1999},
  month = apr,
  volume = {12},
  pages = {7--45},
  issn = {1388-3690, 1573-0557},
  doi = {10.1023/A:1010016816429},
  abstract = {Scheme and Smalltalk continuations may have unlimited extent. This means that a purely stack-based implementation of continuations, as suffices for most languages, is inadequate. We review several implementation strategies for continuations and compare their performance using instruction counts for the normal case and continuation-intensive synthetic benchmarks for other scenarios, including coroutines and multitasking. All of the strategies constrain a compiler in some way, resulting in indirect costs that are hard to measure directly. We use related measurements on a set of benchmarks to calculate upper bounds for these indirect costs.},
  journal = {Higher-Order and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),continuations,coroutines,heap allocation,multitasking,Numeric Computing,Programming Languages; Compilers; Interpreters,Scheme,Smalltalk,Software Engineering/Programming and Operating Systems,stacks},
  language = {en},
  number = {1}
}

@inproceedings{Clouston2015Programming,
  title = {Programming and {{Reasoning}} with {{Guarded Recursion}} for {{Coinductive Types}}},
  booktitle = {Foundations of {{Software Science}} and {{Computation Structures}}},
  author = {Clouston, Ranald and Bizjak, Ale{\v s} and Grathwohl, Hans Bugge and Birkedal, Lars},
  editor = {Pitts, Andrew},
  year = {2015},
  pages = {407--421},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-46678-0_26},
  abstract = {We present the guarded lambda-calculus, an extension of the simply typed lambda-calculus with guarded recursive and coinductive types. The use of guarded recursive types ensures the productivity of well-typed programs. Guarded recursive types may be transformed into coinductive types by a type-former inspired by modal logic and Atkey-McBride clock quantification, allowing the typing of acausal functions. We give a call-by-name operational semantics for the calculus, and define adequate denotational semantics in the topos of trees. The adequacy proof entails that the evaluation of a program always terminates. We demonstrate the expressiveness of the calculus by showing the definability of solutions to Rutten's behavioural differential equations. We introduce a program logic with L{\"o}b induction for reasoning about the contextual equivalence of programs.},
  isbn = {978-3-662-46678-0},
  keywords = {Denotational Semantic,Natural Deduction,Operational Semantic,Program Logic,Reduction Rule},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Cockx2014pattern,
  title = {Pattern {{Matching Without K}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank},
  year = {2014},
  pages = {257--268},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628139},
  abstract = {Dependent pattern matching is an intuitive way to write programs and proofs in dependently typed languages. It is reminiscent of both pattern matching in functional languages and case analysis in on-paper mathematics. However, in general it is incompatible with new type theories such as homotopy type theory (HoTT). As a consequence, proofs in such theories are typically harder to write and to understand. The source of this incompatibility is the reliance of dependent pattern matching on the so-called K axiom - also known as the uniqueness of identity proofs - which is inadmissible in HoTT. The Agda language supports an experimental criterion to detect definitions by pattern matching that make use of the K axiom, but so far it lacked a formal correctness proof. In this paper, we propose a new criterion for dependent pattern matching without K, and prove it correct by a translation to eliminators in the style of Goguen et al. (2006). Our criterion both allows more good definitions than existing proposals, and solves a previously undetected problem in the criterion offered by Agda. It has been implemented in Agda and is the first to be supported by a formal proof. Thus it brings the benefits of dependent pattern matching to contexts where we cannot assume K, such as HoTT. It also points the way to new forms of dependent pattern matching, for example on higher inductive types.},
  isbn = {978-1-4503-2873-9},
  keywords = {Agda,dependent pattern matching,homotopy type theory,k axiom},
  series = {{{ICFP}} '14}
}

@inproceedings{Cohen2018Cubical,
  title = {Cubical {{Type Theory}}: {{A Constructive Interpretation}} of the {{Univalence Axiom}}},
  shorttitle = {Cubical {{Type Theory}}},
  booktitle = {21st {{International Conference}} on {{Types}} for {{Proofs}} and {{Programs}} ({{TYPES}} 2015)},
  author = {Cohen, Cyril and Coquand, Thierry and Huber, Simon and M{\"o}rtberg, Anders},
  editor = {Uustalu, Tarmo},
  year = {2018},
  volume = {69},
  pages = {5:1--5:34},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.TYPES.2015.5},
  isbn = {978-3-95977-030-9},
  keywords = {cubical sets,dependent type theory,univalence axiom},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@article{Compagnoni2003Typed,
  title = {Typed Operational Semantics for Higher-Order Subtyping},
  author = {Compagnoni, Adriana and Goguen, Healfdene},
  year = {2003},
  month = aug,
  volume = {184},
  pages = {242--297},
  issn = {0890-5401},
  doi = {10.1016/S0890-5401(03)00062-2},
  abstract = {Bounded operator abstraction is a language construct relevant to object oriented programming languages and to ML2000, the successor to Standard ML. In this paper, we introduce F{$\omega\leqslant$}, a variant of F{$<$}:{$\omega$} with this feature and with Cardelli and Wegner's kernel Fun rule for quantifiers. We define a typed-operational semantics with subtyping and prove that it is equivalent with F{$\omega\leqslant$}, using logical relations to prove soundness. The typed-operational semantics provides a powerful and uniform technique to study metatheoretic properties of F{$\omega\leqslant$}, such as Church\textendash{}Rosser, subject reduction, the admissibility of structural rules, and the equivalence with the algorithmic presentation of the system that performs weak-head reductions. Furthermore, we can show decidability of subtyping using the typed-operational semantics and its equivalence with the usual presentation. Hence, this paper demonstrates for the first time that logical relations can be used to show decidability of subtyping.},
  journal = {Information and Computation},
  keywords = {Dependent kinds,Lambda calculus,Subtyping,Type theory,Typed-operational semantics},
  number = {2}
}

@inproceedings{Consel1990binding,
  title = {Binding {{Time Analysis}} for {{High Order Untyped Functional Languages}}},
  booktitle = {Proceedings of the 1990 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Consel, Charles},
  year = {1990},
  pages = {264--272},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/91556.91668},
  abstract = {When some inputs of a program are known at compile-time, certain expressions can be processed statically; this is the basis of the notion of partial evaluation. Identifying these early computations can be determined independently of the actual values of the input by a static analysis called binding time analysis. Then, to process a program, one simply follows the binding time information: evaluate compile-time expressions and defer the others to run-time.
Using abstract interpretation, we present a binding time analysis for an untyped functional language which provides an effective treatment of both higher order functions and data structures. To our knowledge it is the first such analysis. It has been implemented and is used in a partial evaluator for a side-effect free dialect of Scheme. The analysis is general enough, however, to be valid for non-strict typed functional languages such as Haskell. Our approach and the system we have developed solve and go beyond the open problem of partially evaluating higher order functions described in [3] since we also provide a method to handle data structures.
Our analysis improves on previous work [5, 15, 4] in that: (1) it treats both higher order functions and data structures, (2) it does not impose syntactic restrictions on the program being processed, and (3) it does not require a preliminary phase to collect the set of possible functions that may occur at each site of application.},
  isbn = {0-89791-368-X},
  series = {{{LFP}} '90}
}

@inproceedings{Consel1993tutorial,
  title = {Tutorial {{Notes}} on {{Partial Evaluation}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Consel, Charles and Danvy, Olivier},
  year = {1993},
  pages = {493--501},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/158511.158707},
  abstract = {The last years have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes survey the field and present a critical assessment of the state of the art.},
  isbn = {0-89791-560-7},
  series = {{{POPL}} '93}
}

@inproceedings{Cook1989denotational,
  title = {A {{Denotational Semantics}} of {{Inheritance}} and {{Its Correctness}}},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}}},
  author = {Cook, W. and Palsberg, J.},
  year = {1989},
  pages = {433--443},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/74877.74922},
  abstract = {This paper presents a denotational model of inheritance. The model is based on an intuitive motivation of the purpose of inheritance. The correctness of the model is demonstrated by proving it equivalent to an operational semantics of inheritance based upon the method-lookup algorithm of object-oriented languages. Although it was originally developed to explain inheritance in object-oriented languages, the model shows that inheritance is a general mechanism that may be applied to any form of recursive definition.},
  isbn = {0-89791-333-7},
  series = {{{OOPSLA}} '89}
}

@inproceedings{Cook1990inheritance,
  title = {Inheritance Is {{Not Subtyping}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cook, William R. and Hill, Walter and Canning, Peter S.},
  year = {1990},
  pages = {125--135},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/96709.96721},
  abstract = {In typed object-oriented languages the subtype relation is typically based on the inheritance hierarchy. This approach, however, leads either to insecure type-systems or to restrictions on inheritance that make it less flexible than untyped Smalltalk inheritance. We present a new typed model of inheritance that allows more of the flexibility of Smalltalk inheritance within a statically-typed system. Significant features of our analysis are the introduction of polymorphism into the typing of inheritance and the uniform application of inheritance to objects, classes and types. The resulting notion of type inheritance allows us to show that the type of an inherited object is an inherited type but not always a subtype.},
  isbn = {0-89791-343-4},
  series = {{{POPL}} '90}
}

@inproceedings{Cook1992interfaces,
  title = {Interfaces and {{Specifications}} for the {{Smalltalk}}-80 {{Collection Classes}}},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  author = {Cook, William R.},
  year = {1992},
  pages = {1--15},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/141936.141938},
  isbn = {0-201-53372-3},
  series = {{{OOPSLA}} '92}
}

@techreport{Coquand1986analysis,
  title = {An Analysis of {{Girard}}'s Paradox},
  author = {Coquand, T.},
  year = {1986},
  month = may,
  institution = {{INRIA}},
  language = {en},
  type = {Report}
}

@article{Coquand1988calculus,
  title = {The Calculus of Constructions},
  author = {Coquand, Thierry and Huet, G{\'e}rard},
  year = {1988},
  month = feb,
  volume = {76},
  pages = {95--120},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(88)90005-3},
  journal = {Information and Computation},
  number = {2}
}

@inproceedings{Coquand1988Inductively,
  title = {Inductively Defined Types},
  booktitle = {{{COLOG}}-88},
  author = {Coquand, Thierry and Paulin, Christine},
  year = {1988},
  month = dec,
  pages = {50--66},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-52335-9_47},
  isbn = {978-3-540-52335-2 978-3-540-46963-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Coquand1990Proof,
  title = {A {{Proof}} of {{Strong Normalization}} for the {{Theory}} of {{Constructions Using}} a {{Kripke}}-{{Like Interpretation}}},
  author = {Coquand, Thierry and Gallier, Jean},
  year = {1990},
  month = jul,
  journal = {Technical Reports (CIS)}
}

@article{Coquand1992paradox,
  title = {The Paradox of Trees in Type Theory},
  author = {Coquand, Thierry},
  year = {1992},
  month = mar,
  volume = {32},
  pages = {10--14},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01995104},
  abstract = {We show how to represent a paradox similar to Russell's paradox in Type Theory withW-types and a type of all types, and how to use this in order to represent a fixed-point operator in such a theory. It is still open whether such a construction is possible without theW-type.},
  journal = {BIT Numerical Mathematics},
  language = {en},
  number = {1}
}

@incollection{Coquand1995new,
  title = {A New Paradox in Type Theory},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  author = {Coquand, Thierry},
  editor = {Prawitz, Dag and Skyrms, Brian and Westerst{\aa}hl, Dag},
  year = {1995},
  month = jan,
  volume = {134},
  pages = {555--570},
  publisher = {{Elsevier}},
  doi = {10.1016/S0049-237X(06)80062-5},
  abstract = {This chapter presents a new paradox in type theory that is a type-theoretic refinement of Reynolds' result that there is no set-theoretic model of polymorphism. One application of this paradox, which shows unexpected connections between the principles of excluded middle and the axiom of description in impredicative type theories, is discussed in the chapter. The lambda terms will always be considered up to {$\beta$}-conversion. The types of minimal higher order logic consist of one basic type ``o'' and function types of the form {$\alpha$} \textrightarrow{} {$\beta$}. The chapter also defines other logical connectives. The original logic of Church was formulated for classical logic and had a ground type of individuals. Yet another difference was the introduction of a description operator. It is possible to interpret classical higher order propositional logic in minimal higher order logic. Minimal higher order logic has a direct set-theoretic semantics. Second-order lambda calculus is also introduced in the chapter. One motivation is to provide syntax for polymorphic procedure.},
  series = {Logic, {{Methodology}} and {{Philosophy}} of {{Science IX}}}
}

@article{Coquand2002Formalised,
  title = {A {{Formalised Proof}} of the {{Soundness}} and {{Completeness}} of a {{Simply Typed Lambda}}-{{Calculus}} with {{Explicit Substitutions}}},
  author = {Coquand, Catarina},
  year = {2002},
  month = mar,
  volume = {15},
  pages = {57--90},
  issn = {1388-3690, 1573-0557},
  doi = {10.1023/A:1019964114625},
  abstract = {We present a simply-typed {$\lambda$}-calculus with explicit substitutions and we give a fully formalised proof of its soundness and completeness with respect to Kripke models. We further give conversion rules for the calculus and show also for them that they are sound and complete with respect to extensional equality in the Kripke model. A decision algorithm for conversion is given and proven correct. We use the technique ``normalisation by evaluation'' in order to prove these results. An important aspect of this work is that it is not a formalisation of an existing proof, instead the proof has been done in interaction with the proof system, ALF.},
  journal = {Higher-Order and Symbolic Computation},
  language = {en},
  number = {1}
}

@article{Coquand2013isomorphism,
  title = {Isomorphism Is Equality},
  author = {Coquand, Thierry and Danielsson, Nils Anders},
  year = {2013},
  month = nov,
  volume = {24},
  pages = {1105--1120},
  issn = {0019-3577},
  doi = {10.1016/j.indag.2013.09.002},
  abstract = {The setting of this work is dependent type theory extended with the univalence axiom. We prove that, for a large class of algebraic structures, isomorphic instances of a structure are equal\textemdash{}in fact, isomorphism is in bijective correspondence with equality. The class of structures includes monoids whose underlying types are ``sets'', and also posets where the underlying types are sets and the ordering relations are pointwise ``propositional''. For monoids on sets equality coincides with the usual notion of isomorphism from universal algebra, and for posets of the kind mentioned above equality coincides with order isomorphism.},
  journal = {Indagationes Mathematicae},
  keywords = {Dependent type theory,proof assistants,Univalence},
  number = {4},
  series = {In Memory of {{N}}.{{G}}. ({{Dick}}) de {{Bruijn}} (1918\textendash{}2012)}
}

@misc{coursera2013Brief,
  title = {A {{Brief History}} of {{Humankind}} with {{Dr}}. {{Yuval Noah Harari}}},
  year = {2013},
  month = jan,
  abstract = {The course A Brief History of Humankind by Dr. Yuval Noah Harari from Hebrew University of Jerusalem will be offered free of charge to everyone on the Coursera platform. Sign up at http://www.coursera.org/course/humankind.},
  collaborator = {{coursera}}
}

@inproceedings{Cousineau2007embedding,
  title = {Embedding {{Pure Type Systems}} in the {{Lambda}}-Pi-Calculus {{Modulo}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Typed Lambda Calculi}} and {{Applications}}},
  author = {Cousineau, Denis and Dowek, Gilles},
  year = {2007},
  pages = {102--117},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  abstract = {The lambda-Pi-calculus allows to express proofs of minimal predicate logic. It can be extended, in a very simple way, by adding computation rules. This leads to the lambda-Pi-calculus modulo. We show in this paper that this simple extension is surprisingly expressive and, in particular, that all functional Pure Type Systems, such as the system F, or the Calculus of Constructions, can be embedded in it. And, moreover, that this embedding is conservative under termination hypothesis.},
  isbn = {978-3-540-73227-3},
  series = {{{TLCA}}'07}
}

@inproceedings{Crafa2015Chemical,
  title = {The {{Chemical Approach}} to {{Typestate}}-Oriented {{Programming}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Crafa, Silvia and Padovani, Luca},
  year = {2015},
  pages = {917--934},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814270.2814287},
  abstract = {We study a novel approach to typestate-oriented programming based on the chemical metaphor: state and operations on objects are molecules of messages and state transformations are chemical reactions. This approach allows us to investigate typestate in an inherently concurrent setting, whereby objects can be accessed and modified concurrently by several processes, each potentially changing only part of their state. We introduce a simple behavioral type theory to express in a uniform way both the private and the public interfaces of objects, to describe and enforce structured object protocols consisting of possibilities, prohibitions, and obligations, and to control object sharing.},
  isbn = {978-1-4503-3689-5},
  keywords = {behavioral types,concurrency,join calculus,Typestate},
  series = {{{OOPSLA}} 2015}
}

@inproceedings{Crary1998intensional,
  title = {Intensional Polymorphism in Type-Erasure Semantics},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Crary, Karl and Weirich, Stephanie and Morrisett, Greg},
  year = {1998},
  pages = {301--312},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289459},
  abstract = {Intensional polymorphism, the ability to dispatch to different routines based on types at run time, enables a variety of advanced implementation techniques for polymorphic languages, including tag-free garbage collection, unboxed function arguments, polymorphic marshalling, and flattened data structures. To date, languages that support intensional polymorphism have required a type-passing (as opposed to type-erasure) interpretation where types are constructed and passed to polymorphic functions at run time. Unfortunately, type-passing suffers from a number of drawbacks: it requires duplication of constructs at the term and type levels, it prevents abstraction, and it severely complicates polymorphic closure conversion.We present a type-theoretic framework that supports intensional polymorphism, but avoids many of the disadvantages of type passing. In our approach, run-time type information is represented by ordinary terms. This avoids the duplication problem, allows us to recover abstraction, and avoids complications with closure conversion. In addition, our type system provides another improvement in expressiveness; it allows unknown types to be refined in place thereby avoiding certain beta-expansions required by other frameworks.},
  isbn = {1-58113-024-4},
  series = {{{ICFP}} '98}
}

@inproceedings{Crary1999simple,
  title = {A Simple Proof Technique for Certain Parametricity Results},
  booktitle = {Proceedings of the {{Fourth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Crary, Karl},
  year = {1999},
  pages = {82--89},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/317636.317787},
  abstract = {Many properties of parametric, polymorphic functions can be determined simply by inspection of their types. Such results are usually proven using Reynolds's parametricity theorem. However, Reynolds's theorem can be difficult to show in some settings, particularly ones involving computational effects. I present an alternative technique for proving some parametricity results. This technique is considerably simpler and easily generalizes to effectful settings. It works by instantiating polymorphic functions with singleton types that fully specify the behavior of the functions. Using this technique, I show that callers' stacks are protected from corruption during function calls in Typed Assembly Language programs.},
  isbn = {978-1-58113-111-6},
  series = {{{ICFP}} '99}
}

@inproceedings{Crary1999what,
  title = {What Is a Recursive Module?},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1999 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Crary, Karl and Harper, Robert and Puri, Sidd},
  year = {1999},
  pages = {50--63},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/301618.301641},
  abstract = {A hierarchical module system is an effective tool for structuring large programs. Strictly hierarchical module systems impose an acyclic ordering on import dependencies among program units. This can impede modular programming by forcing mutually-dependent components to be consolidated into a single module. Recently there have been several proposals for module systems that admit cyclic dependencies, but it is not clear how these proposals relate to one another, nor how one might integrate them into an expressive module system such as that of ML.To address this question we provide a type-theoretic analysis of the notion of a recursive module in the context of a "phase-distinction" formalism for higher-order module systems. We extend this calculus with a recursive module mechanism and a new form of signature, called a recursively dependent signature, to support the definition of recursive modules. These extensions are justified by an interpretation in terms of more primitive language constructs. This interpretation may also serve as a guide for implementation.},
  isbn = {1-58113-094-5},
  series = {{{PLDI}} '99}
}

@article{Crary2007syntactic,
  title = {Syntactic Logical Relations for Polymorphic and Recursive Types},
  author = {Crary, Karl and Harper, Robert},
  year = {2007},
  month = apr,
  volume = {172},
  pages = {259--299},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2007.02.010},
  abstract = {The method of logical relations assigns a relational interpretation to types that expresses operational invariants satisfied by all terms of a type. The method is widely used in the study of typed languages, for example to establish contextual equivalences of terms. The chief difficulty in using logical relations is to establish the existence of a suitable relational interpretation. We extend work of Pitts and Birkedal and Harper on constructing relational interpretations of types to polymorphism and recursive types, and apply it to establish parametricity and representation independence properties in a purely operational setting. We argue that, once the existence of a relational interpretation has been established, it is straightforward to use it to establish properties of interest.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {data abstraction,lambda calculus and related systems,logics of programs,operational semantics,Operational semantics,polymorphism,type structure,Type structure},
  series = {Computation, {{Meaning}}, and {{Logic}}: {{Articles}} Dedicated to {{Gordon Plotkin}}}
}

@inproceedings{Crary2009syntactic,
  title = {A Syntactic Account of Singleton Types via Hereditary Substitution},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{Logical Frameworks}} and {{Meta}}-{{Languages}}: {{Theory}} and {{Practice}}},
  author = {Crary, Karl},
  year = {2009},
  pages = {21--29},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1577824.1577829},
  abstract = {We give a syntactic proof of decidability and consistency of equivalence for the singleton type calculus, which lies at the foundation of modern module systems such as that of ML. Unlike existing proofs, which work by constructing a model, our syntactic proof makes few demands on the underlying proof theory and mathematical foundation. Consequently, it can be --- and has been --- entirely formulated in the Twelf meta-logic, and provides an important piece of a Twelf-formalized type-safety proof for Standard ML. The proof works by translation of the singleton type calculus into a canonical presentation, adapted from work on logical frameworks, in which equivalent terms are written identically. Canonical forms are not preserved under standard substitution, so we employ an alternative definition of substitution called hereditary substitution, which contracts redices that arise during substitution.},
  isbn = {978-1-60558-529-1},
  keywords = {_tablet,logical frameworks,mechanized metatheory,singleton types},
  series = {{{LFMTP}} '09}
}

@inproceedings{Crary2017Modules,
  title = {Modules, Abstraction, and Parametric Polymorphism},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Crary, Karl},
  year = {2017},
  pages = {100--113},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009892},
  abstract = {Reynolds's Abstraction theorem forms the mathematical foundation for data abstraction. His setting was the polymorphic lambda calculus. Today, many modern languages, such as the ML family, employ rich module systems designed to give more expressive support for data abstraction than the polymorphic lambda calculus, but analogues of the Abstraction theorem for such module systems have lagged far behind.   We give an account of the Abstraction theorem for a modern module calculus supporting generative and applicative functors, higher-order functors, sealing, and translucent signatures. The main issues to be overcome are: (1) the fact that modules combine both types and terms, so they must be treated as both simultaneously, (2) the effect discipline that models the distinction between transparent and opaque modules, and (3) a very rich language of type constructors supporting singleton kinds. We define logical equivalence for modules and show that it coincides with contextual equivalence. This substantiates the folk theorem that modules are good for data abstraction. All our proofs are formalized in Coq.},
  isbn = {978-1-4503-4660-3},
  keywords = {Abstraction,logical relations,modules,parametricity},
  series = {{{POPL}} 2017}
}

@incollection{Cremet2006core,
  title = {A {{Core Calculus}} for {{Scala Type Checking}}},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2006},
  author = {Cremet, Vincent and Garillot, Fran{\c c}ois and Lenglet, Sergue{\"i} and Odersky, Martin},
  editor = {Kr{\'a}lovi{\v c}, Rastislav and Urzyczyn, Pawe{\l}},
  year = {2006},
  month = jan,
  pages = {1--23},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present a minimal core calculus that captures interesting constructs of the Scala programming language: nested classes, abstract types, mixin composition, and path dependent types. We show that the problems of type assignment and subtyping in this calculus are decidable.},
  copyright = {\textcopyright{}2006 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-37791-7 978-3-540-37793-1},
  keywords = {Algorithm Analysis and Problem Complexity,Algorithms,Computation by Abstract Devices,Data Structures,Discrete Mathematics in Computer Science,Logics and Meanings of Programs},
  number = {4162},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Cretin2012Power,
  title = {On the {{Power}} of {{Coercion Abstraction}}},
  booktitle = {Proceedings of the 39th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Cretin, Julien and R{\'e}my, Didier},
  year = {2012},
  pages = {361--372},
  publisher = {{ACM}},
  address = {{Philadelphia, PA, USA}},
  doi = {10.1145/2103656.2103699},
  abstract = {Erasable coercions in System F-eta, also known as retyping functions, are well-typed eta-expansions of the identity. They may change the type of terms without changing their behavior and can thus be erased before reduction. Coercions in F-eta can model subtyping of known types and some displacement of quantifiers, but not subtyping assumptions nor certain forms of delayed type instantiation. We generalize F-eta by allowing abstraction over retyping functions. We follow a general approach where computing with coercions can be seen as computing in the lambda-calculus but keeping track of which parts of terms are coercions. We obtain a language where coercions do not contribute to the reduction but may block it and are thus not erasable. We recover erasable coercions by choosing a weak reduction strategy and restricting coercion abstraction to value-forms or by restricting abstraction to coercions that are polymorphic in their domain or codomain. The latter variant subsumes F-eta, F-sub, and MLF in a unified framework.},
  isbn = {978-1-4503-1083-3},
  keywords = {bounded polymorphism,coercion,conversion,f-eta,polymorphism,retyping functions,subtyping,system f,type,type containment},
  series = {{{POPL}} '12}
}

@article{Cretin2014Erasable,
  title = {Erasable Coercions: A Unified Approach to Type Systems},
  shorttitle = {Erasable Coercions},
  author = {Cretin, Julien},
  year = {2014},
  month = jan,
  abstract = {Functional programming languages, like OCaml or Haskell, rely on the lambda calculus for their core language. Although they have different reduction strategies and type system features, their proof of soundness and normalization (in the absence of recursion) should be factorizable. This thesis does such a factorization for theoretical type systems featuring recursive types, subtyping, bounded polymorphism, and constraint polymorphism. Interestingly, soundness and normalization for strong reduction imply soundness and normalization for all usual strategies. Our observation is that a generalization of existing coercions permits to describe all type system features stated above in an erasable and composable way. We illustrate this by proposing two concrete type systems: first, an explicit type system with a restricted form of coercion abstraction to express subtyping and bounded polymorphism; and an implicit type system with unrestricted coercion abstraction that generalizes the explicit type system with recursive types and constraint polymorphism---but without the subject reduction property. A side technical result is an adaptation of the step-indexed proof technique for type-soundness to calculi equipped with a strong notion of reduction.},
  language = {en}
}

@techreport{Cretin2014System,
  title = {System {{F}} with {{Coercion Constraints}}},
  author = {Cretin, Julien and R{\'e}my, Didier},
  year = {2014},
  month = jan,
  abstract = {We present a second-order lambda-calculus with coercion constraints that generalizes a previous extension of System F with parametric coercion abstractions by allowing multiple but simultaneous type and coercion abstractions, as well as recursive coercions and equi-recursive types. This allows to present in a uniform way several type system features that had previously been studied separately: type containment, bounded and instance-bounded polymorphism, which are already encodable with parametric coercion abstraction, and ML-style subtyping constraints. Our framework allows for a clear separation of language constructs with and without computational content. We also distinguish coherent coercions that are fully erasable from potentially incoherent coercions that suspend the evaluation---and enable the encoding of GADTs. Technically, type coercions that witness subtyping relations between types are replaced by a more expressive notion of typing coercions that witness subsumption relations between typings, e.g. pairs composed of a typing environment and a type. Our calculus is equipped with a strong notion of reduction that allows reduction under abstractions---but we also introduce a form of weak reduction as reduction cannot proceed under incoherent type abstractions. Type soundness is proved by adapting the step-indexed semantics technique to strong reduction strategies, moving indices inside terms so as to control the reduction steps internally.},
  language = {en},
  type = {Report}
}

@misc{CretinCoherent,
  title = {Coherent {{Coercion Abstraction}} with a Step-Indexed Strong-Reduction Semantics},
  author = {Cretin, Julien and R{\'e}my, Didier},
  abstract = {The usual notion of type coercions that witness subtyping relations between types is generalized to a more expressive notion of typing coercions that witness subsumption relations between typings, e.g. pairs composed of a typing environment and a type. This is more expressive and allows for a clearer separation of language constructs with and without computational content. This is illustrated on a second-order calculus of implicit coercions that allows multiple but simultaneous type and coercion abstractions and has recursive coercions and general recursive types. The calculus is equipped with a very liberal notion of reduction. It models a wide range of type features including type containment, bounded and instance-bounded polymorphism, as well as subtyping constraints as used for ML-style type inference with subtyping. Type soundness is proved by adapting the step-indexed semantics technique to strong reduction strategies, moving indices inside terms so as to control the reduction steps internally.}
}

@inproceedings{Curien2000duality,
  title = {The {{Duality}} of {{Computation}}},
  booktitle = {Proceedings of the {{Fifth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Curien, Pierre-Louis and Herbelin, Hugo},
  year = {2000},
  pages = {233--243},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/351240.351262},
  abstract = {We present the \&mu; -calculus, a syntax for \&lambda;-calculus + control operators exhibiting symmetries such as program/context and call-by-name/call-by-value. This calculus is derived from implicational Gentzen's sequent calculus LK, a key classical logical system in proof theory. Under the Curry-Howard correspondence between proofs and programs, we can see LK, or more precisely a formulation called LK\&mu; , as a syntax-directed system of simple types for \&mu; -calculus. For \&mu; -calculus, choosing a call-by-name or call-by-value discipline for reduction amounts to choosing one of the two possible symmetric orientations of a critical pair. Our analysis leads us to revisit the question of what is a natural syntax for call-by-value functional computation. We define a translation of \&lambda;\&mu;-calculus into \&mu; -calculus and two dual translations back to \&lambda;-calculus, and we recover known CPS translations by composing these translations.},
  isbn = {1-58113-202-6},
  series = {{{ICFP}} '00}
}

@article{Curien2007definability,
  title = {Definability and {{Full Abstraction}}},
  author = {Curien, Pierre-Louis},
  year = {2007},
  month = apr,
  volume = {172},
  pages = {301--310},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2007.02.011},
  abstract = {Game semantics has renewed denotational semantics. It offers among other things an attractive classification of programming features, and has brought a bunch of new definability results. In parallel, in the denotational semantics of proof theory, several full completeness results have been shown since the early nineties. In this note, we review the relation between definability and full abstraction, and we put a few old and recent results of this kind in perspective.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Denotational semantics,operational semantics,sequentiality},
  series = {Computation, {{Meaning}}, and {{Logic}}: {{Articles}} Dedicated to {{Gordon Plotkin}}}
}

@article{Curien2014revisiting,
  title = {Revisiting the Categorical Interpretation of Dependent Type Theory},
  author = {Curien, Pierre-Louis and Garner, Richard and Hofmann, Martin},
  year = {2014},
  month = aug,
  volume = {546},
  pages = {99--119},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2014.03.003},
  abstract = {We show that Hofmann's and Curien's interpretations of Martin-L{\"o}f's type theory, which were both designed to cure a mismatch between syntax and semantics in Seely's original interpretation in locally cartesian closed categories, are related via a natural isomorphism. As an outcome, we obtain a new proof of the coherence theorem needed to show the soundness after all of Seely's interpretation.},
  journal = {Theoretical Computer Science},
  keywords = {categorical semantics,Coherence,dependent types,Grothendieck fibrations,Locally cartesian closed categories,Monoidal categories,type theory},
  series = {Models of {{Interaction}}: {{Essays}} in {{Honour}} of {{Glynn Winskel}}}
}

@incollection{Czajka2014coinductive,
  title = {A {{Coinductive Confluence Proof}} for {{Infinitary Lambda}}-{{Calculus}}},
  booktitle = {Rewriting and {{Typed Lambda Calculi}}},
  author = {Czajka, {\L}ukasz},
  editor = {Dowek, Gilles},
  year = {2014},
  month = jul,
  pages = {164--178},
  publisher = {{Springer International Publishing}},
  abstract = {We give a coinductive proof of confluence, up to equivalence of root-active subterms, of infinitary lambda-calculus. We also show confluence of B{\"o}hm reduction (with respect to root-active terms) in infinitary lambda-calculus. In contrast to previous proofs, our proof makes heavy use of coinduction and does not employ the notion of descendants.},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-08917-1 978-3-319-08918-8},
  keywords = {Computing Methodologies,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Mathematics of Computing,Symbolic and Algebraic Manipulation},
  language = {en},
  number = {8560},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Danielsson2006fast,
  title = {Fast and {{Loose Reasoning}} Is {{Morally Correct}}},
  booktitle = {Conference {{Record}} of the 33rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Danielsson, Nils Anders and Hughes, John and Jansson, Patrik and Gibbons, Jeremy},
  year = {2006},
  pages = {206--217},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1111037.1111056},
  abstract = {Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.},
  isbn = {1-59593-027-2},
  keywords = {equational reasoning,inductive and coinductive types,lifted types,non-strict and strict languages,partial and infinite values,partial and total languages},
  series = {{{POPL}} '06}
}

@article{Danilewski2016Building,
  title = {Building {{Code}} with {{Dynamic Staging}}},
  author = {Danilewski, Piotr and Slusallek, Philipp},
  year = {2016},
  month = dec,
  abstract = {When creating a new domain-specific language (DSL) it is common to embed it as a part of a flexible host language, rather than creating it entirely from scratch. The semantics of an embedded DSL (EDSL) is either given directly as a set of functions (shallow embedding), or an AST is constructed that is later processed (deep embedding). Typically, the deep embedding is used when the EDSL specifies domain-specific optimizations (DSO) in a form of AST transformations. In this paper we show that deep embedding is not necessary to specify most optimizations. We define language semantics as action functions that are executed during parsing. These actions build incrementally a new, arbitrary complex program function. The EDSL designer is able to specify many aspects of the semantics as a runnable code, such as variable scoping rules, custom type checking, arbitrary control flow structures, or DSO. A sufficiently powerful staging mechanism helps assembling the code from different actions, as well as evaluate the semantics in arbitrarily many stages. In the end, we obtain code that is as efficient as one written by hand. We never create any object representation of the code. No external traversing algorithm is used to process the code. All program fragments are functions with their entire semantics embedded within the function bodies. This approach allows reusing the code between EDSL and the host language, as well as combining actions of many different EDSLs.},
  archivePrefix = {arXiv},
  eprint = {1612.01325},
  eprinttype = {arxiv},
  journal = {arXiv:1612.01325 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Danilewski2016ManyDSL,
  title = {{{ManyDSL}}: {{A Host}} for {{Many Languages}}},
  shorttitle = {{{ManyDSL}}},
  author = {Danilewski, Piotr and Slusallek, Philipp},
  year = {2016},
  month = dec,
  abstract = {Domain-specific languages are becoming increasingly important. Almost every application touches multiple domains. But how to define, use, and combine multiple DSLs within the same application? The most common approach is to split the project along the domain boundaries into multiple pieces and files. Each file is then compiled separately. Alternatively, multiple languages can be embedded in a flexible host language: within the same syntax a new domain semantic is provided. In this paper we follow a less explored route of metamorphic languages. These languages are able to modify their own syntax and semantics on the fly, thus becoming a more flexible host for DSLs. Our language allows for dynamic creation of grammars and switching languages where needed. We achieve this through a novel concept of Syntax-Directed Execution. A language grammar includes semantic actions that are pieces of functional code executed immediately during parsing. By avoiding additional intermediate representation, connecting actions from different languages and domains is greatly simplified. Still, actions can generate highly specialized code though lambda encapsulation and Dynamic Staging.},
  archivePrefix = {arXiv},
  eprint = {1612.03488},
  eprinttype = {arxiv},
  journal = {arXiv:1612.03488 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Danner2015denotational,
  title = {Denotational {{Cost Semantics}} for {{Functional Languages}} with {{Inductive Types}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Danner, Norman and Licata, Daniel R. and Ramyaa, Ramyaa},
  year = {2015},
  pages = {140--151},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784749},
  abstract = {A central method for analyzing the asymptotic complexity of a functional program is to extract and then solve a recurrence that expresses evaluation cost in terms of input size. The relevant notion of input size is often specific to a datatype, with measures including the length of a list, the maximum element in a list, and the height of a tree. In this work, we give a formal account of the extraction of cost and size recurrences from higher-order functional programs over inductive datatypes. Our approach allows a wide range of programmer-specified notions of size, and ensures that the extracted recurrences correctly predict evaluation cost. To extract a recurrence from a program, we first make costs explicit by applying a monadic translation from the source language to a complexity language, and then abstract datatype values as sizes. Size abstraction can be done semantically, working in models of the complexity language, or syntactically, by adding rules to a preorder judgement. We give several different models of the complexity language, which support different notions of size. Additionally, we prove by a logical relations argument that recurrences extracted by this process are upper bounds for evaluation cost; the proof is entirely syntactic and therefore applies to all of the models we consider.},
  isbn = {978-1-4503-3669-7},
  keywords = {analysis,complexity,Semi-automatic},
  series = {{{ICFP}} 2015}
}

@article{Danvy1992representing,
  title = {Representing {{Control}}: A {{Study}} of the {{CPS Transformation}}},
  shorttitle = {Representing {{Control}}},
  author = {Danvy, Oliver and Filinski, Andrzex},
  year = {1992},
  month = dec,
  volume = {2},
  pages = {361--391},
  issn = {1469-8072},
  doi = {10.1017/S0960129500001535},
  abstract = {This paper investigates the transformation of {$\lambda\nu$}-terms into continuation-passing style (CPS). We show that by appropriate {$\eta$}-expansion of Fisher and Plotkin's two-pass equational specification of the CPS transform, we can obtain a static and context-free separation of the result terms into ``essential'' and ``administrative'' constructs. Interpreting the former as syntax builders and the latter as directly executable code, We obtain a simple and efficient one-pass transformation algorithm, easily extended to conditional expressions, recursive definitions, and similar constructs. This new transformation algorithm leads to a simpler proof of Plotkin's simulation and indifference results.We go on to show how CPS-based control operators similar to, more general then, Scheme's call/cc can be naturally accommodated by the new transformation algorithm. To demonstrate the expressive power of these operators, we use them to present an equivalent but even more concise formulation of the efficient CPS transformation algorithm. Finally, we relate the fundamental ideas underlying this derivation to similar concepts from other work on program manipulation; we derive a one-pass CPS transformation of {$\lambda$}n-terms; and we outline some promising areas for future research.},
  journal = {Mathematical Structures in Computer Science},
  number = {04}
}

@inproceedings{Danvy1997lambdadropping,
  title = {Lambda-Dropping: {{Transforming Recursive Equations}} into {{Programs}} with {{Block Structure}}},
  shorttitle = {Lambda-Dropping},
  booktitle = {Proceedings of the 1997 {{ACM SIGPLAN Symposium}} on {{Partial Evaluation}} and {{Semantics}}-Based {{Program Manipulation}}},
  author = {Danvy, Olivier and Schultz, Ulrik P.},
  year = {1997},
  pages = {90--106},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/258993.259007},
  abstract = {Lambda-lifting a functional program transforms it into a set of recursive equations. We present the symmetric transformation: lambda-dropping. Lambda-dropping a set of recursive equations restores block structure and lexical scope.For lack of scope, recursive equations must carry around all the parameters that any of their callees might possibly need. Both lambda-lifting and lambda-dropping thus require one to compute a transitive closure over the call graph:\&bull; for lambda-lifting: to establish the Def/Use path of each free variable (these free variables are then added as parameters to each of the functions in the call path);\&bull; for lambda-dropping: to establish the Def/Use path of each parameter (parameters whose use occurs in the same scope as their definition do not need to be passed along in the call path).Without free variables, a program is scope-insensitive. Its blocks are then free to float (for lambda-lifting) or to sink (for lambda-dropping) along the vertices of the scope tree.We believe lambda-lifting and lambda-dropping are interesting per se, both in principle and in practice, but our prime application is partial evaluation: except for Malmkj\&aelig;r and \&Oslash;rb\&aelig;k's case study presented at PEPM '95, most polyvariant specializers for procedural programs operate on recursive equations. To this end, in a pre-processing phase, they lambda-lift source programs into recursive equations, As a result, residual programs are also expressed as recursive equations, often with dozens of parameters, which most compilers do not handle efficiently. Lambda-dropping in a post-processing phase restores their block structure and lexical scope thereby significantly reducing both the compile time and the run time of residual programs.},
  isbn = {0-89791-917-3},
  series = {{{PEPM}} '97}
}

@incollection{Danvy2002lambdalifting,
  title = {Lambda-{{Lifting}} in {{Quadratic Time}}},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Danvy, Olivier and Schultz, Ulrik P.},
  editor = {Hu, Zhenjiang and {Rodr{\'i}guez-Artalejo}, Mario},
  year = {2002},
  month = sep,
  pages = {134--151},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}2002 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-44233-2 978-3-540-45788-6},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {2441},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Danvy2005There,
  title = {There and Back Again},
  author = {Danvy, Olivier and Goldberg, Mayer},
  year = {2005},
  month = jan,
  volume = {66},
  pages = {397--413},
  issn = {0169-2968},
  abstract = {We present a programming pattern where a recursive function defined over a data structure traverses another data structure at return time. The idea is that the recursive calls get us 'there' by traversing the first data structure and the returns get us 'back again' while traversing the second data structure. We name this programming pattern of traversing a data structure at call time and another data structure at return time "There And Back Again" (TABA). The TABA pattern directly applies to computing symbolic convolutions and to multiplying polynomials. It also blends well with other programming patterns such as dynamic programming and traversing a list at double speed. We illustrate TABA and dynamic programming with Catalan numbers. We illustrate TABA and traversing a list at double speed with palindromes and we obtain a novel solution to this traditional exercise. Finally, through a variety of tree traversals, we show how to apply TABA to other data structures than lists. A TABA-based function written in direct style makes full use of an ALGOL-like control stack and needs no heap allocation. Conversely, in a TABA-based function written in continuation-passing style and recursively defined over a data structure (traversed at call time), the continuation acts as an iterator over a second data structure (traversed at return time). In general, the TABA pattern saves one from accumulating intermediate data structures at call time.},
  journal = {Fundam. Inf.},
  keywords = {continuations,defunctionalization,Recursive programming,TABA},
  number = {4}
}

@article{Dargaye2010verified,
  title = {A Verified Framework for Higher-Order Uncurrying Optimizations},
  author = {Dargaye, Zaynah and Leroy, Xavier},
  year = {2010},
  month = jan,
  volume = {22},
  pages = {199--231},
  issn = {1388-3690, 1573-0557},
  doi = {10.1007/s10990-010-9050-z},
  abstract = {Function uncurrying is an important optimization for the efficient execution of functional programming languages. This optimization replaces curried functions by uncurried, multiple-argument functions, while preserving the ability to evaluate partial applications. First-order uncurrying (where curried functions are optimized only in the static scopes of their definitions) is well understood and implemented by many compilers, but its extension to higher-order functions (where uncurrying can also be performed on parameters and results of higher-order functions) is challenging. This article develops a generic framework that expresses higher-order uncurrying optimizations as type-directed insertion of coercions, and prove its correctness. The proof uses step-indexed logical relations and was entirely mechanized using the Coq proof assistant.},
  journal = {Higher-Order and Symbolic Computation},
  language = {en},
  number = {3}
}

@article{Davies2001modal,
  title = {A {{Modal Analysis}} of {{Staged Computation}}},
  author = {Davies, Rowan and Pfenning, Frank},
  year = {2001},
  month = may,
  volume = {48},
  pages = {555--604},
  issn = {0004-5411},
  doi = {10.1145/382780.382785},
  abstract = {We show that a type system based on the intuitionistic modal logic S4  provides an expressive framework for specifying and analyzing computation stages in the context of typed \&lgr;-calculi and functional languages. We directly demonstrate the sense in which our   l\textrightarrow{$\square$}e -calculus captures staging, and also give a conservative embeddng of Nielson and Nielson's two-level functional language in our functional language Mini-ML  {$\square$} , thus proving  that binding-time correctness is equivalent to modal correctness on this fragment. In addition,   Mini-ML{$\square$}    can also express immediate evaluation and sharing of code across multiple stages, thus supporting run-time code generation as well as partial evaluation.},
  journal = {J. ACM},
  keywords = {binding times,run-time code generation,staged computation},
  number = {3}
}

@article{Davis2017Nobrainer,
  title = {No-Brainer {{CPS Conversion}} ({{Functional Pearl}})},
  author = {Davis, Milo and Meehan, William and Shivers, Olin},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {23:1--23:25},
  issn = {2475-1421},
  doi = {10.1145/3110267},
  abstract = {Algorithms that convert direct-style {\^I}\guillemotright{}-calculus terms to their equivalent terms in continuation-passing style (CPS) typically introduce so-called {\^a}administrative redexes:{\^a} useless artifacts of the conversion that must be cleaned up by a subsequent pass over the result to reduce them away. We present a simple, linear-time algorithm for CPS conversion that introduces no administrative redexes. In fact, the output term is a normal form in a reduction system that generalizes the notion of {\^a}administrative redexes{\^a} to what we call {\^a}no-brainer redexes,{\^a} that is, redexes whose reduction shrinks the size of the term. We state the theorems which establish the algorithm{\^a}s desireable properties, along with sketches of the full proofs.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {compiler,continuation,Continuation-passing Style,CPS,functional language,lambda calculus},
  number = {ICFP}
}

@article{Delaware2013Feature,
  title = {Feature Modularity in Mechanized Reasoning},
  author = {Delaware, Benjamin James},
  year = {2013},
  month = dec,
  abstract = {Complex systems are naturally understood as combinations of their
distinguishing characteristics or \textbackslash{}definit\{features\}. Distinct features
differentiate between variations of configurable systems and also
identify the novelties of extensions. The implementation of a
conceptual feature is often scattered throughout an artifact, forcing
designers to understand the entire artifact in order to reason about
the behavior of a single feature. It is particularly challenging to
independently develop novel extensions to complex systems as a
result.

This dissertation shows how to modularly reason about the
implementation of conceptual features in both the formalizations of
programming languages and object-oriented software product lines. In
both domains, modular verification of features can be leveraged to
reason about the behavior of artifacts in which they are included:
fully mechanized metatheory proofs for programming languages can be
synthesized from independently developed proofs, and programs built
from well-formed feature modules are guaranteed to be well-formed
without needing to be typechecked. Modular reasoning about individual
features can furthermore be used to efficiently reason about families
of languages and programs which share a common set of features.},
  language = {en\_US}
}

@inproceedings{Delaware2013metatheory,
  title = {Meta-Theory {\`a} {{La Carte}}},
  booktitle = {Proceedings of the 40th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Delaware, Benjamin and {d. S. Oliveira}, Bruno C. and Schrijvers, Tom},
  year = {2013},
  pages = {207--218},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2429069.2429094},
  abstract = {Formalizing meta-theory, or proofs about programming languages, in a proof assistant has many well-known benefits. Unfortunately, the considerable effort involved in mechanizing proofs has prevented it from becoming standard practice. This cost can be amortized by reusing as much of existing mechanized formalizations as possible when building a new language or extending an existing one. One important challenge in achieving reuse is that the inductive definitions and proofs used in these formalizations are closed to extension. This forces language designers to cut and paste existing definitions and proofs in an ad-hoc manner and to expend considerable effort to patch up the results. The key contribution of this paper is the development of an induction technique for extensible Church encodings using a novel reinterpretation of the universal property of folds. These encodings provide the foundation for a framework, formalized in Coq, which uses type classes to automate the composition of proofs from modular components. This framework enables a more structured approach to the reuse of meta-theory formalizations through the composition of modular inductive definitions and proofs. Several interesting language features, including binders and general recursion, illustrate the capabilities of our framework. We reuse these features to build fully mechanized definitions and proofs for a number of languages, including a version of mini-ML. Bounded induction enables proofs of properties for non-inductive semantic functions, and mediating type classes enable proof adaptation for more feature-rich languages.},
  isbn = {978-1-4503-1832-7},
  keywords = {coq,extensible church encodings,modular mechanized meta-theory},
  series = {{{POPL}} '13}
}

@inproceedings{Delaware2013modular,
  title = {Modular {{Monadic Meta}}-Theory},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Delaware, Benjamin and Keuchel, Steven and Schrijvers, Tom and Oliveira, Bruno C.d.S.},
  year = {2013},
  pages = {319--330},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500587},
  abstract = {This paper presents 3MT, a framework for modular mechanized meta-theory of languages with effects. Using 3MT, individual language features and their corresponding definitions -- semantic functions, theorem statements and proofs-- can be built separately and then reused to create different languages with fully mechanized meta-theory. 3MT combines modular datatypes and monads to define denotational semantics with effects on a per-feature basis, without fixing the particular set of effects or language constructs. One well-established problem with type soundness proofs for denotational semantics is that they are notoriously brittle with respect to the addition of new effects. The statement of type soundness for a language depends intimately on the effects it uses, making it particularly challenging to achieve modularity. 3MT solves this long-standing problem by splitting these theorems into two separate and reusable parts: a feature theorem that captures the well-typing of denotations produced by the semantic function of an individual feature with respect to only the effects used, and an effect theorem that adapts well-typings of denotations to a fixed superset of effects. The proof of type soundness for a particular language simply combines these theorems for its features and the combination of their effects. To establish both theorems, 3MT uses two key reasoning techniques: modular induction and algebraic laws about effects. Several effectful language features, including references and errors, illustrate the capabilities of 3MT. A case study reuses these features to build fully mechanized definitions and proofs for 28 languages, including several versions of mini-ML with effects.},
  isbn = {978-1-4503-2326-0},
  keywords = {mechanized meta-theory,modularity,Monads,side-effects},
  series = {{{ICFP}} '13}
}

@inproceedings{DellaToffola2015performance,
  title = {Performance {{Problems You Can Fix}}: {{A Dynamic Analysis}} of {{Memoization Opportunities}}},
  shorttitle = {Performance {{Problems You Can Fix}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Della Toffola, Luca and Pradel, Michael and Gross, Thomas R.},
  year = {2015},
  pages = {607--622},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814270.2814290},
  abstract = {Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.},
  isbn = {978-1-4503-3689-5},
  keywords = {caching,memoization,performance bugs,profiling},
  series = {{{OOPSLA}} 2015}
}

@article{Delling2016Rethinking,
  title = {Rethinking {{Experimental Methods}} in {{Computing}} ({{Dagstuhl Seminar}} 16111)},
  author = {Delling, Daniel and Demetrescu, Camil and Johnson, David S. and Vitek, Jan},
  editor = {Delling, Daniel and Demetrescu, Camil and Johnson, David S. and Vitek, Jan},
  year = {2016},
  volume = {6},
  pages = {24--43},
  issn = {2192-5283},
  doi = {10.4230/DagRep.6.3.24},
  journal = {Dagstuhl Reports},
  keywords = {Algorithms,Benchmarks,Data sets,Experiments,Repeatability,Reproducibility,Software Artifacts,Statistics},
  number = {3}
}

@inproceedings{Demers1995reflection,
  title = {Reflection in Logic, Functional and Object-Oriented Programming: A {{Short Comparative Study}}},
  shorttitle = {Reflection in Logic, Functional and Object-Oriented Programming},
  booktitle = {In {{IJCAI}} '95 {{Workshop}} on {{Reflection}} and {{Metalevel Architectures}} and Their {{Applications}} in {{AI}}},
  author = {Demers, Fran{\c c}ois-Nicola and Malenfant, Jacques},
  year = {1995},
  pages = {29--38},
  abstract = {Reflection is a wide-ranging concept that has been studied independently in many different areas of science in general, and computer science in particular. Even in the sub-area of programming languages, it has been applied to different paradigms, especially the logic, functional and objectoriented ones. Partly because of different past influences, but also because researchers in these communities scarcely talk to each others, concepts have evolved separately, sometimes to the point where it is hard for people in one community to recognize similarities in the work of others, not to speak about cross-fertilization among them. In this paper, we propose a synthesis covering mainly the application of computation reflection to programming languages. We compare the different approaches and try to identify similar concepts hidden behind different names or constructs. We also point out the different emphasis that has been given to different concepts in each of them. We do not claim neither comp...}
}

@article{Demetrescu2014reactive,
  title = {Reactive {{Imperative Programming}} with {{Dataflow Constraints}}},
  author = {Demetrescu, Camil and Finocchi, Irene and Ribichini, Andrea},
  year = {2014},
  month = nov,
  volume = {37},
  pages = {3:1--3:53},
  issn = {0164-0925},
  doi = {10.1145/2623200},
  abstract = {Dataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes in their environment. In this article, we show that one-way dataflow constraints, largely explored in the context of interactive applications, can be seamlessly integrated in any imperative language and can be used as a general paradigm for writing performance-critical reactive applications that require efficient incremental computations. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as ``reactive.'' Reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically re-executed every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend on the values of other variables. The constraint-solving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using off-the-shelf compilers. We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, data structure repair, and software visualization. The performance of our implementation is compared to problem-specific change propagation algorithms, as well as to language-centric approaches such as self-adjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {constraint solving,data structure repair,dataflow programming,imperative programming,incremental computation,observer design pattern,one-way dataflow constraints,reactive programming,software visualization},
  number = {1}
}

@article{Demetrescu2017Programming,
  title = {Programming {{Language Techniques}} for {{Incremental}} and {{Reactive Computing}} ({{Dagstuhl Seminar}} 16402)},
  author = {Demetrescu, Camil and Erdweg, Sebastian and Hammer, Matthew A. and Krishnamurthi, Shriram},
  editor = {Demetrescu, Camil and Erdweg, Sebastian and Hammer, Matthew A. and Krishnamurthi, Shriram},
  year = {2017},
  volume = {6},
  pages = {1--12},
  issn = {2192-5283},
  doi = {10.4230/DagRep.6.10.1},
  journal = {Dagstuhl Reports},
  keywords = {change propagation,dataflow programming,dynamic dependency graph,Incremental computing,live programming,memoization,reactive programming},
  number = {10}
}

@incollection{Devai2015edsls,
  title = {The {{EDSL}}'s {{Struggle}} for {{Their Sources}}},
  booktitle = {Central {{European Functional Programming School}}},
  author = {D{\'e}vai, Gergely and Lesk{\'o}, D{\'a}niel and Tejfel, M{\'a}t{\'e}},
  editor = {Zs{\'o}k, Vikt{\'o}ria and Horv{\'a}th, Zolt{\'a}n and Csat{\'o}, Lehel},
  year = {2015},
  pages = {300--335},
  publisher = {{Springer International Publishing}},
  abstract = {Embedded Domain Specific Languages make language design and implementation easier, because lexical and syntactical analysis and part of the semantic checks can be completed by the compiler of the host language. On the other hand, by the nature of embedding, EDSL compilers have to work with a syntax tree that stores no information about the source file processed and the location of the program entities within the source file. This makes it hard to produce user-friendly error messages and connect the generated target code with the source code for debugging and profiling purposes. This lecture note presents this problem in detail and shows possible solutions. The first, lightweight solution uses macro preprocessing. The second one is based on syntax tree transformations to add missing source-related information. This is more powerful, but also more heavyweight. The last technique avoids the problem by turning the embedded language implementation to a standalone one (with own parser) after the experimental phase of the language development process: It turns out that most of the embedded implementation can be reused in the standalone one.},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-15939-3 978-3-319-15940-9},
  keywords = {Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {8606},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{DeVito2013terra,
  title = {Terra: {{A Multi}}-Stage {{Language}} for {{High}}-Performance {{Computing}}},
  shorttitle = {Terra},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {DeVito, Zachary and Hegarty, James and Aiken, Alex and Hanrahan, Pat and Vitek, Jan},
  year = {2013},
  pages = {105--116},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2491956.2462166},
  abstract = {High-performance computing applications, such as auto-tuners and domain-specific languages, rely on generative programming techniques to achieve high performance and portability. However, these systems are often implemented in multiple disparate languages and perform code generation in a separate process from program execution, making certain optimizations difficult to engineer. We leverage a popular scripting language, Lua, to stage the execution of a novel low-level language, Terra. Users can implement optimizations in the high-level language, and use built-in constructs to generate and execute high-performance Terra code. To simplify meta-programming, Lua and Terra share the same lexical environment, but, to ensure performance, Terra code can execute independently of Lua's runtime. We evaluate our design by reimplementing existing multi-language systems entirely in Terra. Our Terra-based auto-tuner for BLAS routines performs within 20\% of ATLAS, and our DSL for stencil computations runs 2.3x faster than hand-written C.},
  isbn = {978-1-4503-2014-6},
  keywords = {DSL,lua,staged computation},
  series = {{{PLDI}} '13}
}

@inproceedings{DeVito2015design,
  title = {The {{Design}} of {{Terra}}: {{Harnessing}} the {{Best Features}} of {{High}}-{{Level}} and {{Low}}-{{Level Languages}}},
  booktitle = {1st {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2015)},
  author = {DeVito, Zachary and Hanrahan, Pat},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  year = {2015},
  volume = {32},
  pages = {79--89},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.79},
  abstract = {Applications are often written using a combination of high-level and low-level languages since it allows performance critical parts to be carefully optimized, while other parts can be written more productively. This approach is used in web development, game programming, and in build systems for applications themselves. However, most languages were not designed with interoperability in mind, resulting in glue code and duplicated features that add complexity. We propose a two-language system where both languages were designed to interoperate. Lua is used for our high-level language since it was originally designed with interoperability in mind. We create a new low-level language, Terra, that we designed to interoperate with Lua. It is embedded in Lua, and meta-programmed from it, but has a low level of abstraction suited for writing high-performance code. We discuss important design decisions - compartmentalized runtimes, glue-free interoperation, and meta-programming features - that enable Lua and Terra to be more powerful than the sum of their parts.},
  isbn = {978-3-939897-80-4},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-50186}
}

@inproceedings{deVries2014true,
  title = {True {{Sums}} of {{Products}}},
  booktitle = {Proceedings of the 10th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {{de Vries}, Edsko and L{\"o}h, Andres},
  year = {2014},
  pages = {83--94},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2633628.2633634},
  abstract = {We introduce the sum-of-products (SOP) view for datatype-generic programming (in Haskell). While many of the libraries that are commonly in use today represent datatypes as arbitrary combinations of binary sums and products, SOP reflects the structure of datatypes more faithfully: each datatype is a single n-ary sum, where each component of the sum is a single n-ary product. This representation turns out to be expressible accurately in GHC with today's extensions. The resulting list-like structure of datatypes allows for the definition of powerful high-level traversal combinators, which in turn encourage the definition of generic functions in a compositional and concise style. A major plus of the SOP view is that it allows to separate function-specific metadata from the main structural representation and recombining this information later.},
  isbn = {978-1-4503-3042-8},
  keywords = {Datatype-generic programming,generic views,json,lenses,metadata,sums of products,universes},
  series = {{{WGP}} '14}
}

@inproceedings{Devriese2013typed,
  title = {Typed {{Syntactic Meta}}-Programming},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Devriese, Dominique and Piessens, Frank},
  year = {2013},
  pages = {73--86},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500575},
  abstract = {We present a novel set of meta-programming primitives for use in a dependently-typed functional language. The types of our meta-programs provide strong and precise guarantees about their termination, correctness and completeness. Our system supports type-safe construction and analysis of terms, types and typing contexts. Unlike alternative approaches, they are written in the same style as normal programs and use the language's standard functional computational model. We formalise the new meta-programming primitives, implement them as an extension of Agda, and provide evidence of usefulness by means of two compelling applications in the fields of datatype-generic programming and proof tactics.},
  isbn = {978-1-4503-2326-0},
  keywords = {Datatype-generic programming,dependent types,meta-programming,tactics},
  series = {{{ICFP}} '13}
}

@inproceedings{Devriese2016Reasoning,
  title = {Reasoning about {{Object Capabilities}} with {{Logical Relations}} and {{Effect Parametricity}}},
  booktitle = {2016 {{IEEE European Symposium}} on {{Security}} and {{Privacy}} ({{EuroS P}})},
  author = {Devriese, D. and Birkedal, L. and Piessens, F.},
  year = {2016},
  month = mar,
  pages = {147--162},
  doi = {10.1109/EuroSP.2016.22},
  abstract = {Object capabilities are a technique for fine-grained privilege separation in programming languages and systems, with important applications in security. However, current formal characterisations do not fully capture capability-safety of a programming language and are not sufficient for verifying typical applications. Using state-of-the-art techniques from programming languages research, we define a logical relation for a core calculus of JavaScript that better characterises capability-safety. The relation is powerful enough to reason about typical capability patterns and supports evolvable invariants on shared data structures, capabilities with restricted authority over them and isolated components with restricted communication channels. We use a novel notion of effect parametricity for deriving properties about effects. Our results imply memory access bounds that have previously been used to characterise capability-safety.},
  keywords = {Calculus,capability-safety,Cognition,Computer languages,Data Structures,effect parametricity,Electronic mail,fine-grained privilege separation,high level languages,inference mechanisms,java,javascript,Logic programming,logical relations,object capabilities,programming languages,reasoning,security,security of data,Syntactics}
}

@article{Dezani-Ciancaglini2003infinitary,
  title = {Infinitary Lambda Calculus and Discrimination of {{Berarducci}} Trees},
  author = {{Dezani-Ciancaglini}, Mariangiola and Severi, Paula and {de Vries}, Fer-Jan},
  year = {2003},
  month = apr,
  volume = {298},
  pages = {275--302},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(02)00809-5},
  abstract = {We propose an extension of lambda calculus for which the Berarducci trees equality coincides with observational equivalence, when we observe rootstable or rootactive behavior of terms. In one direction the proof is an adaptation of the classical B{\"o}hm out technique. In the other direction the proof is based on confluence for strongly converging reductions in this extension.},
  journal = {Theoretical Computer Science},
  keywords = {Berarducci trees,Böhm out technique,Infinitary lambda calculus,Observational equivalence},
  number = {2},
  series = {Australasian {{Computing Theory}}}
}

@article{Diehl2018Generic,
  title = {Generic {{Zero}}-{{Cost Reuse}} for {{Dependent Types}}},
  author = {Diehl, Larry and Firsov, Denis and Stump, Aaron},
  year = {2018},
  month = mar,
  abstract = {Dependently typed languages are well known for having a problem with code reuse. Traditional non-indexed algebraic datatypes (e.g. lists) appear alongside a plethora of indexed variations (e.g. vectors). Functions are often rewritten for both non-indexed and indexed versions of essentially the same datatype, which is a source of code duplication. We work in a Curry-style dependent type theory, where the same untyped term may be classified as both the non-indexed and indexed versions of a datatype. Many solutions have been proposed for the problem of dependently typed reuse, but we exploit Curry-style type theory in our solution to not only reuse data and programs, but do so at zero-cost (without a runtime penalty). Our work is an exercise in dependently typed generic programming, and internalizes the process of zero-cost reuse as the identity function in a Curry-style theory.},
  archivePrefix = {arXiv},
  eprint = {1803.08150},
  eprinttype = {arxiv},
  journal = {arXiv:1803.08150 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Dietz1987two,
  title = {Two {{Algorithms}} for {{Maintaining Order}} in a {{List}}},
  booktitle = {Proceedings of the {{Nineteenth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Dietz, P. and Sleator, D.},
  year = {1987},
  pages = {365--372},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/28395.28434},
  abstract = {The order maintenance problem is that of maintaining a list under a sequence of Insert and Delete operations, while answering Order queries (determine which of two elements comes first in the list). We give two new algorithms for this problem. The first algorithm matches the O(1) amortized time per operation of the best previously known algorithm, and is much simpler. The second algorithm permits all operations to be performed in O(1) worst-case time.},
  isbn = {0-89791-221-7},
  series = {{{STOC}} '87}
}

@techreport{Dijkstra2008efficient,
  title = {Efficient {{Functional Unification}} and {{Substitution}}},
  author = {Dijkstra, Atze and Middelkoop, Arie and Swierstra, S. Doaitse},
  year = {2008},
  institution = {{Department of Information and Computing Sciences, Utrecht University}},
  number = {UU-CS-2008-027},
  pubcat = {techreport},
  urlpdf = {http://www.cs.uu.nl/research/techreps/repo/CS-2008/2008-027.pdf}
}

@inproceedings{Dockins2009Fresh,
  title = {A {{Fresh Look}} at {{Separation Algebras}} and {{Share Accounting}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Dockins, Robert and Hobor, Aquinas and Appel, Andrew W.},
  editor = {Hu, Zhenjiang},
  year = {2009},
  pages = {161--177},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Separation Algebras serve as models of Separation Logics; Share Accounting allows reasoning about concurrent-read/exclusive- write resources in Separation Logic. In designing a Concurrent Separation Logic and in mechanizing proofs of its soundness, we found previous axiomatizations of separation algebras and previous systems of share accounting to be useful but imperfect. We adjust the axioms of separation algebras; we demonstrate an operator calculus for constructing new separation algebras; we present a more powerful system of share accounting with a new, simple model; and we provide a reusable Coq development.},
  isbn = {978-3-642-10672-9},
  keywords = {Bounded Distributive Lattice,Lift Operator,Separation Logic,Share Model,Token Counting},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{Dockins2012Constructing,
  title = {Constructing {{Hereditary Worlds Within Worlds}}},
  author = {Dockins, Robert and Hobor, Aquinas},
  year = {2012},
  abstract = {Abstract. Although they appear unrelated, the type system of the polymorphic {$\lambda$}-calculus with references and the assertions of concurrent separation logic with first-class locks share a critical feature: an unsound contravariant circularity in their na{\"i}ve semantic model. We developed indirection theory to automatically construct, and cleanly axiomatize, step-indexed approximations to these na{\"i}ve models, as well as a large number of others [HDA10b]. Unfortunately, the previous axiomatization had a flaw. One is usually only interested in using hereditary predicates: those which are closed under the action of approximation. As previously presented, indirection theory allows nonhereditary predicates to exist in certain parts of the construction. Although not fatal, this flaw requires workarounds that are not entirely obvious to the uninitiated. We correct this flaw by presenting a new axiomatization of indirection theory that only permits heredity predicates and show that the new interface is sound by constructing a model. The new axiomatization is somewhat more subtle than the previous one, but it retains the same flavor, cleanliness, and metatheoretic properties. In contrast, the new construction is markedly more complex, especially in a mechanized context. Indeed, our Coq mechanization is one of our key contributions, and accordingly we present it in considerable detail. 1},
  keywords = {_tablet},
  note = {00000}
}

@article{Dodds2016Verifying,
  title = {Verifying Custom Synchronization Constructs Using Higher-Order Separation Logic},
  author = {Dodds, Mike and Jagannathan, Suresh and Parkinson, Matthew J. and Svendsen, Kasper and Birkedal, Lars},
  year = {2016},
  month = jan,
  volume = {38},
  pages = {4:1--4:72},
  issn = {0164-0925},
  doi = {10.1145/2818638},
  abstract = {Synchronization constructs lie at the heart of any reliable concurrent program. Many such constructs are standard (e.g., locks, queues, stacks, and hash-tables). However, many concurrent applications require custom synchronization constructs with special-purpose behavior. These constructs present a significant challenge for verification. Like standard constructs, they rely on subtle racy behavior, but unlike standard constructs, they may not have well-understood abstract interfaces. As they are custom built, such constructs are also far more likely to be unreliable. This article examines the formal specification and verification of custom synchronization constructs. Our target is a library of channels used in automated parallelization to enforce sequential behavior between program statements. Our high-level specification captures the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure sequential behavior. We connect the high-level specification with the low-level library implementation to prove that a client's requirements are satisfied. Significantly, we can reason about program and library correctness without breaking abstraction boundaries. To achieve this, we use a program logic called iCAP (impredicative Concurrent Abstract Predicates) based on separation logic. iCAP supports both high-level abstraction and low-level reasoning about races. We use this to show that our high-level channel specification abstracts three different, increasingly complex low-level implementations of the library. iCAP's support for higher-order reasoning lets us prove that sequential dependencies are respected, while iCAP's next-generation semantic model lets us avoid ugly problems with cyclic dependencies.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {concurrency,concurrent abstract predicates,Separation logic},
  number = {2}
}

@inproceedings{Dolan2017Polymorphism,
  title = {Polymorphism, {{Subtyping}}, and {{Type Inference}} in {{MLsub}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dolan, Stephen and Mycroft, Alan},
  year = {2017},
  pages = {60--72},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009882},
  abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs.   This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.},
  isbn = {978-1-4503-4660-3},
  keywords = {Algebra,polymorphism,subtyping,type inference},
  series = {{{POPL}} 2017}
}

@inproceedings{Dovier1991flogg,
  title = {Flogg: {{A Logic Programming Language}} with {{Finite Sets}}},
  shorttitle = {Flogg},
  booktitle = {Proc. 8th {{Int}}. {{Conf}}. on {{Logic Programming}}},
  author = {Dovier, Agostino and Omodeo, Eugenio G. and Pontelli, Enrico and Rossi, Gianfranco},
  year = {1991},
  pages = {111--124},
  publisher = {{MIT Press}},
  abstract = {An extended logic programming language embodying sets is developed in successive stages, introducing at each stage simple set dictions and operations, and discussing their operational as well as declarative semantics. First, by means of special set terms added to definite Horn Clause logic, one is enabled to define enumerated sets. A new unification algorithm which can cope with set terms is developed and proved to terminate. Moreover, distinguished predicates representing set membership and equality are added to the base language along with their negative counterparts, and SLD resolution is modified accordingly. It is shown that the resulting language allows restricted universal quantifiers in goals and clause bodies to be defined quite simply within the language itself. Finally, abstraction set terms are made available as intensional designations of sets. It is shown that also such terms become directly definable within the language, provided the latter is endowed with negation, which may occur in goals and clause bodies.},
  keywords = {_tablet}
}

@inproceedings{Downen2015structures,
  title = {Structures for {{Structural Recursion}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Downen, Paul and {Johnson-Freyd}, Philip and Ariola, Zena M.},
  year = {2015},
  pages = {127--139},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784762},
  abstract = {Our goal is to develop co-induction from our understanding of induction, putting them on level ground as equal partners for reasoning about programs. We investigate several structures which represent well-founded forms of recursion in programs. These simple structures encapsulate reasoning by primitive and noetherian induction principles, and can be composed together to form complex recursion schemes for programs operating over a wide class of data and co-data types. At its heart, this study is guided by duality: each structure for recursion has a dual form, giving perfectly symmetric pairs of equal and opposite data and co-data types for representing recursion in programs. Duality is brought out through a framework presented in sequent style, which inherently includes control effects that are interpreted logically as classical reasoning principles. To accommodate the presence of effects, we give a calculus parameterized by a notion of strategy, which is strongly normalizing for a wide range of strategies. We also present a more traditional calculus for representing effect-free functional programs, but at the cost of losing some of the founding dualities.},
  isbn = {978-1-4503-3669-7},
  keywords = {Classical Logic,coinduction,Duality,Induction,recursion,sequent calculus,Strong Normalization,Structures},
  series = {{{ICFP}} 2015}
}

@inproceedings{Dreyer2003type,
  title = {A Type System for Higher-Order Modules},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dreyer, Derek and Crary, Karl and Harper, Robert},
  year = {2003},
  pages = {236--249},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/604131.604151},
  abstract = {We present a type theory for higher-order modules that accounts for many central issues in module system design, including translucency, applicativity, generativity, and modules as first-class values. Our type system harmonizes design elements from previous work, resulting in a simple, economical account of modular programming. The main unifying principle is the treatment of abstraction mechanisms as computational effects. Our language is the first to provide a complete and practical formalization of all of these critical issues in module system design.},
  isbn = {1-58113-628-5},
  keywords = {abstract data types,computational effects,functors,generativity,modularity,singleton types,type theory},
  series = {{{POPL}} '03}
}

@inproceedings{Dreyer2004Type,
  title = {A {{Type System}} for {{Well}}-Founded {{Recursion}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dreyer, Derek},
  year = {2004},
  pages = {293--305},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/964001.964026},
  abstract = {In the interest of designing a recursive module extension to ML that is as simple and general as possible, we propose a novel type system for general recursion over effectful expressions. The presence of effects seems to necessitate a backpatching semantics for recursion similar to that of Scheme. Our type system ensures statically that recursion is well-founded---that the body of a recursive expression will evaluate without attempting to access the undefined recursive variable---which avoids some unnecessary run-time costs associated with backpatching. To ensure well-founded recursion in the presence of multiple recursive variables and separate compilation, we track the usage of individual recursive variables, represented statically by "names". So that our type system may eventually be integrated smoothly into ML's, reasoning involving names is only required inside code that uses our recursive construct and need not infect existing ML code, although instrumentation of some existing code can help to improve the precision of our type system.},
  isbn = {978-1-58113-729-3},
  keywords = {effect systems,recursion,recursive modules,yype systems},
  series = {{{POPL}} '04}
}

@inproceedings{Dreyer2005Recursive,
  title = {Recursive {{Type Generativity}}},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dreyer, Derek},
  year = {2005},
  pages = {41--53},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1086365.1086372},
  abstract = {Existential types provide a simple and elegant foundation for understanding generative abstract data types, of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so.We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first "forward-declare" them by generating their names, and then define each one secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among "transparent" type definitions.In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a destination-passing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.},
  isbn = {978-1-59593-064-4},
  keywords = {abstract data types,effect systems,generativity,recursion,recursive modules,type systems},
  series = {{{ICFP}} '05}
}

@inproceedings{Dreyer2007modular,
  title = {Modular {{Type Classes}}},
  booktitle = {Proceedings of the 34th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dreyer, Derek and Harper, Robert and Chakravarty, Manuel M. T. and Keller, Gabriele},
  year = {2007},
  pages = {63--70},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1190216.1190229},
  abstract = {ML modules and Haskell type classes have proven to be highly effective tools for program structuring. Modules emphasize explicit configuration of program components and the use of data abstraction. Type classes emphasize implicit program construction and ad hoc polymorphism. In this paper, we show how the implicitly-typed style of type class programming may be supported within the framework of an explicitly-typed module language by viewing type classes as a particular mode of use of modules. This view offers a harmonious integration of modules and type classes, where type class features, such as class hierarchies and associated types, arise naturally as uses of existing module-language constructs, such as module hierarchies and type components. In addition, programmers have explicit control over which type class instances are available for use by type inference in a given scope. We formalize our approach as a Harper-Stone-style elaboration relation, and provide a sound type inference algorithm as a guide to implementation.},
  isbn = {1-59593-575-4},
  keywords = {modules,type classes,type inference,type systems},
  series = {{{POPL}} '07}
}

@article{Dreyer2007Recursive,
  title = {Recursive Type Generativity},
  author = {Dreyer, Derek},
  year = {2007},
  month = jul,
  volume = {17},
  pages = {433--471},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796807006429},
  abstract = {Existential types provide a simple and elegant foundation for understanding generative abstract data types of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so. We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first `forward-declare' them by generating their names, and then supply each one's identity secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among `transparent' type definitions. In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a destination-passing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {4-5}
}

@inproceedings{Dreyer2007Type,
  title = {A {{Type System}} for {{Recursive Modules}}},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dreyer, Derek},
  year = {2007},
  pages = {289--302},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1291151.1291196},
  abstract = {There has been much work in recent years on extending ML with recursive modules. One of the most difficult problems in the development of such an extension is the double vision problem, which concerns the interaction of recursion and data abstraction. In previous work, I defined a type system called RTG, which solves the double vision problem at the level of a System-F-style core calculus. In this paper, I scale the ideas and techniques of RTG to the level of a recursive ML-style module calculus called RMC, thus establishing that no tradeoff between data abstraction and recursive modules is necessary. First, I describe RMC's typing rules for recursive modules informally and discuss some of the design questions that arose in developing them. Then, I present the formal semantics of RMC, which is interesting in its own right. The formalization synthesizes aspects of both the Definition and the Harper-Stone interpretation of Standard ML, and includes a novel two-pass algorithm for recursive module typechecking in which the coherence of the two passes is emphasized by their representation in terms of the same set of inference rules.},
  isbn = {978-1-59593-815-2},
  keywords = {abstract data types,modules,recursion,type systems},
  series = {{{ICFP}} '07}
}

@inproceedings{Dreyer2008mixin,
  title = {Mixin' up the {{ML}} Module System},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dreyer, Derek and Rossberg, Andreas},
  year = {2008},
  pages = {307--320},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1411204.1411248},
  abstract = {ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent MLstyle data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.},
  isbn = {978-1-59593-919-7},
  keywords = {_tablet,abstract data types,hierarchical composability,mixin modules,ML modules,recursive modules,type systems},
  series = {{{ICFP}} '08}
}

@inproceedings{Dreyer2010Impact,
  title = {The {{Impact}} of {{Higher}}-Order {{State}} and {{Control Effects}} on {{Local Relational Reasoning}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dreyer, Derek and Neis, Georg and Birkedal, Lars},
  year = {2010},
  pages = {143--156},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863543.1863566},
  abstract = {Reasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages - languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higher-order state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the "semantic cube": fully abstract game-semantic characterizations of various axes in the design space of ML-like languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences. In this paper, we marry the aspirations of the semantic cube to the powerful proof method of step-indexed Kripke logical relations. Building on recent work of Ahmed, Dreyer, and Rossberg, we define the first fully abstract logical relation for an ML-like language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power our language - namely, the restriction to first-order state and/or the removal of call/cc - we can enhance the proving power of our possible-worlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.},
  isbn = {978-1-60558-794-3},
  keywords = {_tablet,biorthogonality,exceptions,first-class continuations,higher-order state,local state,Observational equivalence,state transition systems,step-indexed kripke logical relations},
  series = {{{ICFP}} '10}
}

@inproceedings{Dreyer2010Relational,
  title = {A {{Relational Modal Logic}} for {{Higher}}-Order {{Stateful ADTs}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Dreyer, Derek and Neis, Georg and Rossberg, Andreas and Birkedal, Lars},
  year = {2010},
  pages = {185--198},
  publisher = {{ACM}},
  address = {{Madrid, Spain}},
  doi = {10.1145/1706299.1706323},
  abstract = {The method of logical relations is a classic technique for proving the equivalence of higher-order programs that implement the same observable behavior but employ different internal data representations. Although it was originally studied for pure, strongly normalizing languages like System F, it has been extended over the past two decades to reason about increasingly realistic languages. In particular, Appel and McAllester's idea of step-indexing has been used recently to develop syntactic Kripke logical relations for ML-like languages that mix functional and imperative forms of data abstraction. However, while step-indexed models are powerful tools, reasoning with them directly is quite painful, as one is forced to engage in tedious step-index arithmetic to derive even simple results. In this paper, we propose a logic LADR for equational reasoning about higher-order programs in the presence of existential type abstraction, general recursive types, and higher-order mutable state. LADR exhibits a novel synthesis of features from Plotkin-Abadi logic, G{\"o}del-L{\"o}b logic, S4 modal logic, and relational separation logic. Our model of LADR is based on Ahmed, Dreyer, and Rossberg's state-of-the-art step-indexed Kripke logical relation, which was designed to facilitate proofs of representation independence for "state-dependent" ADTs. LADR enables one to express such proofs at a much higher level, without counting steps or reasoning about the subtle, step-stratified construction of possible worlds.},
  isbn = {978-1-60558-479-9},
  keywords = {abstract data types,local state,modal logic,plotkin-abadi logic,separation logic,step-indexed logical relations},
  series = {{{POPL}} '10}
}

@article{Dreyer2011Logical,
  title = {Logical {{Step}}-{{Indexed Logical Relations}}},
  author = {Dreyer, Derek and Ahmed, Amal and Birkedal, Lars},
  year = {2011},
  month = jun,
  volume = {7},
  issn = {18605974},
  doi = {10.2168/LMCS-7(2:16)2011},
  abstract = {Appel and McAllester's "step-indexed" logical relations have proven to be a simple and effective technique for reasoning about programs in languages with semantically interesting types, such as general recursive types and general reference types. However, proofs using step-indexed models typically involve tedious, error-prone, and proof-obscuring step-index arithmetic, so it is important to develop clean, high-level, equational proof principles that avoid mention of step indices. In this paper, we show how to reason about binary step-indexed logical relations in an abstract and elegant way. Specifically, we define a logic LSLR, which is inspired by Plotkin and Abadi's logic for parametricity, but also supports recursively defined relations by means of the modal "later" operator from Appel, Melli\textbackslash{}`es, Richards, and Vouillon's "very modal model" paper. We encode in LSLR a logical relation for reasoning relationally about programs in call-by-value System F extended with general recursive types. Using this logical relation, we derive a set of useful rules with which we can prove contextual equivalence and approximation results without counting steps.},
  archivePrefix = {arXiv},
  eprint = {1103.0510},
  eprinttype = {arxiv},
  journal = {Logical Methods in Computer Science},
  keywords = {_tablet_modified,Computer Science - Logic in Computer Science,Computer Science - Programming Languages,D.3.3; F.3.1; F.3.3},
  number = {2}
}

@article{Dreyer2012impact,
  title = {The Impact of Higher-Order State and Control Effects on Local Relational Reasoning},
  author = {Dreyer, Derek and Neis, Georg and Birkedal, Lars},
  year = {2012},
  month = sep,
  volume = {22},
  pages = {477--528},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S095679681200024X},
  abstract = {AbstractReasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages\textemdash{}languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higher-order state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the ``semantic cube'': fully abstract game-semantic characterizations of various axes in the design space of ML-like languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences.In this paper, we marry the aspirations of the semantic cube to the powerful proof method of step-indexed Kripke logical relations. Building on recent work of Ahmed et al. (2009), we define the first fully abstract logical relation for an ML-like language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power of our language\textemdash{}namely, the restriction to first-order state and/or the removal of call/cc\textemdash{}we can enhance the proving power of our possible-worlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.},
  journal = {Journal of Functional Programming},
  keywords = {_tablet_modified},
  number = {4-5}
}

@article{Drossopoulou1999Java,
  title = {Is the {{Java}} Type System Sound?},
  author = {Drossopoulou, Sophia and Eisenbach, Susan and Khurshid, Sarfraz},
  year = {1999},
  month = jan,
  volume = {5},
  pages = {3--24},
  issn = {1096-9942},
  doi = {10.1002/(SICI)1096-9942(199901/03)5:1<3::AID-TAPO2>3.0.CO;2-T},
  abstract = {A proof of the soundness of the Java type system is a first, necessary step towards demonstrating which Java programs won't compromise computer security. We consider a subset of Java describing primitive types, classes, inheritance, instance variables and methods, interfaces, shadowing, dynamic method binding, object creation, null, arrays, and exception throwing and handling. We argue that for this subset the type system is sound, by proving that program execution preserves the types, up to subclasses/subinterfaces. \textcopyright{} 1999 John Wiley \& Sons, Inc.},
  copyright = {Copyright \textcopyright{} 1999 John Wiley \& Sons, Inc.},
  journal = {Theory and Practice of Object Systems},
  language = {en},
  number = {1}
}

@inproceedings{Duggan1996mixin,
  title = {Mixin {{Modules}}},
  booktitle = {Proceedings of the {{First ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Duggan, Dominic and Sourelis, Constantinos},
  year = {1996},
  pages = {262--273},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/232627.232654},
  abstract = {Mixin modules are proposed as a new construct for module languages, allowing recursive definitions to span module boundaries. Mixin modules are proposed specifically for the Standard ML language. Several applications are described, including the resolution of cycles in module import dependency graphs, as well as functionality related to Haskell type classes and CLOS generic functions, though without any complications to the core language semantics. Mixin modules require no changes to the core ML type system, and only a very minor change to its run-time semantics. A type system and reduction semantics are provided, and the former is verified to be sound relative to the latter.},
  isbn = {0-89791-770-7},
  series = {{{ICFP}} '96}
}

@inproceedings{Dunfield2003Type,
  title = {Type {{Assignment}} for {{Intersections}} and {{Unions}} in {{Call}}-by-{{Value Languages}}},
  booktitle = {Foundations of {{Software Science}} and {{Computation Structures}}},
  author = {Dunfield, Joshua and Pfenning, Frank},
  editor = {Gordon, Andrew D.},
  year = {2003},
  pages = {250--266},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We develop a system of type assignment with intersection types, union types, indexed types, and universal and existential dependent types that is sound in a call-by-value functional language. The combination of logical and computational principles underlying our formulation naturally leads to the central idea of type-checking subterms in evaluation order. We thereby provide a uniform generalization and explanation of several earlier isolated systems. The proof of progress and type preservation, usually formulated for closed terms only, relies on a notion of definite substitution.},
  isbn = {978-3-540-36576-1},
  keywords = {Dependent Type,Intersection Type,Property Type,Typing Rule,Union Type},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Dunfield2012elaborating,
  title = {Elaborating {{Intersection}} and {{Union Types}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dunfield, Joshua},
  year = {2012},
  pages = {17--28},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364527.2364534},
  abstract = {Designing and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features. In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation. We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary {$\lambda$}-calculus terms. The key feature is a Forsythe-like merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary call-by-value evaluation of the elaborated program corresponds to a type-preserving evaluation of the source program. We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.},
  isbn = {978-1-4503-1054-3},
  keywords = {intersection,types},
  series = {{{ICFP}} '12}
}

@article{Dunfield2013Complete,
  title = {Complete and {{Easy Bidirectional Typechecking}} for {{Higher}}-{{Rank Polymorphism}}},
  author = {Dunfield, Joshua and Krishnaswami, Neelakantan R.},
  year = {2013},
  month = jun,
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike Damas-Milner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higher-rank polymorphism, grounded in proof theory; this calculus enjoys many properties such as eta-reduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and well-behaved, despite being both sound and complete.},
  archivePrefix = {arXiv},
  eprint = {1306.6032},
  eprinttype = {arxiv},
  journal = {arXiv:1306.6032 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Dunfield2013complete,
  title = {Complete and {{Easy Bidirectional Typechecking}} for {{Higher}}-Rank {{Polymorphism}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dunfield, Joshua and Krishnaswami, Neelakantan R.},
  year = {2013},
  pages = {429--442},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500582},
  abstract = {Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike Damas-Milner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higher-rank polymorphism, grounded in proof theory; this calculus enjoys many properties such as eta-reduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and well-behaved, despite being both sound and complete.},
  isbn = {978-1-4503-2326-0},
  keywords = {bidirectional typechecking,higher-rank polymorphism},
  series = {{{ICFP}} '13}
}

@article{Dunfield2014elaborating,
  title = {Elaborating Intersection and Union Types},
  author = {Dunfield, Joshua},
  year = {2014},
  month = may,
  volume = {24},
  pages = {133--165},
  issn = {1469-7653},
  doi = {10.1017/S0956796813000270},
  abstract = {Designing and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features. In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation. We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary {$\lambda$}-calculus terms. The key feature is a Forsythe-like merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary call-by-value evaluation of the elaborated program corresponds to a type-preserving evaluation of the source program. We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.},
  journal = {Journal of Functional Programming},
  number = {Special Issue 2-3}
}

@inproceedings{Dunfield2015elaborating,
  title = {Elaborating {{Evaluation}}-Order {{Polymorphism}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Dunfield, Joshua},
  year = {2015},
  pages = {256--268},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784744},
  abstract = {We classify programming languages according to evaluation order: each language fixes one evaluation order as the default, making it transparent to program in that evaluation order, and troublesome to program in the other. This paper develops a type system that is impartial with respect to evaluation order. Evaluation order is implicit in terms, and explicit in types, with by-value and by-name versions of type connectives. A form of intersection type quantifies over evaluation orders, describing code that is agnostic over (that is, polymorphic in) evaluation order. By allowing such generic code, programs can express the by-value and by-name versions of a computation without code duplication. We also formulate a type system that only has by-value connectives, plus a type that generalizes the difference between by-value and by-name connectives: it is either a suspension (by name) or a "no-op" (by value). We show a straightforward encoding of the impartial type system into the more economical one. Then we define an elaboration from the economical language to a call-by-value semantics, and prove that elaborating a well-typed source program, where evaluation order is implicit, produces a well-typed target program where evaluation order is explicit. We also prove a simulation between evaluation of the target program and reductions (either by-value or by-name) in the source program. Finally, we prove that typing, elaboration, and evaluation are faithful to the type annotations given in the source program: if the programmer only writes by-value types, no by-name reductions can occur at run time.},
  isbn = {978-1-4503-3669-7},
  keywords = {evaluation order,intersection types,polymorphism},
  series = {{{ICFP}} 2015}
}

@inproceedings{Dunphy2004parametric,
  title = {Parametric Limits},
  booktitle = {Proceedings of the 19th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}, 2004},
  author = {Dunphy, B. and Reddy, U.S.},
  year = {2004},
  month = jul,
  pages = {242--251},
  doi = {10.1109/LICS.2004.1319618},
  abstract = {We develop a categorical model of polymorphic lambda calculi using the notion of parametric limits, which extend the notion of limits in categories to reflexive graphs of categories. We show that a number of parametric models of polymorphism can be captured in this way. We also axiomatize the structure of reflexive graphs needed for modelling parametric polymorphism based on ideas of fibrations, and show that it leads to proofs of representation results such as the initial algebra and final coalgebra properties one expects in polymorphic lambda calculi.},
  keywords = {categorical model,Computer science,fibrations,final coalgebra properties,initial algebra,lambda calculus,Logic,parametric limits,parametric models,parametric polymorphism,polymorphic lambda calculi,reflexive graphs,relational parametricity,representation proofs,topology}
}

@article{Dwork1984sequential,
  title = {On the Sequential Nature of Unification},
  author = {Dwork, Cynthia and Kanellakis, Paris C. and Mitchell, John C.},
  year = {1984},
  month = jun,
  volume = {1},
  pages = {35--50},
  issn = {0743-1066},
  doi = {10.1016/0743-1066(84)90022-0},
  abstract = {The problem of unification of terms is log-space complete for P. In deriving this lower bound no use is made of the potentially concise representation of terms by directed acyclic graphs. In addition, the problem remains complete even if infinite substitutions are allowed. A consequence of this result is that parallelism cannot significantly improve on the best sequential solutions for unification. However, we show that for the problem of term matching, an important subcase of unification, there is a good parallel algorithm using O(log2n) time and nO(1) processors on a PRAM. For the O(log2n) parallel time upper bound we assume that the terms are represented by directed acyclic graphs; if the longer string representation is used we obtain an O(log n) parallel time bound.},
  journal = {The Journal of Logic Programming},
  number = {1}
}

@article{Dybjer1994inductive,
  title = {Inductive Families},
  author = {Dybjer, Peter},
  year = {1994},
  month = jul,
  volume = {6},
  pages = {440--465},
  issn = {0934-5043, 1433-299X},
  doi = {10.1007/BF01211308},
  abstract = {A general formulation of inductive and recursive definitions in Martin-L{\"o}f's type theory is presented. It extends Backhouse's `Do-It-Yourself Type Theory' to include inductive definitions of families of sets and definitions of functions by recursion on the way elements of such sets are generated. The formulation is in natural deduction and is intended to be a natural generalisation to type theory of Martin-L{\"o}f's theory of iterated inductive definitions in predicate logic. Formal criteria are given for correct formation and introduction rules of a new set former capturing definition by strictly positive, iterated, generalised induction. Moreover, there is an inversion principle for deriving elimination and equality rules from the formation and introduction rules. Finally, there is an alternative schematic presentation of definition by recursion. The resulting theory is a flexible and powerful language for programming and constructive mathematics. We hint at the wealth of possible applications by showing several basic examples: predicate logic, generalised induction, and a formalisation of the untyped lambda calculus.},
  journal = {Formal Aspects of Computing},
  keywords = {Computational Mathematics and Numerical Analysis,Inductive definitions,Intuitionistic type theory,Math Applications in Computer Science,natural deduction,Theory of Computation},
  language = {en},
  number = {4}
}

@inproceedings{Dybjer2002Normalization,
  title = {Normalization and {{Partial Evaluation}}},
  booktitle = {Applied {{Semantics}}},
  author = {Dybjer, Peter and Filinski, Andrzej},
  editor = {Barthe, Gilles and Dybjer, Peter and Pinto, Lu{\'i}s and Saraiva, Jo{\~a}o},
  year = {2002},
  pages = {137--192},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We give an introduction to normalization by evaluation and type-directed partial evaluation. We first present normalization by evaluation for a combinatory version of G{\"o}del System T. Then we show normalization by evaluation for typed lambda calculus with {$\beta$} and {$\eta$} conversion. Finally, we introduce the notion of binding time, and explain the method of type-directed partial evaluation for a small PCF-style functional programming language. We give algorithms for both call-by-name and call-by-value versions of this language.},
  isbn = {978-3-540-45699-5},
  keywords = {Denotational Semantic,Functional Language,Functional Programming,Normal Form,Partial Evaluation},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Eichberg2014software,
  title = {A {{Software Product Line}} for {{Static Analyses}}: {{The OPAL Framework}}},
  shorttitle = {A {{Software Product Line}} for {{Static Analyses}}},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on the {{State}} of the {{Art}} in {{Java Program Analysis}}},
  author = {Eichberg, Michael and Hermann, Ben},
  year = {2014},
  pages = {1--6},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2614628.2614630},
  abstract = {Implementations of static analyses are usually tailored toward a single goal to be efficient, hampering reusability and adaptability of the components of an analysis. To solve these issues, we propose to implement static analyses as highly-configurable software product lines (SPLs). Furthermore, we also discuss an implementation of an SPL for static analyses -- called OPAL -- that uses advanced language features offered by the Scala programming language to get an easily adaptable and (type-)safe software product line. OPAL is a general purpose library for static analysis of Java Bytecode that is already successfully used. We present OPAL and show how a design based on software produce line engineering benefits the implementation of static analyses with the framework.},
  isbn = {978-1-4503-2919-4},
  keywords = {abstract interpretation,design,program analysis,software product line engineering,static analysis},
  series = {{{SOAP}} '14}
}

@inproceedings{Eisenberg2014closed,
  title = {Closed {{Type Families}} with {{Overlapping Equations}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Eisenberg, Richard A. and Vytiniotis, Dimitrios and Peyton Jones, Simon and Weirich, Stephanie},
  year = {2014},
  pages = {671--683},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2535838.2535856},
  abstract = {Open, type-level functions are a recent innovation in Haskell that move Haskell towards the expressiveness of dependent types, while retaining the look and feel of a practical programming language. This paper shows how to increase expressiveness still further, by adding closed type functions whose equations may overlap, and may have non-linear patterns over an open type universe. Although practically useful and simple to implement, these features go beyond conventional dependent type theory in some respects, and have a subtle metatheory.},
  isbn = {978-1-4503-2544-8},
  keywords = {Haskell,system fc,type families,type-level computation},
  series = {{{POPL}} '14}
}

@article{Eisenberg2016Dependent,
  title = {Dependent Types in {{Haskell}}: Theory and Practice},
  shorttitle = {Dependent Types in Haskell},
  author = {Eisenberg, Richard A.},
  year = {2016},
  month = oct,
  abstract = {Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been adding new type-level programming features for some time. Many of these features---chiefly: generalized algebraic datatypes (GADTs), type families, kind polymorphism, and promoted datatypes---have brought Haskell to the doorstep of dependent types. Many dependently typed programs can even currently be encoded, but often the constructions are painful. In this dissertation, I describe Dependent Haskell, which supports full dependent types via a backward-compatible extension to today's Haskell. An important contribution of this work is an implementation, in GHC, of a portion of Dependent Haskell, with the rest to follow. The features I have implemented are already released, in GHC 8.0. This dissertation contains several practical examples of Dependent Haskell code, a full description of the differences between Dependent Haskell and today's Haskell, a novel type-safe dependently typed lambda-calculus (called Pico) suitable for use as an intermediate language for compiling Dependent Haskell, and a type inference and elaboration algorithm, Bake, that translates Dependent Haskell to type-correct Pico.},
  archivePrefix = {arXiv},
  eprint = {1610.07978},
  eprinttype = {arxiv},
  journal = {arXiv:1610.07978 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Eisenberg2018Type,
  title = {Type Variables in Patterns},
  author = {Eisenberg, Richard A. and Breitner, Joachim and Jones, Simon Peyton},
  year = {2018},
  month = jun,
  abstract = {For many years, GHC has implemented an extension to Haskell that allows type variables to be bound in type signatures and patterns, and to scope over terms. This extension was never properly specified. We rectify that oversight here. With the formal specification in hand, the otherwise-labyrinthine path toward a design for binding type variables in patterns becomes blindingly clear. We thus extend ScopedTypeVariables to bind type variables explicitly, obviating the Proxy workaround to the dustbin of history.},
  archivePrefix = {arXiv},
  eprint = {1806.03476},
  eprinttype = {arxiv},
  journal = {arXiv:1806.03476 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@phdthesis{Elliott1990extensions,
  title = {Extensions and {{Applications}} of {{Higher}}-Order {{Unification}}},
  author = {Elliott, Conal Mullen},
  year = {1990},
  address = {{Pittsburgh, PA, USA}},
  note = {AAI9107557},
  school = {Carnegie Mellon University}
}

@article{Elliott2003compiling,
  title = {Compiling Embedded Languages},
  author = {Elliott, Conal and Finne, Sigbj{\o}rn and De Moor, Oege},
  year = {2003},
  month = may,
  volume = {13},
  pages = {455--481},
  issn = {1469-7653},
  doi = {10.1017/S0956796802004574},
  abstract = {Functional languages are particularly well-suited to the interpretive implementations of Domain-Specific Embedded Languages (DSELs). We describe an implemented technique for producing optimizing compilers for DSELs, based on Kamin\&apos;s idea of DSELs for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization results from chains of domain-independent rewrites on basic types. New DSELs are defined directly in terms of the basic syntactic types, plus host language functions and tuples. This definition style makes compilers easy to write and, in fact, almost identical to the simplest embedded interpreters. We illustrate this technique with a language Pan for the computationally intensive domain of image synthesis and manipulation.},
  journal = {Journal of Functional Programming},
  number = {03}
}

@techreport{Elliott2008simply,
  title = {Simply Efficient Functional Reactivity},
  author = {Elliott, Conal},
  year = {2008},
  month = apr,
  institution = {{LambdaPix}},
  abstract = {Functional reactive programming (FRP) has simple and powerful semantics, but has resisted efficient implementation. In particular, most past implementations have used demand-driven sampling, which accommodates FRP's continuous time semantics and fits well with the nature of functional programming. Consequently, values are wastefully recomputed even when inputs don't change, and reaction latency can be as high as the sampling period.

This paper presents a way to implement FRP that combines data- and demand-driven evaluation, in which values are recomputed only when necessary, and reactions are nearly instantaneous. The implementation is rooted in a new simple formulation of FRP and its semantics and so is easy to understand and reason about.

On the road to efficiency and simplicity, we'll meet some old friends (monoids, functors, applicative functors, monads, morphisms, and improving values) and make some new friends (functional future values, reactive normal form, and concurrent "unambiguous choice").},
  number = {2008-01}
}

@techreport{Elliott2009denotational,
  title = {Denotational Design with Type Class Morphisms (Extended Version)},
  author = {Elliott, Conal},
  year = {2009},
  month = mar,
  institution = {{LambdaPix}},
  abstract = {Type classes provide a mechanism for varied implementations of standard interfaces. Many of these interfaces are founded in mathematical tradition and so have regularity not only of types but also of properties (laws) that must hold. Types and properties give strong guidance to the library implementor, while leaving freedom as well. Some of this remaining freedom is in how the implementation works, and some is in what it accomplishes.

To give additional guidance to the what, without impinging on the how, this paper proposes a principle of type class morphisms (TCMs), which further refines the compositional style of denotational semantics. The TCM idea is simply that the instance's meaning is the meaning's instance. This principle determines the meaning of each type class instance, and hence defines correctness of implementation. It also serves to transfer laws about a type's semantic model, such as the class laws, to hold for the type itself. In some cases, it also provides a systematic guide to implementation, and in some cases, valuable design feedback.

The paper is illustrated with several examples of types, meanings, and morphisms.},
  number = {2009-01}
}

@inproceedings{Elliott2009pushpull,
  title = {Push-Pull {{Functional Reactive Programming}}},
  booktitle = {Proceedings of the {{2Nd ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Elliott, Conal M.},
  year = {2009},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596638.1596643},
  abstract = {Functional reactive programming (FRP) has simple and powerful semantics, but has resisted efficient implementation. In particular, most past implementations have used demand-driven sampling, which accommodates FRP's continuous time semantics and fits well with the nature of functional programming. Consequently, values are wastefully recomputed even when inputs don't change, and reaction latency can be as high as the sampling period. This paper presents a way to implement FRP that combines data- and demand-driven evaluation, in which values are recomputed only when necessary, and reactions are nearly instantaneous. The implementation is rooted in a new simple formulation of FRP and its semantics and so is easy to understand and reason about. On the road to a new implementation, we'll meet some old friends (monoids, functors, applicative functors, monads, morphisms, and improving values) and make some new friends (functional future values, reactive normal form, and concurrent "unambiguous choice").},
  isbn = {978-1-60558-508-6},
  keywords = {concurrency,data-driven,demand-driven,functional reactive programming,semantics},
  series = {Haskell '09}
}

@article{Elliott2017Compiling,
  title = {Compiling to {{Categories}}},
  author = {Elliott, Conal},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {27:1--27:27},
  issn = {2475-1421},
  doi = {10.1145/3110271},
  abstract = {It is well-known that the simply typed lambda-calculus is modeled by any cartesian closed category (CCC). This correspondence suggests giving typed functional programs a variety of interpretations, each corresponding to a different category. A convenient way to realize this idea is as a collection of meaning-preserving transformations added to an existing compiler, such as GHC for Haskell. This paper describes such an implementation and demonstrates its use for a variety of interpretations including hardware circuits, automatic differentiation, incremental computation, and interval analysis. Each such interpretation is a category easily defined in Haskell (outside of the compiler). The general technique appears to provide a compelling alternative to deeply embedded domain-specific languages.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {category theory,compile-time optimization,domain-specific languages},
  number = {ICFP}
}

@article{Elsman2018Static,
  title = {Static {{Interpretation}} of {{Higher}}-Order {{Modules}} in {{Futhark}}: {{Functional GPU Programming}} in the {{Large}}},
  shorttitle = {Static {{Interpretation}} of {{Higher}}-Order {{Modules}} in {{Futhark}}},
  author = {Elsman, Martin and Henriksen, Troels and Annenkov, Danil and Oancea, Cosmin E.},
  year = {2018},
  month = jul,
  volume = {2},
  pages = {97:1--97:30},
  issn = {2475-1421},
  doi = {10.1145/3236792},
  abstract = {We present a higher-order module system for the purely functional data-parallel array language Futhark. The module language has the property that it is completely eliminated at compile time, yet it serves as a powerful tool for organizing libraries and complete programs. The presentation includes a static and a dynamic semantics for the language in terms of, respectively, a static type system and a provably terminating elaboration of terms into terms of an underlying target language. The development is formalised in Coq using a novel encoding of semantic objects based on products, sets, and finite maps. The module language features a unified treatment of module type abstraction and core language polymorphism and is rich enough for expressing practical forms of module composition.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {compilers,functional languages,GPGPU,modules},
  number = {ICFP}
}

@article{Emir2007Objectoriented,
  title = {Object-Oriented Pattern Matching},
  author = {Emir, Burak},
  year = {2007},
  doi = {10.5075/epfl-thesis-3899},
  abstract = {Pattern matching is a programming language construct considered essential in functional programming. Its purpose is to inspect and decompose data. Instead, object-oriented programming languages do not have a dedicated construct for this purpose. A possible reason for this is that pattern matching is useful when data is defined separately from operations on the data \textendash{} a scenario that clashes with the object-oriented motto of grouping data and operations. However, programmers are frequently confronted with situations where there is no alternative to expressing data and operations separately \textendash{} because most data is neither stored in nor does it originate from an object-oriented context. Consequently, object-oriented programmers, too, are in need for elegant and concise solutions to the problem of decomposing data. To this end, we propose a built-in pattern matching construct compatible with object-oriented programming. We claim that it leads to more concise and readable code than standard object-oriented approaches. A pattern in our approach is any computable way of testing and deconstructing an object and binding relevant parts to local names. We introduce pattern matching in two variants, case classes and extractors. We compare the readability, extensibility and performance of built-in pattern matching in these two variants with standard decomposition techniques. It turns out that standard object-oriented approaches to decomposing data are not extensible. Case classes, which have been studied before, require a low notational overhead, but expose their representation, making them hard to change later. The novel extractor mechanism offers loose coupling and extensibility, but comes with a performance overhead. We present a formalization of object-oriented pattern matching with extractors. This is done by giving definitions and proving standard properties for a calculus that provides pattern matching as described before. We then give a formal, optimizing translation from the calculus including pattern matching to its fragment without pattern matching, and prove it correct. Finally, we consider non-obvious interactions between the pattern matching and parametric polymorphism. We review the technique of generalized algebraic data types from functional programming, and show how it can be carried over to the object-oriented style. The main tool is the extension of the type system with subtype constraints, which leads to a very expressive metatheory. Through this theory, we are able to express patterns that operate on existentially quantified types purely by universally quantified extractors. Emir, Burak},
  language = {en}
}

@inproceedings{Endrullis2013infinitary,
  title = {Infinitary {{Rewriting Coinductively}}},
  booktitle = {18th {{International Workshop}} on {{Types}} for {{Proofs}} and {{Programs}} ({{TYPES}} 2011)},
  author = {Endrullis, J{\"o}rg and Polonsky, Andrew},
  editor = {Danielsson, Nils Anders and Nordstr{\"o}m, Bengt},
  year = {2013},
  volume = {19},
  pages = {16--27},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.TYPES.2011.16},
  isbn = {978-3-939897-49-1},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-38971}
}

@inproceedings{Erdweg2011sugarj,
  title = {{{SugarJ}}: {{Library}}-Based {{Syntactic Language Extensibility}}},
  shorttitle = {{{SugarJ}}},
  booktitle = {Proceedings of the 2011 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K{\"a}stner, Christian and Ostermann, Klaus},
  year = {2011},
  pages = {391--406},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2048066.2048099},
  abstract = {Existing approaches to extend a programming language with syntactic sugar often leave a bitter taste, because they cannot be used with the same ease as the main extension mechanism of the programming language - libraries. Sugar libraries are a novel approach for syntactically extending a programming language within the language. A sugar library is like an ordinary library, but can, in addition, export syntactic sugar for using the library. Sugar libraries maintain the composability and scoping properties of ordinary libraries and are hence particularly well-suited for embedding a multitude of domain-specific languages into a host language. They also inherit self-applicability from libraries, which means that sugar libraries can provide syntactic extensions for the definition of other sugar libraries. To demonstrate the expressiveness and applicability of sugar libraries, we have developed SugarJ, a language on top of Java, SDF and Stratego, which supports syntactic extensibility. SugarJ employs a novel incremental parsing technique, which allows changing the syntax within a source file. We demonstrate SugarJ by five language extensions, including embeddings of XML and closures in Java, all available as sugar libraries. We illustrate the utility of self-applicability by embedding XML Schema, a metalanguage to define XML languages.},
  isbn = {978-1-4503-0940-0},
  keywords = {DSL embedding,language composition,language extensibility,Libraries,sugarj,syntactic sugar},
  series = {{{OOPSLA}} '11}
}

@inproceedings{Erdweg2014CaptureAvoiding,
  title = {Capture-{{Avoiding}} and {{Hygienic Program Transformations}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Erdweg, Sebastian and van der Storm, Tijs and Dai, Yi},
  year = {2014},
  month = jul,
  pages = {489--514},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44202-9_20},
  abstract = {Program transformations in terms of abstract syntax trees compromise referential integrity by introducing variable capture. Variable capture occurs when in the generated program a variable declaration accidentally shadows the intended target of a variable reference. Existing transformation systems either do not guarantee the avoidance of variable capture or impair the implementation of transformations.We present an algorithm called name-fix that automatically eliminates variable capture from a generated program by systematically renaming variables. name-fix is guided by a graph representation of the binding structure of a program, and requires name-resolution algorithms for the source language and the target language of a transformation. name-fix is generic and works for arbitrary transformations in any transformation system that supports origin tracking for names. We verify the correctness of name-fix and identify an interesting class of transformations for which name-fix provides hygiene. We demonstrate the applicability of name-fix for implementing capture-avoiding substitution, inlining, lambda lifting, and compilers for two domain-specific languages.},
  language = {en}
}

@inproceedings{Erdweg2014CaptureAvoidinga,
  title = {Capture-{{Avoiding}} and {{Hygienic Program Transformations}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Erdweg, Sebastian and {van der Storm}, Tijs and Dai, Yi},
  editor = {Jones, Richard},
  year = {2014},
  pages = {489--514},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44202-9_20},
  abstract = {Program transformations in terms of abstract syntax trees compromise referential integrity by introducing variable capture. Variable capture occurs when in the generated program a variable declaration accidentally shadows the intended target of a variable reference. Existing transformation systems either do not guarantee the avoidance of variable capture or impair the implementation of transformations.We present an algorithm called name-fix that automatically eliminates variable capture from a generated program by systematically renaming variables. name-fix is guided by a graph representation of the binding structure of a program, and requires name-resolution algorithms for the source language and the target language of a transformation. name-fix is generic and works for arbitrary transformations in any transformation system that supports origin tracking for names. We verify the correctness of name-fix and identify an interesting class of transformations for which name-fix provides hygiene. We demonstrate the applicability of name-fix for implementing capture-avoiding substitution, inlining, lambda lifting, and compilers for two domain-specific languages.},
  isbn = {978-3-662-44202-9},
  keywords = {Abstract Syntax,Program Transformation,Source Program,State Machine,Target Language},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Erdweg2014modular,
  title = {Modular {{Specification}} and {{Dynamic Enforcement}} of {{Syntactic Language Constraints}} When {{Generating Code}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Modularity}}},
  author = {Erdweg, Sebastian and Vergu, Vlad and Mezini, Mira and Visser, Eelco},
  year = {2014},
  pages = {241--252},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2577080.2577089},
  abstract = {A key problem in metaprogramming and specifically in generative programming is to guarantee that generated code is well-formed with respect to the context-free and context-sensitive constraints of the target language. We propose typesmart constructors as a dynamic approach to enforcing the well-formedness of generated code. A typesmart constructor is a function that is used in place of a regular constructor to create values, but it may reject the creation of values if the given data violates some language-specific constraint. While typesmart constructors can be implemented individually, we demonstrate how to derive them automatically from a grammar, so that the grammar remains the sole specification of a language's syntax and is not duplicated. We have integrated support for typesmart constructors into the run-time system of Stratego to enforce usage of typesmart constructors implicitly whenever a regular constructor is called. We evaluate the applicability, performance, and usefulness of typesmart constructors for syntactic constraints in a compiler for MiniJava developed with Spoofax and in various language extensions of Java and Haskell implemented with SugarJ and SugarHaskell.},
  isbn = {978-1-4503-2772-5},
  keywords = {abstract syntax tree,dynamic analysis,generative programming,program transformation,spoofax,sugarj,typesmart constructors,well-formedness checks},
  series = {{{MODULARITY}} '14}
}

@incollection{Ernst2001family,
  title = {Family {{Polymorphism}}},
  booktitle = {{{ECOOP}} 2001 \textemdash{} {{Object}}-{{Oriented Programming}}},
  author = {Ernst, Erik},
  editor = {Knudsen, J{\o}rgen Lindskov},
  year = {2001},
  month = jan,
  pages = {303--326},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper takes polymorphism to the multi-object level. Traditional inheritance, polymorphism, and late binding interact nicely to provide both flexibility and safety \textemdash{} when a method is invoked on an object via a polymorphic reference, late binding ensures that we get the appropriate implementation of that method for the actual object. We are granted the flexibility of using different kinds of objects and different method implementations, and we are guaranteed the safety of the combination. Nested classes, polymorphism, and late binding of nested classes interact similarly to provide both safety and flexibility at the level of multi-object systems. We are granted the flexibility of using different families of kinds of objects, and we are guaranteed the safety of the combination. This paper highlights the inability of traditional polymorphism to handle multiple objects, and presents family polymorphism as a way to overcome this problem. Family polymorphism has been implemented in the programming language gbeta, a generalized version of Beta, and the source code of this implementation is available under GPL.1},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-42206-8 978-3-540-45337-6},
  keywords = {Business Information Systems,Computer Communication Networks,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {2072},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Ernst2006virtual,
  title = {A {{Virtual Class Calculus}}},
  booktitle = {Conference {{Record}} of the 33rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ernst, Erik and Ostermann, Klaus and Cook, William R.},
  year = {2006},
  pages = {270--282},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1111037.1111062},
  abstract = {Virtual classes are class-valued attributes of objects. Like virtual methods, virtual classes are defined in an object's class and may be redefined within subclasses. They resemble inner classes, which are also defined within a class, but virtual classes are accessed through object instances, not as static components of a class. When used as types, virtual classes depend upon object identity -- each object instance introduces a new family of virtual class types. Virtual classes support large-scale program composition techniques, including higher-order hierarchies and family polymorphism. The original definition of virtual classes in BETA left open the question of static type safety, since some type errors were not caught until runtime. Later the languages Caesar and gbeta have used a more strict static analysis in order to ensure static type safety. However, the existence of a sound, statically typed model for virtual classes has been a long-standing open question. This paper presents a virtual class calculus, VC, that captures the essence of virtual classes in these full-fledged programming languages. The key contributions of the paper are a formalization of the dynamic and static semantics of VC and a proof of the soundness of VC.},
  isbn = {1-59593-027-2},
  keywords = {soundness,virtual classes},
  series = {{{POPL}} '06}
}

@incollection{Erwig2012semantics,
  title = {Semantics {{First}}!},
  booktitle = {Software {{Language Engineering}}},
  author = {Erwig, Martin and Walkingshaw, Eric},
  editor = {Sloane, Anthony and A{\ss}mann, Uwe},
  year = {2012},
  pages = {243--262},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The design of languages is still more of an art than an engineering discipline. Although recently tools have been put forward to support the language design process, such as language workbenches, these have mostly focused on a syntactic view of languages. While these tools are quite helpful for the development of parsers and editors, they provide little support for the underlying design of the languages. In this paper we illustrate how to support the design of languages by focusing on their semantics first. Specifically, we will show that powerful and general language operators can be employed to adapt and grow sophisticated languages out of simple semantics concepts. We use Haskell as a metalanguage and will associate generic language concepts, such as semantics domains, with Haskell-specific ones, such as data types. We do this in a way that clearly distinguishes our approach to language design from the traditional syntax-oriented one. This will reveal some unexpected correlations, such as viewing type classes as language multipliers. We illustrate the viability of our approach with several real-world examples.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-28829-6 978-3-642-28830-2},
  keywords = {Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Software Engineering,User Interfaces and Human Computer Interaction},
  language = {en},
  number = {6940},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Farmer2008seven,
  title = {The Seven Virtues of Simple Type Theory},
  author = {Farmer, William M.},
  year = {2008},
  month = sep,
  volume = {6},
  pages = {267--286},
  issn = {1570-8683},
  doi = {10.1016/j.jal.2007.11.001},
  abstract = {Simple type theory, also known as higher-order logic, is a natural extension of first-order logic which is simple, elegant, highly expressive, and practical. This paper surveys the virtues of simple type theory and attempts to show that simple type theory is an attractive alternative to first-order logic for practical-minded scientists, engineers, and mathematicians. It recommends that simple type theory be incorporated into introductory logic courses offered by mathematics departments and into the undergraduate curricula for computer science and software engineering students.},
  journal = {Journal of Applied Logic},
  keywords = {Complete ordered field,higher-order logic,Nonstandard models,Peano arithmetic,Practical logics,type theory},
  number = {3}
}

@inproceedings{Farmer2012hermit,
  title = {The {{HERMIT}} in the {{Machine}}: {{A Plugin}} for the {{Interactive Transformation}} of {{GHC Core Language Programs}}},
  shorttitle = {The {{HERMIT}} in the {{Machine}}},
  booktitle = {Proceedings of the 2012 {{Haskell Symposium}}},
  author = {Farmer, Andrew and Gill, Andy and Komp, Ed and Sculthorpe, Neil},
  year = {2012},
  pages = {1--12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364506.2364508},
  abstract = {The importance of reasoning about and refactoring programs is a central tenet of functional programming. Yet our compilers and development toolchains only provide rudimentary support for these tasks. This paper introduces a programmatic and compiler-centric interface that facilitates refactoring and equational reasoning. To develop our ideas, we have implemented HERMIT, a toolkit enabling informal but systematic transformation of Haskell programs from inside the Glasgow Haskell Compiler's optimization pipeline. With HERMIT, users can experiment with optimizations and equational reasoning, while the tedious heavy lifting of performing the actual transformations is done for them. HERMIT provides a transformation API that can be used to build higher-level rewrite tools. One use-case is prototyping new optimizations as clients of this API before being committed to the GHC toolchain. We describe a HERMIT application - a read-eval-print shell for performing transformations using HERMIT. We also demonstrate using this shell to prototype an optimization on a specific example, and report our initial experiences and remaining challenges.},
  isbn = {978-1-4503-1574-6},
  keywords = {dsls,equational reasoning,ghc,optimization,strategic programming},
  series = {Haskell '12}
}

@inproceedings{Farmer2014hermit,
  title = {The {{HERMIT}} in the {{Stream}}: {{Fusing Stream Fusion}}'s {{concatMap}}},
  shorttitle = {The {{HERMIT}} in the {{Stream}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2014 {{Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Farmer, Andrew and {Hoener zu Siederdissen}, Christian and Gill, Andy},
  year = {2014},
  pages = {97--108},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2543728.2543736},
  abstract = {Stream Fusion, a popular deforestation technique in the Haskell community, cannot fuse the concatMap combinator. This is a serious limitation, as concatMap represents computations on nested streams. The original implementation of Stream Fusion used the Glasgow Haskell Compiler's user-directed rewriting system. A transformation which allows the compiler to fuse many uses of concatMap has previously been proposed, but never implemented, because the host rewrite system was not expressive enough to implement the proposed transformation. In this paper, we develop a custom optimization plugin which implements the proposed concatMap transformation, and study the effectiveness of the transformation in practice. We also provide a new translation scheme for list comprehensions which enables them to be optimized. Within this framework, we extend the transformation to monadic streams. Code featuring uses of concatMap experiences significant speedup when compiled with this optimization. This allows Stream Fusion to outperform its rival, foldr/build, on many list computations, and enables performance-sensitive code to be expressed at a higher level of abstraction.},
  isbn = {978-1-4503-2619-3},
  keywords = {deforestation,functional programming,ghc,Haskell,optimization,program fusion,program transformation,stream fusion},
  series = {{{PEPM}} '14}
}

@article{Fegaras2015incremental,
  title = {Incremental {{Query Processing}} on {{Big Data Streams}}},
  author = {Fegaras, Leonidas},
  year = {2015},
  month = nov,
  abstract = {This paper addresses online processing for large-scale, incremental computations on a distributed stream processing engine (DSPE). Our goal is to convert any distributed batch query to an incremental DSPE program automatically. In contrast to other approaches, we derive incremental programs that return accurate results, not approximate answers, by retaining a minimal state during the query evaluation lifetime and by using incremental evaluation techniques to return an accurate snapshot answer at each time interval that depends on the current state and the latest batches of data. Our methods can handle many forms of queries, including iterative and nested queries, group-by with aggregation, and joins on one-to-many relationships. Finally, we report on a prototype implementation of our framework using MRQL running on top of Spark and we experimentally validate the effectiveness of our methods.},
  archivePrefix = {arXiv},
  eprint = {1511.07846},
  eprinttype = {arxiv},
  journal = {arXiv:1511.07846 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@article{Felleisen1987syntactic,
  title = {A Syntactic Theory of Sequential Control},
  author = {Felleisen, Matthias and Friedman, Daniel P. and Kohlbecker, Eugene and Duba, Bruce},
  year = {1987},
  volume = {52},
  pages = {205--237},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(87)90109-5},
  abstract = {Sequential control operators like J and call/cc are often found in implementations of the {$\lambda$}-calculus as a programming language. Their semantics is always defined by the evaluation function of an abstract machine. We show that, given such a machine semantics, one can derive an algebraic extension of the {$\lambda\upsilon$}-calculus. The extended calculus satisfies the diamond property and contains a Church-Rosser subcalculus. This underscores that the interpretation of control operators is to a certain degree independent of a specific evaluation strategy. We also prove a standardization theorem and use it to investigate the correspondence between the machine and the calculus. Together, the calculus and the rewriting machine form a syntactic theory of control, which provides a natural basis for reasoning about programs with nonfunctional control operators.},
  journal = {Theoretical Computer Science},
  number = {3}
}

@article{Felleisen1992revised,
  title = {The Revised Report on the Syntactic Theories of Sequential Control and State},
  author = {Felleisen, Matthias and Hieb, Robert},
  year = {1992},
  month = sep,
  volume = {103},
  pages = {235--271},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(92)90014-7},
  abstract = {The syntactic theories of control and state are conservative extensions of the {$\lambda\upsilon$}-calculus for equational reasoning about imperative programming facilities in higher-order languages. Unlike the simple {$\lambda\upsilon$}-calculus, the extended theories are mixtures of equivalence relations and compatible congruence relations on the term language, which significantly complicates the reasoning process. In this paper we develop fully compatible equational theories of the same imperative higher-order programming languages. The new theories subsume the original calculi of control and state and satisfy the usual Church\textendash{}Rosser and Standardization Theorems. With the new calculi, equational reasoning about imperative programs becomes as simple as reasoning about functional programs.},
  journal = {Theoretical Computer Science},
  number = {2}
}

@incollection{Filinski1989declarative,
  title = {Declarative Continuations: {{An}} Investigation of Duality in Programming Language Semantics},
  shorttitle = {Declarative Continuations},
  booktitle = {Category {{Theory}} and {{Computer Science}}},
  author = {Filinski, Andrzej},
  editor = {Pitt, David H. and Rydeheard, David E. and Dybjer, Peter and Pitts, Andrew M. and Poign{\'e}, Axel},
  year = {1989},
  month = jan,
  pages = {224--249},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper presents a formalism for including first-class continuations in a programming language as a declarative concept, rather than an imperative one. A symmetric extension of the typed {$\lambda$}-calculus is introduced, where values and continuations play dual roles, permitting mirror-image syntax for dual categorical concepts like products and coproducts. An implementable semantic description and a static type system for this calculus are presented. We also give a categorical description of the language, by presenting a correspondence with a system of combinatory logic, similar to a cartesian closed category, but with a completely symmetrical set of axioms.},
  copyright = {\textcopyright{}1989 Springer-Verlag},
  isbn = {978-3-540-51662-0 978-3-540-46740-3},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {389},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Filinski1994representing,
  title = {Representing {{Monads}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Filinski, Andrzej},
  year = {1994},
  pages = {446--457},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/174675.178047},
  abstract = {We show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a call-by-value language with ``composable continuations''. As part of the development, we extend Meyer and Wand's characterization of the relationship between continuation-passing and direct style to one for continuation-passing vs. general ``monadic'' style. We further show that the composable-continuations construct can itself be represented using ordinary, non-composable first-class continuations and a single piece of state. Thus, in the presence of two specific computational effects - storage and escapes - any expressible monadic structure (e.g., nondeterminism as represented by the list monad) can be added as a purely definitional extension, without requiring a reinterpretation of the whole language. The paper includes an implementation of the construction (in Standard ML with some New Jersey extensions) and several examples.},
  isbn = {0-89791-636-0},
  series = {{{POPL}} '94}
}

@inproceedings{Filinski1999representing,
  title = {Representing {{Layered Monads}}},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Filinski, Andrzej},
  year = {1999},
  pages = {175--188},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/292540.292557},
  abstract = {There has already been considerable research on constructing modular, monad-based specifications of computational effects (state, exceptions, nondeterminism, etc.) in programming languages. We present a simple framework in this tradition, based on a Church-style effect-typing system for an ML-like language. The semantics of this language is formally defined by a series of monadic translations, each one expanding away a layer of effects. Such a layered specification is easy to reason about, but its direct implementation (whether by parameterized interpretation or by actual translation) is often prohibitively inefficient.By exploiting deeper semantic properties of monads, however, it is also possible to derive a vastly more efficient implementation: we show that each layer of effects can be uniformly simulated by continuation-passing, and further that multiple such layers can themselves be simulated by a standard semantics for call/cc and mutable state. Thus, even multi-effect programs can be executed in Scheme or SML/NJ at full native speed, generalizing an earlier single-effect result. As an example, we show how a simple resumption-based semantics of concurrency allows us to directly simulate a shared-state program across all possible dynamic interleavings of execution threads.},
  isbn = {1-58113-095-3},
  series = {{{POPL}} '99}
}

@incollection{Filinski2001normalization,
  title = {Normalization by {{Evaluation}} for the {{Computational Lambda}}-{{Calculus}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Filinski, Andrzej},
  editor = {Abramsky, Samson},
  year = {2001},
  month = jan,
  pages = {151--165},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We show how a simple semantic characterization of normalization by evaluation for the {$\lambda\beta\eta$}-calculus can be extended to a similar construction for normalization of terms in the computational {$\lambda$}-calculus. Specifically, we show that a suitable residualizing interpretation of base types, constants, and computational effects allows us to extract a syntactic normal form from a term's denotation. The required interpretation can itself be constructed as the meaning of a suitable functional program in an ML-like language, leading directly to a practical normalization algorithm. The results extend easily to product and sum types, and can be seen as a formal basis for call-by-value type-directed partial evaluation.},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-41960-0 978-3-540-45413-7},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {2044},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Filinski2007relations,
  title = {On the Relations between Monadic Semantics},
  author = {Filinski, Andrzej},
  year = {2007},
  month = may,
  volume = {375},
  pages = {41--75},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.12.027},
  abstract = {We present a simple computational metalanguage with general recursive types and multiple notions of effects, through which a variety of concrete denotational semantics can be conveniently factored, by suitably interpreting the effects as monads. We then propose a methodology for relating two such interpretations of the metalanguage, with the aim of showing that the semantics they induce agree for complete programs. As a prototypical instance of such a relation, we use the framework to show agreement between a direct and a continuation semantics of the simple, untyped functional language from Reynolds's original paper on the subject.},
  journal = {Theoretical Computer Science},
  keywords = {Computational metalanguage,continuations,logical relations,Monads,recursive types},
  number = {1\textendash{}3},
  series = {Festschrift for {{John C}}. {{Reynolds}}'s 70th Birthday}
}

@inproceedings{Filinski2010monads,
  title = {Monads in {{Action}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Filinski, Andrzej},
  year = {2010},
  pages = {483--494},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706299.1706354},
  abstract = {In functional programming, monadic characterizations of computational effects are normally understood denotationally: they describe how an effectful program can be systematically expanded or translated into a larger, pure program, which can then be evaluated according to an effect-free semantics. Any effect-specific operations expressible in the monad are also given purely functional definitions, but these definitions are only directly executable in the context of an already translated program. This approach thus takes an inherently Church-style view of effects: the nominal meaning of every effectful term in the program depends crucially on its type. We present here a complementary, operational view of monadic effects, in which an effect definition directly induces an imperative behavior of the new operations expressible in the monad. This behavior is formalized as additional operational rules for only the new constructs; it does not require any structural changes to the evaluation judgment. Specifically, we give a small-step operational semantics of a prototypical functional language supporting programmer-definable, layered effects, and show how this semantics naturally supports reasoning by familiar syntactic techniques, such as showing soundness of a Curry-style effect-type system by the progress+preservation method.},
  isbn = {978-1-60558-479-9},
  keywords = {computational effects,modular semantics,Monads},
  series = {{{POPL}} '10}
}

@inproceedings{Filinski2011comprehensive,
  title = {Towards a {{Comprehensive Theory}} of {{Monadic Effects}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Filinski, Andrzej},
  year = {2011},
  pages = {1--1},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2034773.2034775},
  abstract = {It has been more than 20 years since monads were proposed as a unifying concept for computational effects, in both formal semantics and functional programs. Over that period, there has been substantial incremental progress on several fronts within the ensuing research area, including denotational, operational, and axiomatic characterizations of effects; principles and frameworks for combining effects; prescriptive vs. descriptive effect-type systems; specification vs. implementation of effects; and realizations of effect-related theoretical constructions in practical functional languages, both eager and lazy. Yet few would confidently claim that programs with computational effects are by now as well understood, and as thoroughly supported by formal reasoning techniques, as types and terms in purely functional settings. This talk outlines (one view of) the landscape of effectful functional programming, and attempts to assess our collective progress towards the goal of a broad yet coherent theory of monadic effects. We are not quite there yet, but intriguingly, many potential ingredients of such a theory have been repeatedly discovered and developed, with only minor variations, in seemingly unrelated contexts. Some stronger-than-expected ties between the research topics mentioned above also instill hope that there is indeed a natural, comprehensive theory of monadic effects, waiting to be fully explicated.},
  isbn = {978-1-4503-0865-6},
  keywords = {computational effects,Monads},
  series = {{{ICFP}} '11}
}

@article{Finn1997Partial,
  title = {Partial {{Functions}} in a {{Total Setting}}},
  author = {Finn, Simon and Fourman, Michael P. and Longley, John},
  year = {1997},
  month = feb,
  volume = {18},
  pages = {85--104},
  issn = {0168-7433, 1573-0670},
  doi = {10.1023/A:1005702928286},
  abstract = {We discuss a scheme for defining and reasoning about partial recursive functions within a classical two-valued logic in which all terms denote. We show how a total extension of the partial function introduced by a recursive declaration may be axiomatized within a classical logic, and illustrate by an example the kind of reasoning that our scheme supports. By presenting a naive set-theoretic semantics, we show that the system we propose is logically consistent. Our work is motivated largely by the pragmatic issues arising from mechanical theorem proving \textendash{} we discuss some of the practical benefits and limitations of our scheme for mechanical verification of software and hardware systems.},
  journal = {Journal of Automated Reasoning},
  language = {en},
  number = {1}
}

@inproceedings{Fiore1999abstract,
  title = {Abstract Syntax and Variable Binding},
  booktitle = {14th {{Symposium}} on {{Logic}} in {{Computer Science}}, 1999. {{Proceedings}}},
  author = {Fiore, M. and Plotkin, G. and Turi, D.},
  year = {1999},
  pages = {193--202},
  doi = {10.1109/LICS.1999.782615},
  abstract = {We develop a theory of abstract syntax with variable binding. To every binding signature we associate a category of models consisting of variable sets endowed with compatible algebra and substitution structures. The syntax generated by the signature is the initial model. This gives a notion of initial algebra semantics encompassing the traditional one; besides compositionality, it automatically verifies the semantic substitution lemma},
  keywords = {abstract syntax,Algebra,Carbon capture and storage,category of models,compatible algebra,Computer languages,Concrete,initial algebra semantics,Integral equations,Logic,process algebra,Production,Proposals,semantic substitution lemma,substitution structures,Tree graphs,variable binding}
}

@article{Fiore2002objects,
  title = {Objects of {{Categories}} as {{Complex Numbers}}},
  author = {Fiore, Marcelo and Leinster, Tom},
  year = {2002},
  month = dec,
  abstract = {In many everyday categories (sets, spaces, modules, ...) objects can be both added and multiplied. The arithmetic of such objects is a challenge because there is usually no subtraction. We prove a family of cases of the following principle: if an arithmetic statement about the objects can be proved by pretending that they are complex numbers, then there also exists an honest proof.},
  archivePrefix = {arXiv},
  eprint = {math/0212377},
  eprinttype = {arxiv},
  journal = {arXiv:math/0212377},
  keywords = {Mathematics - Category Theory,Mathematics - Commutative Algebra,Mathematics - Rings and Algebras}
}

@incollection{Firsov2016Purely,
  title = {Purely {{Functional Incremental Computing}}},
  booktitle = {Programming {{Languages}}},
  author = {Firsov, Denis and Jeltsch, Wolfgang},
  editor = {Castor, Fernando and Liu, Yu David},
  year = {2016},
  month = sep,
  pages = {62--77},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-45279-1_5},
  abstract = {Many applications have to maintain evolving data sources as well as views on these sources. If sources change, the corresponding views have to be adapted. Complete recomputation of views is typically too expensive. An alternative is to convert source changes into view changes and apply these to the views. This is the key idea of incremental computing. In this paper, we use Haskell to develop an incremental computing framework. We illustrate the concepts behind this framework by implementing several example computations on sequences. Our framework allows the user to implement incremental computations using arbitrary monad families that encapsulate mutable state. This makes it possible to use highly efficient algorithms for core computations.},
  copyright = {\textcopyright{}2016 Springer International Publishing Switzerland},
  isbn = {978-3-319-45278-4 978-3-319-45279-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {9889},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Fischer2009purely,
  title = {Purely {{Functional Lazy Non}}-Deterministic {{Programming}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Fischer, Sebastian and Kiselyov, Oleg and Shan, Chung-chieh},
  year = {2009},
  pages = {11--22},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596550.1596556},
  abstract = {Functional logic programming and probabilistic programming have demonstrated the broad benefits of combining laziness (non-strict evaluation with sharing of the results) with non-determinism. Yet these benefits are seldom enjoyed in functional programming, because the existing features for non-strictness, sharing, and non-determinism in functional languages are tricky to combine. We present a practical way to write purely functional lazy non-deterministic programs that are efficient and perspicuous. We achieve this goal by embedding the programs into existing languages (such as Haskell, SML, and OCaml) with high-quality implementations, by making choices lazily and representing data with non-deterministic components, by working with custom monadic data types and search strategies, and by providing equational laws for the programmer to reason about their code.},
  isbn = {978-1-60558-332-7},
  keywords = {call-time choice,continuations,Monads,side effects},
  series = {{{ICFP}} '09}
}

@inproceedings{Fisher1999design,
  title = {The {{Design}} of a {{Class Mechanism}} for {{Moby}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1999 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Fisher, Kathleen and Reppy, John},
  year = {1999},
  pages = {37--49},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/301618.301638},
  abstract = {Typical class-based languages, such as C++ and JAVA, provide complex class mechanisms but only weak module systems. In fact, classes in these languages incorporate many of the features found in richer module mechanisms. In this paper, we describe an alternative approach to designing a language that has both classes and modules. In our design, we rely on a rich ML-style module system to provide features such as visibility control and parameterization, while providing a minimal class mechanism that includes only those features needed to support inheritance. Programmers can then use the combination of modules and classes to implement the full range of class-based features and idioms. Our approach has the advantage that it provides a full-featured module system (useful in its own right), while keeping the class mechanism quite simple.We have incorporated this design in MOBY, which is an ML-style language that supports class-based object-oriented programming. In this paper, we describe our design via a series of simple examples, show how various class-based features and idioms are realized in MOBY, compare our design with others, and sketch its formal semantics.},
  isbn = {1-58113-094-5},
  series = {{{PLDI}} '99}
}

@incollection{Fisher2000extending,
  title = {Extending {{Moby}} with {{Inheritance}}-{{Based Subtyping}}},
  booktitle = {{{ECOOP}} 2000 \textemdash{} {{Object}}-{{Oriented Programming}}},
  author = {Fisher, Kathleen and Reppy, John},
  editor = {Bertino, Elisa},
  year = {2000},
  month = jan,
  pages = {83--107},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Classes play a dual role in mainstream statically-typed object-oriented languages, serving as both object generators and object types. In such languages, inheritance implies subtyping. In contrast, the theoretical language community has viewed this linkage as a mistake and has focused on subtyping relationships determined by the structure of object types, without regard to their underlying implementations. In this paper,we explore why inheritance-based subtyping relations are useful and we present an extension to the MOBY programming language that supports both inheritance-based and structural subtyping relations. In addition, we present a formal accounting of this extension.},
  copyright = {\textcopyright{}2000 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-67660-7 978-3-540-45102-0},
  keywords = {Business Information Systems,Computer Communication Networks,Logics and Meanings of Programs,Programming Techniques,Software Engineering},
  language = {en},
  number = {1850},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Fisher2016miniAdapton,
  title = {{{miniAdapton}}: {{A Minimal Implementation}} of {{Incremental Computation}} in {{Scheme}}},
  shorttitle = {{{miniAdapton}}},
  author = {Fisher, Dakota and Hammer, Matthew A. and Byrd, William and Might, Matthew},
  year = {2016},
  month = sep,
  abstract = {We describe a complete Scheme implementation of miniAdapton, which implements the core functionality of the Adapton system for incremental computation (also known as self-adjusting computation). Like Adapton, miniAdapton allows programmers to safely combine mutation and memoization. miniAdapton is built on top of an even simpler system, microAdapton. Both miniAdapton and microAdapton are designed to be easy to understand, extend, and port to host languages other than Scheme. We also present adapton variables, a new interface in Adapton for variables intended to represent expressions.},
  archivePrefix = {arXiv},
  eprint = {1609.05337},
  eprinttype = {arxiv},
  journal = {arXiv:1609.05337 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Flanagan1993Essence,
  title = {The {{Essence}} of {{Compiling}} with {{Continuations}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1993 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  year = {1993},
  pages = {237--247},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/155090.155113},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the ``continuation''). Since the nai\textasciidieresis{}ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.
A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the nai\textasciidieresis{}ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  isbn = {0-89791-598-4},
  series = {{{PLDI}} '93}
}

@article{Flanagan2004essence,
  title = {The {{Essence}} of {{Compiling}} with {{Continuations}}},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and Felleisen, Matthias},
  year = {2004},
  month = apr,
  volume = {39},
  pages = {502--514},
  issn = {0362-1340},
  doi = {10.1145/989393.989443},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the "continuation"). Since the na{\"i}ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.A thorough analysis of the abstract machine for CPS terms shows that the actions of the code generator invert the na{\"i}ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.},
  journal = {SIGPLAN Not.},
  number = {4}
}

@inproceedings{Flatt2002Composable,
  title = {Composable and {{Compilable Macros}}: {{You Want}} It {{When}}?},
  shorttitle = {Composable and {{Compilable Macros}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Flatt, Matthew},
  year = {2002},
  pages = {72--83},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/581478.581486},
  abstract = {Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.},
  isbn = {978-1-58113-487-2},
  keywords = {language tower,macros,modules},
  series = {{{ICFP}} '02}
}

@inproceedings{Flatt2007adding,
  title = {Adding {{Delimited}} and {{Composable Control}} to a {{Production Programming Environment}}},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Flatt, Matthew and Yu, Gang and Findler, Robert Bruce and Felleisen, Matthias},
  year = {2007},
  pages = {165--176},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1291151.1291178},
  abstract = {Operators for delimiting control and for capturing composable continuations litter the landscape of theoretical programming language research. Numerous papers explain their advantages, how the operators explain each other (or don't), and other aspects of the operators' existence. Production programming languages, however, do not support these operators, partly because their relationship to existing and demonstrably useful constructs - such as exceptions and dynamic binding - remains relatively unexplored. In this paper, we report on our effort of translating the theory of delimited and composable control into a viable implementation for a production system. The report shows how this effort involved a substantial design element, including work with a formal model, as well as significant practical exploration and engineering. The resulting version of PLT Scheme incorporates the expressive combination of delimited and composable control alongside dynamic-wind, dynamic binding, and exception handling. None of the additional operators subvert the intended benefits of existing control operators, so that programmers can freely mix and match control operators.},
  isbn = {978-1-59593-815-2},
  series = {{{ICFP}} '07}
}

@article{Fluet2006Phantom,
  title = {Phantom Types and Subtyping},
  author = {Fluet, Matthew and Pucella, Riccardo},
  year = {2006},
  month = nov,
  volume = {16},
  pages = {751--791},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796806006046},
  abstract = {We investigate a technique from the literature, called the phantom-types technique, that uses parametric polymorphism, type constraints, and unification of polymorphic types to model a subtyping hierarchy. Hindley-Milner type systems, such as the one found in Standard ML, can be used to enforce the subtyping relation, at least for first-order values. We show that this technique can be used to encode any finite subtyping hierarchy (including hierarchies arising from multiple interface inheritance). We formally demonstrate the suitability of the phantom-types technique for capturing first-order subtyping by exhibiting a type-preserving translation from a simple calculus with bounded polymorphism to a calculus embodying the type system of SML.},
  journal = {Journal of Functional Programming},
  number = {6}
}

@article{Forster2017Expressive,
  title = {On the {{Expressive Power}} of {{User}}-Defined {{Effects}}: {{Effect Handlers}}, {{Monadic Reflection}}, {{Delimited Control}}},
  shorttitle = {On the {{Expressive Power}} of {{User}}-Defined {{Effects}}},
  author = {Forster, Yannick and Kammar, Ohad and Lindley, Sam and Pretnar, Matija},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {13:1--13:29},
  issn = {2475-1421},
  doi = {10.1145/3110257},
  abstract = {We compare the expressive power of three programming abstractions for user-defined computational effects: Plotkin and Pretnar's effect handlers, Filinski's monadic reflection, and delimited control without answer-type-modification. This comparison allows a precise discussion about the relative expressiveness of each programming abstraction. It also demonstrates the sensitivity of the relative expressiveness of user-defined effects to seemingly orthogonal language features.   We present three calculi, one per abstraction, extending Levy's call-by-push-value. For each calculus, we present syntax, operational semantics, a natural type-and-effect system, and, for effect handlers and monadic reflection, a set-theoretic denotational semantics. We establish their basic metatheoretic properties: safety, termination, and, where applicable, soundness and adequacy. Using Felleisen's notion of a macro translation, we show that these abstractions can macro-express each other, and show which translations preserve typeability. We use the adequate finitary set-theoretic denotational semantics for the monadic calculus to show that effect handlers cannot be macro-expressed while preserving typeability either by monadic reflection or by delimited control. Our argument fails with simple changes to the type system such as polymorphism and inductive types. We supplement our development with a mechanised Abella formalisation.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {algebraic effects and handlers,call-by-push-value,computational effects,delimited control,Denotational semantics,lambda calculus,language extension,macro expressiveness,monadic reflection,Monads,reify and reflect,shift and reset,type-and-effect systems},
  number = {ICFP}
}

@inproceedings{Foster2005combinators,
  title = {Combinators for {{Bi}}-Directional {{Tree Transformations}}: {{A Linguistic Approach}} to the {{View Update Problem}}},
  shorttitle = {Combinators for {{Bi}}-Directional {{Tree Transformations}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Foster, J. Nathan and Greenwald, Michael B. and Moore, Jonathan T. and Pierce, Benjamin C. and Schmitt, Alan},
  year = {2005},
  pages = {233--246},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1040305.1040325},
  abstract = {We propose a novel approach to the well-known view update problem for the case of tree-structured data: a domain-specific programming language in which all expressions denote bi-directional transformations on trees. In one direction, these transformations---dubbed lenses---map a "concrete" tree into a simplified "abstract view"; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses.We identify a natural space of well-behaved bi-directional transformations over arbitrary structures, study definedness and continuity in this setting, and state a precise connection with the classical theory of "update translation under a constant complement" from databases. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, copying, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bi-directional list-processing transformations as derived forms.},
  isbn = {1-58113-830-X},
  keywords = {bi-directional programming,harmony,lenses,view update problem,XML},
  series = {{{POPL}} '05}
}

@article{Foster2007Combinators,
  title = {Combinators for {{Bidirectional Tree Transformations}}: {{A Linguistic Approach}} to the {{View}}-Update {{Problem}}},
  shorttitle = {Combinators for {{Bidirectional Tree Transformations}}},
  author = {Foster, J. Nathan and Greenwald, Michael B. and Moore, Jonathan T. and Pierce, Benjamin C. and Schmitt, Alan},
  year = {2007},
  month = may,
  volume = {29},
  issn = {0164-0925},
  doi = {10.1145/1232420.1232424},
  abstract = {We propose a novel approach to the view-update problem for tree-structured data: a domain-specific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations---dubbed lenses---map a concrete tree into a simplified abstract view; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses. We begin by identifying a natural space of well-behaved bidirectional transformations over arbitrary structures, studying definedness and continuity in this setting. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe bidirectional transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional list-processing transformations as derived forms. An extended example shows how our combinators can be used to define a lens that translates between a native HTML representation of browser bookmarks and a generic abstract bookmark format.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {Bidirectional programming,harmony,lenses,view update problem,XML},
  number = {3}
}

@inproceedings{Fourtounis2013supporting,
  title = {Supporting {{Separate Compilation}} in a {{Defunctionalizing Compiler}}},
  booktitle = {2nd {{Symposium}} on {{Languages}}, {{Applications}} and {{Technologies}}},
  author = {Fourtounis, Georgios and Papaspyrou, Nikolaos S.},
  editor = {Leal, Jos{\'e} Paulo and Rocha, Ricardo and Sim{\~o}es, Alberto},
  year = {2013},
  volume = {29},
  pages = {39--49},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/OASIcs.SLATE.2013.39},
  isbn = {978-3-939897-52-1},
  series = {{{OpenAccess Series}} in {{Informatics}} ({{OASIcs}})},
  urn = {urn:nbn:de:0030-drops-40294}
}

@article{Fragkoulis2015interactive,
  title = {An Interactive {{SQL}} Relational Interface for Querying Main-Memory Data Structures},
  author = {Fragkoulis, Marios and Spinellis, Diomidis and Louridas, Panos},
  year = {2015},
  month = apr,
  pages = {1--24},
  issn = {0010-485X, 1436-5057},
  doi = {10.1007/s00607-015-0452-y},
  abstract = {Query formalisms and facilities have received significant attention in the past decades resulting in the development of query languages with varying characteristics; many of them resemble sql. Query facilities typically ship as part of database management systems or, sometimes, bundled with programming languages. For applications written in imperative programming languages, database management systems impose an expensive model transformation. In-memory data structures can represent sophisticated relationships in a manner that is efficient in terms of storage and processing overhead, but most general purpose programming languages lack an interpreter and/or an expressive query language for manipulating interactive queries. Issuing interactive ad-hoc queries on program data structures is tough. This work presents a method and an implementation for representing an application's arbitrary imperative programming data model as a queryable relational one. The Pico COllections Query Library (pico ql) uses a domain specific language to define a relational representation of application data structures and an sql interface implementation. Queries are issued interactively and are type safe. We demonstrate our relational representation for objects and the library's usefulness on three large c++ projects. pico ql enhances query expressiveness and boosts productivity compared to querying via traditional programming constructs.},
  journal = {Computing},
  keywords = {68N15,Artificial Intelligence (incl. Robotics),Computer Appl. in Administrative Data Processing,Computer Communication Networks,Computer Science; general,Information Systems Applications (incl. Internet),Interactive,Main-memory,Object,Query,Software Engineering,sql},
  language = {en}
}

@inproceedings{Frandsen2006singular,
  title = {A {{Singular Choice}} for {{Multiple Choice}}},
  booktitle = {Working {{Group Reports}} on {{ITiCSE}} on {{Innovation}} and {{Technology}} in {{Computer Science Education}}},
  author = {Frandsen, Gudmund S. and Schwartzbach, Michael I.},
  year = {2006},
  pages = {34--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1189215.1189164},
  abstract = {How should multiple choice tests be scored and graded, in particular when students are allowed to check several boxes to convey partial knowledge? Many strategies may seem reasonable, but we demonstrate that five self-evident axioms are sufficient to determine completely the correct strategy. We also discuss how to measure robustness of the obtained grades. Our results have practical advantages and also suggest criteria for designing multiple choice questions.},
  isbn = {1-59593-603-3},
  keywords = {grading scales,multiple choice,scoring strategies,theory},
  series = {{{ITiCSE}}-{{WGR}} '06}
}

@inproceedings{Freyd1990Recursive,
  title = {Recursive Types Reduced to Inductive Types},
  booktitle = {[1990] {{Proceedings}}. {{Fifth Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Freyd, P.},
  year = {1990},
  month = jun,
  pages = {498--507},
  doi = {10.1109/LICS.1990.113772},
  abstract = {A setting called complete partial ordering (CPO) categories and the notion of dialgebra are described. Free dialgebras on CPO-categories are shown to be the same as minimal invariant objects. In the case that the bifunctor is independent of its contravariant variable (hence construable as a covariant functor), it is shown that minimal invariant objects serve simultaneously as initial algebras and final coalgebras. The reduction to inductive types is shown in a two-step process. First let T be a bifunctor contravariant in its first variable, convariant in the second. For each A it is possible to consider the convariant functor that sends X to TAX. If FA denotes a minimal invariant object of this covariant functor, one for each A, then F becomes a contrainvariant functor. It is shown that the minimal invariant objects of F are minimal invariant objects of the original bifunctor T. Secondly, let T be a contrainvariant functor. It is shown that the square of the functor (necessarily covariant) has the same minimal invariant objects.{$<$}{$>$}},
  keywords = {Algebra,bifunctor,complete partial ordering,contravariant variable,covariant functor,CPO-categories,data structures,dialgebras,final coalgebras,formal logic,inductive types,initial algebras,minimal invariant object,minimal invariant objects,Upper bound}
}

@inproceedings{Friedman1984reification,
  title = {Reification: {{Reflection Without Metaphysics}}},
  shorttitle = {Reification},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  author = {Friedman, Daniel P. and Wand, Mitchell},
  year = {1984},
  pages = {348--355},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800055.802051},
  abstract = {We consider how the data structures of an interpreter may be made available to the program it is running, and how the program may alter its interpreter's structures. We refer to these processes as reification and reflection. We show how these processes may be considered as an extension of the fexpr concept in which not only the form and the environment, but also the continuation, are made available to the body of the procedure. We show how such a construct can be used to effectively add an unlimited variety of ``special forms'' to a very small base language. We consider some trade-offs in how interpreter objects are reified. Our version of this construct is similar to one in 3-Lisp [Smith 82, 84], but is independent of the rest of 3-Lisp. In particular, it does not rely on the notion of a tower of interpreters.},
  isbn = {0-89791-142-3},
  series = {{{LFP}} '84}
}

@inproceedings{Frisby2012pattern,
  title = {A {{Pattern}} for {{Almost Homomorphic Functions}}},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Frisby, Nicolas and Gill, Andy and Alexander, Perry},
  year = {2012},
  pages = {1--12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364394.2364396},
  abstract = {Modern type systems present the programmer with a trade-off between correctness and code complexity--more precise, or exact, types that allow only legal values prevent runtime errors while less precise types enable more reuse. Unfortunately, the software engineering benefits of reuse and avoiding duplicate code currently outweigh assurance gains of exact types. We factor out a pattern common in conversions that result from using exact types as a reusable function, extending existing generic programming techniques to avoid code duplication and enable reuse.},
  isbn = {978-1-4503-1576-0},
  keywords = {data types,generic programming,type invariants,type-level programming},
  series = {{{WGP}} '12}
}

@inproceedings{Fu2014Self,
  title = {Self {{Types}} for {{Dependently Typed Lambda Encodings}}},
  booktitle = {Rewriting and {{Typed Lambda Calculi}}},
  author = {Fu, Peng and Stump, Aaron},
  editor = {Dowek, Gilles},
  year = {2014},
  pages = {224--239},
  publisher = {{Springer International Publishing}},
  abstract = {We revisit lambda encodings of data, proposing new solutions to several old problems, in particular dependent elimination with lambda encodings. We start with a type-assignment form of the Calculus of Constructions, restricted recursive definitions and Miquel's implicit product. We add a type construct {$\iota$}x.T, called a self type, which allows T to refer to the subject of typing. We show how the resulting System S with this novel form of dependency supports dependent elimination with lambda encodings, including induction principles. Strong normalization of S is established by defining an erasure from S to a version of F {$\omega$} with positive recursive type definitions, which we analyze. We also prove type preservation for S.},
  isbn = {978-3-319-08918-8},
  keywords = {Induction Principle,Lambda Calculus,Strong Normalization,Type Construct,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Fulop2012survey,
  title = {A Survey of Proof Nets and Matrices for Substructural Logics},
  author = {Fulop, Sean A.},
  year = {2012},
  month = mar,
  abstract = {This paper is a survey of two kinds of "compressed" proof schemes, the \textbackslash{}emph\{matrix method\} and \textbackslash{}emph\{proof nets\}, as applied to a variety of logics ranging along the substructural hierarchy from classical all the way down to the nonassociative Lambek system. A novel treatment of proof nets for the latter is provided. Descriptions of proof nets and matrices are given in a uniform notation based on sequents, so that the properties of the schemes for the various logics can be easily compared.},
  archivePrefix = {arXiv},
  eprint = {1203.4912},
  eprinttype = {arxiv},
  journal = {arXiv:1203.4912 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@article{Futamura1999partial,
  title = {Partial {{Evaluation}} of {{Computation Process}}--{{An Approach}} to a {{Compiler}}-{{Compiler}}},
  author = {Futamura, Yoshihiko},
  year = {1999},
  month = dec,
  volume = {12},
  pages = {381--391},
  issn = {1388-3690, 1573-0557},
  doi = {10.1023/A:1010095604496},
  abstract = {This paper reports the relationship between formal description of semantics (i.e., interpreter) of a programming language and an actual compiler. The paper also describes a method to automatically generate an actual compiler from a formal description which is, in some sense, the partial evaluation of a computation process. The compiler-compiler inspired by this method differs from conventional ones in that the compiler-compiler based on our method can describe an evaluation procedure (interpreter) in defining the semantics of a programming language, while the conventional one describes a translation process.},
  journal = {Higher-Order and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),compiler,Futamura projections,interpreter,Numeric Computing,partial evaluation,program transformation,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {4}
}

@inproceedings{Gabriel2012structure,
  title = {The {{Structure}} of a {{Programming Language Revolution}}},
  booktitle = {Proceedings of the {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {Gabriel, Richard P.},
  year = {2012},
  pages = {195--214},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2384592.2384611},
  abstract = {Engineering often precedes science. Incommensurability is real.},
  isbn = {978-1-4503-1562-3},
  keywords = {engineering,incommensurability,paradigms,science},
  series = {Onward! '12}
}

@article{Gallier1989Girard,
  title = {On {{Girard}}'s "{{Candidats}} de {{Reductibilit{\'e}}}"},
  author = {Gallier, Jean},
  year = {1989},
  month = nov,
  journal = {Technical Reports (CIS)}
}

@article{Gallier1995proving,
  title = {Proving Properties of Typed {$\lambda$}-Terms Using Realizability, Covers, and Sheaves},
  author = {Gallier, Jean},
  year = {1995},
  month = may,
  volume = {142},
  pages = {299--368},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(94)00280-0},
  abstract = {The main purpose of this paper is to take apart the reducibility method in order to understand how its pieces fit together, and in particular, to recast the conditions on candidates of reducibility as sheaf conditions. There has been a feeling among experts on this subject that it should be possible to present the reducibility method using more semantic means, and that a deeper understanding would then be gained. This paper gives mathematical substance to this feeling, by presenting a generalization of the reducibility method based on a semantic notion of realizability which uses the notion of a cover algebra (as in abstract sheaf theory). A key technical ingredient is the introduction of a new class of semantic structures equipped with preorders, called pre-applicative structures. These structures need not be extensional. In this framework, a general realizability theorem can be shown. Kleene's recursive realizability and a variant of Kreisel's modified realizability both fit into this framework. We are then able to prove a metatheorem which shows that if a property of realizers satisfies some simple conditions, then it holds for the semantic interpretations of all terms. Applying this theorem to the special case of the term model, yields a general theorem for proving properties of typed {$\lambda$}-terms, in particular, strong normalization and confluence. This approach clarifies the reducibility method by showing that the closure conditions on candidates of reducibility can be viewed as sheaf conditions. The above approach is applied to the simply-typed {$\lambda$}-calculus (with types \textrightarrow, \texttimes, +, and {$\perp$}), and to the second-order (polymorphic) {$\lambda$}-calculus (with types \textrightarrow{} and {$\forall$}2), for which it yields a new theorem.},
  journal = {Theoretical Computer Science},
  number = {2}
}

@article{Gapeyev2002recursive,
  title = {Recursive Subtyping Revealed},
  author = {Gapeyev, Vladimir and Levin, Michael Y. and Pierce, Benjamin C.},
  year = {2002},
  volume = {12},
  pages = {511--548},
  issn = {1469-7653},
  doi = {10.1017/S0956796802004318},
  abstract = {Algorithms for checking subtyping between recursive types lie at the core of many programming language implementations. But the fundamental theory of these algorithms and how they relate to simpler declarative specifications is not widely understood, due in part to the difficulty of the available introductions to the area. This tutorial paper offers an `end-to-end' introduction to recursive types and subtyping algorithms, from basic theory to efficient implementation, set in the unifying mathematical framework of coinduction.},
  journal = {Journal of Functional Programming},
  number = {06}
}

@article{Garrigue1998programming,
  title = {Programming with Polymorphic Variants},
  author = {Garrigue, J},
  year = {1998},
  abstract = {Type inference for structural polymorphism ---i.e. record and variant polymorphism--- has been an active area of research since more than 10 years ago, and many results have been obtained. However these results are yet to be applied to real programming languages. Based on our experience with the Objective Label system, we describe how variant polymorphism can be integrated in a programming language, and what are the benefits. We give a detailed account of our type inference and compilation...},
  keywords = {pattern-matching,polymorphic-variants,polymorphism,structural-types,subtyping,type-inference}
}

@article{Garrigue2000code,
  title = {Code Reuse through Polymorphic Variants},
  author = {Garrigue, J},
  year = {2000},
  abstract = {Their support for code reuse has made object-oriented languages popular. However,

they do not succeed equally in all areas, particularly when data has a complex

structure, making hard to keep the parallel between data and code.

On the other hand, functional programming languages, which separate data from

code, are better at handling complex structures, but they do not provide direct ways

to reuse code for a different datatype.

We show here a way to achieve code reuse, even when data and...},
  keywords = {expression-problem,extensibility,polymorphic-variants,polymorphism}
}

@inproceedings{Gasiunas2007Dependent,
  title = {Dependent {{Classes}}},
  booktitle = {Proceedings of the 22nd {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}} and {{Applications}}},
  author = {Gasiunas, Vaidas and Mezini, Mira and Ostermann, Klaus},
  year = {2007},
  pages = {133--152},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1297027.1297038},
  abstract = {Virtual classes allow nested classes to be refined in subclasses. In this way nested classes can be seen as dependent abstractions of the objects of the enclosing classes. Expressing dependency via nesting, however, has two limitations: Abstractions that depend on more than one object cannot be modeled and a class must know all classes that depend on its objects. This paper presents dependent classes, a generalization of virtual classes that expresses similar semantics by parameterization rather than by nesting. This increases expressivity of class variations as well as the flexibility of their modularization. Besides, dependent classes complement multimethods in scenarios where multi-dispatched abstractions rather than multi-dispatched methods are needed. They can also be used to express more precise signatures of multimethods and even extend their dispatch semantics. We present a formal semantics of dependent classes and a machine-checked type soundness proof in Isabelle/HOL [29], the first of this kind for a language with virtual classes and path-dependent types.},
  isbn = {978-1-59593-786-5},
  keywords = {dependent classes,dynamic dispatch,multimethods,multiple dispatch,variability,virtual classes},
  series = {{{OOPSLA}} '07}
}

@inproceedings{Gerakios2013forsaking,
  title = {Forsaking {{Inheritance}}: {{Supercharged Delegation}} in {{DelphJ}}},
  shorttitle = {Forsaking {{Inheritance}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN International Conference}} on {{Object Oriented Programming Systems Languages}} \&\#38; {{Applications}}},
  author = {Gerakios, Prodromos and Biboudis, Aggelos and Smaragdakis, Yannis},
  year = {2013},
  pages = {233--252},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2509136.2509535},
  abstract = {We propose DelphJ: a Java-based OO language that eschews inheritance completely, in favor of a combination of class morphing and (deep) delegation. Compared to past delegation approaches, the novel aspect of our design is the ability to emulate the best aspects of inheritance while retaining maximum flexibility: using morphing, a class can select any of the methods of its delegatee and export them (if desired) or transform them (e.g., to add extra arguments or modify type signatures), yet without needing to name these methods explicitly and handle them one-by-one. Compared to past work on morphing, our approach adopts and adapts advanced delegation mechanisms, in order to add late binding capabilities and, thus, provide a full substitute of inheritance. Additionally, we explore complex semantic issues in the interaction of delegation with late binding. We present our language design both informally, with numerous examples, and formally in a core calculus.},
  isbn = {978-1-4503-2374-1},
  keywords = {delegation,language extensions,meta-programming,morphing,object composition,static reflection},
  series = {{{OOPSLA}} '13}
}

@inproceedings{Geuvers1994churchrosser,
  title = {On the {{Church}}-{{Rosser}} Property for Expressive Type Systems and Its Consequences for Their Metatheoretic Study},
  booktitle = {, {{Symposium}} on {{Logic}} in {{Computer Science}}, 1994. {{LICS}} '94. {{Proceedings}}},
  author = {Geuvers, H. and Werner, B.},
  year = {1994},
  month = jul,
  pages = {320--329},
  doi = {10.1109/LICS.1994.316057},
  abstract = {We consider two alternative definitions for the conversion rule in pure type systems. We study the consequences of this choice for the metatheory and point out the related implementation issues. We relate two open problems by showing that if a PTS allows the construction of a fixed point combinator, then Church-Rosser for {$\beta\eta$}-reduction fails. We present a new formalization of Russell's paradox in a slight extension of Martin-Lof's inconsistent theory with Type:Type and show that the resulting term leads to a fix-point construction. The main consequence is that the corresponding system is non-confluent. This example shows that in some typed {$\lambda$}-calculi, the Church-Rosser proof for the {$\beta\eta$}-reduction is not purely combinatorial anymore, as in pure {$\lambda$}-calculus, but relies on the normalization and thus the logical consistency of the system},
  keywords = {Buildings,Calculus,Church-Rosser property,Computer science,conversion rule,expressive type systems,fixed point combinator,formal logic,implementation issues,lambda calculus,logical consistency,Mathematical model,Mathematics,metatheoretic study,metatheory,pure type systems,theorem proving,type theory}
}

@article{Geuvers2001Inconsistency,
  title = {Inconsistency of Classical Logic in Type Theory},
  author = {Geuvers, Herman},
  year = {2001},
  month = dec,
  abstract = {Introduction In this note, we show the inconsistency of a strong version of classical logic in the type theory of Coq. More precisely, we show that from the assumption 8A:PropfAg + f:Ag, we can derive ?. The type 8A::PropfAg + f:Ag (in Coq notation: (A:Prop)\{A\}+\{\textasciitilde{}A\}) is more powerful than the `ordinary classical axiom 8A::PropA \_ :A (in Coq notation: (A:Prop)A / \textasciitilde{}A), because it allows to dene a function to a Set type by cases. This is precisely one of the crucial steps in the proof of the inconsistency: the construction of two functions b2P : bool!Prop and P2b :}
}

@article{Ghelli1997Termination,
  title = {Termination of {{System F}}-Bounded: {{A Complete Proof}}},
  shorttitle = {Termination of {{SystemF}}-Bounded},
  author = {Ghelli, Giorgio},
  year = {1997},
  month = nov,
  volume = {139},
  pages = {39--56},
  issn = {0890-5401},
  doi = {10.1006/inco.1997.2662},
  abstract = {System F-bounded is a second-order typed {$\lambda$}-calculus with subtyping which has been defined to carry out foundational studies about the type systems of object-oriented languages. The almost recursive nature of the essential feature of this system makes one wonder whether it retains the strong normalization property, with respect to first- and second-order {$\beta\eta$}-reduction of system F{$\leqslant$}. We prove that this is the case. The proof is carried out to the last detail to allow the reader to be convinced of its correctness.},
  journal = {Information and Computation},
  keywords = {_tablet},
  number = {1}
}

@inproceedings{Giarrusso2013Open,
  title = {Open {{GADTs}} and Declaration-Site Variance: {{A}} Problem Statement},
  shorttitle = {Open Gadts and Declaration-Site Variance},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Scala}}},
  author = {Giarrusso, Paolo G.},
  year = {2013},
  pages = {5:1--5:4},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2489837.2489842},
  abstract = {Generalized algebraic data types (GADTs) allow embedding extensible typed ASTs and transformations on them. Such transformations on typed ASTs are useful for code optimization in deeply embedded DSLs, for instance when using Lightweight Modular Staging (LMS). However, in Scala it is hard to make transformations for typed ASTs type-safe. Therefore, AST transformations in LMS are often not fully typechecked, preventing bugs from being caught early and without extensive testing. We show that writing type-safe transformations in such embeddings is in fact not just hard, but impossible without using unsafe casts or significantly restricting extensibility: Declaration-site variance opens GADTs representing typed ASTs not only to desirable extensions, but also to extensions that introduce exotic terms. We make the problem concrete on an embedding of {$\lambda{}<$}: through covariant GADTs. We discuss solution approaches, and sketch a Scala extension to address this problem without either introducing unsafe casts or restricting extensibility. We believe a complete solution would significantly ease writing transformations by allowing type-checking to verify them, and thus would ease their development.},
  isbn = {978-1-4503-2064-1},
  keywords = {DSL embedding,Lambda-calculus,lightweight modular staging,Scala,soundness,Type-safety},
  series = {{{SCALA}} '13}
}

@inproceedings{Giarrusso2019Incremental,
  title = {Incremental {$\lambda$}-Calculus in Cache-Transfer Style},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Giarrusso, Paolo G. and {R{\'e}gis-Gianas}, Yann and Schuster, Philipp},
  editor = {Caires, Lu{\'i}s},
  year = {2019},
  pages = {553--580},
  publisher = {{Springer International Publishing}},
  abstract = {Incremental computation requires propagating changes and reusing intermediate results of base computations. Derivatives, as produced by static differentiation [7], propagate changes but do not reuse intermediate results, leading to wasteful recomputation. As a solution, we introduce conversion to Cache-Transfer-Style, an additional program transformations producing purely incremental functional programs that create and maintain nested tuples of intermediate results. To prove CTS conversion correct, we extend the correctness proof of static differentiation from STLC to untyped {$\mathsl{\lambda}\lambda\backslash$}lambda -calculus via step-indexed logical relations, and prove sound the additional transformation via simulation theorems.To show ILC-based languages can improve performance relative to from-scratch recomputation, and that CTS conversion can extend its applicability, we perform an initial performance case study. We provide derivatives of primitives for operations on collections and incrementalize selected example programs using those primitives, confirming expected asymptotic speedups.},
  isbn = {978-3-030-17184-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Gibbons2007datatypegeneric,
  title = {Datatype-{{Generic Programming}}},
  booktitle = {Datatype-{{Generic Programming}}},
  author = {Gibbons, Jeremy},
  editor = {Backhouse, Roland and Gibbons, Jeremy and Hinze, Ralf and Jeuring, Johan},
  year = {2007},
  pages = {1--71},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Generic programming aims to increase the flexibility of programming languages, by expanding the possibilities for parametrization \textemdash{} ideally, without also expanding the possibilities for uncaught errors. The term means different things to different people: parametric polymorphism, data abstraction, meta-programming, and so on. We use it to mean polytypism, that is, parametrization by the shape of data structures rather than their contents. To avoid confusion with other uses, we have coined the qualified term datatype-generic programming for this purpose. In these lecture notes, we expand on the definition of datatype-generic programming, and present some examples of datatype-generic programs. We also explore the connection with design patterns in object-oriented programming; in particular, we argue that certain design patterns are just higher-order datatype-generic programs.},
  copyright = {\textcopyright{}2007 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-76785-5 978-3-540-76786-2},
  keywords = {Data Structures,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {4719},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Gibbons2014folding,
  title = {Folding {{Domain}}-Specific {{Languages}}: {{Deep}} and {{Shallow Embeddings}} ({{Functional Pearl}})},
  shorttitle = {Folding {{Domain}}-Specific {{Languages}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Gibbons, Jeremy and Wu, Nicolas},
  year = {2014},
  pages = {339--347},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628138},
  abstract = {A domain-specific language can be implemented by embedding within a general-purpose host language. This embedding may be deep or shallow, depending on whether terms in the language construct syntactic or semantic representations. The deep and shallow styles are closely related, and intimately connected to folds; in this paper, we explore that connection.},
  isbn = {978-1-4503-2873-9},
  keywords = {deep and shallow embedding,domain-specific languages,folds},
  series = {{{ICFP}} '14}
}

@inproceedings{Gifford1986Integrating,
  title = {Integrating Functional and Imperative Programming},
  booktitle = {Proceedings of the 1986 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Gifford, David K. and Lucassen, John M.},
  year = {1986},
  pages = {28--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/319838.319848},
  isbn = {978-0-89791-200-6},
  series = {{{LFP}} '86}
}

@inproceedings{Gil2009Are,
  title = {Are {{We Ready}} for a {{Safer Construction Environment}}?},
  booktitle = {{{ECOOP}} 2009 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Gil, Joseph (Yossi) and Shragai, Tali},
  editor = {Drossopoulou, Sophia},
  year = {2009},
  pages = {495--519},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The semantics of many OO languages dictates that the constructor of a derived class is a refining extension of one of the base class constructors. As this base constructor runs, it may invoke dynamically bound methods which are overridden in the derived class. These invocations receive an ``half baked object'', i.e., an object whose derived class portion is uninitialized. Such a situation may lead to confusing semantics and to hidden coupling between the base and the derived. Dynamic binding within constructors also makes it difficult to enhance the programming language with advanced mechanisms for expressing design intent, such as non-null annotation (denoting reference values which can never be null), read-only annotation for fields and variables (expressing the intention that these cannot be modified after they are completely created) and class invariants (part of the famous design by contract methodology). A read-only field for example becomes immutable only after the creation of the enclosing object is complete.},
  isbn = {978-3-642-03013-0},
  keywords = {Base Class,Dynamic Binding,Initializer Object,Manual Inspection,Polymorphic Behavior},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Gilbert2019Definitional,
  title = {Definitional {{Proof}}-Irrelevance {{Without K}}},
  author = {Gilbert, Ga{\"e}tan and Cockx, Jesper and Sozeau, Matthieu and Tabareau, Nicolas},
  year = {2019},
  month = jan,
  volume = {3},
  pages = {3:1--3:28},
  issn = {2475-1421},
  doi = {10.1145/3290316},
  abstract = {Loading...},
  journal = {Proc. ACM Program. Lang.},
  keywords = {proof assistants,proof irrelevance,type theory},
  number = {POPL}
}

@inproceedings{Gill2009typesafe,
  title = {Type-Safe {{Observable Sharing}} in {{Haskell}}},
  booktitle = {Proceedings of the {{2Nd ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Gill, Andy},
  year = {2009},
  pages = {117--128},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596638.1596653},
  abstract = {Haskell is a great language for writing and supporting embedded Domain Specific Languages (DSLs). Some form of observable sharing is often a critical capability for allowing so-called deep DSLs to be compiled and processed. In this paper, we describe and explore uses of an IO function for reification which allows direct observation of sharing.},
  isbn = {978-1-60558-508-6},
  keywords = {DSL compilation,observable sharing},
  series = {Haskell '09}
}

@article{Gill2014domainspecific,
  title = {Domain-Specific {{Languages}} and {{Code Synthesis Using Haskell}}},
  author = {Gill, Andy},
  year = {2014},
  month = apr,
  volume = {12},
  pages = {30:30--30:43},
  issn = {1542-7730},
  doi = {10.1145/2611429.2617811},
  abstract = {Looking at embedded DSLs},
  journal = {Queue},
  number = {4}
}

@inproceedings{Gill2015remote,
  title = {The {{Remote Monad Design Pattern}}},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Gill, Andy and Sculthorpe, Neil and Dawson, Justin and Eskilson, Aleksander and Farmer, Andrew and Grebe, Mark and Rosenbluth, Jeffrey and Scott, Ryan and Stanton, James},
  year = {2015},
  pages = {59--70},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2804302.2804311},
  abstract = {Remote Procedure Calls are expensive. This paper demonstrates how to reduce the cost of calling remote procedures from Haskell by using the remote monad design pattern, which amortizes the cost of remote calls. This gives the Haskell community access to remote capabilities that are not directly supported, at a surprisingly inexpensive cost. We explore the remote monad design pattern through six models of remote execution patterns, using a simulated Internet of Things toaster as a running example. We consider the expressiveness and optimizations enabled by each remote execution model, and assess the feasibility of our approach. We then present a full-scale case study: a Haskell library that provides a Foreign Function Interface to the JavaScript Canvas API. Finally, we discuss existing instances of the remote monad design pattern found in Haskell libraries.},
  isbn = {978-1-4503-3808-0},
  keywords = {Design Pattern,FFI,Monads,Remote Procedure Call},
  series = {Haskell 2015}
}

@article{Glew2013Formalisation,
  title = {Formalisation of the Lambda Aleph {{Runtime}}},
  author = {Glew, Neal and Sweeney, Tim and Petersen, Leaf},
  year = {2013},
  month = jul,
  abstract = {In previous work we describe a novel approach to dependent typing based on a multivalued term language. In this technical report we formalise the runtime, a kind of operational semantics, for that language. We describe a fairly comprehensive core language, and then give a small-step operational semantics based on an abstract machine. Errors are explicit in the semantics. We also prove several simple properties: that every non-terminated machine state steps to something and that reduction is deterministic once input is fixed.},
  archivePrefix = {arXiv},
  eprint = {1307.5277},
  eprinttype = {arxiv},
  journal = {arXiv:1307.5277 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Godefroid2005DART,
  title = {{{DART}}: {{Directed Automated Random Testing}}},
  shorttitle = {{{DART}}},
  booktitle = {Proceedings of the 2005 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  year = {2005},
  pages = {213--223},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1065010.1065036},
  abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  isbn = {978-1-59593-056-9},
  keywords = {automated test generation,interfaces,program verification,random testing,software testing},
  series = {{{PLDI}} '05}
}

@article{Goguen1977initial,
  title = {Initial {{Algebra Semantics}} and {{Continuous Algebras}}},
  author = {Goguen, J. A. and Thatcher, J. W. and Wagner, E. G. and Wright, J. B.},
  year = {1977},
  month = jan,
  volume = {24},
  pages = {68--95},
  issn = {0004-5411},
  doi = {10.1145/321992.321997},
  abstract = {Many apparently divergent approaches to specifying formal semantics of programming languages are applications of initial algebra semantics. In this paper an overview of initial algebra semantics is provided. The major technical feature is an initial continuous algebra which permits unified algebraic treatment of iterative and recursive semantic features in the same framework as more basic operations.},
  journal = {J. ACM},
  number = {1}
}

@inproceedings{Goguen1989what,
  title = {What Is {{Unification}}? {{A Categorical View}} of {{Substitution}}, {{Equation}} and {{Solution}}},
  shorttitle = {What Is {{Unification}}?},
  booktitle = {Resolution of {{Equations}} in {{Algebraic Structures}}, {{Volume}} 1: {{Algebraic Techniques}}},
  author = {Goguen, Joseph A.},
  year = {1989},
  pages = {217--261},
  publisher = {{Academic}},
  abstract = {From a general perspective, a substitution is a transformation from one space to another, an equation is a pair of such substitutions, and a solution  to an equation is a substitution that yields the same value when composed with (i.e., when substituted into) the substitutions that constitute the given equation. In some special cases, solutions are called unifiers. Other examples include Scott domain equations, unification grammars, type inference, and differential equations. The intuition that the composition of substitutions should be associative when defined, and should have identities, motivates a general concept of substitution system based on category theory. Notions of morphism, congruence, and quotient are given for substitution systems, each with the expected properties, and some general cardinality bounds are proved for most general solution sets (which are minimal sets of solutions with the property that any other solution is a substitution instance of one in the set). The notions of equation and solution are also generalized to systems of equations, i.e., to constraint solving, and applied t oclarify the notions of "compositionality" and "unification" in linguistic unification grammar. This paper is self-contained as regards category theory, and indeed, could be used as an introductory tutorial on that subject.}
}

@article{Goguen1994Typed,
  title = {A {{Typed Operational Semantics}} for {{Type Theory}}},
  author = {Goguen, Healfdene},
  year = {1994},
  month = jul,
  abstract = {Untyped reduction provides a natural operational semantics for type theory. Normalization results say that such a semantics is sound. However, this reduction does not take type information into account and gives no information about canonical forms for terms. We introduce a new operational semantics, which we call typed operational semantics, which defines a reduction to normal form for terms which are well-typed in the type theory. 
 
The central result of the thesis is soundness of the typed operational semantics for the original system. Completeness of the semantics is straightforward. We demonstrate that this equivalence between the declarative and operational presentations of type theory has important metatheoretic consequences: results such as strengthening, subject reduction and strong normalization follow by straightforward induction on derivations in the new system. 
 
We introduce these ideas in the setting of the simply typed lambda calculus. We then extend the techniques to Luo's system UTT, which is Martin-L{\"o}f's Logical Framework extended by a general mechanism for inductive types, a predicative universe and an impredicative universe of propositions. We also give a proof-irrelevant set-theoretic semantics for UTT.},
  language = {en}
}

@incollection{Goguen2006Eliminating,
  title = {Eliminating {{Dependent Pattern Matching}}},
  booktitle = {Algebra, {{Meaning}}, and {{Computation}}: {{Essays}} Dedicated to {{Joseph A}}. {{Goguen}} on the {{Occasion}} of {{His}} 65th {{Birthday}}},
  author = {Goguen, Healfdene and McBride, Conor and McKinna, James},
  editor = {Futatsugi, Kokichi and Jouannaud, Jean-Pierre and Meseguer, Jos{\'e}},
  year = {2006},
  pages = {521--540},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11780274_27},
  abstract = {This paper gives a reduction-preserving translation from Coquand's dependent pattern matching [4] into a traditional type theory [11] with universes, inductive types and relations and the axiom K [22]. This translation serves as a proof of termination for structurally recursive pattern matching programs, provides an implementable compilation technique in the style of functional programming languages, and demonstrates the equivalence with a more easily understood type theory.},
  isbn = {978-3-540-35464-2},
  keywords = {Inductive Type,Logical Framework,Pattern Match,Recursive Call,Type Theory},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Goltz2014design,
  title = {Design for Future: Managed Software Evolution},
  shorttitle = {Design for Future},
  author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M{\"a}rtin, Lukas and {Vogel-Heuser}, Birgit},
  year = {2014},
  month = oct,
  pages = {1--11},
  issn = {1865-2034, 1865-2042},
  doi = {10.1007/s00450-014-0273-9},
  abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future\textemdash{}Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
  journal = {Computer Science - Research and Development},
  keywords = {Co-evolution,Computer Hardware,Computer Science; general,Computer Systems Organization and Communication Networks,Data Structures; Cryptology and Information Theory,Design; maintenance and operation,Knowledge carrying software,Legacy systems,Software Engineering/Programming and Operating Systems,Software life cycle,Theory of Computation},
  language = {en}
}

@article{Goncharov2018Metalanguage,
  title = {A {{Metalanguage}} for {{Guarded Iteration}}},
  author = {Goncharov, Sergey and Rauch, Christoph and Schr{\"o}der, Lutz},
  year = {2018},
  month = jul,
  abstract = {Notions of guardedness serve to delineate admissible recursive definitions in various settings in a compositional manner. In recent work, we have introduced an axiomatic notion of guardedness in symmetric monoidal categories, which serves as a unifying framework for various examples from program semantics, process algebra, and beyond. In the present paper, we propose a generic metalanguage for guarded iteration based on combining this notion with the fine-grain call-by-value paradigm, which we intend as a unifying programming language for guarded and unguarded iteration in the presence of computational effects. We give a generic (categorical) semantics of this language over a suitable class of strong monads supporting guarded iteration, and show it to be in touch with the standard operational behaviour of iteration by giving a concrete big-step operational semantics for a certain specific instance of the metalanguage and establishing adequacy for this case.},
  archivePrefix = {arXiv},
  eprint = {1807.11256},
  eprinttype = {arxiv},
  journal = {arXiv:1807.11256 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@inproceedings{Gonthier1992geometry,
  title = {The {{Geometry}} of {{Optimal Lambda Reduction}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Gonthier, Georges and Abadi, Mart{\'i}n and L{\'e}vy, Jean-Jacques},
  year = {1992},
  pages = {15--26},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/143165.143172},
  abstract = {Lamping discovered an optimal graph-reduction implementation of the \&lgr;-calculus. Simultaneously, Girard invented the geometry of interaction, a mathematical foundation for operational semantics. In this paper, we connect and explain the geometry of interaction and Lamping's graphs. The geometry of interaction provides a suitable semantic basis for explaining and improving Lamping's system. On the other hand, graphs similar to Lamping's provide a concrete representation of the geometry of interaction. Together, they offer a new understanding of computation, as well as ideas for efficient and correct implementations.},
  isbn = {0-89791-453-8},
  series = {{{POPL}} '92}
}

@inproceedings{Gonthier2011how,
  title = {How to {{Make Ad Hoc Proof Automation Less Ad Hoc}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Gonthier, Georges and Ziliani, Beta and Nanevski, Aleksandar and Dreyer, Derek},
  year = {2011},
  pages = {163--175},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2034773.2034798},
  abstract = {Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself. We propose a novel approach to proof automation in Coq that allows the user to specify the behavior of custom automated routines in terms of Coq's own type system. Our approach involves a sophisticated application of Coq's canonical structures, which generalize Haskell type classes and facilitate a flexible style of dependently-typed logic programming. Specifically, just as Haskell type classes are used to infer the canonical implementation of an overloaded term at a given type, canonical structures can be used to infer the canonical proof of an overloaded lemma for a given instantiation of its parameters. We present a series of design patterns for canonical structure programming that enable one to carefully and predictably coax Coq's type inference engine into triggering the execution of user-supplied algorithms during unification, and we illustrate these patterns through several realistic examples drawn from Hoare Type Theory. We assume no prior knowledge of Coq and describe the relevant aspects of Coq type inference from first principles.},
  isbn = {978-1-4503-0865-6},
  keywords = {canonical structures,coq,custom proof automation,hoare type theory,interactive theorem proving,tactics,type classes},
  series = {{{ICFP}} '11}
}

@article{Gonthier2013how,
  title = {How to Make Ad Hoc Proof Automation Less Ad Hoc},
  author = {Gonthier, Georges and Ziliani, Beta and Nanevski, Aleksandar and Dreyer, Derek},
  year = {2013},
  volume = {23},
  pages = {357--401},
  doi = {10.1017/S0956796813000051},
  abstract = {Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself.},
  journal = {Journal of Functional Programming},
  number = {Special Issue 04}
}

@techreport{Gonthier2015Small,
  title = {A {{Small Scale Reflection Extension}} for the {{Coq}} System},
  author = {Gonthier, Georges and Mahboubi, Assia and Tassi, Enrico},
  year = {2015},
  institution = {{Inria Saclay Ile de France}},
  abstract = {This is the user manual of Ssreflect, a set of extensions to the proof scripting language of the Coq proof assistant. While these extensions were developed to support a particular proof methodology - small-scale reflection - most of them actually are of a quite general nature, improving the functionality of Coq in basic areas such as script layout and structuring, proof context management, and rewriting. Consequently, and in spite of the title of this document, most of the extensions described here should be of interest for all Coq users, whether they embrace small-scale reflection or not.},
  language = {en},
  type = {Report}
}

@article{Gordon2019Sequential,
  title = {Sequential {{Effect Systems}} with {{Control Operators}}},
  author = {Gordon, Colin S.},
  year = {2019},
  month = aug,
  abstract = {Sequential effect systems are a class of effect system that exploits information about program order, rather than discarding it as traditional commutative effect systems do. This extra expressive power allows effect systems to reason about behavior over time, capturing properties such as atomicity, unstructured lock ownership, or even general safety properties. While we now understand the essential denotational (categorical) models fairly well, application of these ideas to real software is hampered by the sheer variety of source level control flow constructs in real languages. Denotational approaches are general enough to accommodate any particular control flow construct, but provide no guidance on the details, let alone applications. We address this new problem by appeal to a classic idea: macro-expression of commonly-used programming constructs in terms of control operators. We give an effect system for a subset of Racket's tagged delimited control operators, as a lifting of an effect system for a language without direct control operators. This gives the first account of sequential effects in the presence of general control operators. Using this system, we also re-derive the sequential effect system rules for control flow constructs previously shown sound directly, and derive sequential effect rules for new constructs not previously studied in the context of source-level sequential effect systems. This offers a way to directly extend source-level support for sequential effect systems to real programming languages.},
  archivePrefix = {arXiv},
  eprint = {1811.12285},
  eprinttype = {arxiv},
  journal = {arXiv:1811.12285 [cs]},
  keywords = {Computer Science - Programming Languages,F.3.2},
  primaryClass = {cs}
}

@techreport{Gorlatch1997optimizing,
  title = {Optimizing {{Compositions}} of {{Scans}} and {{Reductions}} in {{Parallel Program Derivation}}},
  author = {Gorlatch, Sergei},
  year = {1997},
  abstract = {Introduction We study two popular programming schemas: scan (also known as prefix sums, parallel prefix, etc.) and reduction (also known as fold). Originally from the functional world [3], they are becoming increasingly popular as primitives of parallel programming. The reasons are that, first, such higher-order combinators are adequate and useful for a broad class of applications [4], second, they encourage well-structured, coarse-grained parallel programming and, third, their implementation in the MPI standard [14] makes the target programs portable across different parallel architectures with predictable performance. Our contributions are as follows: -- We formally prove two optimization rules: the first rule transforms a sequential composition of scan and reduction into a single reduction, the second rule transforms a composition of two scans into a single scan. -- We apply the first rule in the formal derivation of a parallel algorithm for the}
}

@article{Gotsman2011Precision,
  title = {Precision and the {{Conjunction Rule}} in {{Concurrent Separation Logic}}},
  author = {Gotsman, Alexey and Berdine, Josh and Cook, Byron},
  year = {2011},
  month = sep,
  volume = {276},
  pages = {171--190},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2011.09.021},
  abstract = {Concurrent separation logic is a Hoare logic for modular reasoning about concurrent heap-manipulating programs synchronising via locks. It achieves modular reasoning by partitioning the program state into thread-local and lock-protected parts, and assigning resource invariants to the latter. Surprisingly, the logic is unsound unless resource invariants are precise, i.e., unambiguously carve out an area of the heap. The counterexample showing the unsoundness involves the conjunction rule. However, to date it has been an open question whether concurrent separation logic without the conjunction rule is sound when the restriction on resource invariants is dropped: all the published proofs have the precision restriction baked in. In this paper we present a single proof that shows the soundness of the logic with imprecise resource invariants, but without the conjunction rule, as well as its classical version, where resource invariants are required to be precise and the conjunction rule is included. Our proof yields a precise and direct formulation of O'Hearn's Separation Property and provides a semantic analysis of the logic that is much more elementary than previous proofs.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {concurrency,conjunction rule,precision,Separation logic},
  series = {Twenty-Seventh {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXVII}})}
}

@misc{Gowerstwo,
  title = {The {{Two Cultures}} of {{Mathematics}}},
  author = {Gowers, W. T.},
  abstract = {This article was for an IMU volume entitled Mathematics: Frontiers and Perspectives. It is basically a defence of combinatorics against some of the criticisms commonly made of it. Judging from some of the reaction to the article, it is perhaps worth saying that when I quote the views of `theoreticians', it is because I agree with them. My aim is not to dispute their criteria for what constitutes worthwhile mathematics, but rather to show that combinatorics meets these criteria (suitably, and I hope reasonably, interpreted).},
  note = {Info found at https://www.dpmms.cam.ac.uk/\textasciitilde{}wtg10/papers.html}
}

@article{Grabmayer2014maximal,
  title = {Maximal {{Sharing}} in the {{Lambda Calculus}} with Letrec},
  author = {Grabmayer, Clemens and Rochel, Jan},
  year = {2014},
  month = jan,
  abstract = {Increasing sharing in programs is desirable to compactify the code, and to avoid duplication of reduction work at run-time, thereby speeding up execution. We show how a maximal degree of sharing can be obtained for programs expressed as terms in the lambda calculus with letrec. We introduce a notion of `maximal compactness' for lambda-letrec-terms among all terms with the same infinite unfolding. Instead of defined purely syntactically, this notion is based on a graph semantics. lambda-letrec-terms are interpreted as first-order term graphs so that unfolding equivalence between terms is preserved and reflected through bisimilarity of the term graph interpretations. Compactness of the term graphs can then be compared via functional bisimulation. We describe practical and efficient methods for the following two problems: transforming a lambda-letrec-term into a maximally compact form; and deciding whether two lambda-letrec-terms are unfolding-equivalent. The transformation of a lambda-letrec-term \$L\$ into maximally compact form \$L\_0\$ proceeds in three steps: (i) translate L into its term graph \$G = [[ L ]]\$; (ii) compute the maximally shared form of \$G\$ as its bisimulation collapse \$G\_0\$; (iii) read back a lambda-letrec-term \$L\_0\$ from the term graph \$G\_0\$ with the property \$[[ L\_0 ]] = G\_0\$. This guarantees that \$L\_0\$ and \$L\$ have the same unfolding, and that \$L\_0\$ exhibits maximal sharing. The procedure for deciding whether two given lambda-letrec-terms \$L\_1\$ and \$L\_2\$ are unfolding-equivalent computes their term graph interpretations \$[[ L\_1 ]]\$ and \$[[ L\_2 ]]\$, and checks whether these term graphs are bisimilar. For illustration, we also provide a readily usable implementation.},
  archivePrefix = {arXiv},
  eprint = {1401.1460},
  eprinttype = {arxiv},
  journal = {arXiv:1401.1460 [cs]},
  keywords = {Computer Science - Programming Languages,D.1.1,F.3.3},
  primaryClass = {cs}
}

@inproceedings{Gramoli2014why,
  title = {Why {{Inheritance Anomaly}} Is {{Not Worth Solving}}},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Implementation}}, {{Compilation}}, {{Optimization}} of {{Object}}-{{Oriented Languages}}, {{Programs}} and {{Systems PLE}}},
  author = {Gramoli, Vincent and Santosa, Andrew E.},
  year = {2014},
  pages = {6:1--6:12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2633301.2633307},
  abstract = {Modern computers improve their predecessors with additional parallelism but require concurrent software to exploit it. Object-orientation is instrumental in simplifying sequential programming, however, in a concurrent setting, programmers adding new methods in a subclass typically have to modify the code of the superclass, which inhibits reuse, a problem known as inheritance anomaly. There have been much efforts by researchers in the last two decades to solve the problem by deriving anomaly-free languages. Yet, these proposals have not ended up as practical solutions, thus one may ask why. In this article, we investigate from a theoretical perspective if a solution of the problem would introduce extra code complexity. We model object behavior as a regular language, and show that freedom from inheritance anomaly necessitates a language where ensuring Liskov-Wing substitutability becomes a language containment problem, which in our modeling is PSPACE hard. This indicates that we cannot expect programmers to manually ensure that subtyping holds in an anomaly-free language. Anomaly freedom thus predictably leads to software bugs and we doubt the value of providing it. From the practical perspective, the problem is already solved. Inheritance anomaly is part of the general fragile base class problem of object-oriented programming, that arises due to code coupling in implementation inheritance. In modern software practice, the fragile base class problem is circumvented by interface abstraction to avoid implementation inheritance, and opting for composition as means for reuse. We discuss concurrent programming issues with composition for reuse.},
  isbn = {978-1-4503-2914-9},
  keywords = {concurrent programming,inheritance anomaly,object-oriented programming},
  series = {{{ICOOOLPS}} '14}
}

@inproceedings{Grebe2017Rewriting,
  title = {Rewriting a {{Shallow DSL Using}} a {{GHC Compiler Extension}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Grebe, Mark and Young, David and Gill, Andy},
  year = {2017},
  pages = {246--258},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3136040.3136048},
  abstract = {Embedded Domain Specific Languages are a powerful tool for developing customized languages to fit specific problem domains. Shallow EDSLs allow a programmer to program using many of the features of a host language and its syntax, but sacrifice performance. Deep EDSLs provide better performance and flexibility, through the ability to manipulate the abstract syntax tree of the DSL program, but sacrifice syntactical similarity to the host language. Using Haskino, an EDSL designed for small embedded systems based on the Arduino line of microcontrollers, and a compiler plugin for the Haskell GHC compiler, we show a method for combining the best aspects of shallow and deep EDSLs. The programmer is able to write in the shallow EDSL, and have it automatically transformed into the deep EDSL. This allows the EDSL user to benefit from powerful aspects of the host language, Haskell, while meeting the demanding resource constraints of the small embedded processing environment.},
  isbn = {978-1-4503-5524-7},
  keywords = {_tablet,Arduino,EDSL,GHC,Haskell,Transformations},
  series = {{{GPCE}} 2017}
}

@article{Grechanik2013Supercompilation,
  title = {Supercompilation by Hypergraph Transformation},
  author = {Grechanik, Sergei},
  year = {2013},
  abstract = {This paper presents a reformulation of the notion of multi-result supercompilation in terms of graph transformations. For this purpose we use a hypergraph-based representation of the program being transformed. The presented approach bridges the gap between supercompilation and equality saturation. We also show how higher-level supercompilation naturally arises in this setting.},
  note = {00000}
}

@inproceedings{Grechanik2014Inductive,
  title = {Inductive {{Prover Based}} on {{Equality Saturation}} for a {{Lazy Functional Language}}},
  booktitle = {Perspectives of {{System Informatics}}},
  author = {Grechanik, Sergei},
  year = {2014},
  month = jun,
  pages = {127--141},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-46823-4_11},
  abstract = {The present paper shows how the idea of equality saturation can be used to build an inductive prover for a non-total first-order lazy functional language. We adapt equality saturation approach to a functional language by using transformations borrowed from supercompilation. A special transformation called merging by bisimilarity is used to perform proof by induction of equivalence between nodes of the E-graph. Equalities proved this way are just added to the E-graph. We also experimentally compare our prover with HOSC and HipSpec.},
  isbn = {978-3-662-46822-7 978-3-662-46823-4},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Greenman2014getting,
  title = {Getting {{F}}-Bounded {{Polymorphism}} into {{Shape}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Greenman, Ben and Muehlboeck, Fabian and Tate, Ross},
  year = {2014},
  pages = {89--99},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594308},
  abstract = {We present a way to restrict recursive inheritance without sacrificing the benefits of F-bounded polymorphism. In particular, we distinguish two new concepts, materials and shapes, and demonstrate through a survey of 13.5 million lines of open-source generic-Java code that these two concepts never actually overlap in practice. With this Material-Shape Separation, we prove that even na{\"i}ve type-checking algorithms are sound and complete, some of which address problems that were unsolvable even under the existing proposals for restricting inheritance. We illustrate how the simplicity of our design reflects the design intuitions employed by programmers and potentially enables new features coming into demand for upcoming programming languages.},
  isbn = {978-1-4503-2784-8},
  keywords = {decidability,F-bounded polymorphism,higher-kinded types,joins,materials,separation,shapes,subtyping,variance},
  series = {{{PLDI}} '14}
}

@inproceedings{Grigore2017Java,
  title = {Java Generics Are {{Turing}} Complete},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Grigore, Radu},
  year = {2017},
  pages = {73--85},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009871},
  abstract = {This paper describes a reduction from the halting problem of Turing machines to subtype checking in Java. It follows that subtype checking in Java is undecidable, which answers a question posed by Kennedy and Pierce in 2007. It also follows that Java's type checker can recognize any recursive language, which improves a result of Gill and Levy from 2016. The latter point is illustrated by a parser generator for fluent interfaces.},
  isbn = {978-1-4503-4660-3},
  keywords = {decidability,fluent interface,Java,parser generator,subtype checking,Turing machine},
  series = {{{POPL}} 2017}
}

@article{Gross2014experience,
  title = {Experience {{Implementing}} a {{Performant Category}}-{{Theory Library}} in {{Coq}}},
  author = {Gross, Jason and Chlipala, Adam and Spivak, David I.},
  year = {2014},
  month = jan,
  abstract = {We describe our experience implementing a broad category-theory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coq's logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful.},
  archivePrefix = {arXiv},
  eprint = {1401.7694},
  eprinttype = {arxiv},
  journal = {arXiv:1401.7694 [cs, math]},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  primaryClass = {cs, math}
}

@inproceedings{Grust2009FERRY,
  title = {{{FERRY}}: {{Database}}-Supported {{Program Execution}}},
  shorttitle = {{{FERRY}}},
  booktitle = {Proceedings of the 2009 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Grust, Torsten and Mayr, Manuel and Rittinger, Jan and Schreiber, Tom},
  year = {2009},
  pages = {1063--1066},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1559845.1559982},
  abstract = {We demonstrate the language Ferry and its editing, compilation, and execution environment FerryDeck. Ferry's type system and operations match those of scripting or programming languages; its compiler has been designed to emit (bundles of) compliant and efficient SQL:1999 statements. Ferry acts as glue that permits a programming style in which developers access database tables using their programming language's own syntax and idioms -- the Ferry-expressible fragments of such programs may be executed by a relational database back-end, i.e., close to the data. The demonstrator FerryDeck implements compile-and-execute-as-you-type interactivity for Ferry and offers a variety of (graphical) hooks to explore and inspect this approach to database-supported program execution.},
  isbn = {978-1-60558-551-2},
  keywords = {ferry,LINQ,pathfinder,sql:1999},
  series = {{{SIGMOD}} '09}
}

@article{Grust2013FirstClass,
  title = {First-{{Class Functions}} for {{First}}-{{Order Database Engines}}},
  author = {Grust, Torsten and Ulrich, Alexander},
  year = {2013},
  month = aug,
  abstract = {We describe Query Defunctionalization which enables off-the-shelf first-order database engines to process queries over first-class functions. Support for first-class functions is characterized by the ability to treat functions like regular data items that can be constructed at query runtime, passed to or returned from other (higher-order) functions, assigned to variables, and stored in persistent data structures. Query defunctionalization is a non-invasive approach that transforms such function-centric queries into the data-centric operations implemented by common query processors. Experiments with XQuery and PL/SQL database systems demonstrate that first-order database engines can faithfully and efficiently support the expressive "functions as data" paradigm.},
  archivePrefix = {arXiv},
  eprint = {1308.0158},
  eprinttype = {arxiv},
  journal = {arXiv:1308.0158 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Programming Languages,D.3.2,H.2.3},
  primaryClass = {cs}
}

@article{Grust2013Functions,
  title = {Functions {{Are Data Too}}: {{Defunctionalization}} for {{PL}}/{{SQL}}},
  shorttitle = {Functions {{Are Data Too}}},
  author = {Grust, Torsten and Schweinsberg, Nils and Ulrich, Alexander},
  year = {2013},
  month = aug,
  volume = {6},
  pages = {1214--1217},
  issn = {2150-8097},
  doi = {10.14778/2536274.2536279},
  abstract = {We demonstrate a full-fledged implementation of first-class functions for the widely used PL/SQL database programming language. Functions are treated as regular data items that may be (1) constructed at query runtime, (2) stored in and retrieved from tables, (3) assigned to variables, and (4) passed to and from other (higher-order) functions. The resulting PL/SQL dialect concisely and elegantly expresses a wide range of new query idioms which would be cumbersome to formulate if functions remained second-class citizens. We include a diverse set of application scenarios that make these advantages tangible. First-class PL/SQL functions require featherweight syntactic extensions only and come with a non-invasive implementation-- the defunctionalization transformation--that can entirely be built on top of existing relational DBMS infrastructure. An interactive demonstrator helps users to experiment with the "function as data" paradigm and to earn a solid intuition of its inner workings.},
  journal = {Proc. VLDB Endow.},
  number = {12}
}

@misc{Guarded,
  title = {Guarded {{Traced Categories}} | {{SpringerLink}}},
  howpublished = {https://link.springer.com/chapter/10.1007/978-3-319-89366-2\_17}
}

@inproceedings{Guatto2018Generalized,
  title = {A {{Generalized Modality}} for {{Recursion}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Guatto, Adrien},
  year = {2018},
  pages = {482--491},
  publisher = {{ACM}},
  address = {{Oxford, United Kingdom}},
  doi = {10.1145/3209108.3209148},
  abstract = {Nakano's later modality allows types to express that the output of a function does not immediately depend on its input, and thus that computing its fixpoint is safe. This idea, guarded recursion, has proved useful in various contexts, from functional programming with infinite data structures to formulations of step-indexing internal to type theory. Categorical models have revealed that the later modality corresponds in essence to a simple reindexing of the discrete time scale. Unfortunately, existing guarded type theories suffer from significant limitations for programming purposes. These limitations stem from the fact that the later modality is not expressive enough to capture precise input-output dependencies of functions. As a consequence, guarded type theories reject many productive definitions. Combining insights from guarded type theories and synchronous programming languages, we propose a new modality for guarded recursion. This modality can apply any well-behaved reindexing of the time scale to a type. We call such reindexings time warps. Several modalities from the literature, including later, correspond to fixed time warps, and thus arise as special cases of ours.},
  isbn = {978-1-4503-5583-4},
  keywords = {Category Theory,Functional Programming,Guarded Recursion,Streams,Synchronous Programming,Type Systems},
  series = {{{LICS}} '18}
}

@inproceedings{Gvero2013complete,
  title = {Complete {{Completion Using Types}} and {{Weights}}},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Gvero, Tihomir and Kuncak, Viktor and Kuraj, Ivan and Piskac, Ruzica},
  year = {2013},
  pages = {27--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2491956.2462192},
  abstract = {Developing modern software typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. As the basis of our technique we use type inhabitation for lambda calculus terms in long normal form. We introduce a succinct representation for type judgements that merges types into equivalence classes to reduce the search space, then reconstructs any desired number of solutions on demand. Furthermore, we introduce a method to rank solutions based on weights derived from a corpus of code. We implemented the algorithm and deployed it as a plugin for the Eclipse IDE for Scala. We show that the techniques we incorporated greatly increase the effectiveness of the approach. Our evaluation benchmarks are code examples from programming practice; we make them available for future comparisons.},
  isbn = {978-1-4503-2014-6},
  keywords = {code completion,program synthesis,ranking,type inhabitation,type-driven synthesis},
  series = {{{PLDI}} '13}
}

@inproceedings{Hackett2012fast,
  title = {Fast and {{Precise Hybrid Type Inference}} for {{JavaScript}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hackett, Brian and Guo, Shu-yu},
  year = {2012},
  pages = {239--250},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2254064.2254094},
  abstract = {JavaScript performance is often bound by its dynamically typed nature. Compilers do not have access to static type information, making generation of efficient, type-specialized machine code difficult. We seek to solve this problem by inferring types. In this paper we present a hybrid type inference algorithm for JavaScript based on points-to analysis. Our algorithm is fast, in that it pays for itself in the optimizations it enables. Our algorithm is also precise, generating information that closely reflects the program's actual behavior even when analyzing polymorphic code, by augmenting static analysis with run-time type barriers. We showcase an implementation for Mozilla Firefox's JavaScript engine, demonstrating both performance gains and viability. Through integration with the just-in-time (JIT) compiler in Firefox, we have improved performance on major benchmarks and JavaScript-heavy websites by up to 50\%. Inference-enabled compilation is the default compilation mode as of Firefox 9.},
  isbn = {978-1-4503-1205-9},
  keywords = {hybrid,just-in-time compilation,type inference},
  series = {{{PLDI}} '12}
}

@techreport{Hagino1987categorical,
  title = {A {{Categorical Programming Language}}},
  author = {Hagino, Tatsuya},
  year = {1987},
  abstract = {A theory of data types and a programming language based on category theory are presented. Data types play a crucial role in programming. They enable us to write programs easily and elegantly. Various programming languages have been developed, each of which may use different kinds of data types. Therefore, it becomes important to organize data types systematically so that we can understand the relationship between one data type and another and investigate future directions which lead us to discover exciting new data types. There have been several approaches to systematically organize data types: algebraic specification methods using algebras, domain theory using complete partially ordered sets and type theory using the connection between logics and data types. Here, we use category theory. Category theory has proved to be remarkably good at revealing the nature of mathematical objects, and we use it to understand the true nature of data types in programming.}
}

@incollection{Hagino1987typed,
  title = {A Typed Lambda Calculus with Categorical Type Constructors},
  booktitle = {Category {{Theory}} and {{Computer Science}}},
  author = {Hagino, Tatsuya},
  editor = {Pitt, David H. and Poign{\'e}, Axel and Rydeheard, David E.},
  year = {1987},
  pages = {140--157},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A typed lambda calculus with categorical type constructors is introduced. It has a uniform category theoretic mechanism to declare new types. Its type structure includes categorical objects like products and coproducts as well as recursive types like natural numbers and lists. It also allows duals of recursive types, i.e. lazy types, like infinite lists. It has generalized iterators for recursive types and duals of iterators for lazy types. We will give reduction rules for this simply typed lambda calculus and show that they are strongly normalizing even though it has infinite things like infinite lists.},
  copyright = {\textcopyright{}1987 Springer-Verlag},
  isbn = {978-3-540-18508-6 978-3-540-48006-8},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  language = {en},
  number = {283},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Hagino1989codatatypes,
  title = {Codatatypes in {{ML}}},
  author = {Hagino, Tatsuya},
  year = {1989},
  month = dec,
  volume = {8},
  pages = {629--650},
  issn = {0747-7171},
  doi = {10.1016/S0747-7171(89)80065-3},
  abstract = {A new data type declaration mechanism of defining codatatypes is introduced to a functional programming language ML. Codatatypes are dual to datatypes for which ML already has a mechanism of defining. Sums and finite lists are defined as datatypes, but their duals, products and infinite lists, could not be defined in ML. This new facility gives ML the missing half of data types and makes ML symmetric. Categorical and domain-theoretic characterization of codatatypes are also given.},
  journal = {Journal of Symbolic Computation},
  number = {6}
}

@incollection{Haller2008Implementing,
  title = {Implementing Joins Using Extensible Pattern Matching},
  booktitle = {Coordination {{Models}} and {{Languages}}},
  author = {Haller, Philipp and Cutsem, Tom Van},
  year = {2008},
  month = jun,
  pages = {135--152},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-68265-3_9},
  abstract = {Join patterns are an attractive declarative way to synchronize both threads and asynchronous distributed computations. We explore joins in the context of extensible pattern matching that recently appeared in languages such as F\# and Scala. Our implementation supports join patterns with multiple synchronous events, and guards. Furthermore, we integrated joins into an existing actor-based concurrency framework. It enables join patterns to be used in the context of more advanced synchronization modes, such as future-type message sending and token-passing continuations.},
  isbn = {978-3-540-68264-6 978-3-540-68265-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Halpern2015why,
  title = {Why {{Bother With Syntax}}?},
  author = {Halpern, Joseph Y.},
  year = {2015},
  month = jun,
  abstract = {This short note discusses the role of syntax vs. semantics and the interplay between logic, philosophy, and language in computer science and game theory.},
  archivePrefix = {arXiv},
  eprint = {1506.05282},
  eprinttype = {arxiv},
  journal = {arXiv:1506.05282 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@inproceedings{Hamana2011Foundation,
  title = {A Foundation for {{GADTs}} and Inductive Families: {{Dependent}} Polynomial Functor Approach},
  shorttitle = {A Foundation for Gadts and Inductive Families},
  booktitle = {Proceedings of the {{Seventh ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Hamana, Makoto and Fiore, Marcelo},
  year = {2011},
  pages = {59--70},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2036918.2036927},
  abstract = {Every Algebraic Datatype (ADT) is characterised as the initial algebra of a polynomial functor on sets. This paper extends the characterisation to the case of more advanced datatypes: Generalised Algebraic Datatypes (GADTs) and Inductive Families. Specifically, we show that GADTs and Inductive Families are characterised as initial algebras of dependent polynomial functors. The theoretical tool we use throughout is an abstract notion of polynomial between sets together with its associated general form of polynomial functor between categories of indexed sets introduced by Gambino and Hyland. In the context of ADTs, this fundamental result is the basis for various generic functional programming techniques. To establish the usefulness of our approach for such developments in the broader context of inductively defined dependent types, we apply the theory to construct zippers for Inductive Families.},
  isbn = {978-1-4503-0861-8},
  keywords = {categorical semantics,dependent types},
  series = {{{WGP}} '11}
}

@inproceedings{Hamin2018DeadlockFree,
  title = {Deadlock-{{Free Monitors}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Hamin, Jafar and Jacobs, Bart},
  editor = {Ahmed, Amal},
  year = {2018},
  pages = {415--441},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-89884-1_15},
  abstract = {Monitors constitute one of the common techniques to synchronize threads in multithreaded programs, where calling a {$\mathsf{w}\mathsf{a}\mathsf{i}\mathsf{t}$}wait\textbackslash{}mathsf \{wait\} command on a condition variable suspends the caller thread and notifying a condition variable causes the threads waiting for that condition variable to resume their execution. One potential problem with these programs is that a waiting thread might be suspended forever leading to deadlock, a state where each thread of the program is waiting for a condition variable or a lock. In this paper, a modular verification approach for deadlock-freedom of such programs is presented, ensuring that in any state of the execution of the program if there are some threads suspended then there exists at least one thread running. The main idea behind this approach is to make sure that for any condition variable v for which a thread is waiting there exists a thread obliged to fulfil an obligation for v that only waits for a waitable object whose wait level, an arbitrary number associated with each waitable object, is less than the wait level of v. The relaxed precedence relation introduced in this paper, aiming to avoid cycles, can also benefit some other verification approaches, verifying deadlock-freedom of other synchronization constructs such as channels and semaphores, enabling them to accept a wider range of deadlock-free programs. We encoded the proposed proof rules in the VeriFast program verifier and by defining some appropriate invariants for the locks associated with some condition variables succeeded in verifying some popular use cases of monitors including unbounded/bounded buffer, sleeping barber, barrier, and readers-writers locks. A soundness proof for the presented approach is provided; some of the trickiest lemmas in this proof have been machine-checked with Coq.},
  isbn = {978-3-319-89884-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hammer2008Memory,
  title = {Memory Management for Self-Adjusting Computation},
  booktitle = {Proceedings of the 7th {{International Symposium}} on {{Memory Management}}},
  author = {Hammer, Matthew A. and Acar, Umut A.},
  year = {2008},
  pages = {51--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1375634.1375642},
  abstract = {The cost of reclaiming space with traversal-based garbage collection is inversely proportional to the amount of free memory, i.e., O(1/(1-f)), where f is the fraction of memory that is live. Consequently, the cost of garbage collection can be very high when the size of the live data remains large relative to the available free space. Intuitively, this is because allocating a small amount of memory space will require the garbage collector to traverse a significant fraction of the memory only to discover little garbage. This is unfortunate because in some application domains the size of the memory-resident data can be generally high. This can cause high GC overheads, especially when generational assumptions do not hold. One such application domain is self-adjusting computation, where computations use memory-resident execution traces in order to respond to changes to their state (e.g., inputs) efficiently. This paper proposes memory-management techniques for self-adjusting computation that remain efficient even when the size of the live data is large. More precisely, the proposed techniques guarantee O(1) amortized cost for each reclaimed memory object. We propose a set of primitives for self-adjusting computation that support the proposed memory management techniques. The primitives provide an operation for allocating memory; we reclaim unused memory automatically. We implement a library for supporting the primitives in the C language and perform an experimental evaluation. Our experiments show that the approach can be implemented with reasonably small constant-factor overheads and that the programs written using the library behave optimally. Compared to previous implementations, we measure up to an order of magnitude improvement in performance and up to a 75\% reduction in space usage.},
  isbn = {978-1-60558-134-7},
  keywords = {computational geometry,dynamic algorithms,dynamic dependency graphs,garbage collection,memoization,memory management,performance,self-adjusting computation},
  series = {{{ISMM}} '08}
}

@inproceedings{Hammer2014adapton,
  title = {Adapton: {{Composable}}, {{Demand}}-Driven {{Incremental Computation}}},
  shorttitle = {Adapton},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  year = {2014},
  pages = {156--166},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594324},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered. To address these problems, we present {$\lambda$}iccdd, a core calculus that applies a demand-driven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. {$\lambda$}iccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures {$\lambda$}iccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing {$\lambda$}iccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms state-of-the-art IC approaches.},
  isbn = {978-1-4503-2784-8},
  keywords = {call-by-push-value (CBPV),demanded computation graph (DCG) incremental computation,laziness,self-adjusting computation,thunks},
  series = {{{PLDI}} '14}
}

@inproceedings{Hammer2014adaptona,
  title = {Adapton: {{Composable}}, {{Demand}}-Driven {{Incremental Computation}} ({{Extended}} Version)},
  shorttitle = {Adapton},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.},
  year = {2014},
  publisher = {{ACM}},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered. To address these problems, we present {$\lambda$}iccdd, a core calculus that applies a demand-driven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. {$\lambda$}iccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures {$\lambda$}iccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing {$\lambda$}iccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms state-of-the-art IC approaches.},
  isbn = {978-1-4503-2784-8},
  keywords = {call-by-push-value (CBPV),demanded computation graph (DCG) incremental computation,laziness,self-adjusting computation,thunks}
}

@article{Hammer2015incremental,
  title = {Incremental {{Computation}} with {{Names}}},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Labich, Nicholas and Foster, Jeffrey S. and Hicks, Michael and Van Horn, David},
  year = {2015},
  month = mar,
  abstract = {Over the past thirty years, there has been significant progress in developing general-purpose, language-based approaches to incremental computation, which aims to efficiently update the result of a computation when an input is changed. A key design challenge in such approaches is how to provide efficient incremental support for a broad range of programs. In this paper, we argue that first-class names are a critical linguistic feature for efficient incremental computation. Names identify computations to be reused across differing runs of a program, and making them first class gives programmers a high level of control over reuse. We demonstrate the benefits of names by presenting Nominal Adapton, an ML-like language for incremental computation with names. We describe how to use Nominal Adapton to efficiently incrementalize several standard programming patterns---including maps, folds, and unfolds---and show how to build efficient, incremental probabilistic trees and tries. Since Nominal Adapton's implementation is subtle, we formalize it as a core calculus and prove it is from-scratch consistent, meaning it always produces the same answer as simply re-running the computation. Finally, we demonstrate that Nominal Adapton can provide large speedups over both from-scratch computation and Adapton, a previous state-of-the-art incremental system.},
  archivePrefix = {arXiv},
  eprint = {1503.07792},
  eprinttype = {arxiv},
  journal = {arXiv:1503.07792 [cs]},
  keywords = {_tablet,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Hammer2015Incremental,
  title = {Incremental Computation with Names},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Labich, Nicholas and Foster, Jeffrey S. and Hicks, Michael and Van Horn, David},
  year = {2015},
  pages = {748--766},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814270.2814305},
  abstract = {Over the past thirty years, there has been significant progress in developing general-purpose, language-based approaches to incremental computation, which aims to efficiently update the result of a computation when an input is changed. A key design challenge in such approaches is how to provide efficient incremental support for a broad range of programs. In this paper, we argue that first-class names are a critical linguistic feature for efficient incremental computation. Names identify computations to be reused across differing runs of a program, and making them first class gives programmers a high level of control over reuse. We demonstrate the benefits of names by presenting Nominal Adapton, an ML-like language for incremental computation with names. We describe how to use Nominal Adapton to efficiently incrementalize several standard programming patterns---including maps, folds, and unfolds---and show how to build efficient, incremental probabilistic trees and tries. Since Nominal Adapton's implementation is subtle, we formalize it as a core calculus and prove it is from-scratch consistent, meaning it always produces the same answer as simply re-running the computation. Finally, we demonstrate that Nominal Adapton can provide large speedups over both from-scratch computation and Adapton, a previous state-of-the-art incremental computation system.},
  isbn = {978-1-4503-3689-5},
  keywords = {call-by-push-value (CBPV),demanded computation graph (DCG),incremental compu- tation,laziness,memoization,nominal matching,self-adjusting computation,structural matching,thunks},
  series = {{{OOPSLA}} 2015}
}

@article{Hammer2016Refinement,
  title = {Refinement Types for Precisely Named Cache Locations},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  year = {2016},
  month = oct,
  abstract = {Many programming language techniques for incremental computation employ programmer-specified names for cached information. At runtime, each name identifies a "cache location" for a dynamic data value or a sub-computation; in sum, these cache location choices guide change propagation and incremental (re)execution. We call a cache location name precise when it identifies at most one value or subcomputation; we call all other names imprecise, or ambiguous. At a minimum, cache location names must be precise to ensure that change propagation works correctly; yet, reasoning statically about names in incremental programs remains an open problem. As a first step, this paper defines and solves the precise name problem, where we verify that incremental programs with explicit names use them precisely. To do so, we give a refinement type and effect system, and prove it sound (every well-typed program uses names precisely). We also demonstrate that this type system is expressive by verifying example programs that compute over efficient representations of incremental sequences and sets. Beyond verifying these programs, our type system also describes their dynamic naming strategies, e.g., for library documentation purposes.},
  archivePrefix = {arXiv},
  eprint = {1610.00097},
  eprinttype = {arxiv},
  journal = {arXiv:1610.00097 [cs]},
  keywords = {_tablet,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hammer2016Refinementa,
  title = {Refinement Types for Precisely Named Cache Locations},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  year = {2016},
  month = oct,
  abstract = {State-of-the-art programming language techniques for incremental computation employ programmer-specified names, whose role is critical for the asymptotic efficiency of many online algorithms. These names identify "cache locations" for dynamic data and sub-computations---the program's dynamic dependencies. To work well, or at all, these names must be precise, meaning that for an evaluation derivation in question, each name identifies at most one value or subcomputation; we call all other names imprecise, or ambiguous. The precise name problem consists of statically verifying that a program allocates names precisely, for all inputs. Past theoretical work ignores this problem by not permitting programs to use names directly, and past implementations, which necessarily permit explicit names, employ ad hoc workarounds for imprecise incremental programs. In this work, we give the first static verification approach to the precise naming problem. Specifically, we define a refinement type system that gives name term and index term sub-languages for approximating programmer-specified names of dynamic data and sub-computations. We prove that our type system enforces that these names are precise. We demonstrate the practical value of our proposed type system by verifying examples of incremental sequences and sets from a recent collections library, including both library client and implementation code. Drawing closer to an implementation of our type system, we derive a bidirectional version, and prove that it corresponds to our declarative type system. A key challenge in implementing the bidirectional system is handling constraints over names, name terms and name sets; toward this goal, we give decidable, syntactic rules to guide these checks.},
  archivePrefix = {arXiv},
  eprint = {1610.00097},
  eprinttype = {arxiv},
  journal = {arXiv:1610.00097 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hammer2016Typed,
  title = {Typed {{Adapton}}: {{Refinement}} Types for Incremental Computations with Precise Names},
  shorttitle = {Typed {{Adapton}}},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Economou, Dimitrios J. and Narasimhamurthy, Monal},
  year = {2016},
  month = oct,
  abstract = {Over the past decade, programming language techniques for incremental computation have demonstrated that incremental programs with precise, carefully chosen dynamic names for data and sub-computations can dramatically outperform non-incremental programs, as well as those using traditional memoization (without such names). However, prior work lacks a verification mechanism to solve the ambiguous name problem, the problem of statically enforcing precise names. We say that an allocated pointer name is precise for an evaluation derivation when it identifies at most one value or subcomputation, and ambiguous otherwise. In this work, we define a refinement type system that gives practical static approximations to enforce precise, deterministic allocation names in otherwise functional programs. We show that this type system permits expressing familiar functional programs, and generic, composable library components. We prove that our type system enforces that well-typed programs name their values and sub-computations precisely, without ambiguity. Drawing closer to an implementation, we derive a bidirectional version of the type system, and prove that it corresponds to our declarative type system. A key challenge in implementing the bidirectional system is handling constraints over names, name terms and name sets; toward this goal, we give decidable, syntactic rules to guide these checks.},
  archivePrefix = {arXiv},
  eprint = {1610.00097},
  eprinttype = {arxiv},
  journal = {arXiv:1610.00097 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hammer2016Typeda,
  title = {Typed {{Adapton}}: {{Refinement}} Types for Nominal Memoization of Purely Functional Incremental Programs},
  shorttitle = {Typed {{Adapton}}},
  author = {Hammer, Matthew A. and Dunfield, Joshua},
  year = {2016},
  month = oct,
  abstract = {Nominal memoization combines memoized functional programming with a controlled form of imperative cache effects. By leveraging these imperative effects, nominal memoization can dramatically outperform traditional ("structural") memoization. However, the nominal memoization programming model is error-prone: when the programmer unintentionally misuses names, their incremental program ceases to correspond with a purely functional specification. This paper develops a refinement type system for nominal memoization that enforces a program's correspondence with a purely functional specification. Our type system employs set-indexed types in the style of DML (Xi and Pfenning 1999), extended with polymorphism over kinds and index functions. We prove that our type system enforces the dynamic side conditions proposed by Hammer et al 2015. Past work shows that these conditions suffice to write useful examples of nominal memoization while also guaranteeing from-scratch consistency of the incremental programs. These features contribute to its overall goal of expressing generic naming strategies in type-generic incremental code. In particular, we show various forms of namespace parametricity and illustrate through these examples its importance for expressing nominal memoization in library code. We also show how extensions to our type system can permit controlled forms of naming strategies that encode incremental churn and feedback. We speculate that certain feedback and churn patterns constitute naming strategies that encode existing forms of functional reactive computation as a mode of use of nominal memoization.},
  archivePrefix = {arXiv},
  eprint = {1610.00097},
  eprinttype = {arxiv},
  journal = {arXiv:1610.00097 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hammer2018Memory,
  title = {Memory Management for Self-Adjusting Computation},
  shorttitle = {Fungi},
  author = {Hammer, Matthew A. and Dunfield, Joshua and Headley, Kyle and Narasimhamurthy, Monal and Economou, Dimitrios J.},
  year = {2018},
  month = aug,
  abstract = {Incremental computations attempt to exploit input similarities over time, reusing work that is unaffected by input changes. To maximize this reuse in a general-purpose programming setting, programmers need a mechanism to identify dynamic allocations (of data and subcomputations) that correspond over time. We present Fungi, a typed functional language for incremental computation with names. Unlike prior general-purpose languages for incremental computing, Fungi's notion of names is formal, general, and statically verifiable. Fungi's type-and-effect system permits the programmer to encode (program-specific) local invariants about names, and to use these invariants to establish global uniqueness for their composed programs, the property of using names correctly. We prove that well-typed Fungi programs respect global uniqueness. We derive a bidirectional version of the type and effect system, and we have implemented a prototype of Fungi in Rust. We apply Fungi to a library of incremental collections, showing that it is expressive in practice.},
  archivePrefix = {arXiv},
  eprint = {1808.07826},
  eprinttype = {arxiv},
  journal = {arXiv:1808.07826 [cs]},
  keywords = {_tablet,Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hamza2019System,
  title = {System {{FR}} as {{Foundations}} for {{Stainless}}},
  author = {Hamza, Jad and Voirol, Nicolas and Kun{\v c}ak, Viktor},
  year = {2019},
  month = apr,
  abstract = {We present the design, implementation, and foundation of a verifier for higher-order functional programs with generics and recursive data types. Our system supports proving safety and termination using preconditions, postconditions and assertions. It supports writing proof hints using assertions and recursive calls. To formalize the soundness of the system we introduce System FR, a calculus supporting System F polymorphism, dependent refinement types, and recursive types (including recursion through contravariant positions of function types). Through the use of sized types, System FR supports reasoning about termination of lazy data structures such as streams. We formalize a reducibility argument using the Coq proof assistant and prove the soundness of a type-checker with respect to call-by-value semantics, ensuring type safety and normalization for typeable programs. Our program verifier is implemented as an alternative verification-condition generator for the Stainless tool, which relies on existing SMT-based solver backend for automation. We demonstrate the efficiency of our approach by verifying a collection of higher-order functional programs comprising around 14000 lines of polymorphic higher-order Scala code, including graph search algorithms, basic number theory, monad laws, functional data structures, and assignments from popular Functional Programming MOOCs.},
  archivePrefix = {arXiv},
  eprint = {1904.03482},
  eprinttype = {arxiv},
  journal = {arXiv:1904.03482 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@inproceedings{Hanenberg2010faith,
  title = {Faith, {{Hope}}, and {{Love}}: {{An Essay}} on {{Software Science}}'s {{Neglect}} of {{Human Factors}}},
  shorttitle = {Faith, {{Hope}}, and {{Love}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Hanenberg, Stefan},
  year = {2010},
  pages = {933--946},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1869459.1869536},
  abstract = {Research in the area of programming languages has different facets -- from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers -- and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.},
  isbn = {978-1-4503-0203-6},
  keywords = {empirical research,programming language research,research methods,Software Engineering},
  series = {{{OOPSLA}} '10}
}

@inproceedings{Hanenberg2014why,
  title = {Why {{Do We Know So Little About Programming Languages}}, and {{What Would Have Happened}} If {{We Had Known More}}?},
  booktitle = {Proceedings of the 10th {{ACM Symposium}} on {{Dynamic Languages}}},
  author = {Hanenberg, Stefan},
  year = {2014},
  pages = {1--1},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2661088.2661102},
  abstract = {Programming language research in the last decades was mainly driven by mathematical methods (such as formal semantics, correctness proofs, type soundness proofs, etc.) or run-time arguments based on benchmark tests. This happened despite the frequent discussion over programming language usability. We have now been through decade after decade of one language after another domainating the field, forcing companies to switch languages and migrate libraries. Now that Javascript seems to be the next language to dominate, people start to ask old questions anew. The first goal of this talk is to discuss why the application of empirical methods is (still) relatively rare in PL research, and to discuss what could be done in empirical methods to make them a substantial part of PL research. The second goal is to speculate about the possible effects that concrete empirical knowledge could have had on the programming language community. For example, what would have happened to programming languages if current knowledge would have been available 30 years ago? What if knowledge about programming languages from the year 2050 would be available today?},
  isbn = {978-1-4503-3211-8},
  keywords = {controlled experiments,empirical studies,programming languages},
  series = {{{DLS}} '14}
}

@inproceedings{Harper1990Higherorder,
  title = {Higher-Order {{Modules}} and the {{Phase Distinction}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Harper, Robert and Mitchell, John C. and Moggi, Eugenio},
  year = {1990},
  pages = {341--354},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/96709.96744},
  abstract = {In earlier work, we used a typed function calculus, XML, with dependent types to analyze several aspects of the Standard ML type system. In this paper, we introduce a refinement of XML with a clear compile-time/run-time phase distinction, and a direct compile-time type checking algorithm. The calculus uses a finer separation of types into universes than XML and enforces the phase distinction using a nonstandard equational theory for module and signature expressions. While unusual from a type-theoretic point of view, the nonstandard equational theory arises naturally from the well-known Grothendieck construction on an indexed category.},
  isbn = {978-0-89791-343-0},
  series = {{{POPL}} '90}
}

@article{Harper1992constructing,
  title = {Constructing Type Systems over an Operational Semantics},
  author = {Harper, Robert},
  year = {1992},
  month = jul,
  volume = {14},
  pages = {71--84},
  issn = {0747-7171},
  doi = {10.1016/0747-7171(92)90026-Z},
  abstract = {Type theories in the sense of Martin-L{\"o}f and the NuPRL system are based on taking as primitive a type-free programming language given by an operational semantics, and defining types as partial equivalence relations on the set of closed terms. The construction of a type system is based on a general form of inductive definition that may either be taken as acceptable in its own right, or further explicated in terms of other patterns of induction. One such account, based on a general theory of inductively-defined relations, was given by Allen. An alternative account, based on an essentially set-theoretic argument, is presented.},
  journal = {Journal of Symbolic Computation},
  number = {1}
}

@article{Harper1993Type,
  title = {On the Type Structure of {{Standard ML}}},
  author = {Harper, Robert and Mitchell, John C.},
  year = {1993},
  month = apr,
  volume = {15},
  pages = {211--252},
  issn = {0164-0925},
  doi = {10.1145/169701.169696},
  abstract = {Standard ML is a useful programming language with a polymorphic type system and a flexible module facility. One notable feature of the core expression language of ML is that it is implicitly typed: no explicit type information need be supplied by the programmer. In contrast, the module language of ML is explicitly typed; in particular, the types of parameters in parametric modules must be supplied by the programmer. We study the type structure of Standard ML by giving an explicitly-typed, polymorphic function calculus that captures many of the essential aspects of both the core and module language. In this setting, implicitly-typed core language expressions are regarded as a convenient short-hand for an explicitly-typed counterpart in our function calculus. In contrast to the Girard-Reynolds polymorphic calculus, our function calculus is predicative: the type system may be built up by induction on type levels. We show that, in a precise sense, the language becomes inconsistent if restrictions imposed by type levels are relaxed. More specifically, we prove that the important programming features of ML cannot be added to any impredicative language, such as the Girard-Reynolds calculus, without implicitly assuming a type of all types.},
  journal = {ACM Trans. Program. Lang. Syst.},
  number = {2}
}

@inproceedings{Harper1994typetheoretic,
  title = {A Type-Theoretic Approach to Higher-Order Modules with Sharing},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Harper, Robert and Lillibridge, Mark},
  year = {1994},
  pages = {123--137},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/174675.176927},
  abstract = {The design of a module system for constructing and maintaining large programs is a difficult task that raises a number of theoretical and practical issues. A fundamental issue is the management of the flow of information between program units at compile time via the notion of an interface. Experience has shown that fully opaque interfaces are awkward to use in practice since too much information is hidden, and that fully transparent interfaces lead to excessive interdependencies, creating problems for maintenance and separate compilation. The ``sharing'' specifications of Standard ML address this issue by allowing the programmer to specify equational relationships between types in separated modules, but are not expressive enough to allow the programmer complete control over the propagation of type information between modules.
These problems are addressed from a type-theoretic viewpoint by considering a calculus based on Girard's system F\&ohgr;. The calculus differs form those considered in previous studies by relying exclusively on a new form of weak sum type to propagate information at compile-time, in contrast to approaches based on strong sums which rely on substitution. The new form of sum type allows for the specification of equational, as well as type and kind, information in interfaces. This provides complete control over the propagation of compile-time information between program units and is sufficient to encode in a straightforward way most users of type sharing specifications in Standard ML.
Modules are treated as ``first-class'' citizens, and therefore the system supports higher-order modules and some object-oriented programming idioms; the language may be easily restricted to ``second-class'' modules found in ML-like languages.},
  isbn = {978-0-89791-636-3},
  series = {{{POPL}} '94}
}

@inproceedings{Harper1995compiling,
  title = {Compiling {{Polymorphism Using Intensional Type Analysis}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Harper, Robert and Morrisett, Greg},
  year = {1995},
  pages = {130--141},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/199448.199475},
  abstract = {Traditional techniques for implementing polymorphism use a universal representation for objects of unknown type. Often, this forces a compiler to use universal representations even if the types of objects are known. We examine an alternative approach for compiling polymorphism where types are passed as arguments to polymorphic routines in order to determine the representation of an object. This approach allows monomorphic code to use natural, efficient representations, supports separate compilation of polymorphic definitions and, unlike coercion-based implementations of polymorphism, natural representations can be used for mutable objects such as refs and arrays.We are particularly interested in the typing properties of an intermediate language that allows run-time type  analysis to be coded within the language. This allows us to compile many representation transformations and many language features without adding new primitive operations to the language. In this paper, we provide a core target language where type-analysis operators can be coded within the language and the types of such operators can be accurately tracked. The target language is powerful enough to code a variety of useful features, yet type checking remains decidable. We show how to translate an ML-like language into the target language so that primitive operators can analyze types to produce efficient representations. We demonstrate the power of the ``user-level'' operators by coding flattened tuples, marshalling, type classes, and a form of type dynamic within the  language.},
  isbn = {0-89791-692-1},
  series = {{{POPL}} '95}
}

@article{Harper2000typetheoretic,
  title = {A {{Type}}-{{Theoretic Interpretation}} of {{Standard ML}}},
  author = {Harper, Robert and Stone, Christopher},
  year = {2000},
  month = may,
  journal = {Proof, Language, and Interaction Essays in Honour of Robin Milner Edited by Gordon Plotkin, Colin Stirling and Mads Tofte}
}

@article{Harper2007mechanizing,
  title = {Mechanizing Metatheory in a Logical Framework},
  author = {Harper, Robert and Licata, Daniel R.},
  year = {2007},
  volume = {17},
  pages = {613--673},
  issn = {1469-7653},
  doi = {10.1017/S0956796807006430},
  abstract = {The LF logical framework codifies a methodology for representing deductive systems, such as programming languages and logics, within a dependently typed {$\lambda$}-calculus. In this methodology, the syntactic and deductive apparatus of a system is encoded as the canonical forms of associated LF types; an encoding is correct (adequate) if and only if it defines a compositional bijection between the apparatus of the deductive system and the associated canonical forms. Given an adequate encoding, one may establish metatheoretic properties of a deductive system by reasoning about the associated LF representation. The Twelf implementation of the LF logical framework is a convenient and powerful tool for putting this methodology into practice. Twelf supports both the representation of a deductive system and the mechanical verification of proofs of metatheorems about it. The purpose of this article is to provide an up-to-date overview of the LF {$\lambda$}-calculus, the LF methodology for adequate representation, and the Twelf methodology for mechanizing metatheory. We begin by defining a variant of the original LF language, called Canonical LF, in which only canonical forms (long {$\beta\eta$}-normal forms) are permitted. This variant is parameterized by a subordination relation, which enables modular reasoning about LF representations. We then give an adequate representation of a simply typed {$\lambda$}-calculus in Canonical LF, both to illustrate adequacy and to serve as an object of analysis. Using this representation, we formalize and verify the proofs of some metatheoretic results, including preservation, determinacy, and strengthening. Each example illustrates a significant aspect of using LF and Twelf for formalized metatheory.},
  journal = {Journal of Functional Programming},
  number = {4-5}
}

@inproceedings{Harrison1993subjectoriented,
  title = {Subject-Oriented {{Programming}}: {{A Critique}} of {{Pure Objects}}},
  shorttitle = {Subject-Oriented {{Programming}}},
  booktitle = {Proceedings of the {{Eighth Annual Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  author = {Harrison, William and Ossher, Harold},
  year = {1993},
  pages = {411--428},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/165854.165932},
  isbn = {0-89791-587-9},
  series = {{{OOPSLA}} '93}
}

@incollection{Harrison2006selfverification,
  title = {Towards {{Self}}-Verification of {{HOL Light}}},
  booktitle = {Automated {{Reasoning}}},
  author = {Harrison, John},
  editor = {Furbach, Ulrich and Shankar, Natarajan},
  year = {2006},
  month = jan,
  pages = {177--191},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The HOL Light prover is based on a logical kernel consisting of about 400 lines of mostly functional OCaml, whose complete formal verification seems to be quite feasible. We would like to formally verify (i) that the abstract HOL logic is indeed correct, and (ii) that the OCaml code does correctly implement this logic. We have performed a full verification of an imperfect but quite detailed model of the basic HOL Light core, without definitional mechanisms, and this verification is entirely conducted with respect to a set-theoretic semantics within HOL Light itself. We will duly explain why the obvious logical and pragmatic difficulties do not vitiate this approach, even though it looks impossible or useless at first sight. Extension to include definitional mechanisms seems straightforward enough, and the results so far allay most of our practical worries.},
  copyright = {\textcopyright{}2006 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-37187-8 978-3-540-37188-5},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering},
  language = {en},
  number = {4130},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hatcliff1994generic,
  title = {A {{Generic Account}} of {{Continuation}}-Passing {{Styles}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Hatcliff, John and Danvy, Olivier},
  year = {1994},
  pages = {458--471},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/174675.178053},
  abstract = {We unify previous work on the continuation-passing style (CPS) transformations in a generic framework based on Moggi's computational meta-language. This framework is used to obtain CPS transformations for a variety of evaluation strategies and to characterize the corresponding administrative reductions and inverse transformations. We establish generic formal connections between operational semantics and equational theories. Formal properties of transformations for specific evaluation orders follow as corollaries.
Essentially, we factor transformations through Moggi's computational m
eta-language. Mapping \&lgr;-terms into the meta-language captures computation properties (e.g., partiality, strictness) and evaluation order explicitly in both the term and the type structure of the meta-language. The CPS transformation is then obtained by applying a generic transformation from terms and types in the meta-language to CPS terms and types, based on a typed term representation of the continuation monad. We prove an adequacy property for the generic transformation and establish an equational correspondence between the meta-language and CPS terms.
These generic results generalize Plotkin's seminal theorems, subsume more recent results, and enable new uses of CPS transformations and their inverses. We discuss how to aply these results to compilation.},
  isbn = {0-89791-636-0},
  series = {{{POPL}} '94}
}

@article{Havelund2000model,
  title = {Model Checking {{JAVA}} Programs Using {{JAVA PathFinder}}},
  author = {Havelund, Klaus and Pressburger, Thomas},
  year = {2000},
  month = mar,
  volume = {2},
  pages = {366--381},
  issn = {1433-2779},
  doi = {10.1007/s100090050043},
  abstract = {This paper describes a translator called Java PathFinder (Jpf), which translates from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated into similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA's areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi-threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications.},
  journal = {International Journal on Software Tools for Technology Transfer},
  keywords = {Key words: Program verification – Java – Model checking – Spin – Concurrent programming – Assertions – Deadlocks},
  language = {en},
  number = {4}
}

@article{Headley2016Random,
  title = {The {{Random Access Zipper}}: {{Simple}}, {{Purely}}-{{Functional Sequences}}},
  shorttitle = {The {{Random Access Zipper}}},
  author = {Headley, Kyle and Hammer, Matthew A.},
  year = {2016},
  month = aug,
  abstract = {We introduce the Random Access Zipper (RAZ), a simple, purely-functional data structure for editable sequences. A RAZ combines the structure of a zipper with that of a tree: like a zipper, edits at the cursor require constant time; by leveraging tree structure, relocating the edit cursor in the sequence requires logarithmic time. While existing data structures provide these time bounds, none do so with the same simplicity and brevity of code as the RAZ. The simplicity of the RAZ provides the opportunity for more programmers to extend the structure to their own needs, and we provide some suggestions for how to do so.},
  archivePrefix = {arXiv},
  eprint = {1608.06009},
  eprinttype = {arxiv},
  journal = {arXiv:1608.06009 [cs]},
  keywords = {_tablet,Computer Science - Data Structures and Algorithms,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Hedges2014monad,
  title = {Monad {{Transformers}} for {{Backtracking Search}}},
  author = {Hedges, Jules},
  year = {2014},
  month = jun,
  volume = {153},
  pages = {31--50},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.153.3},
  abstract = {This paper extends Escardo and Oliva's selection monad to the selection monad transformer, a general monadic framework for expressing backtracking search algorithms in Haskell. The use of the closely related continuation monad transformer for similar purposes is also discussed, including an implementation of a DPLL-like SAT solver with no explicit recursion. Continuing a line of work exploring connections between selection functions and game theory, we use the selection monad transformer with the nondeterminism monad to obtain an intuitive notion of backward induction for a certain class of nondeterministic games.},
  archivePrefix = {arXiv},
  eprint = {1406.2058},
  eprinttype = {arxiv},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  keywords = {Computer Science - Programming Languages}
}

@inproceedings{Henglein2010Generic,
  title = {Generic {{Multiset Programming}} for {{Language}}-Integrated {{Querying}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Henglein, Fritz and Larsen, Ken Friis},
  year = {2010},
  pages = {49--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863495.1863503},
  abstract = {This paper demonstrates how relational algebraic programming based on efficient symbolic representations of multisets and operations on them can be applied to the query sublanguage of SQL in a type-safe fashion. In essence, it provides a library for na{\"i}ve programming with multisets in a generalized SQL-style fashion, but avoids many cases of asymptotically inefficient nested iteration through cross-products.},
  isbn = {978-1-4503-0251-7},
  keywords = {Algebra,bag,discrimination,equivalence,filter,GADT,generic,Haskell,join,lazy,LINQ,map,multiset,ordering,programming,project,Query,querying,relational,select,sql,symbolic},
  series = {{{WGP}} '10}
}

@inproceedings{Henglein2010Optimizing,
  title = {Optimizing {{Relational Algebra Operations Using Generic Equivalence Discriminators}} and {{Lazy Products}}},
  booktitle = {Proceedings of the 2010 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Henglein, Fritz},
  year = {2010},
  pages = {73--82},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706356.1706372},
  abstract = {We show how to efficiently evaluate generic map-filter-product queries, generalizations of select-project-join (SPJ) queries in relational algebra, based on a combination of two novel techniques: generic discrimination-based joins and lazy (formal) products. Discrimination-based joins are based on the notion of (equivalence) discriminator. A discriminator partitions a list of values according to a user-specified equivalence relation on keys the values are associated with. Equivalence relations can be specified in an expressive embedded language for denoting equivalence relations. We show that discriminators can be constructed generically (by structural recursion on equivalence expressions), purely functionally, and efficiently (worst-case linear time). The array-based basic multiset discrimination algorithm of Cai and Paige (1995) provides a base discriminator that is both asymptotically and practically efficient. In contrast to hashing, discrimination is fully abstract (only depends on which equivalences hold on its inputs), and in contrast to comparison-based sorting, it does not require an ordering relation on its inputs. In particular, it is applicable to references (pointers). Furthermore, it has better asymptotic computational complexity than both sorting and hashing. We represent cross-products and unions lazily (symbolically) as formal products of the argument sets (relations). This allows the selection operation to recognize on the fly whenever it is applied to a cross-product and invoke an efficient equijoin implementation. In particular, queries can still be formulated naively, using filter, map and product without an explicit join operation, yet garner the advantages of efficient join-algorithms during evaluation. The techniques subsume many of the optimization techniques based on relational algebra equalities, without need for a query preprocessing phase. They require no indexes and behave purely functionally. They can be considered a form of symbolic execution of set expressions that automate and encapsulate dynamic program transformation of such expressions and lead to asymptotic performance improvements over naive execution in many cases.},
  isbn = {978-1-60558-727-1},
  keywords = {Algebra,cross-product,discrimination,disriminator,equivalence,evaluation,formal,generic,join,lazy,optimization,optimize,optimizing,product,Query,relational,symbolic},
  series = {{{PEPM}} '10}
}

@incollection{Herman2008theory,
  title = {A {{Theory}} of {{Hygienic Macros}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  year = {2008},
  month = jan,
  pages = {48--62},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Hygienic macro systems, such as Scheme's, automatically rename variables to prevent unintentional variable capture\textemdash{}in short, they ``just work.'' Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  copyright = {\textcopyright{}2008 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-78738-9 978-3-540-78739-6},
  keywords = {Data Structures,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {4960},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Hermida2014Logical,
  title = {Logical {{Relations}} and {{Parametricity}} \textendash{} {{A Reynolds Programme}} for {{Category Theory}} and {{Programming Languages}}},
  author = {Hermida, Claudio and Reddy, Uday S. and Robinson, Edmund P.},
  year = {2014},
  month = mar,
  volume = {303},
  pages = {149--180},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2014.02.008},
  abstract = {In his seminal paper on ``Types, Abstraction and Parametric Polymorphism,'' John Reynolds called for homomorphisms to be generalized from functions to relations. He reasoned that such a generalization would allow type-based ``abstraction'' (representation independence, information hiding, naturality or parametricity) to be captured in a mathematical theory, while accounting for higher-order types. However, after 30 years of research, we do not yet know fully how to do such a generalization. In this article, we explain the problems in doing so, summarize the work carried out so far, and call for a renewed attempt at addressing the problem.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Category Theory,Data abstraction,Definability,Fibrations,Homomorphisms,Information hiding,Logical Relations,Natural Transformations,Parametric polymorphism,Reflexive Graphs,Relation lifting,Relational Parametricity,Universal algebra},
  series = {Proceedings of the {{Workshop}} on {{Algebra}}, {{Coalgebra}} and {{Topology}} ({{WACT}} 2013)}
}

@inproceedings{Hess2014automatic,
  title = {Automatic {{Locality}}-Friendly {{Interface Extension}} of {{Numerical Functions}}},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Hess, Benjamin and Gross, Thomas R. and P{\"u}schel, Markus},
  year = {2014},
  pages = {83--92},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2658761.2658772},
  abstract = {Raising the level of abstraction is a key concern of software engineering, and libraries (either used directly or as a target of a program generation system) are a successful technique to raise programmer productivity and to improve software quality. Unfortunately successful libraries may contain functions that may not be general enough. For example, many numeric performance libraries contain functions that work on one- or higher-dimensional arrays. A problem arises if a program wants to invoke such a function on a non-contiguous subarray (e.g., in C the column of a matrix or a subarray of an image). If the library developer did not foresee this scenario, the client program must include explicit copy steps before and after the library function call, incurring a possibly high performance penalty. A better solution would be an enhanced library function that allows for the desired access pattern. Exposing the access pattern allows the compiler to optimize for the intended usage scenario(s). As we do not want the library developer to generate all interesting versions manually, we present a tool that takes a library function written in C and generates such a customized function for typical accesses. We describe the approach, discuss limitations, and report on the performance. As example access patterns we consider those most common in numerical applications: striding and block striding, general permutations, as well as scaling. We evaluate the tool on various library functions including filters, scans, reductions, sorting, FFTs, and linear algebra operations. The automatically generated custom version is in most cases significantly faster than using individual steps, offering speed-ups that are typically in the range of 1.2-1.8x.},
  isbn = {978-1-4503-3161-6},
  keywords = {components,interface extension,Libraries,performance,preprocessors,programming language features interaction,software product lines},
  series = {{{GPCE}} 2014}
}

@inproceedings{Heydon2000caching,
  title = {Caching {{Function Calls Using Precise Dependencies}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Heydon, Allan and Levin, Roy and Yu, Yuan},
  year = {2000},
  pages = {311--320},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/349299.349341},
  abstract = {This paper describes the implementation of a purely functional
programming language for building software systems. In this language,
external tools like compilers and linkers are invoked by function calls. Because some function calls are extremely expensive, it is obviously important to reuse the results of previous function calls whenever possible. Caching a function call requires the language interpreter to record all values on which the function call depends. For optimal caching, it is important to record precise dependencies that are both dynamic and fine-grained. The paper sketches how we compute such dependencies, describes the implementation of an efficient function cache, and evaluates our implementation's performance.},
  isbn = {1-58113-199-2},
  series = {{{PLDI}} '00}
}

@inproceedings{Hinze2000memo,
  title = {Memo {{Functions}}, {{Polytypically}}!},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Generic Programming}}, {{Ponte}} De},
  author = {Hinze, Ralf},
  year = {2000},
  pages = {17--32},
  abstract = {. This paper presents a polytypic implementation of memo  functions that are based on digital search trees. A memo function can  be seen as the composition of a tabulation function that creates a memo  table and a look-up function that queries the table. We show that tabulation  can be derived from look-up by inverse function construction. The  type of memo tables is dened by induction on the structure of argument  types and is parametric with respect to the result type of memo  functions. A memo table for a xed argument type is then a functor and  look-up and tabulation are natural isomorphisms. We provide simple  polytypic proofs of these properties.  1 Introduction  A memo function [11] is like an ordinary function except that it caches previously computed values. If it is applied a second time to a particular argument, it immediately returns the cached result, rather than recomputing it. For storing arguments and results a memo function internally employs an index structure, the ...}
}

@article{Hinze2002polytypic,
  title = {Polytypic Values Possess Polykinded Types},
  author = {Hinze, Ralf},
  year = {2002},
  month = may,
  volume = {43},
  pages = {129--159},
  issn = {0167-6423},
  doi = {10.1016/S0167-6423(02)00025-4},
  abstract = {A polytypic value is one that is defined by induction on the structure of types. In Haskell types are assigned so-called kinds that distinguish between manifest types like the type of integers and functions on types like the list type constructor. Previous approaches to polytypic programming were restricted in that they only allowed to parameterize values by types of one fixed kind. In this paper, we show how to define values that are indexed by types of arbitrary kinds. It turns out that these polytypic values possess types that are indexed by kinds. We present several examples that demonstrate that the additional flexibility is useful in practice. One paradigmatic example is the mapping function, which describes the functorial action on arrows. A single polytypic definition yields mapping functions for data types of arbitrary kinds including first- and higher-order functors.

Haskell's type system essentially corresponds to the simply typed lambda calculus with kinds playing the role of types. We show that the specialization of a polytypic value to concrete instances of data types can be phrased as an interpretation of the simply typed lambda calculus. This allows us to employ logical relations to prove properties of polytypic values. Among other things, we show that the polytypic mapping function satisfies suitable generalizations of the functorial laws.},
  journal = {Science of Computer Programming},
  number = {2\textendash{}3},
  series = {Mathematics of {{Program Construction}} ({{MPC}} 2000)}
}

@article{Hinze2006generics,
  title = {Generics for the Masses},
  author = {Hinze, Ralf},
  year = {2006},
  volume = {16},
  pages = {451--483},
  doi = {10.1017/S0956796806006022},
  abstract = {A generic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of generic functions are the functions that can be derived in Haskell, such as show, read, and `=='. The recent years have seen a number of proposals that support the definition of generic functions. Some of the proposals define new languages, some define extensions to existing languages. As a common characteristic none of the proposals can be made to work within Haskell 98: they all require something extra, either a more sophisticated type system or an additional language construct. The purpose of this paper is to show that one can, in fact, program generically within Haskell 98 obviating to some extent the need for fancy type systems or separate tools. Haskell's type classes are at the heart of this approach: they ensure that generic functions can be defined succinctly and, in particular, that they can be used painlessly. We detail three different implementations of generics both from a practical and from a theoretical perspective.},
  journal = {Journal of Functional Programming},
  number = {4-5}
}

@inproceedings{Hinze2010reason,
  title = {Reason {{Isomorphically}}!},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Hinze, Ralf and James, Daniel W.H.},
  year = {2010},
  pages = {85--96},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863495.1863507},
  abstract = {When are two types the same? In this paper we argue that isomorphism is a more useful notion than equality. We explain a succinct and elegant approach to establishing isomorphisms, with our focus on showing their existence over deriving the witnesses. We use category theory as a framework, but rather than chasing diagrams or arguing with arrows, we present our proofs in a calculational style. In particular, we hope to showcase to the reader why the Yoneda lemma and adjunctions should be in their reasoning toolbox.},
  isbn = {978-1-4503-0251-7},
  keywords = {adjunctions,category theory,isomorphism,yoneda lemma},
  series = {{{WGP}} '10}
}

@inproceedings{Hinze2011categorical,
  title = {Towards a {{Categorical Foundation}} for {{Generic Programming}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Hinze, Ralf and Wu, Nicolas},
  year = {2011},
  pages = {47--58},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2036918.2036926},
  abstract = {Generic Haskell is an extension of Haskell that supports datatype generic programming. The central idea of Generic Haskell is to interpret a type by a function, the so-called instance of a generic function at that type. Since types in Haskell include parametric types such as 'list of', Generic Haskell represents types by terms of the simply-typed lambda calculus. This paper puts the idea of interpreting types as functions on a firm theoretical footing, exploiting the fact that the simply-typed lambda calculus can be interpreted in a cartesian closed category. We identify a suitable target category, a subcategory of Cat, and argue that slice, coslice and comma categories are a good fit for interpreting generic functions at base types.},
  isbn = {978-1-4503-0861-8},
  keywords = {category theory,comma category,generic programming,slice category},
  series = {{{WGP}} '11}
}

@incollection{Hinze2012generic,
  title = {Generic {{Programming}} with {{Adjunctions}}},
  booktitle = {Generic and {{Indexed Programming}}},
  author = {Hinze, Ralf},
  editor = {Gibbons, Jeremy},
  year = {2012},
  month = jan,
  pages = {47--129},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Adjunctions are among the most important constructions in mathematics. These lecture notes show they are also highly relevant to datatype-generic programming. First, every fundamental datatype\textemdash{}sums, products, function types, recursive types\textemdash{}arises out of an adjunction. The defining properties of an adjunction give rise to well-known laws of the algebra of programming. Second, adjunctions are instrumental in unifying and generalising recursion schemes. We discuss a multitude of basic adjunctions and show that they are directly relevant to programming and to reasoning about programs.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-32201-3 978-3-642-32202-0},
  keywords = {Data Structures,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  number = {7470},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Hinze2012kan,
  title = {Kan {{Extensions}} for {{Program Optimisation Or}}: {{Art}} and {{Dan Explain}} an {{Old Trick}}},
  shorttitle = {Kan {{Extensions}} for {{Program Optimisation Or}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Hinze, Ralf},
  editor = {Gibbons, Jeremy and Nogueira, Pablo},
  year = {2012},
  month = jan,
  pages = {324--362},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Many program optimisations involve transforming a program in direct style to an equivalent program in continuation-passing style. This paper investigates the theoretical underpinnings of this transformation in the categorical setting of monads. We argue that so-called absolute Kan Extensions underlie this program optimisation. It is known that every Kan extension gives rise to a monad, the codensity monad, and furthermore that every monad is isomorphic to a codensity monad. The end formula for Kan extensions then induces an implementation of the monad, which can be seen as the categorical counterpart of continuation-passing style. We show that several optimisations are instances of this scheme: Church representations and implementation of backtracking using success and failure continuations, among others. Furthermore, we develop the calculational properties of Kan extensions, powers and ends. In particular, we propose a two-dimensional notation based on string diagrams that aims to support effective reasoning with Kan extensions.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-31112-3 978-3-642-31113-0},
  keywords = {adjunction,backtracking,Church representation,codensity monad,CPS,Discrete Mathematics in Computer Science,end,Haskell,Kan extension,Logics and Meanings of Programs,Math Applications in Computer Science,Mathematical Logic and Formal Languages,power,Programming Languages; Compilers; Interpreters,Software Engineering,string diagram},
  language = {en},
  number = {7342},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hinze2015conjugate,
  title = {Conjugate {{Hylomorphisms}} \textendash{} {{Or}}: {{The Mother}} of {{All Structured Recursion Schemes}}},
  shorttitle = {Conjugate {{Hylomorphisms}} \textendash{} {{Or}}},
  booktitle = {Proceedings of the {{42Nd Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Hinze, Ralf and Wu, Nicolas and Gibbons, Jeremy},
  year = {2015},
  pages = {527--538},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2676726.2676989},
  abstract = {The past decades have witnessed an extensive study of structured recursion schemes. A general scheme is the hylomorphism, which captures the essence of divide-and-conquer: a problem is broken into sub-problems by a coalgebra; sub-problems are solved recursively; the sub-solutions are combined by an algebra to form a solution. In this paper we develop a simple toolbox for assembling recursive coalgebras, which by definition ensure that their hylo equations have unique solutions, whatever the algebra. Our main tool is the conjugate rule, a generic rule parametrized by an adjunction and a conjugate pair of natural transformations. We show that many basic adjunctions induce useful recursion schemes. In fact, almost every structured recursion scheme seems to arise as an instance of the conjugate rule. Further, we adapt our toolbox to the more expressive setting of parametrically recursive coalgebras, where the original input is also passed to the algebra. The formal development is complemented by a series of worked-out examples in Haskell.},
  isbn = {978-1-4503-3300-9},
  keywords = {adjunctions,hylomorphisms,recursion schemes},
  series = {{{POPL}} '15}
}

@incollection{Hirzel2008Matchete,
  title = {Matchete: Paths through the Pattern Matching Jungle},
  shorttitle = {Matchete},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Hirzel, Martin and Nystrom, Nathaniel and Bloom, Bard and Vitek, Jan},
  year = {2008},
  month = jan,
  pages = {150--166},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77442-6_11},
  abstract = {Pattern matching is a programming language feature for selecting a handler based on the structure of data while binding names to sub-structures. By combining selection and binding, pattern matching facilitates many common tasks such as date normalization, red-black tree manipulation, conversion of XML documents, or decoding TCP/IP packets. Matchete is a language extension to Java that unifies different approaches to pattern matching: regular expressions, structured term patterns, XPath, and bit-level patterns. Matchete naturally allows nesting of these different patterns to form composite patterns. We present the Matchete syntax and describe a prototype implementation.},
  isbn = {978-3-540-77441-9 978-3-540-77442-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Hoare1969axiomatic,
  title = {An {{Axiomatic Basis}} for {{Computer Programming}}},
  author = {Hoare, C. A. R.},
  year = {1969},
  month = oct,
  volume = {12},
  pages = {576--580},
  issn = {0001-0782},
  doi = {10.1145/363235.363259},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
  journal = {Commun. ACM},
  keywords = {axiomatic method,formal language definition,machine-independent programming,program documentation,programming language design,theory of programming' proofs of programs},
  number = {10}
}

@incollection{Hoare1989notes,
  title = {Notes on an {{Approach}} to {{Category Theory}} for {{Computer Scientists}}},
  booktitle = {Constructive {{Methods}} in {{Computing Science}}},
  author = {Hoare, C. a. R.},
  editor = {Broy, Manfred},
  year = {1989},
  month = jan,
  pages = {245--305},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {These notes have been designed for the benefit of theoretical computer scientists who are not sure whether they want to study category theory, and who are too busy to devote the long period of continuous study required to master the subject from standard texts. The notes are therefore structured into three independent chapters, of which the earlier ones are the simplest and most clearly relevant to computer science. Each chapter introduces a number of essential categorical concepts, illustrates them by examples intended to be familiar to computer scientists, and presents theorems describing their most important properties. Each chapter may therefore be studied at widely separated intervals of time; further, the material of each chapter is organised so that there is no need to finish one chapter (or even one section) before starting the next. Finally, the reader who decides to abandon the study of category theory before completing the notes will still have obtained benefit from the effort expended.},
  copyright = {\textcopyright{}1989 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-74886-8 978-3-642-74884-4},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Techniques,Software Engineering},
  language = {en},
  number = {55},
  series = {{{NATO ASI Series}}}
}

@article{Hoare2009viewpointretrospective,
  title = {{{ViewpointRetrospective}}: An Axiomatic Basis for Computer Programming},
  shorttitle = {{{ViewpointRetrospective}}},
  author = {Hoare, C.A.R.},
  year = {2009},
  month = oct,
  volume = {52},
  pages = {30},
  issn = {00010782},
  doi = {10.1145/1562764.1562779},
  journal = {Communications of the ACM},
  language = {en},
  number = {10}
}

@inproceedings{Hobor2010Theory,
  title = {A {{Theory}} of {{Indirection}} via {{Approximation}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Hobor, Aquinas and Dockins, Robert and Appel, Andrew W.},
  year = {2010},
  pages = {171--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706299.1706322},
  abstract = {Building semantic models that account for various kinds of indirect reference has traditionally been a difficult problem. Indirect reference can appear in many guises, such as heap pointers, higher-order functions, object references, and shared-memory mutexes. We give a general method to construct models containing indirect reference by presenting a "theory of indirection". Our method can be applied in a wide variety of settings and uses only simple, elementary mathematics. In addition to various forms of indirect reference, the resulting models support powerful features such as impredicative quantification and equirecursion; moreover they are compatible with the kind of powerful substructural accounting required to model (higher-order) separation logic. In contrast to previous work, our model is easy to apply to new settings and has a simple axiomatization, which is complete in the sense that all models of it are isomorphic. Our proofs are machine-checked in Coq.},
  isbn = {978-1-60558-479-9},
  keywords = {_tablet_modified,indirection theory,step-indexed models},
  series = {{{POPL}} '10}
}

@inproceedings{Hofer2008polymorphic,
  title = {Polymorphic {{Embedding}} of {{DSLs}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Hofer, Christian and Ostermann, Klaus and Rendel, Tillmann and Moors, Adriaan},
  year = {2008},
  pages = {137--148},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1449913.1449935},
  abstract = {The influential pure embedding methodology of embedding domain-specific languages (DSLs) as libraries into a general-purpose host language forces the DSL designer to commit to a single semantics. This precludes the subsequent addition of compilation, optimization or domain-specific analyses. We propose polymorphic embedding of DSLs, where many different interpretations of a DSL can be provided as reusable components, and show how polymorphic embedding can be realized in the programming language Scala. With polymorphic embedding, the static type-safety, modularity, composability and rapid prototyping of pure embedding are reconciled with the flexibility attainable by external toolchains.},
  isbn = {978-1-60558-267-2},
  keywords = {algebraic semantics,compositionality,domain-specific languages,extensibility,pure embedding,Scala},
  series = {{{GPCE}} '08}
}

@inproceedings{Hofer2010modular,
  title = {Modular {{Domain}}-Specific {{Language Components}} in {{Scala}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Hofer, Christian and Ostermann, Klaus},
  year = {2010},
  pages = {83--92},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1868294.1868307},
  abstract = {Programs in domain-\-specific embedded languages (DSELs) can be represented in the host language in different ways, for instance implicitly as libraries, or explicitly in the form of abstract syntax trees. Each of these representations has its own strengths and weaknesses. The implicit approach has good composability properties, whereas the explicit approach allows more freedom in making syntactic program transformations. Traditional designs for DSELs fix the form of representation, which means that it is not possible to choose the best representation for a particular interpretation or transformation. We propose a new design for implementing DSELs in Scala which makes it easy to use different program representations at the same time. It enables the DSL implementor to define modular language components and to compose transformations and interpretations for them.},
  isbn = {978-1-4503-0154-1},
  keywords = {domain-specific languages,embedded languages,Scala,term representation,visitor pattern},
  series = {{{GPCE}} '10}
}

@inproceedings{Hofer2010Modular,
  title = {Modular Domain-Specific Language Components in {{Scala}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Hofer, Christian and Ostermann, Klaus},
  year = {2010},
  pages = {83--92},
  publisher = {{ACM}},
  address = {{Eindhoven, The Netherlands}},
  doi = {10.1145/1868294.1868307},
  abstract = {Programs in domain-\-specific embedded languages (DSELs) can be represented in the host language in different ways, for instance implicitly as libraries, or explicitly in the form of abstract syntax trees. Each of these representations has its own strengths and weaknesses. The implicit approach has good composability properties, whereas the explicit approach allows more freedom in making syntactic program transformations. Traditional designs for DSELs fix the form of representation, which means that it is not possible to choose the best representation for a particular interpretation or transformation. We propose a new design for implementing DSELs in Scala which makes it easy to use different program representations at the same time. It enables the DSL implementor to define modular language components and to compose transformations and interpretations for them.},
  isbn = {978-1-4503-0154-1},
  keywords = {domain-specific languages,embedded languages,scala,term representation,visitor pattern},
  series = {{{GPCE}} '10}
}

@misc{Hofmann1995extensional,
  title = {Extensional Concepts in Intensional Type Theory},
  author = {Hofmann, Martin},
  year = {1995},
  abstract = {Theories of dependent types have been proposed as a foundation of constructive mathematics and as a framework in which to construct certified programs. In these applications an important role is played by identity types which internalise equality and therefore are essential for accommodating proofs and programs in the same formal system.

This thesis attempts to reconcile the two different ways that type theories deal with identity types. In extensional type theory the propositional equality induced by the identity types is identified with definitional equality, i.e. conversion. This renders type-checking and well-formedness of propositions undecidable and leads to non-termination in the presence of universes. In intensional type theory propositional equality is coarser than definitional equality, the latter being confined to definitional expansion and normalisation. Then type-checking and well-formedness are decidable, and this variant is therefore adopted by most implementations.

However, the identity type in intensional type theory is not powerful enough for formalisation of mathematics and program development. Notably, it does not identify pointwise equal functions (functional extensionality) and provides no means of redefining equality on a type as a given relation, i.e. quotient types. We call such capabilities extensional concepts. Other extensional concepts of interest are uniqueness of proofs and more specifically of equality proofs, subset types, and propositional extensionality---the identification of equivalent propositions. In this work we investigate to what extent these extensional concepts may be added to intensional type theory without sacrificing decidability and existence of canonical forms. The method we use is the translation of identity types into equivalence relations defined by induction on the type structure. In this way type theory with extensional concepts can be understood as a high-level language for working with equivalence relations instead of equality. Such translations of type theory into itself turn out to be best described using categorical models of type theory.

We thus begin with a thorough treatment of categorical models with particular emphasis on the interpretation of type-theoretic syntax in such models. We then show how pairs of types and predicates can be organised into a model of type theory in which subset types are available and in which any two proofs of a proposition are equal. This model has applications in the areas of program extraction from proofs and modules for functional programs. For us its main purpose is to clarify the idea of syntactic translations via categorical model constructions.

The main result of the thesis consists of the construction of two models in which functional extensionality and quotient types are available. In the first one types are modelled by types together with proposition-valued partial equivalence relations. This model is rather simple and in addition provides subset types and propositional extensionality. However, it does not furnish proper dependent types such as vectors or matrices. We try to overcome this disadvantage by using another model based on families of type-valued equivalence relations which is however much more complicated and validates certain conversion rules only up to propositional equality.

We illustrate the use of these models by several small examples taken from both formalised mathematics and program development.

We also establish various syntactic properties of propositional equality including a proof of the undecidability of typing in extensional type theory and a correspondence between derivations in extensional type theory and terms in intensional type theory with extensional concepts added. Furthermore we settle affirmatively the hitherto open question of the independence of unicity of equality proofs in intensional type theory which implies that the addition of pattern matching to intensional type theory does not yield a conservative extension.},
  howpublished = {http://www.lfcs.inf.ed.ac.uk/reports/95/ECS-LFCS-95-327/index.html}
}

@phdthesis{Hofmann1995Extensional,
  title = {Extensional Concepts in Intensional Type Theory},
  author = {Hofmann, Martin},
  year = {1995},
  abstract = {Theories of dependent types have been proposed as a foundation of constructive mathematics and as a framework in which to construct certified programs. In these applications an important role is played by identity types which internalise equality and therefore are essential for accommodating proofs and programs in the same formal system.

This thesis attempts to reconcile the two different ways that type theories deal with identity types. In extensional type theory the propositional equality induced by the identity types is identified with definitional equality, i.e. conversion. This renders type-checking and well-formedness of propositions undecidable and leads to non-termination in the presence of universes. In intensional type theory propositional equality is coarser than definitional equality, the latter being confined to definitional expansion and normalisation. Then type-checking and well-formedness are decidable, and this variant is therefore adopted by most implementations.

However, the identity type in intensional type theory is not powerful enough for formalisation of mathematics and program development. Notably, it does not identify pointwise equal functions (functional extensionality) and provides no means of redefining equality on a type as a given relation, i.e. quotient types. We call such capabilities extensional concepts. Other extensional concepts of interest are uniqueness of proofs and more specifically of equality proofs, subset types, and propositional extensionality---the identification of equivalent propositions. In this work we investigate to what extent these extensional concepts may be added to intensional type theory without sacrificing decidability and existence of canonical forms. The method we use is the translation of identity types into equivalence relations defined by induction on the type structure. In this way type theory with extensional concepts can be understood as a high-level language for working with equivalence relations instead of equality. Such translations of type theory into itself turn out to be best described using categorical models of type theory.

We thus begin with a thorough treatment of categorical models with particular emphasis on the interpretation of type-theoretic syntax in such models. We then show how pairs of types and predicates can be organised into a model of type theory in which subset types are available and in which any two proofs of a proposition are equal. This model has applications in the areas of program extraction from proofs and modules for functional programs. For us its main purpose is to clarify the idea of syntactic translations via categorical model constructions.

The main result of the thesis consists of the construction of two models in which functional extensionality and quotient types are available. In the first one types are modelled by types together with proposition-valued partial equivalence relations. This model is rather simple and in addition provides subset types and propositional extensionality. However, it does not furnish proper dependent types such as vectors or matrices. We try to overcome this disadvantage by using another model based on families of type-valued equivalence relations which is however much more complicated and validates certain conversion rules only up to propositional equality.

We illustrate the use of these models by several small examples taken from both formalised mathematics and program development.

We also establish various syntactic properties of propositional equality including a proof of the undecidability of typing in extensional type theory and a correspondence between derivations in extensional type theory and terms in intensional type theory with extensional concepts added. Furthermore we settle affirmatively the hitherto open question of the independence of unicity of equality proofs in intensional type theory which implies that the addition of pattern matching to intensional type theory does not yield a conservative extension.}
}

@incollection{Hofmann1997syntax,
  title = {Syntax and Semantics of Dependent Types},
  booktitle = {Extensional {{Concepts}} in {{Intensional Type Theory}}},
  author = {Hofmann, Martin},
  year = {1997},
  pages = {13--54},
  publisher = {{Springer London}},
  abstract = {In this chapter we fix a particular syntax for a dependently typed calculus and define an abstract notion of model as well as a general interpretation function mapping syntactical objects to entities in a model. This interpretation function is shown to be sound with respect to the syntax.},
  copyright = {\textcopyright{}1997 Springer-Verlag London},
  isbn = {978-1-4471-1243-3 978-1-4471-0963-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  language = {en},
  series = {{{CPHC}}/{{BCS Distinguished Dissertations}}}
}

@incollection{Hofmann1997syntaxa,
  title = {Syntax and {{Semantics}} of {{Dependent Types}}},
  booktitle = {Semantics and {{Logics}} of {{Computation}}},
  author = {Hofmann, Martin},
  year = {1997},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-511-52661-9},
  series = {Publications of the {{Newton Institute}}}
}

@incollection{Holdermans2006generic,
  title = {Generic {{Views}} on {{Data Types}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Holdermans, Stefan and Jeuring, Johan and L{\"o}h, Andres and Rodriguez, Alexey},
  editor = {Uustalu, Tarmo},
  year = {2006},
  pages = {209--234},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A generic function is defined by induction on the structure of types. The structure of a data type can be defined in several ways. For example, in PolyP a pattern functor gives the structure of a data type viewed as a fixed point, and in Generic Haskell a structural representation type gives an isomorphic type view of a data type in terms of sums of products. Depending on this generic view on the structure of data types, some generic functions are easier, more difficult, or even impossible to define. Furthermore, the efficiency of some generic functions can be improved by choosing a different view. This paper introduces generic views on data types and shows why they are useful. Furthermore, it shows how generic views have been added to Generic Haskell, an extension of the functional programming language Haskell that supports the construction of generic functions. The separation between inductive definitions on type structure and generic views allows us to combine many approaches to generic programming in a single framework.},
  copyright = {\textcopyright{}2006 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-35631-8 978-3-540-35632-5},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {4014},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Homer2012Patterns,
  title = {Patterns as Objects in {{Grace}}},
  booktitle = {Proceedings of the 8th {{Symposium}} on {{Dynamic Languages}}},
  author = {Homer, Michael and Noble, James and Bruce, Kim B. and Black, Andrew P. and Pearce, David J.},
  year = {2012},
  pages = {17--28},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2384577.2384581},
  abstract = {Object orientation and pattern matching are often seen as conflicting approaches to program design. Object-oriented programs place type-dependent behavior inside objects and invoke it via dynamic dispatch, while pattern-matching programs place type-dependent behavior outside data structures and invoke it via multiway conditionals (case statements). Grace is a new, dynamic, object-oriented language designed to support teaching: to this end, Grace needs to support both styles. We explain how this conflict can be resolved gracefully: by modelling patterns and cases as partial functions, reifying those functions as objects, and then building up complex patterns from simpler ones using pattern combinators. We describe the implementation of this design as an object-oriented framework, and a case study of its effectiveness.},
  isbn = {978-1-4503-1564-7},
  keywords = {education,grace,minigrace,object orientation,pattern matching},
  series = {{{DLS}} '12}
}

@inproceedings{Hong2018Path,
  title = {Path Dependent Types with Path-Equality},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Hong, Jaemin and Park, Jihyeok and Ryu, Sukyoung},
  year = {2018},
  pages = {35--39},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3241653.3241657},
  abstract = {While the Scala type system provides expressive features like objects with type members, the lack of equality checking between path-dependent types prohibits some programming idioms. One such an example is abstract domain combinators in implementing static analyzers. In this paper, we propose to extend the Scala type system with path-equality, and formalize it as a DOT variant, {$\pi$} DOT, which supports records with type members and elds. We show that {$\pi$} DOT has the normalization property and prove its type soundness.},
  isbn = {978-1-4503-5836-1},
  keywords = {DOT,path equality,Scala},
  series = {Scala 2018}
}

@article{Horn2018Incremental,
  title = {Incremental Relational Lenses},
  author = {Horn, Rudi and Perera, Roly and Cheney, James},
  year = {2018},
  month = jul,
  volume = {2},
  pages = {74:1--74:30},
  issn = {2475-1421},
  doi = {10.1145/3236769},
  abstract = {Lenses are a popular approach to bidirectional transformations, a generalisation of the view update problem in databases, in which we wish to make changes to source tables to effect a desired change on a view. However, perhaps surprisingly, lenses have seldom actually been used to implement updatable views in databases. Bohannon, Pierce and Vaughan proposed an approach to updatable views called relational lenses, but to the best of our knowledge this proposal has not been implemented or evaluated to date. We propose incremental relational lenses, that equip relational lenses with change-propagating semantics that map small changes to the view to (potentially) small changes to the source tables. We also present a language-integrated implementation of relational lenses and a detailed experimental evaluation, showing orders of magnitude improvement over the non-incremental approach. Our work shows that relational lenses can be used to support expressive and efficient view updates at the language level, without relying on updatable view support from the underlying database.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {bidirectional transformations,incremental computation,lenses,relational calculus},
  number = {ICFP}
}

@inproceedings{Hoshino2012Step,
  title = {Step {{Indexed Realizability Semantics}} for a {{Call}}-by-{{Value Language Based}} on {{Basic Combinatorial Objects}}},
  booktitle = {Proceedings of the 2012 27th {{Annual IEEE}}/{{ACM Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Hoshino, Naohiko},
  year = {2012},
  pages = {385--394},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/LICS.2012.74},
  abstract = {We propose a mathematical framework for step indexed realizability semantics of a call-by-value polymorphic lambda calculus with recursion, existential types and recursive types. Our framework subsumes step indexed realizability semantics by untyped call-by-value lambda calculi as well as categorical abstract machines. Starting from an extension of Hofstra's basic combinatorial objects, we construct a step indexed categorical realizability semantics. Our main result is soundness and adequacy of our step indexed realizability semantics. As an application, we show that a small step operational semantics captures the big step operational semantics of the call-by-value polymorphic lambda calculus. We also give a safe implementation of the call-by-value polymorphic lambda calculus into a categorical abstract machine.},
  isbn = {978-0-7695-4769-5},
  keywords = {Denotational semantics,operational semantics,Semantics of Programming Languages},
  series = {{{LICS}} '12}
}

@article{Hu1997formal,
  title = {Formal {{Derivation}} of {{Efficient Parallel Programs}} by {{Construction}} of {{List Homomorphisms}}},
  author = {Hu, Zhenjiang and Iwasaki, Hideya and Takechi, Masato},
  year = {1997},
  month = may,
  volume = {19},
  pages = {444--461},
  issn = {0164-0925},
  doi = {10.1145/256167.256201},
  abstract = {It has been attracting much attention to make use of list homomorphisms in parallel programming because they ideally suit the divide-and-conquer parallel paradigm. However, they have been usually treated rather informally and ad hoc in the development of efficient parallel programs. What is worse is that some interesting functions, e.g., the maximum segment sum problem, are basically not list homomorphisms. In this article, we propose a systematic and formal way for the construction of a list homomorphism for a given problem so that an efficient parallel program is derived. We show, with several well-known but nontrivial problems, how a straightforward, and ``obviously'' correct, but quite inefficient solution to the problem can be successfully turned into a semantically  equivalent ``almost list homomorphism.'' The derivation is based on two transformations, namely tupling and fusion, which are defined according to the specific recursive structures of list homomorphisms.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {list homomorphism,parallel functional programming,program transformation and derivation},
  number = {3}
}

@incollection{Hu2002accumulative,
  title = {An {{Accumulative Parallel Skeleton}} for {{All}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Hu, Zhenjiang and Iwasaki, Hideya and Takeichi, Masato},
  editor = {M{\'e}tayer, Daniel Le},
  year = {2002},
  month = jan,
  pages = {83--97},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Parallel skeletons intend to encourage programmers to build a parallel program from ready-made components for which efficient implementations are known to exist, making the parallelization process simpler. However, it is neither easy to develop efficient parallel programs using skeletons nor to use skeletons to manipulate irregular data, and moreover there lacks a systematic way to optimize skeletal parallel programs. To remedy this situation, we propose a novel parallel skeleton, called accumulate, which not only efficiently describes data dependency in computation but also exhibits nice algebraic properties for manipulation. We show that this skeleton significantly eases skeletal parallel programming in practice, efficiently manipulating both regular and irregular data, and systematically optimizing skeletal parallel programs.},
  copyright = {\textcopyright{}2002 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-43363-7 978-3-540-45927-9},
  keywords = {Data Structures,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {2305},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Hu2015how,
  title = {How {{Functional Programming Mattered}}},
  author = {Hu, Zhenjiang and Hughes, John and Wang, Meng},
  year = {2015},
  month = jul,
  pages = {nwv042},
  issn = {2095-5138, 2053-714X},
  doi = {10.1093/nsr/nwv042},
  abstract = {In 1989 when functional programming was still considered a niche topic, Hughes wrote a visionary paper arguing convincingly ``why functional programming matters''. More than two decades have passed. Has functional programming really mattered? Our answer is a resounding ``Yes!''. Functional programming is now at the forefront of a new generation of programming technologies, and enjoying increasing popularity and influence. In this paper, we review the impact of functional programming, focusing on how it has changed the way we may construct programs, the way we may verify programs, and fundamentally the way we may think about programs.},
  journal = {National Science Review},
  keywords = {declarative programming,equational reasoning,functional languages,functional programming},
  language = {en}
}

@article{Hu2019Undecidability,
  title = {Undecidability of \${{D}}\_\{{$<$}:\}\$ and Its Decidable Fragments},
  shorttitle = {Undecidability of \$d\_\{{$<\vphantom\}$}},
  author = {Hu, Jason and Lhot{\'a}k, Ond{\v r}ej},
  year = {2019},
  month = aug,
  abstract = {Dependent Object Types (DOT) is a calculus with path dependent types, intersection types, and object self-references, which serves as the core calculus of Scala 3. Although the calculus has been proven sound, it remains open whether type checking in DOT is decidable. In this paper, we establish undecidability proofs of type checking and subtyping of \$D\_\{{$<$}:\}\$, a syntactic subset of DOT. It turns out that even for \$D\_\{{$<$}:\}\$, undecidability is surprisingly difficult to show, as evidenced by counterexamples for past attempts. To prove undecidability, we discover an equivalent definition of the \$D\_\{{$<$}:\}\$ subtyping rules in normal form. Besides being easier to reason about, this definition makes the phenomenon of bad bounds explicit as a single inference rule. After removing this rule, we discover two decidable fragments of \$D\_\{{$<$}:\}\$ subtyping and identify algorithms to decide them. We prove soundness and completeness of the algorithms with respect to the fragments, and we prove that the algorithms terminate. Our proofs are mechanized in a combination of Coq and Agda.},
  archivePrefix = {arXiv},
  eprint = {1908.05294},
  eprinttype = {arxiv},
  journal = {arXiv:1908.05294 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@incollection{Hudak2003Arrows,
  title = {Arrows, {{Robots}}, and {{Functional Reactive Programming}}},
  booktitle = {Advanced {{Functional Programming}}},
  author = {Hudak, Paul and Courtney, Antony and Nilsson, Henrik and Peterson, John},
  editor = {Jeuring, Johan and Jones, Simon L. Peyton},
  year = {2003},
  pages = {159--187},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-540-44833-4_6},
  abstract = {Functional reactive programming},
  copyright = {\textcopyright{}2003 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-40132-2 978-3-540-44833-4},
  keywords = {Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {2638},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hudak2007history,
  title = {A {{History}} of {{Haskell}}: {{Being Lazy}} with {{Class}}},
  shorttitle = {A {{History}} of {{Haskell}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN Conference}} on {{History}} of {{Programming Languages}}},
  author = {Hudak, Paul and Hughes, John and Peyton Jones, Simon and Wadler, Philip},
  year = {2007},
  pages = {12-1--12-55},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1238844.1238856},
  abstract = {This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.},
  isbn = {978-1-59593-766-7},
  series = {{{HOPL III}}}
}

@phdthesis{Huesca2015Incrementality,
  title = {Incrementality and Effect Simulation in the Simply Typed Lambda Calculus},
  author = {Huesca, Lourdes del Carmen Gonzalez},
  year = {2015},
  month = nov,
  abstract = {Certified programming is a framework in which any program is correct by construction. Proof assistants and dependently typed programming languages are the representatives of this paradigm where the proof and implementation of a program are done at the same time. However, it has some limitations: a program in Type Theory is built only with pure and total functions. Our objective is to write efficient and certified programs. The contributions of this work are the formalization, in the Simply Typed Lambda Calculus, of two mechanisms to achieve efficiency: to validate impure computations and to optimize computations by incrementality. An impure computation, that is a program with effects, and its validation in a functional and total language is done through a posteriori simulation. The simulation is performed afterwards on a monadic procedure and is guided by a prophecy. An efficient oracle is responsible for producing prophecies which is actually, the monadic procedure itself translated into an effectful programming language. The second contribution is an optimization to perform incremental computations. Incrementality as a way to propagate an input change into a corresponding output change is guided by formal change descriptions over terms and dynamic differentiation of functions. Displaceable types represent data-changes while an extension of the simply typed lambda calculus with differentials and partial derivatives offers a language to reason about incrementality.},
  language = {en},
  school = {Universite Paris Diderot-Paris VII}
}

@incollection{Huet1986cartesian,
  title = {Cartesian Closed Categories and Lambda-Calculus},
  booktitle = {Combinators and {{Functional Programming Languages}}},
  author = {Huet, G{\'e}rard},
  editor = {Cousineau, Guy and Curien, Pierre-Louis and Robinet, Bernard},
  year = {1986},
  pages = {123--135},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}1986 Springer-Verlag},
  isbn = {978-3-540-17184-3 978-3-540-47253-7},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Programming Techniques},
  language = {en},
  number = {242},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Hughes1996Type,
  title = {Type Specialisation for the {$\lambda$}-Calculus; or, a New Paradigm for Partial Evaluation Based on Type Inference},
  booktitle = {Partial {{Evaluation}}},
  author = {Hughes, John},
  year = {1996},
  pages = {183--215},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-61580-6_10},
  isbn = {978-3-540-61580-4 978-3-540-70589-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hur2012Marriage,
  title = {The {{Marriage}} of {{Bisimulations}} and {{Kripke Logical Relations}}},
  booktitle = {Proceedings of the 39th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Hur, Chung-Kil and Dreyer, Derek and Neis, Georg and Vafeiadis, Viktor},
  year = {2012},
  pages = {59--72},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2103656.2103666},
  abstract = {There has been great progress in recent years on developing effective techniques for reasoning about program equivalence in ML-like languages---that is, languages that combine features like higher-order functions, recursive types, abstract types, and general mutable references. Two of the most prominent types of techniques to have emerged are *bisimulations* and *Kripke logical relations (KLRs)*. While both approaches are powerful, their complementary advantages have led us and other researchers to wonder whether there is an essential tradeoff between them. Furthermore, both approaches seem to suffer from fundamental limitations if one is interested in scaling them to inter-language reasoning. In this paper, we propose *relation transition systems (RTSs)*, which marry together some of the most appealing aspects of KLRs and bisimulations. In particular, RTSs show how bisimulations' support for reasoning about recursive features via *coinduction* can be synthesized with KLRs' support for reasoning about local state via *state transition systems*. Moreover, we have designed RTSs to avoid the limitations of KLRs and bisimulations that preclude their generalization to inter-language reasoning. Notably, unlike KLRs, RTSs are transitively composable.},
  isbn = {978-1-4503-1083-3},
  keywords = {abstract types,bisimulations,contextual equivalence,global vs. local knowledge,higher-order state,Kripke Logical Relations,recursive types,relation transition systems,transitivity},
  series = {{{POPL}} '12}
}

@inproceedings{Hurkens1995simplification,
  title = {A Simplification of {{Girard}}'s Paradox},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Hurkens, Antonius J. C.},
  year = {1995},
  month = apr,
  pages = {266--278},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0014058},
  abstract = {In 1972 J.-Y. Girard showed that the Burali-Forti paradox can be formalised in the type system U. In 1991 Th. Coquand formalised another paradox in U-. The corresponding proof terms (that have no normal form) are large. We present a shorter term of type {$\perp$} in the Pure Type System {$\lambda$}U- and analyse its reduction behaviour. The idea is to construct a universe U and two functions such that a certain equality holds. Using this equality, we prove and disprove that a certain object in U is well-founded.},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Hutchins2010Pure,
  title = {Pure {{Subtype Systems}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Hutchins, DeLesley S.},
  year = {2010},
  pages = {287--298},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706299.1706334},
  abstract = {This paper introduces a new approach to type theory called pure subtype systems . Pure subtype systems differ from traditional approaches to type theory (such as pure type systems) because the theory is based on subtyping, rather than typing. Proper types and typing are completely absent from the theory; the subtype relation is defined directly over objects. The traditional typing relation is shown to be a special case of subtyping, so the loss of types comes without any loss of generality. Pure subtype systems provide a uniform framework which seamlessly integrates subtyping with dependent and singleton types. The framework was designed as a theoretical foundation for several problems of practical interest, including mixin modules, virtual classes, and feature-oriented programming. The cost of using pure subtype systems is the complexity of the meta-theory. We formulate the subtype relation as an abstract reduction system, and show that the theory is sound if the underlying reductions commute. We are able to show that the reductions commute locally, but have thus far been unable to show that they commute globally. Although the proof is incomplete, it is ``close enough'' to rule out obvious counter-examples. We present it as an open problem in type theory.},
  isbn = {978-1-60558-479-9},
  keywords = {abstract reduction systems,dependent types,singleton types,subtyping,transitivity elimination},
  series = {{{POPL}} '10}
}

@inproceedings{Hutton1998fold,
  title = {Fold and {{Unfold}} for {{Program Semantics}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Hutton, Graham},
  year = {1998},
  pages = {280--288},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289457},
  abstract = {In this paper we explain how recursion operators can be used to structure and reason about program semantics within a functional language. In particular, we show how the recursion operator fold can be used to structure denotational semantics, how the dual recursion operator unfold can be used to structure operational semantics, and how algebraic properties of these operators can be used to reason about program semantics. The techniques are explained with the aid of two main examples, the first concerning arithmetic expressions, and the second concerning Milner's concurrent language CCS. The aim of the paper is to give functional programmers new insights into recursion operators, program semantics, and the relationships between them.},
  isbn = {1-58113-024-4},
  series = {{{ICFP}} '98}
}

@incollection{Hyland1982effective,
  title = {The {{Effective Topos}}},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  author = {Hyland, J. M. E.},
  editor = {{A.S. Troelstra and D. van Dalen}},
  year = {1982},
  volume = {Volume 110},
  pages = {165--216},
  publisher = {{Elsevier}},
  abstract = {This chapter describes the most accessible of the series of toposes that can be constructed from notions of realizability: it is that based on the original notion of recursive realizability and presents the abstract approach to recursive realizability in some detail. The chapter introduces effective topos and discusses the notion of a negative formula that arises naturally in the theory of sheaves. The chapter presents features of effective topos, where the power-set matters: uniformity principles and properties of j-operators. The chapter recommends that while constructing a topos from a tripos, one must add new subobjects of the sets one has started with to represent the nonstandard predicates and take quotients of these by the nonstandard equivalence relations.},
  isbn = {0049-237X},
  series = {The {{L}}. {{E}}. {{J}}. {{Brouwer Centenary Symposium Proceedings}} of the {{Conference}} Held in {{Noordwijkerhout}}}
}

@article{Hyland1988small,
  title = {A Small Complete Category},
  author = {Hyland, J. M. E.},
  year = {1988},
  month = nov,
  volume = {40},
  pages = {135--165},
  issn = {0168-0072},
  doi = {10.1016/0168-0072(88)90018-8},
  journal = {Annals of Pure and Applied Logic},
  number = {2}
}

@article{Hyland2007combining,
  title = {Combining Algebraic Effects with Continuations},
  author = {Hyland, Martin and Levy, Paul Blain and Plotkin, Gordon and Power, John},
  year = {2007},
  month = may,
  volume = {375},
  pages = {20--40},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.12.026},
  abstract = {We consider the natural combinations of algebraic computational effects such as side-effects, exceptions, interactive input/output, and nondeterminism with continuations. Continuations are not an algebraic effect, but previously developed combinations of algebraic effects given by sum and tensor extend, with effort, to include commonly used combinations of the various algebraic effects with continuations. Continuations also give rise to a third sort of combination, that given by applying the continuations monad transformer to an algebraic effect. We investigate the extent to which sum and tensor extend from algebraic effects to arbitrary monads, and the extent to which Felleisen et~al.'s C operator extends from continuations to its combination with algebraic effects. To do all this, we use Dubuc's characterisation of strong monads in terms of enriched large Lawvere theories.},
  journal = {Theoretical Computer Science},
  keywords = {Computational effect,Lawvere theory,modularity,Monad},
  number = {1\textendash{}3},
  series = {Festschrift for {{John C}}. {{Reynolds}}'s 70th Birthday}
}

@article{Ilik2013Continuationpassing,
  title = {Continuation-Passing Style Models Complete for Intuitionistic Logic},
  author = {Ilik, Danko},
  year = {2013},
  month = jun,
  volume = {164},
  pages = {651--662},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2012.05.003},
  abstract = {A class of models is presented, in the form of continuation monads polymorphic for first-order individuals, that is sound and complete for minimal intuitionistic predicate logic (including disjunction and the existential quantifier). The proofs of soundness and completeness are constructive and the computational content of their composition is, in particular, a {$\beta$}-normalisation-by-evaluation program for simply typed lambda calculus with sum types. Although the inspiration comes from Danvy's type-directed partial evaluator for the same lambda calculus, the use of delimited control operators (i.e. computational effects) is avoided. The role of polymorphism is crucial \textendash{} dropping it allows one to obtain a notion of model complete for classical predicate logic.},
  journal = {Annals of Pure and Applied Logic},
  keywords = {Completeness,Double-negation shift,Intuitionistic logic,Kripke models,Normalisation by evaluation},
  note = {00000},
  number = {6},
  series = {Classical {{Logic}} and {{Computation}} 2010({{CLAC}} 2010)}
}

@article{Ilik2013interpretation,
  title = {An Interpretation of the {{Sigma}}-2 Fragment of Classical {{Analysis}} in {{System T}}},
  author = {Ilik, Danko},
  year = {2013},
  month = jan,
  abstract = {We show that it is possible to define a realizability interpretation for the \$\textbackslash{}Sigma\_2\$-fragment of classical Analysis using G\textbackslash{}"odel's System T only. This supplements a previous result of Schwichtenberg regarding bar recursion at types 0 and 1 by showing how to avoid using bar recursion altogether. Our result is proved via a conservative extension of System T with an operator for composable continuations from the theory of programming languages due to Danvy and Filinski. The fragment of Analysis is therefore essentially constructive, even in presence of the full Axiom of Choice schema: Weak Church's Rule holds of it in spite of the fact that it is strong enough to refute the formal arithmetical version of Church's Thesis.},
  archivePrefix = {arXiv},
  eprint = {1301.5089},
  eprinttype = {arxiv},
  journal = {arXiv:1301.5089 [cs, math]},
  keywords = {03F25; 03F10; 03F60; 03B30; 03B20; 03E25; 03D65; 68N15,Computer Science - Logic in Computer Science,Mathematics - Logic},
  primaryClass = {cs, math}
}

@inproceedings{Im2011syntactic,
  title = {A {{Syntactic Type System}} for {{Recursive Modules}}},
  booktitle = {Proceedings of the 2011 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Im, Hyeonseung and Nakata, Keiko and Garrigue, Jacques and Park, Sungwoo},
  year = {2011},
  pages = {993--1012},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2048066.2048141},
  abstract = {A practical type system for ML-style recursive modules should address at least two technical challenges. First, it needs to solve the double vision problem, which refers to an inconsistency between external and internal views of recursive modules. Second, it needs to overcome the tension between practical decidability and expressivity which arises from the potential presence of cyclic type definitions caused by recursion between modules. Although type systems in previous proposals solve the double vision problem and are also decidable, they fail to typecheck common patterns of recursive modules, such as functor fixpoints, that are essential to the expressivity of the module system and the modular development of recursive modules. This paper proposes a novel type system for recursive modules that solves the double vision problem and typechecks common patterns of recursive modules including functor fixpoints. First, we design a type system with a type equivalence based on weak bisimilarity, which does not lend itself to practical implementation in general, but accommodates a broad range of cyclic type definitions. Then, we identify a practically implementable fragment using a type equivalence based on type normalization, which is expressive enough to typecheck typical uses of recursive modules. Our approach is purely syntactic and the definition of the type system is ready for use in an actual implementation.},
  isbn = {978-1-4503-0940-0},
  keywords = {abstract types,modules,recursion,type systems,weak bisimulations},
  series = {{{OOPSLA}} '11}
}

@incollection{Im2013contractive,
  title = {Contractive {{Signatures}} with {{Recursive Types}}, {{Type Parameters}}, and {{Abstract Types}}},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  author = {Im, Hyeonseung and Nakata, Keiko and Park, Sungwoo},
  editor = {Fomin, Fedor V. and Freivalds, R{\=u}si{\c n}{\v s} and Kwiatkowska, Marta and Peleg, David},
  year = {2013},
  pages = {299--311},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Although theories of equivalence or subtyping for recursive types have been extensively investigated, sophisticated interaction between recursive types and abstract types has gained little attention. The key idea behind type theories for recursive types is to use syntactic contractiveness, meaning every {$\mu$}-bound variable occurs only under a type constructor such as \textrightarrow{} or {${_\ast}$}. This syntactic contractiveness guarantees the existence of the unique solution of recursive equations and thus has been considered necessary for designing a sound theory for recursive types. However, in an advanced type system, such as OCaml, with recursive types, type parameters, and abstract types, we cannot easily define the syntactic contractiveness of types. In this paper, we investigate a sound type system for recursive types, type parameters, and abstract types. In particular, we develop a new semantic notion of contractiveness for types and signatures using mixed induction and coinduction, and show that our type system is sound with respect to the standard call-by-value operational semantics, which eliminates signature sealings. Moreover we show that while non-contractive types in signatures lead to unsoundness of the type system, they may be allowed in modules. We have also formalized the whole system and its type soundness proof in Coq.},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-39211-5 978-3-642-39212-2},
  keywords = {Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Computer Communication Networks,Discrete Mathematics in Computer Science,Information Storage and Retrieval,Information Systems Applications (incl. Internet)},
  language = {en},
  number = {7966},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Intrigila2017Refutation,
  title = {Refutation of {{Sall{\'e}}}'s {{Longstanding Conjecture}}},
  booktitle = {2nd {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}} ({{FSCD}} 2017)},
  author = {Intrigila, Benedetto and Manzonetto, Giulio and Polonsky, Andrew},
  editor = {Miller, Dale},
  year = {2017},
  volume = {84},
  pages = {20:1--20:18},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.FSCD.2017.20},
  isbn = {978-3-95977-047-7},
  keywords = {Böhm trees,lambda calculus,Observational equivalence,omega-rule},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Ishtiaq2001bi,
  title = {{{BI As}} an {{Assertion Language}} for {{Mutable Data Structures}}},
  booktitle = {Proceedings of the 28th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ishtiaq, Samin S. and O'Hearn, Peter W.},
  year = {2001},
  pages = {14--26},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/360204.375719},
  abstract = {Reynolds has developed a logic for reasoning about mutable data structures in which the pre- and postconditions are written in an intuitionistic logic enriched with a spatial form of conjunction. We investigate the approach from the point of view of the logic BI of bunched implications of O'Hearnand Pym. We begin by giving a model in which the law of the excluded middleholds, thus showing that the approach is compatible with classical logic. The relationship between the intuitionistic and classical versions of the system is established by a translation, analogous to a translation from intuitionistic logic into the modal logic S4. We also consider the question of completeness of the axioms. BI's spatial implication is used to express weakest preconditions for object-component assignments, and an axiom for allocating a cons cell is shown to be complete under an interpretation of triplesthat allows a command to be applied to states with dangling pointers. We make this latter a feature, by incorporating an operation, and axiom, for disposing of memory. Finally, we describe a local character enjoyed by specifications in the logic, and show how this enables a class of frame axioms, which say what parts of the heap don't change, to be inferred automatically.},
  isbn = {1-58113-336-7},
  series = {{{POPL}} '01}
}

@inproceedings{Isradisaikul2013Reconciling,
  title = {Reconciling Exhaustive Pattern Matching with Objects},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Isradisaikul, Chinawat and Myers, Andrew C.},
  year = {2013},
  pages = {343--354},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2491956.2462194},
  abstract = {Pattern matching, an important feature of functional languages, is in conflict with data abstraction and extensibility, which are central to object-oriented languages. Modal abstraction offers an integration of deep pattern matching and convenient iteration abstractions into an object-oriented setting; however, because of data abstraction, it is challenging for a compiler to statically verify properties such as exhaustiveness. In this work, we extend modal abstraction in the JMatch language to support static, modular reasoning about exhaustiveness and redundancy. New matching specifications allow these properties to be checked using an SMT solver. We also introduce expressive pattern-matching constructs. Our evaluation shows that these new features enable more concise code and that the performance of checking exhaustiveness and redundancy is acceptable.},
  isbn = {978-1-4503-2014-6},
  keywords = {data abstraction,equality constructor,exhaustiveness,java,jmatch,matching specification,modal abstraction,named constructor,pattern matching,redundancy,subtyping},
  series = {{{PLDI}} '13}
}

@inproceedings{Jackson1994semantic,
  title = {Semantic {{Diff}}: A Tool for Summarizing the Effects of Modifications},
  shorttitle = {Semantic {{Diff}}},
  booktitle = {, {{International Conference}} on {{Software Maintenance}}, 1994. {{Proceedings}}},
  author = {Jackson, D. and Ladd, D.A.},
  year = {1994},
  month = sep,
  pages = {243--252},
  doi = {10.1109/ICSM.1994.336770},
  abstract = {Describes a tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them. Unlike existing tools based on comparison of program dependence graphs, our tool expresses its results in terms of the observable input-output behaviour of the procedure, rather than its syntactic structure. And because the analysis is truly semantic, it requires no prior matching of syntactic components, and generates fewer spurious differences, so that meaning-preserving transformations (such as renaming local variables) are correctly determined to have no visible effect. A preliminary experiment on modifications applied to the code of a large real-time system suggests that the approach is practical},
  keywords = {configuration management,input-output programs,local variable renaming,meaning-preserving transformations,observable input-output behaviour,procedure versions,program dependence graphs,program diagnostics,real-time system,real-time systems,report generation,Semantic Diff,semantic differences,Software fault diagnosis,software modification effects summarization,software tools,spurious differences,syntactic components}
}

@incollection{Jacobs1996objects,
  title = {Objects {{And Classes}}, {{Co}}-{{Algebraically}}  (2)},
  booktitle = {Object {{Orientation}} with {{Parallelism}} and {{Persistence}}},
  author = {Jacobs, Bart},
  editor = {Freitag, Burkhard and Jones, Cliff B. and Lengauer, Christian and Schek, Hans-J{\"o}rg},
  year = {1996},
  month = jan,
  pages = {83--103},
  publisher = {{Springer US}},
  abstract = {The co-algebraic perspective on objects and classes in object-oriented programming is elaborated: classes are described as co-algebras, which may occur as models (implementations) of co-algebraic specifications. These specifications are much like deferred (or virtual) classes with assertions in Eiffel. An object belonging to a class is an element of the state space of the class, as co-algebra. We show how terminal co-algebras of co-algebraic specifications give rise to canonical models (in which all observationally indistinguishable objects are identified). We further describe operational semantics for objects, with an associated notion of bisimulation (for objects in classes modeling the same specification), expressing observational indistinguishability.},
  copyright = {\textcopyright{}1996 Springer-Verlag US},
  isbn = {978-1-4612-8625-7 978-1-4613-1437-0},
  keywords = {Data Structures; Cryptology and Information Theory,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {370},
  series = {The {{Kluwer International Series}} in {{Engineering}} and {{Computer Science}}}
}

@incollection{Jacobs1996objectsa,
  title = {Objects and Classes, Co-Algebraically (1)},
  booktitle = {Object {{Orientation}} with {{Parallelism}} and {{Persistence}}},
  author = {Jacobs, Bart},
  editor = {Freitag, Burkhard and Jones, Cliff B. and Lengauer, Christian and Schek, Hans-J{\"o}rg},
  year = {1996},
  pages = {83--103},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Norwell, MA, USA}},
  isbn = {0-7923-9770-3}
}

@article{Jaffe1993theoretical,
  title = {``{{Theoretical}} Mathematics'': Toward a Cultural Synthesis of Mathematics and Theoretical Physics},
  shorttitle = {``{{Theoretical}} Mathematics''},
  author = {Jaffe, Arthur and Quinn, Frank},
  year = {1993},
  volume = {29},
  pages = {1--13},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0273-0979-1993-00413-0},
  abstract = {Is speculative mathematics dangerous? Recent interactions between physics and mathematics pose the question with some force: traditional mathematical norms discourage speculation, but it is the fabric of theoretical physics. In practice there can be benefits, but there can also be unpleasant and destructive consequences. Serious caution is required, and the issue should be considered before, rather than after, obvious damage occurs. With the hazards carefully in mind, we propose a framework that should allow a healthy and positive role for speculation.},
  journal = {Bulletin of the American Mathematical Society},
  number = {1}
}

@inproceedings{James2012Information,
  title = {Information {{Effects}}},
  booktitle = {Proceedings of the 39th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {James, Roshan P. and Sabry, Amr},
  year = {2012},
  pages = {73--84},
  publisher = {{ACM}},
  address = {{Philadelphia, PA, USA}},
  doi = {10.1145/2103656.2103667},
  abstract = {Computation is a physical process which, like all other physical processes, is fundamentally reversible. From the notion of type isomorphisms, we derive a typed, universal, and reversible computational model in which information is treated as a linear resource that can neither be duplicated nor erased. We use this model as a semantic foundation for computation and show that the "gap" between conventional irreversible computation and logically reversible computation can be captured by a type-and-effect system. Our type-and-effect system is structured as an arrow metalanguage that exposes creation and erasure of information as explicit effect operations. Irreversible computations arise from interactions with an implicit information environment, thus making them a derived notion, much like open systems in Physics. We sketch several applications which can benefit from an explicit treatment of information effects, such as quantitative information-flow security and differential privacy.},
  isbn = {978-1-4503-1083-3},
  keywords = {arrows,linear logic,quantum computing,reversible logic},
  series = {{{POPL}} '12}
}

@article{Jansson1998polytypic,
  title = {Polytypic Unification},
  author = {Jansson, Patrik and Jeuring, Johan},
  year = {1998},
  month = sep,
  volume = {8},
  pages = {527--536},
  issn = {1469-7653},
  doi = {null},
  abstract = {Unification, or two-way pattern matching, is the process of solving an equation involving two first-order terms with variables. Unification is used in type inference in many programming languages and in the execution of logic programs. This means that unification algorithms have to be written over and over again for different term types. Many other functions also make sense for a large class of datatypes; examples are pretty printers, equality checks, maps etc. They can be defined by induction on the structure of user-defined datatypes. Implementations of these functions for different datatypes are closely related to the structure of the datatypes. We call such functions polytypic. This paper describes a unification algorithm parametrised on the type of the terms, and shows how to use polytypism to obtain a unification algorithm that works for all regular term types.},
  journal = {Journal of Functional Programming},
  number = {05}
}

@article{Jaskelioff2011modularity,
  title = {Modularity and {{Implementation}} of {{Mathematical Operational Semantics}}},
  author = {Jaskelioff, Mauro and Ghani, Neil and Hutton, Graham},
  year = {2011},
  month = mar,
  volume = {229},
  pages = {75--95},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2011.02.017},
  abstract = {Structural operational semantics is a popular technique for specifying the meaning of programs by means of inductive clauses. One seeks syntactic restrictions on those clauses so that the resulting operational semantics is well-behaved. This approach is simple and concrete but it has some drawbacks. Turi pioneered a more abstract categorical treatment based upon the idea that operational semantics is essentially a distribution of syntax over behaviour. In this article we take Turi's approach in two new directions. Firstly, we show how to write operational semantics as modular components and how to combine such components to specify complete languages. Secondly, we show how the categorical nature of Turi's operational semantics makes it ideal for implementation in a functional programming language such as Haskell.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {category theory,Haskell,modularity,operational semantics},
  number = {5},
  series = {Proceedings of the {{Second Workshop}} on {{Mathematically Structured Functional Programming}} ({{MSFP}} 2008)}
}

@inproceedings{Jaskelioff2015functional,
  title = {Functional {{Pearl}}: {{A Smart View}} on {{Datatypes}}},
  shorttitle = {Functional {{Pearl}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Jaskelioff, Mauro and Rivas, Exequiel},
  year = {2015},
  pages = {355--361},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784743},
  abstract = {Left-nested list concatenations, left-nested binds on the free monad, and left-nested choices in many non-determinism monads have an algorithmically bad performance. Can we solve this problem without losing the ability to pattern-match on the computation? Surprisingly, there is a deceptively simple solution: use a smart view to pattern-match on the datatype. We introduce the notion of smart view and show how it solves the problem of slow left-nested operations. In particular, we use the technique to obtain fast and simple implementations of lists, of free monads, and of two non-determinism monads.},
  isbn = {978-1-4503-3669-7},
  keywords = {data structure,List,Monad,MonadPlus},
  series = {{{ICFP}} 2015}
}

@article{Jay2016Programs,
  title = {Programs as {{Data Structures}} in {{$\lambda$SF}}-{{Calculus}}},
  author = {Jay, Barry},
  year = {2016},
  month = oct,
  volume = {325},
  pages = {221--236},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2016.09.040},
  abstract = {Lambda-SF-calculus can represent programs as closed normal forms. In turn, all closed normal forms are data structures, in the sense that their internal structure is accessible through queries defined in the calculus, even to the point of constructing the Goedel number of a program. Thus, program analysis and optimisation can be performed entirely within the calculus, without requiring any meta-level process of quotation to produce a data structure.

Lambda-SF-calculus is a confluent, applicative rewriting system derived from lambda-calculus, and the combinatory SF-calculus. Its superior expressive power relative to lambda-calculus is demonstrated by the ability to decide if two programs are syntactically equal, or to determine if a program uses its input. Indeed, there is no homomorphism of applicative rewriting systems from lambda-SF-calculus to lambda-calculus.

Program analysis and optimisation can be illustrated by considering the conversion of a programs to combinators. Traditionally, a program p is interpreted using fixpoint constructions that do not have normal forms, but combinatory techniques can be used to block reduction until the program arguments are given. That is, p is interpreted by a closed normal form M. Then factorisation (by F) adapts the traditional account of lambda-abstraction in combinatory logic to convert M to a combinator N that is equivalent to M in the following two senses. First, N is extensionally equivalent to M where extensional equivalence is defined in terms of eta-reduction. Second, the conversion is an intensional equivalence in that it does not lose any information, and so can be reversed by another definable conversion. Further, the standard optimisations of the conversion process are all definable within lambda-SF-calculus, even those involving free variable analysis.

Proofs of all theorems in the paper have been verified using the Coq theorem prover.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Lambda-calculus,self-interpretation,SF-calculus,xi-rule},
  series = {The {{Thirty}}-Second {{Conference}} on the {{Mathematical Foundations}} of {{Programming Semantics}} ({{MFPS XXXII}})}
}

@inproceedings{Jeuring1991Incremental,
  title = {Incremental {{Algorithms}} on {{Lists}}},
  booktitle = {Proceedings {{SION Computing Science}} in the {{Netherlands}}},
  author = {Jeuring, Johan},
  year = {1991},
  pages = {315--335},
  abstract = {Incremental computations can improve the performance of interactive programs such as spreadsheet programs, program development environments, text editors, etc. Incremental algorithms describe how to compute a required value depending on the input, after the input has been edited. By considering the possible different edit actions on the data type lists, the basic data type used in spreadsheet programs and text editors, we define incremental algorithms on lists. Some theory for the construction of incremental algorithms is developed, and we give an incremental algorithm for a more involved example: formatting a text.  CR categories and descriptors: D11 [Software]: Programming Techniques --- Applicative Programming, D43 [Software]: Programming Languages --- Language constructs, I22 [Artificial Intelligence]: Automatic Programming --- Program transformation. General terms: algorithm, design, theory. Additional keywords and phrases: Bird-Meertens calculus for program construction, incremen...}
}

@techreport{Jeuring1993Theories,
  title = {Theories for {{Algorithm Calculation}}},
  author = {Jeuring, Johan Theodoor},
  year = {1993}
}

@inproceedings{Jia2010dependent,
  title = {Dependent {{Types}} and {{Program Equivalence}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Jia, Limin and Zhao, Jianzhou and Sj{\"o}berg, Vilhelm and Weirich, Stephanie},
  year = {2010},
  pages = {275--286},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706299.1706333},
  abstract = {The definition of type equivalence is one of the most important design issues for any typed language. In dependently typed languages, because terms appear in types, this definition must rely on a definition of term equivalence. In that case, decidability of type checking requires decidability for the term equivalence relation. Almost all dependently-typed languages require this relation to be decidable. Some, such as Coq, Epigram or Agda, do so by employing analyses to force all programs to terminate. Conversely, others, such as DML, ATS, {$\Omega$}mega, or Haskell, allow nonterminating computation, but do not allow those terms to appear in types. Instead, they identify a terminating index language and use singleton types to connect indices to computation. In both cases, decidable type checking comes at a cost, in terms of complexity and expressiveness. Conversely, the benefits to be gained by decidable type checking are modest. Termination analyses allow dependently typed programs to verify total correctness properties. However, decidable type checking is not a prerequisite for type safety. Furthermore, decidability does not imply tractability. A decidable approximation of program equivalence may not be useful in practice. Therefore, we take a different approach: instead of a fixed notion for term equivalence, we parameterize our type system with an abstract relation that is not necessarily decidable. We then design a novel set of typing rules that require only weak properties of this abstract relation in the proof of the preservation and progress lemmas. This design provides flexibility: we compare valid instantiations of term equivalence which range from beta-equivalence, to contextual equivalence, to some exotic equivalences.},
  isbn = {978-1-60558-479-9},
  keywords = {dependent types,program equivalence},
  series = {{{POPL}} '10}
}

@inproceedings{Jim1996what,
  title = {What {{Are Principal Typings}} and {{What Are They Good}} For?},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Jim, Trevor},
  year = {1996},
  pages = {42--53},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/237721.237728},
  isbn = {0-89791-769-3},
  keywords = {_tablet},
  series = {{{POPL}} '96}
}

@inproceedings{Johann2004free,
  title = {Free {{Theorems}} in the {{Presence}} of {{Seq}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Johann, Patricia and Voigtl{\"a}nder, Janis},
  year = {2004},
  pages = {99--110},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/964001.964010},
  abstract = {Parametric polymorphism constrains the behavior of pure functional programs in a way that allows the derivation of interesting theorems about them solely from their types, i.e., virtually for free. Unfortunately, the standard parametricity theorem fails for nonstrict languages supporting a polymorphic strict evaluation primitive like Haskell's seq. Contrary to the folklore surrounding seq and parametricity, we show that not even quantifying only over strict and bottom-reflecting relations in the \$\textbackslash{}forall\$-clause of the underlying logical relation --- and thus restricting the choice of functions with which such relations are instantiated to obtain free theorems to strict and total ones --- is sufficient to recover from this failure. By addressing the subtle issues that arise when propagating up the type hierarchy restrictions imposed on a logical relation in order to accommodate the strictness primitive, we provide a parametricity theorem for the subset of Haskell corresponding to a Girard-Reynolds-style calculus with fixpoints, algebraic datatypes, and seq. A crucial ingredient of our approach is the use of an asymmetric logical relation, which leads to "inequational" versions of free theorems enriched by preconditions guaranteeing their validity in the described setting. Besides the potential to obtain corresponding preconditions for standard equational free theorems by combining some new inequational ones, the latter also have value in their own right, as is exemplified with a careful analysis of seq's impact on familiar program transformations.},
  isbn = {1-58113-729-X},
  keywords = {controlling strict evaluation,correctness proofs,Denotational semantics,Haskell,logical relations,parametricity,program transformations,short cut fusion,theorems for free},
  series = {{{POPL}} '04}
}

@article{Johnson-Freyd2016First,
  title = {First {{Class Call Stacks}}: {{Exploring Head Reduction}}},
  shorttitle = {First {{Class Call Stacks}}},
  author = {{Johnson-Freyd}, Philip and Downen, Paul and Ariola, Zena M.},
  year = {2016},
  month = jun,
  volume = {212},
  pages = {18--35},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.212.2},
  abstract = {Weak-head normalization is inconsistent with functional extensionality in the call-by-name \$\textbackslash{}lambda\$-calculus. We explore this problem from a new angle via the conflict between extensionality and effects. Leveraging ideas from work on the \$\textbackslash{}lambda\$-calculus with control, we derive and justify alternative operational semantics and a sequence of abstract machines for performing head reduction. Head reduction avoids the problems with weak-head reduction and extensionality, while our operational semantics and associated abstract machines show us how to retain weak-head reduction's ease of implementation.},
  archivePrefix = {arXiv},
  eprint = {1606.06378},
  eprinttype = {arxiv},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  keywords = {Computer Science - Programming Languages,F.3.2,F.3.3}
}

@article{Johnson1995memoization,
  title = {Memoization in {{Top}}-down {{Parsing}}},
  author = {Johnson, Mark},
  year = {1995},
  month = sep,
  volume = {21},
  pages = {405--417},
  issn = {0891-2017},
  journal = {Comput. Linguist.},
  number = {3}
}

@incollection{Johnsson1985lambda,
  title = {Lambda Lifting: {{Transforming}} Programs to Recursive Equations},
  shorttitle = {Lambda Lifting},
  booktitle = {Functional {{Programming Languages}} and {{Computer Architecture}}},
  author = {Johnsson, Thomas},
  editor = {Jouannaud, Jean-Pierre},
  year = {1985},
  month = sep,
  pages = {190--203},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}1985 Springer-Verlag},
  isbn = {978-3-540-15975-9 978-3-540-39677-2},
  keywords = {Processor Architectures},
  language = {en},
  number = {201},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Jones1991modular,
  title = {A Modular Fully-Lazy Lambda Lifter in {{Haskell}}},
  author = {Jones, Simon L. Peyton and Lester, David},
  year = {1991},
  month = may,
  volume = {21},
  pages = {479--506},
  issn = {1097-024X},
  doi = {10.1002/spe.4380210505},
  abstract = {An important step in many compilers for functional languages is lambda lifting. In his thesis, Hughes showed that by doing lambda lifting in a particular way, a useful property called full laziness can be preserved. Full laziness has been seen as intertwined with lambda lifting ever since. We show that, on the contrary, full laziness can be regarded as a completely separate process to lambda lifting, thus making it easy to use different lambda lifters following a full-laziness transformation, or to use the full-laziness transformation in compilers which do not require lambda lifting. On the way, we present the complete code for our modular fully-lazy lambda lifter, written in the HASKELL functional programming language.},
  copyright = {Copyright \textcopyright{} 1991 John Wiley \& Sons, Ltd},
  journal = {Software: Practice and Experience},
  keywords = {Full laziness,functional programming,Lambda lifting,program transformation},
  language = {en},
  number = {5}
}

@inproceedings{Jones1992theory,
  title = {A Theory of Qualified Types},
  booktitle = {{{ESOP}} '92},
  author = {Jones, Mark P.},
  year = {1992},
  month = feb,
  pages = {287--306},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-55253-7_17},
  abstract = {No Abstract available for this paper.},
  isbn = {10.1007/3-540-55253-7\_17},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@techreport{Jones1993partial,
  title = {Partial Evaluation for Dictionary-Free Overloading},
  author = {Jones, Mark P},
  year = {1993},
  institution = {{Research Report YALEU/DCS/RR-959, Yale University, New Haven, Connecticut, USA}}
}

@inproceedings{Jones1994dictionaryfree,
  title = {Dictionary-{{Free Overloading}} by {{Partial Evaluation}} (2)},
  booktitle = {{{PEPM}}},
  author = {Jones, Mark P.},
  year = {1994},
  pages = {107--117},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  crossref = {DBLP:conf/pepm/1994}
}

@article{Jones1995dictionaryfree,
  title = {Dictionary-Free Overloading by Partial Evaluation (1)},
  author = {Jones, Mark P.},
  year = {1995},
  month = sep,
  volume = {8},
  pages = {229--248},
  issn = {0892-4635, 1573-0557},
  doi = {10.1007/BF01019005},
  abstract = {One of the most novel features in the functional programming language Haskell is the system oftype classes used to support a combination of overloading and polymorphism. Current implementations of type class overloading are based on the use ofdictionary values, passed as extra parameters to overloaded functions. Unfortunately, this can have a significant effect on run-time performance, for example, by reducing the effectiveness of important program analyses and optimizations. This paper describes how a simple partial evaluator can be used to avoid the need for dictionary values at run-time by generating specialized versions of overloaded functions. This eliminates the run-time costs of overloading. Furthermore, and somewhat surprisingly given the presence of multiple versions of some functions, for all of the examples that we have tried so far, specialization actually leads to a reduction in the size of compiled programs.},
  journal = {LISP and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),Haskell,Numeric Computing,partial evaluation,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems,specialization,type class overloading},
  language = {en},
  number = {3}
}

@inproceedings{Jones1995Functional,
  title = {Functional Programming with Overloading and Higher-Order Polymorphism},
  booktitle = {Advanced {{Functional Programming}}},
  author = {Jones, Mark P.},
  year = {1995},
  month = may,
  pages = {97--136},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-59451-5_4},
  abstract = {The Hindley/Milner type system has been widely adopted as a basis for statically typed functional languages. One of the main reasons for this is that it provides an elegant compromise between flexibility, allowing a single value to be used in different ways, and practicality, freeing the programmer from the need to supply explicit type information. Focusing on practical applications rather than implementation or theoretical details, these notes examine a range of extensions that provide more flexible type systems while retaining many of the properties that have made the original Hindley/Milner system so popular. The topics discussed, some old, but most quite recent, include higher-order polymorphism and type and constructor class overloading. Particular emphasis is placed on the use of these features to promote modularity and reusability.},
  isbn = {978-3-540-59451-2 978-3-540-49270-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Jones1995mix,
  title = {Mix {{Ten Years Later}}},
  booktitle = {Proceedings of the 1995 {{ACM SIGPLAN Symposium}} on {{Partial Evaluation}} and {{Semantics}}-Based {{Program Manipulation}}},
  author = {Jones, Neil D.},
  year = {1995},
  pages = {24--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/215465.215468},
  isbn = {0-89791-720-0},
  series = {{{PEPM}} '95}
}

@inproceedings{Jones1997henk,
  title = {Henk: {{A Typed Intermediate Language}}},
  shorttitle = {Henk},
  booktitle = {In {{Proc}}. {{First Int}}'l {{Workshop}} on {{Types}} in {{Compilation}}},
  author = {Jones, Simon Peyton and Meijer, Erik},
  year = {1997},
  abstract = {There is growing interest in the use of richly-typed intermediate languages in sophisticated compilers for higher-order, typed source languages. These intermediate languages are typically stratified, involving terms, types, and kinds. As the sophistication of the type system increases, these three levels begin to look more and more similar, so an attractive approach is to use a single syntax, and a single data type in the compiler, to represent all three. The theory of so-called pure type systems makes precisely such an identification. This paper describes Henk, a new typed intermediate language based closely on a particular pure type system, the lambda cube. On the way we give a tutorial introduction to the lambda cube. 1 Overview  Many compilers can be divided into three main stages. The  front end translates the source language into an intermediate  language; the middle end transforms the intermediatelanguage into a more efficient form; and the back end translates the intermediate l...}
}

@article{Jones2007Practical,
  title = {Practical Type Inference for Arbitrary-Rank Types},
  author = {Jones, Simon Peyton and Vytiniotis, Dimitrios and Weirich, Stephanie and Shields, Mark},
  year = {2007},
  month = jan,
  volume = {17},
  pages = {1--82},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796806006034},
  abstract = {Haskell's popularity has driven the need for ever more expressive type system features, most of which threaten the decidability and practicality of Damas-Milner type inference. One such feature is the ability to write functions with higher-rank types \textendash{} that is, functions that take polymorphic functions as their arguments. Complete type inference is known to be undecidable for higher-rank (impredicative) type systems, but in practice programmers are more than willing to add type annotations to guide the type inference engine, and to document their code. However, the choice of just what annotations are required, and what changes are required in the type system and its inference algorithm, has been an ongoing topic of research. We take as our starting point a {$\lambda$}-calculus proposed by Odersky and L{\"a}ufer. Their system supports arbitrary-rank polymorphism through the exploitation of type annotations on {$\lambda$}-bound arguments and arbitrary sub-terms. Though elegant, and more convenient than some other proposals, Odersky and L{\"a}ufer's system requires many annotations. We show how to use local type inference (invented by Pierce and Turner) to greatly reduce the annotation burden, to the point where higher-rank types become eminently usable. Higher-rank types have a very modest impact on type inference. We substantiate this claim in a very concrete way, by presenting a complete type-inference engine, written in Haskell, for a traditional Damas-Milner type system, and then showing how to extend it for higher-rank types. We write the type-inference engine using a monadic framework: it turns out to be a particularly compelling example of monads in action. The paper is long, but is strongly tutorial in style. Although we use Haskell as our example source language, and our implementation language, much of our work is directly applicable to any ML-like functional language.},
  journal = {Journal of Functional Programming},
  language = {en},
  number = {1}
}

@inproceedings{Jones2016mechanical,
  title = {A Mechanical Soundness Proof for Subtyping over Recursive Types},
  booktitle = {Proceedings of the 18th {{Workshop}} on {{Formal Techniques}} for {{Java}}-like {{Programs}}},
  author = {Jones, Timothy and Pearce, David J.},
  year = {2016},
  pages = {1:1--1:6},
  publisher = {{ACM}},
  address = {{Rome, Italy}},
  doi = {10.1145/2955811.2955812},
  abstract = {Structural type systems provide an interesting alternative to the more common nominal typing scheme. Several existing languages employ structural types in some form, including Modula-3, Scala and various extensions proposed for Java. However, formalising a recursive structural type system is challenging. In particular, the need to use structural coinduction remains a hindrance for many. We formalise in Agda a simple recursive and structural type system with products and unions. Agda proves useful here because it has explicit support for coinduction and will raise an error if this is misused. The implementation distinguishes between inductively and coinductively defined types: the former corresponds to a finite representation, such as found in source code or the internals of a compiler, while the latter corresponds to a mathematical ideal with which we can coinductively define relations and proofs that are easily applied back to the inductive interpretation. As an application of this, we provide a mechanised proof of subtyping soundness against a semantic embedding of the types into Agda.},
  isbn = {978-1-4503-4439-5},
  series = {{{FTfJP}}'16}
}

@article{Jonnalagedda2014staged,
  title = {On {{Staged Parser Combinators}} for {{Efficient Data Processing}}},
  author = {Jonnalagedda, Manohar and Coppey, Thierry and Stucki, Sandro and Rompf, Tiark and Odersky, Martin},
  year = {2014},
  keywords = {Dynamic programming,parallel processing,parser combinators,staging}
}

@inproceedings{Jonnalagedda2015foldbased,
  title = {Fold-Based {{Fusion As}} a {{Library}}: {{A Generative Programming Pearl}}},
  shorttitle = {Fold-Based {{Fusion As}} a {{Library}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Jonnalagedda, Manohar and Stucki, Sandro},
  year = {2015},
  pages = {41--50},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2774975.2774981},
  abstract = {Fusion is a program optimisation technique commonly implemented using special-purpose compiler support. In this paper, we present an alternative approach, implementing fold-based fusion as a standalone library. We use staging to compose operations on folds; the operations are partially evaluated away, yielding code that does not construct unnecessary intermediate data structures. The technique extends to partitioning and grouping of collections.},
  isbn = {978-1-4503-3626-0},
  keywords = {deforestation,fold,fusion,multi-stage programming,Program optimisation},
  series = {{{SCALA}} 2015}
}

@inproceedings{Jovanovic2014Yinyang,
  title = {Yin-Yang: {{Concealing}} the {{Deep Embedding}} of {{DSLs}}},
  shorttitle = {Yin-Yang},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Jovanovic, Vojin and Shaikhha, Amir and Stucki, Sandro and Nikolaev, Vladimir and Koch, Christoph and Odersky, Martin},
  year = {2014},
  pages = {73--82},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2658761.2658771},
  abstract = {Deeply embedded domain-specific languages (EDSLs) intrinsically compromise programmer experience for improved program performance. Shallow EDSLs complement them by trading program performance for good programmer experience. We present Yin-Yang, a framework for DSL embedding that uses Scala macros to reliably translate shallow EDSL programs to the corresponding deep EDSL programs. The translation allows program prototyping and development in the user friendly shallow embedding, while the corresponding deep embedding is used where performance is important. The reliability of the translation completely conceals the deep em- bedding from the user. For the DSL author, Yin-Yang automatically generates the deep DSL embeddings from their shallow counterparts by reusing the core translation. This obviates the need for code duplication and leads to reliability by construction.},
  isbn = {978-1-4503-3161-6},
  keywords = {Deep Embedding,embedded domain-specific languages,macros,reflection,Shallow Embedding},
  series = {{{GPCE}} 2014}
}

@inproceedings{Jung2015Iris,
  title = {Iris: {{Monoids}} and Invariants as an Orthogonal Basis for Concurrent Reasoning},
  shorttitle = {Iris},
  booktitle = {Proceedings of the {{42Nd Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Jung, Ralf and Swasey, David and Sieczkowski, Filip and Svendsen, Kasper and Turon, Aaron and Birkedal, Lars and Dreyer, Derek},
  year = {2015},
  pages = {637--650},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2676726.2676980},
  abstract = {We present Iris, a concurrent separation logic with a simple premise: monoids and invariants are all you need. Partial commutative monoids enable us to express---and invariants enable us to enforce---user-defined *protocols* on shared state, which are at the conceptual core of most recent program logics for concurrency. Furthermore, through a novel extension of the concept of a *view shift*, Iris supports the encoding of *logically atomic specifications*, i.e., Hoare-style specs that permit the client of an operation to treat the operation essentially as if it were atomic, even if it is not.},
  isbn = {978-1-4503-3300-9},
  keywords = {atomicity,compositional verification,fine-grained concurrency,higher-order logic,invariants,partial commutative monoids,separation logic},
  series = {{{POPL}} '15}
}

@inproceedings{Jung2016Higherorder,
  title = {Higher-Order Ghost State},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Jung, Ralf and Krebbers, Robbert and Birkedal, Lars and Dreyer, Derek},
  year = {2016},
  pages = {256--269},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951943},
  abstract = {The development of concurrent separation logic (CSL) has sparked a long line of work on modular verification of sophisticated concurrent programs. Two of the most important features supported by several existing extensions to CSL are higher-order quantification and custom ghost state. However, none of the logics that support both of these features reap the full potential of their combination. In particular, none of them provide general support for a feature we dub "higher-order ghost state": the ability to store arbitrary higher-order separation-logic predicates in ghost variables.   In this paper, we propose higher-order ghost state as a interesting and useful extension to CSL, which we formalize in the framework of Jung et al.'s recently developed Iris logic. To justify its soundness, we develop a novel algebraic structure called CMRAs ("cameras"), which can be thought of as "step-indexed partial commutative monoids". Finally, we show that Iris proofs utilizing higher-order ghost state can be effectively formalized in Coq, and discuss the challenges we faced in formalizing them.},
  isbn = {978-1-4503-4219-3},
  keywords = {compositional verification,fine-grained concurrency,higher- order logic,interactive theorem proving,Separation logic},
  series = {{{ICFP}} 2016}
}

@article{Jung2017RustBelt,
  title = {{{RustBelt}}: {{Securing}} the Foundations of the {{Rust}} Programming Language},
  shorttitle = {Rustbelt},
  author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {66:1--66:34},
  issn = {2475-1421},
  doi = {10.1145/3158154},
  abstract = {Rust is a new systems programming language that promises to overcome the seemingly fundamental tradeoff between high-level safety guarantees and low-level control over resource management. Unfortunately, none of Rust's safety claims have been formally proven, and there is good reason to question whether they actually hold. Specifically, Rust employs a strong, ownership-based type system, but then extends the expressive power of this core type system through libraries that internally use unsafe features. In this paper, we give the first formal (and machine-checked) safety proof for a language representing a realistic subset of Rust. Our proof is extensible in the sense that, for each new Rust library that uses unsafe features, we can say what verification condition it must satisfy in order for it to be deemed a safe extension to the language. We have carried out this verification for some of the most important libraries that are used throughout the Rust ecosystem.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {concurrency,logical relations,Rust,separation logic,type systems},
  number = {POPL}
}

@article{Jung2018Iris,
  title = {Iris from the Ground up: {{A}} Modular Foundation for Higher-Order Concurrent Separation Logic},
  shorttitle = {Iris from the Ground Up},
  author = {Jung, Ralf and Krebbers, Robbert and Jourdan, Jacques-Henri and Bizjak, Ale{\v s} and Birkedal, Lars and Dreyer, Derek},
  year = {2018/ed},
  volume = {28},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796818000151},
  abstract = {Iris is a framework for higher-order concurrent separation logic, which has been implemented in the Coq proof assistant and deployed very effectively in a wide variety of verification projects. Iris was designed with the express goal of simplifying and consolidating the foundations of modern separation logics, but it has evolved over time, and the design and semantic foundations of Iris itself have yet to be fully written down and explained together properly in one place. Here, we attempt to fill this gap, presenting a reasonably complete picture of the latest version of Iris (version 3.1), from first principles and in one coherent narrative.},
  journal = {Journal of Functional Programming},
  language = {en}
}

@article{K2016LivenessBased,
  title = {Liveness-{{Based Garbage Collection}} for {{Lazy Languages}}},
  author = {K, Prasanna Kumar and Sanyal, Amitabha and Karkare, Amey},
  year = {2016},
  month = apr,
  abstract = {We consider the problem of reducing the memory required to run lazy first-order functional programs. Our approach is to analyze programs for liveness of heap-allocated data. The result of the analysis is used to preserve only live data---a subset of reachable data---during garbage collection. The result is an increase in the garbage reclaimed and a reduction in the peak memory requirement of programs. While this technique has already been shown to yield benefits for eager first-order languages, the lack of a statically determinable execution order and the presence of closures pose new challenges for lazy languages. These require changes both in the liveness analysis itself and in the design of the garbage collector. To show the effectiveness of our method, we implemented a copying collector that uses the results of the liveness analysis to preserve live objects, both evaluated (i.e., in WHNF) and closures. Our experiments confirm that for programs running with a liveness-based garbage collector, there is a significant decrease in peak memory requirements. In addition, a sizable reduction in the number of collections ensures that in spite of using a more complex garbage collector, the execution times of programs running with liveness and reachability-based collectors remain comparable.},
  archivePrefix = {arXiv},
  eprint = {1604.05841},
  eprinttype = {arxiv},
  journal = {arXiv:1604.05841 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Kabir2018kDOT,
  title = {{{$\kappa$DOT}}: {{Scaling DOT}} with Mutation and Constructors},
  shorttitle = {{$K$}dot},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Kabir, Ifaz and Lhot{\'a}k, Ond{\v r}ej},
  year = {2018},
  pages = {40--50},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3241653.3241659},
  abstract = {Scala unifies concepts from object and module systems by allowing for objects with type members which are referenced via path-dependent types. The Dependent Object Types (DOT) calculus of Amin et al. models only this core part of Scala, but does not have many fundamental features of Scala such as strict and mutable fields. Since the most commonly used field types in Scala are strict,the correspondence between DOT and Scala is too weak for us to meaningfully prove static analyses safe for Scala by proving them safe for DOT.  A DOT calculus that can support strict and mutable fields together with constructors that do field initialization would be more suitable for analysis of Scala. Toward this goal, we present {$\kappa$}DOT, an extension of DOT that supports constructors and field mutation and can emulate the different types of fields in Scala. We have proven {$\kappa$}DOT sound through a mechanized proof in Coq. We present the key features of {$\kappa$}DOT and its operational semantics and discuss work-in-progress toward making {$\kappa$}DOT fully strict.},
  isbn = {978-1-4503-5836-1},
  keywords = {dependent object types,mutation,type safety},
  series = {Scala 2018}
}

@inproceedings{Kagawa2006polymorphic,
  title = {Polymorphic {{Variants}} in {{Haskell}}},
  booktitle = {Proceedings of the 2006 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  author = {Kagawa, Koji},
  year = {2006},
  pages = {37--47},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1159842.1159848},
  abstract = {In languages that support polymorphic variants, a single variant value can be passed to many contexts that accept different sets of constructors. Polymorphic variants can be used in order to introduce extensible algebraic datatypes into functional programming languages and are potentially useful for application domains such as interpreters, graphical user interface (GUI) libraries and database interfaces, where the number of necessary constructors cannot be determined in advance. Very few functional languages, however, have a mechanism to extend existing datatypes by adding new constructors. In general, for polymorphic variants to be useful, we would need some mechanisms to reuse existing functions and extend them for new constructors.Actually, the type system of Haskell, when extended with parametric type classes (or multi-parameter type classes with functional dependencies), has enough power not only to mimic polymorphic variants but also to extend existing functions for new constructors.This paper, first, explains how to do this in Haskell's type system (Haskell 98 with popular extensions). However, this encoding of polymorphic variants is difficult to use in practice. This is because it is quite tedious for programmers to write mimic codes by hand and because the problem of ambiguous overloading resolution would embarrass programmers. Therefore, the paper proposes an extension of Haskell's type classes that supports polymorphic variants directly. It has a novel form of instance declarations where records and variants are handled symmetrically.This type system can produce vanilla Haskell codes as a result of type inference. Therefore it behaves as a preprocessor which translates the extended language into plain Haskell. Programmers would be able to use polymorphic variants without worrying nasty problems such as ambiguities.},
  isbn = {1-59593-489-8},
  keywords = {extensibility,Haskell,polymorphic variants,type classes},
  series = {Haskell '06}
}

@inproceedings{Kaiser2017Equivalence,
  title = {Equivalence of {{System F}} and {$\Lambda$}2 in {{Coq Based}} on {{Context Morphism Lemmas}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Conference}} on {{Certified Programs}} and {{Proofs}}},
  author = {Kaiser, Jonas and Tebbi, Tobias and Smolka, Gert},
  year = {2017},
  pages = {222--234},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3018610.3018618},
  abstract = {We give a machine-checked proof of the equivalence of the usual, two-sorted presentation of System F and its single-sorted pure type system variant {\^I}\guillemotright{}2. This is established by reducing the typability problem of F to {\^I}\guillemotright{}2 and vice versa. The difficulty lies in aligning different binding-structures and different contexts (dependent vs. non-dependent). The use of de{\^A} Bruijn syntax, parallel substitutions, and context morphism lemmas leads to an elegant proof. We use the Coq proof assistant and the substitution library Autosubst.},
  isbn = {978-1-4503-4705-1},
  keywords = {Context Morphism Lemmas,de Bruijn Substitutions,pure type systems,system f},
  series = {{{CPP}} 2017}
}

@inproceedings{Kaiser2017Strong,
  title = {Strong Logic for Weak Memory: Reasoning about Release-Acquire Consistency in {{Iris}}},
  shorttitle = {Strong Logic for Weak Memory},
  booktitle = {31st {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2017)},
  author = {Kaiser, Jan-Oliver and Dang, Hoang-Hai and Dreyer, Derek and Lahav, Ori and Vafeiadis, Viktor},
  editor = {M{\"u}ller, Peter},
  year = {2017},
  volume = {74},
  pages = {17:1--17:29},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.ECOOP.2017.17},
  isbn = {978-3-95977-035-4},
  keywords = {concurrency,release-acquire,separation logic,Weak memory models},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Kalibera2013Rigorous,
  title = {Rigorous {{Benchmarking}} in {{Reasonable Time}}},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Memory Management}}},
  author = {Kalibera, Tomas and Jones, Richard},
  year = {2013},
  pages = {63--74},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2464157.2464160},
  abstract = {Experimental evaluation is key to systems research. Because modern systems are complex and non-deterministic, good experimental methodology demands that researchers account for uncertainty. To obtain valid results, they are expected to run many iterations of benchmarks, invoke virtual machines (VMs) several times, or even rebuild VM or benchmark binaries more than once. All this repetition costs time to complete experiments. Currently, many evaluations give up on sufficient repetition or rigorous statistical methods, or even run benchmarks only in training sizes. The results reported often lack proper variation estimates and, when a small difference between two systems is reported, some are simply unreliable. In contrast, we provide a statistically rigorous methodology for repetition and summarising results that makes efficient use of experimentation time. Time efficiency comes from two key observations. First, a given benchmark on a given platform is typically prone to much less non-determinism than the common worst-case of published corner-case studies. Second, repetition is most needed where most uncertainty arises (whether between builds, between executions or between iterations). We capture experimentation cost with a novel mathematical model, which we use to identify the number of repetitions at each level of an experiment necessary and sufficient to obtain a given level of precision. We present our methodology as a cookbook that guides researchers on the number of repetitions they should run to obtain reliable results. We also show how to present results with an effect size confidence interval. As an example, we show how to use our methodology to conduct throughput experiments with the DaCapo and SPEC CPU benchmarks on three recent platforms.},
  isbn = {978-1-4503-2100-6},
  keywords = {benchmarking methodology,dacapo,spec cpu,statistical methods},
  series = {{{ISMM}} '13}
}

@article{Kamin1998research,
  title = {Research on {{Domain}}-Specific {{Embedded Languages}} and {{Program Generators}}},
  author = {Kamin, Samuel N.},
  year = {1998},
  volume = {14},
  pages = {149--168},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(05)80235-X},
  abstract = {Embedding is the process of implementing a language by defining functions in an existing ``host'' Language; the host language with these added functions is the new language. As a consequence; the new language comes equipped with all the features of the host language, with no additional work on the part of the language designer. Embedding works particularly well when the host language is a functional language.

We describe several examples of embedded languages. The first is a language for specifying simple pictures. The others are program generators, that is, languages used to specify programs in other languages. In all of these examples, the host language is Standard ML; in the program generating languages, the target language is C+plus;. The power obtained from the host language is the main emphasis of our presentation.

The author gratefully acknowledges the support provided by the Oregon Graduate Institute, where he was on sabbatical during the preparation of this paper.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{US}}-{{Brazil Joint Workshops}} on the {{Formal Foundations}} of {{Software Systems}}}
}

@inproceedings{Kammar2012algebraic,
  title = {Algebraic {{Foundations}} for {{Effect}}-Dependent {{Optimisations}}},
  booktitle = {Proceedings of the 39th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Kammar, Ohad and Plotkin, Gordon D.},
  year = {2012},
  pages = {349--360},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2103656.2103698},
  abstract = {We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity).},
  isbn = {978-1-4503-1083-3},
  keywords = {algebraic theory of effects,call-by-push-value,code transformations,compiler optimisations,computational effects,Denotational semantics,domain theory,inequational logic,relevant and affine monads,sum and tensor,type and effect systems,universal algebra},
  series = {{{POPL}} '12}
}

@inproceedings{Kammar2013handlers,
  title = {Handlers in {{Action}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Kammar, Ohad and Lindley, Sam and Oury, Nicolas},
  year = {2013},
  pages = {145--158},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500590},
  abstract = {Plotkin and Pretnar's handlers for algebraic effects occupy a sweet spot in the design space of abstractions for effectful computation. By separating effect signatures from their implementation, algebraic effects provide a high degree of modularity, allowing programmers to express effectful programs independently of the concrete interpretation of their effects. A handler is an interpretation of the effects of an algebraic computation. The handler abstraction adapts well to multiple settings: pure or impure, strict or lazy, static types or dynamic types. This is a position paper whose main aim is to popularise the handler abstraction. We give a gentle introduction to its use, a collection of illustrative examples, and a straightforward operational semantics. We describe our Haskell implementation of handlers in detail, outline the ideas behind our OCaml, SML, and Racket implementations, and present experimental results comparing handlers with existing code.},
  isbn = {978-1-4503-2326-0},
  keywords = {algebraic effects,continuations,effect handlers,effect typing,Haskell,modularity,Monads},
  series = {{{ICFP}} '13}
}

@inproceedings{Kang2016Lightweight,
  title = {Lightweight {{Verification}} of {{Separate Compilation}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Kang, Jeehoon and Kim, Yoonseung and Hur, Chung-Kil and Dreyer, Derek and Vafeiadis, Viktor},
  year = {2016},
  pages = {178--190},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2837614.2837642},
  abstract = {Major compiler verification efforts, such as the CompCert project, have traditionally simplified the verification problem by restricting attention to the correctness of whole-program compilation, leaving open the question of how to verify the correctness of separate compilation. Recently, a number of sophisticated techniques have been proposed for proving more flexible, compositional notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the simple "closed simulations" used in verifying whole-program compilation. Applying such techniques to a compiler like CompCert, as Stewart et al. have done, involves major changes and extensions to its original verification. In this paper, we show that if we aim somewhat lower---to prove correctness of separate compilation, but only for a *single* compiler---we can drastically simplify the proof effort. Toward this end, we develop several lightweight techniques that recast the compositional verification problem in terms of whole-program compilation, thereby enabling us to largely reuse the closed-simulation proofs from existing compiler verifications. We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting its verification of whole-program compilation into a verification of separate compilation in less than two person-months. This conversion only required a small number of changes to the original proofs, and uncovered two compiler bugs along the way. The result is SepCompCert, the first verification of separate compilation for the full CompCert compiler.},
  isbn = {978-1-4503-3549-2},
  keywords = {CompCert,Compositional compiler verification,separate compilation},
  series = {{{POPL}} '16}
}

@inproceedings{Karachalias2015GADTs,
  title = {{{GADTs}} Meet Their Match: {{Pattern}}-Matching Warnings That Account for {{GADTs}}, Guards, and Laziness},
  shorttitle = {Gadts Meet Their Match},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
  year = {2015},
  pages = {424--436},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784748},
  abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
  isbn = {978-1-4503-3669-7},
  keywords = {_tablet,Generalized Algebraic Data Types,Haskell,OutsideIn(X),pattern matching},
  series = {{{ICFP}} 2015}
}

@inproceedings{Karlsson2018Extending,
  title = {Extending {{Scala}} with Records: {{Design}}, Implementation, and Evaluation},
  shorttitle = {Extending Scala with Records},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Karlsson, Olof and Haller, Philipp},
  year = {2018},
  pages = {72--82},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3241653.3241661},
  abstract = {This paper presents a design for extensible records in Scala satisfying design goals such as structural subtyping, typesafe polymorphic operations, and separate compilation without runtime bytecode generation. Using new features of Scala 3, the design requires only minimal, local changes to the Scala 3 reference compiler Dotty as well as a small library component. Runtime performance is evaluated experimentally using a novel benchmarking suite generator, showing that the design is competitive with Scala 2's cached reflection for structural field access, and excels at immutable extension and update operations.},
  isbn = {978-1-4503-5836-1},
  keywords = {records,Scala,structural typing},
  series = {Scala 2018}
}

@inproceedings{Kastner2011road,
  title = {The {{Road}} to {{Feature Modularity}}?},
  booktitle = {Proceedings of the 15th {{International Software Product Line Conference}}, {{Volume}} 2},
  author = {K{\"a}stner, Christian and Apel, Sven and Ostermann, Klaus},
  year = {2011},
  pages = {5:1--5:8},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2019136.2019142},
  abstract = {Modularity of feature representations has been a long standing goal of feature-oriented software development. While some researchers regard feature modules and corresponding composition mechanisms as a modular solution, other researchers have challenged the notion of feature modularity and pointed out that most feature-oriented implementation mechanisms lack proper interfaces and support neither modular type checking nor separate compilation. We step back and reflect on the feature-modularity discussion. We distinguish two notions of modularity, cohesion without interfaces and information hiding with interfaces, and point out the different expectations that, we believe, are the root of many heated discussions. We discuss whether feature interfaces should be desired and weigh their potential benefits and costs, specifically regarding crosscutting, granularity, feature interactions, and the distinction between closed-world and open-world reasoning. Because existing evidence for and against feature modularity and feature interfaces is shaky and inconclusive, more research is needed, for which we outline possible directions.},
  isbn = {978-1-4503-0789-5},
  keywords = {crosscutting,feature interactions,feature models,feature modules,granularity,interfaces,modularity,module systems,variability},
  series = {{{SPLC}} '11}
}

@inproceedings{Keller2010hereditary,
  title = {Hereditary {{Substitutions}} for {{Simple Types}}, {{Formalized}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN Workshop}} on {{Mathematically Structured Functional Programming}}},
  author = {Keller, Chantal and Altenkirch, Thorsten},
  year = {2010},
  pages = {3--10},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863597.1863601},
  abstract = {We analyze a normalization function for the simply typed {$\lambda$}-calculus based on hereditary substitutions, a technique developed by Pfenning et al. The normalizer is implemented in Agda, a total language where all programs terminate. It requires no termination proof since it is structurally recursive which is recognized by Agda's termination checker. Using Agda as an interactive theorem prover we establish that our normalization function precisely identifies {$B\eta$}-equivalent terms and hence can be used to decide {$B\eta$}-equality. An interesting feature of this approach is that it is clear from the construction that {$B\eta$}-equality is primitive recursive.},
  isbn = {978-1-4503-0255-5},
  keywords = {decidability of $\\beta$η-equality,hereditary substitutions,normalizer,type theory},
  series = {{{MSFP}} '10}
}

@article{Kellsearch,
  title = {In {{Search}} of {{Types}}},
  author = {Kell, Stephen}
}

@article{Kelly2016Evolving,
  title = {Evolving the {{Incremental}} \{\textbackslash{}lambda\} {{Calculus}} into a {{Model}} of {{Forward Automatic Differentiation}} ({{AD}})},
  author = {Kelly, Robert and Pearlmutter, Barak A. and Siskind, Jeffrey Mark},
  year = {2016},
  month = nov,
  abstract = {Formal transformations somehow resembling the usual derivative are surprisingly common in computer science, with two notable examples being derivatives of regular expressions and derivatives of types. A newcomer to this list is the incremental \$\textbackslash{}lambda\$-calculus, or ILC, a "theory of changes" that deploys a formal apparatus allowing the automatic generation of efficient update functions which perform incremental computation. The ILC is not only defined, but given a formal machine-understandable definition---accompanied by mechanically verifiable proofs of various properties, including in particular correctness of various sorts. Here, we show how the ILC can be mutated into propagating tangents, thus serving as a model of Forward Accumulation Mode Automatic Differentiation. This mutation is done in several steps. These steps can also be applied to the proofs, resulting in machine-checked proofs of the correctness of this model of forward AD.},
  archivePrefix = {arXiv},
  eprint = {1611.03429},
  eprinttype = {arxiv},
  journal = {arXiv:1611.03429 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Kennaway1997infinitary,
  title = {Infinitary Lambda Calculus},
  author = {Kennaway, J. R. and Klop, J. W. and Sleep, M. R. and {de Vries}, F. J.},
  year = {1997},
  month = mar,
  volume = {175},
  pages = {93--125},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(96)00171-5},
  abstract = {In a previous paper we have established the theory of transfinite reduction for orthogonal term rewriting systems. In this paper we perform the same task for the lambda calculus.

From the viewpoint of infinitary rewriting, the B{\"o}hm model of the lambda calculus can be seen as an infinitary term model. In contrast to term rewriting, there are several different possible notions of infinite term, which give rise to different B{\"o}hm-like models, which embody different notions of lazy or eager computation.},
  journal = {Theoretical Computer Science},
  number = {1}
}

@inproceedings{Kennedy2007compiling,
  title = {Compiling with {{Continuations}}, {{Continued}}},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Kennedy, Andrew},
  year = {2007},
  pages = {177--190},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1291151.1291179},
  abstract = {We present a series of CPS-based intermediate languages suitable for functional language compilation, arguing that they have practical benefits over direct-style languages based on A-normal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANF-based languages, inlining involves a re-normalization step that rearranges let expressions and possibly introduces a new 'join point' function, and in monadic languages, commuting conversions must be applied; in contrast, inlining in our CPS language is a simple substitution of variables for variables. We present a contification transformation implemented by simple rewrites on the intermediate language. Exceptions are modelled using so-called 'double-barrelled' CPS. Subtyping on exception constructors then gives a very straightforward effect analysis for exceptions. We also show how a graph-based representation of CPS terms can be implemented extremely efficiently, with linear-time term simplification.},
  isbn = {978-1-59593-815-2},
  keywords = {continuation passing style,continuations,functional programming languages,Monads,optimizing compilation},
  series = {{{ICFP}} '07}
}

@article{Kennedy2018Transposing,
  title = {Transposing {{G}} to {{C}}{$\sharp$}: {{Expressivity}} of Generalized Algebraic Data Types in an Object-Oriented Language},
  shorttitle = {Transposing {{G}} to {{C}}{$\sharp$}},
  author = {Kennedy, Andrew J. and Russo, Claudio V.},
  year = {2018},
  month = apr,
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2018.02.030},
  abstract = {Generalized algebraic datatypes (GADTs) are a hot topic in the functional programming community. Previously we showed that object-oriented languages such as C{$\sharp$} and Java can express GADT declarations using Generics, but only some GADT programs. The addition of equational constraints on type parameters recovers expressivity. We now study this expressivity gap in more depth by extending an earlier translation from System F to C{$\sharp$} to handle GADTs. Our efforts reveal some surprising limitations of Generics and provide further justification for equational constraints.},
  journal = {Theoretical Computer Science},
  keywords = {C\#,Generalized algebraic data types,Generics,Polymorphism,System F}
}

@article{Kent2015occurrence,
  title = {Occurrence {{Typing Modulo Theories}}},
  author = {Kent, Andrew M. and Kempe, David and {Tobin-Hochstadt}, Sam},
  year = {2015},
  month = nov,
  abstract = {We present a new type system combining occurrence typing, previously used to type check programs in dynamically-typed languages such as Racket, JavaScript, and Ruby, with dependent refinement types. We demonstrate that the addition of refinement types allows the integration of arbitrary solver-backed reasoning about logical propositions from external theories. By building on occurrence typing, we can add our enriched type system as an extension of Typed Racket---adding dependency and refinement reuses the existing formalism while increasing its expressiveness. Dependent refinement types allow Typed Racket programmers to express rich type relationships, ranging from data structure invariants such as red-black tree balance to preconditions such as vector bounds. Refinements allow programmers to embed the propositions that occurrence typing in Typed Racket already reasons about into their types. Further, extending occurrence typing to refinements allows us to make the underlying formalism simpler and more powerful. In addition to presenting the design of our system, we present a formal model of the system, show how to integrate it with theories over both linear arithmetic and bitvectors, and evaluate the system in the context of the full Typed Racket implementation. Specifically, we take safe vector access as a case study, and examine all vector accesses in a 56,000 line corpus of Typed Racket programs. Our system is able to prove that 50\% of these are safe with no new annotation, and with a few annotations and modifications, we can capture close to 80\%.},
  archivePrefix = {arXiv},
  eprint = {1511.07033},
  eprinttype = {arxiv},
  journal = {arXiv:1511.07033 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Kiczales2005aspectoriented,
  title = {Aspect-Oriented {{Programming}} and {{Modular Reasoning}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Software Engineering}}},
  author = {Kiczales, Gregor and Mezini, Mira},
  year = {2005},
  pages = {49--58},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1062455.1062482},
  abstract = {Aspects cut new interfaces through the primary decomposition of a system. This implies that in the presence of aspects, the complete interface of a module can only be determined once the complete configuration of modules in the system is known. While this may seem anti-modular, it is an inherent property of crosscutting concerns, and using aspect-oriented programming enables modular reasoning in the presence of such concerns.},
  isbn = {1-58113-963-2},
  keywords = {aspect-oriented programming,modular reasoning,modularity},
  series = {{{ICSE}} '05}
}

@article{Kiselyov2007Lightweight,
  title = {Lightweight {{Static Capabilities}}},
  author = {Kiselyov, Oleg and Shan, Chung-chieh},
  year = {2007},
  month = jun,
  volume = {174},
  pages = {79--104},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2006.10.039},
  abstract = {We describe a modular programming style that harnesses modern type systems to verify safety conditions in practical systems. This style has three ingredients:(i)A compact kernel of trust that is specific to the problem domain.(ii)Unique names (capabilities) that confer rights and certify properties, so as to extend the trust from the kernel to the rest of the application.(iii)Static (type) proxies for dynamic values. We illustrate our approach using examples from the dependent-type literature, but our programs are written in Haskell and OCaml today, so our techniques are compatible with imperative code, native mutable arrays, and general recursion. The three ingredients of this programming style call for (1) an expressive core language, (2) higher-rank polymorphism, and (3) phantom types.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Modular programming,safety property,static types,Verification},
  number = {7},
  series = {Proceedings of the {{Programming Languages}} Meets {{Program Verification}} ({{PLPV}} 2006)}
}

@inproceedings{Kiselyov2009embedded,
  title = {Embedded {{Probabilistic Programming}}},
  booktitle = {Proceedings of the {{IFIP TC}} 2 {{Working Conference}} on {{Domain}}-{{Specific Languages}}},
  author = {Kiselyov, Oleg and Shan, Chung-Chieh},
  year = {2009},
  pages = {360--384},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03034-5_17},
  abstract = {Two general techniques for implementing a domain-specific language (DSL) with less overhead are the {$<$}em{$>$}finally-tagless{$<$}/em{$>$} embedding of object programs and the {$<$}em{$>$}direct-style{$<$}/em{$>$} representation of side effects. We use these techniques to build a DSL for {$<$}em{$>$}probabilistic programming{$<$}/em{$>$} , for expressing countable probabilistic models and performing exact inference and importance sampling on them. Our language is embedded as an ordinary OCaml library and represents probability distributions as ordinary OCaml programs. We use delimited continuations to reify probabilistic programs as lazy search trees, which inference algorithms may traverse without imposing any interpretive overhead on deterministic parts of a model. We thus take advantage of the existing OCaml implementation to achieve competitive performance and ease of use. Inference algorithms can easily be embedded in probabilistic programs themselves.},
  isbn = {978-3-642-03033-8},
  series = {{{DSL}} '09}
}

@inproceedings{Kiselyov2012Iteratees,
  title = {Iteratees},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Functional}} and {{Logic Programming}}},
  author = {Kiselyov, Oleg},
  year = {2012},
  pages = {166--181},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-29822-6_15},
  abstract = {Iteratee IO is a style of incremental input processing with precise resource control. The style encourages building input processors from a user-extensible set of primitives by chaining, layering, pairing and other modes of compositions. The programmer is still able, where needed, to precisely control look-ahead, the allocation of buffers, file descriptors and other resources. The style is especially suitable for processing of communication streams, large amount of data, and data undergone several levels of encoding such as pickling, compression, chunking, framing. It has been used for programming high-performance (HTTP) servers and web frameworks, in computational linguistics and financial trading. We exposit programming with iteratees, contrasting them with Lazy IO and the Handle-based, |stdio|-like IO. We relate them to online parser combinators. We introduce a simple implementation as free monads, which lets us formally reason with iteratees. As an example, we validate several equational laws and use them to optimize iteratee programs. The simple implementation helps understand existing implementations of iteratees and derive new ones.},
  isbn = {978-3-642-29821-9},
  series = {{{FLOPS}}'12}
}

@inproceedings{Kiselyov2015freer,
  title = {Freer {{Monads}}, {{More Extensible Effects}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Kiselyov, Oleg and Ishii, Hiromi},
  year = {2015},
  pages = {94--105},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2804302.2804319},
  abstract = {We present a rational reconstruction of extensible effects, the recently proposed alternative to monad transformers, as the confluence of efforts to make effectful computations compose. Free monads and then extensible effects emerge from the straightforward term representation of an effectful computation, as more and more boilerplate is abstracted away. The generalization process further leads to freer monads, constructed without the Functor constraint. The continuation exposed in freer monads can then be represented as an efficient type-aligned data structure. The end result is the algorithmically efficient extensible effects library, which is not only more comprehensible but also faster than earlier implementations. As an illustration of the new library, we show three surprisingly simple applications: non-determinism with committed choice (LogicT), catching IO exceptions in the presence of other effects, and the semi-automatic management of file handles and other resources through monadic regions. We extensively use and promote the new sort of `laziness', which underlies the left Kan extension: instead of performing an operation, keep its operands and pretend it is done.},
  isbn = {978-1-4503-3808-0},
  keywords = {coroutine,effect handler,effect interaction,free monad,Kan extension,open union,type and effect system},
  series = {Haskell '15}
}

@inproceedings{Kiselyov2017Stream,
  title = {Stream {{Fusion}}, to {{Completeness}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
  year = {2017},
  pages = {285--299},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009880},
  abstract = {Stream processing is mainstream (again): Widely-used stream libraries are now available for virtually all modern OO and functional languages, from Java to C\# to Scala to OCaml to Haskell. Yet expressivity and performance are still lacking. For instance, the popular, well-optimized Java 8 streams do not support the zip operator and are still an order of magnitude slower than hand-written loops.   We present the first approach that represents the full generality of stream processing and eliminates overheads, via the use of staging. It is based on an unusually rich semantic model of stream interaction. We support any combination of zipping, nesting (or flat-mapping), sub-ranging, filtering, mapping\textemdash{}of finite or infinite streams. Our model captures idiosyncrasies that a programmer uses in optimizing stream pipelines, such as rate differences and the choice of a ``for'' vs. ``while'' loops. Our approach delivers hand-written\textendash{}like code, but automatically. It explicitly avoids the reliance on black-box optimizers and sufficiently-smart compilers, offering highest, guaranteed and portable performance.   Our approach relies on high-level concepts that are then readily mapped into an implementation. Accordingly, we have two distinct implementations: an OCaml stream library, staged via MetaOCaml, and a Scala library for the JVM, staged via LMS. In both cases, we derive libraries richer and simultaneously many tens of times faster than past work. We greatly exceed in performance the standard stream libraries available in Java, Scala and OCaml, including the well-optimized Java 8 streams.},
  isbn = {978-1-4503-4660-3},
  keywords = {code generation,multi-stage programming,optimization,stream fusion,streams},
  series = {{{POPL}} 2017}
}

@article{Klin2004adding,
  title = {Adding Recursive Constructs to Bialgebraic Semantics},
  author = {Klin, Bartek},
  year = {2004},
  month = jul,
  volume = {60\textendash{}61},
  pages = {259--286},
  issn = {1567-8326},
  doi = {10.1016/j.jlap.2004.03.005},
  abstract = {This paper aims at fitting a general class of recursive equations into the framework of `well-behaved' structural operational semantics, formalized as bialgebraic semantics by Turi and Plotkin. Rather than interpreting recursive constructs by means of operational rules, separate recursive equations are added to semantic descriptions of languages. The equations, together with the remaining rules, are then interpreted in a suitable category and merged by means of certain fixpoint constructions. For a class of recursive equations called regular unfolding rules, this construction yields distributive laws as analyzed by Turi and Plotkin. This means that regular unfolding rules can be merged seamlessly with abstract operational rules.},
  journal = {The Journal of Logic and Algebraic Programming},
  series = {Structural {{Operational Semantics}}}
}

@inproceedings{Kniesel1999TypeSafe,
  title = {Type-{{Safe Delegation}} for {{Run}}-{{Time Component Adaptation}}},
  booktitle = {{{ECOOP}}' 99 \textemdash{} {{Object}}-{{Oriented Programming}}},
  author = {Kniesel, G{\"u}nter},
  year = {1999},
  month = jun,
  pages = {351--366},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-48743-3_16},
  abstract = {The aim of component technology is the replacement of large monolithic applications with sets of smaller software components, whose particular functionality and interoperation can be adapted to users' needs. However, the adaptation mechanisms of component software are still limited. Most proposals concentrate on adaptations that can be achieved either at compile time or at link time. Current support for dynamic component adaptation, i.e. unanticipated, incremental modifications of a component system at run-time, is not sufficient.This paper proposes object-based inheritance (also known as delegation) as a complement to purely forwarding-based object composition. It presents a typesafe integration of delegation into a class-based object model and shows how it overcomes the problems faced by forwarding-based component interaction, how it supports independent extensibility of components and unanticipated, dynamic component adaptation.},
  isbn = {10.1007/3-540-48743-3\_16},
  keywords = {_tablet},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Knoblock1996data,
  title = {Data {{Specialization}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1996 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Knoblock, Todd B. and Ruf, Erik},
  year = {1996},
  pages = {215--225},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/231379.231428},
  abstract = {Given a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.},
  isbn = {0-89791-795-2},
  series = {{{PLDI}} '96}
}

@article{Knuth1968semantics,
  title = {Semantics of Context-Free Languages},
  author = {Knuth, Donald E.},
  year = {1968},
  month = jun,
  volume = {2},
  pages = {127--145},
  issn = {0025-5661, 1433-0490},
  doi = {10.1007/BF01692511},
  abstract = {``Meaning'' may be assigned to a string in a context-free language by defining ``attributes'' of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are ``synthesized'', i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are ``inherited'', i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  journal = {Mathematical systems theory},
  keywords = {Computational Mathematics and Numerical Analysis,Theory of Computation},
  language = {en},
  number = {2}
}

@article{Knuth1981Breaking,
  title = {Breaking Paragraphs into Lines},
  author = {Knuth, Donald E. and Plass, Michael F.},
  year = {1981},
  month = nov,
  volume = {11},
  pages = {1119--1184},
  issn = {1097-024X},
  doi = {10.1002/spe.4380111102},
  abstract = {This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called `boxes', `glue', and `penalties' provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.},
  journal = {Software: Practice and Experience},
  keywords = {Box/glue/penalty algebra,Composition,Dynamic programming,History of printing,Justification,Layout,Line breaking,Shortest paths,Spacing,TEX (Tau Epsilon Chi),Typesetting,Word processing},
  language = {en},
  number = {11}
}

@inproceedings{Koch2010Incremental,
  title = {Incremental {{Query Evaluation}} in a {{Ring}} of {{Databases}}},
  booktitle = {Proceedings of the {{Twenty}}-Ninth {{ACM SIGMOD}}-{{SIGACT}}-{{SIGART Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Koch, Christoph},
  year = {2010},
  pages = {87--98},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1807085.1807100},
  abstract = {This paper approaches the incremental view maintenance problem from an algebraic perspective. We construct the algebraic structure of a ring of databases and use it as the foundation of the design of a query calculus that allows to express powerful aggregate queries. The query calculus inherits key properties of the ring, such as having a normal form of polynomials and being closed under computing inverses and delta queries. The k-th delta of a polynomial query of degree k without nesting is purely a function of the update, not of the database. This gives rise to a method of eliminating expensive query operators such as joins from programs that perform incremental view maintenance. The main result is that, for non-nested queries, each individual aggregate value can be incrementally maintained using a constant amount of work. This is not possible for nonincremental evaluation.},
  isbn = {978-1-4503-0033-9},
  keywords = {Algebra,Incremental view maintenance},
  series = {{{PODS}} '10}
}

@article{Koch2014dbtoaster,
  title = {{{DBToaster}}: Higher-Order Delta Processing for Dynamic, Frequently Fresh Views},
  shorttitle = {{{DBToaster}}},
  author = {Koch, Christoph and Ahmad, Yanif and Kennedy, Oliver and Nikolic, Milos and N{\"o}tzli, Andres and Lupei, Daniel and Shaikhha, Amir},
  year = {2014},
  month = apr,
  volume = {23},
  pages = {253--278},
  issn = {1066-8888, 0949-877X},
  doi = {10.1007/s00778-013-0348-4},
  abstract = {Applications ranging from algorithmic trading to scientific data analysis require real-time analytics based on views over databases receiving thousands of updates each second. Such views have to be kept fresh at millisecond latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this article, we present the DBToaster system, which keeps materialized views of standard SQL queries continuously fresh as data changes very rapidly. This is achieved by a combination of aggressive compilation techniques and DBToaster's original recursive finite differencing technique which materializes a query and a set of its higher-order deltas as views. These views support each other's incremental maintenance, leading to a reduced overall view maintenance cost. DBToaster supports tens of thousands of complete view refreshes per second for a wide range of queries.},
  journal = {The VLDB Journal},
  keywords = {Compilation,Database Management,Database queries,Incremental view maintenance,Materialized views},
  language = {en},
  number = {2}
}

@article{Koch2014Incremental,
  title = {Incremental {{View Maintenance For Collection Programming}}},
  author = {Koch, Christoph and Lupei, Daniel and Tannen, Val},
  year = {2014},
  month = dec,
  abstract = {In the context of incremental view maintenance (IVM), delta query derivation is an essential technique for speeding up the processing of large, dynamic datasets. The goal is to generate delta queries that, given a small change in the input, can update the materialized view more efficiently than via recomputation. In this work we propose the first solution for the efficient incrementalization of positive nested relational calculus (NRC+) on bags (with integer multiplicities). More precisely, we model the cost of NRC+ operators and classify queries as efficiently incrementalizable if their delta has a strictly lower cost than full re-evaluation. Then, we identify IncNRC+; a large fragment of NRC+ that is efficiently incrementalizable and we provide a semantics-preserving translation that takes any NRC+ query to a collection of IncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is within the complexity class NC0 and we showcase how recursive IVM, a technique that has provided significant speedups over traditional IVM in the case of flat queries [25], can also be applied to IncNRC+.},
  archivePrefix = {arXiv},
  eprint = {1412.4320},
  eprinttype = {arxiv},
  journal = {arXiv:1412.4320 [cs]},
  keywords = {Computer Science - Databases},
  primaryClass = {cs}
}

@inproceedings{Koch2016Incremental,
  title = {Incremental {{View Maintenance For Collection Programming}}},
  booktitle = {Proceedings of the 35th {{ACM SIGMOD}}-{{SIGACT}}-{{SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Koch, Christoph and Lupei, Daniel and Tannen, Val},
  year = {2016},
  pages = {75--90},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2902251.2902286},
  abstract = {In the context of incremental view maintenance (IVM), delta query derivation is an essential technique for speeding up the processing of large, dynamic datasets. The goal is to generate delta queries that, given a small change in the input, can update the materialized view more efficiently than via recomputation. In this work we propose the first solution for the efficient incrementalization of positive nested relational calculus (NRC+) on bags (with integer multiplicities). More precisely, we model the cost of NRC+ operators and classify queries as efficiently incrementalizable if their delta has a strictly lower cost than full re-evaluation. Then, we identify NRC+, a large fragment of NRC+ that is efficiently incrementalizable and we provide a semantics-preserving translation that takes any NRC+ query to a collection of IncNRC+ queries. Furthermore, we prove that incremental maintenance for NRC+ is within the complexity class NC0 and we showcase how recursive IVM, a technique that has provided significant speedups over traditional IVM in the case of flat queries [25], can also be applied to IncNRC+.},
  isbn = {978-1-4503-4191-2},
  keywords = {collection programming,higher-order delta derivation,incremental computation,Incremental view maintenance,nested relational algebra,nested relational calculus,recursive ivm,shredding transformation},
  series = {{{PODS}} '16}
}

@inproceedings{Kohlbecker1986hygienic,
  title = {Hygienic {{Macro Expansion}}},
  booktitle = {Proceedings of the 1986 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Kohlbecker, Eugene and Friedman, Daniel P. and Felleisen, Matthias and Duba, Bruce},
  year = {1986},
  pages = {151--161},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/319838.319859},
  isbn = {0-89791-200-4},
  series = {{{LFP}} '86}
}

@article{Kranz2004orbit,
  title = {Orbit: {{An Optimizing Compiler}} for {{Scheme}}},
  shorttitle = {Orbit},
  author = {Kranz, David and Kelsey, Richard and Rees, Jonathan and Hudak, Paul and Philbin, James and Adams, Norman},
  year = {2004},
  month = apr,
  volume = {39},
  pages = {175--191},
  issn = {0362-1340},
  doi = {10.1145/989393.989414},
  abstract = {Orbit was an optimizing compiler for T, a dialect of Scheme. Its aggressive use of CPS conversion, novel closure representations, and efficient code generation strategies made it the best compiler for a Scheme dialect at the time and for many years to come. The design of T and Orbit directly spawned six PhD theses and one Masters thesis, and influenced many other projects as well, including SML of New Jersey.},
  journal = {SIGPLAN Not.},
  number = {4}
}

@inproceedings{Krebbers2017essence,
  title = {The Essence of Higher-Order Concurrent Separation Logic},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Krebbers, Robbert and Jung, Ralf and Bizjak, Ale{\v s} and Jourdan, Jacques-Henri and Dreyer, Derek and Birkedal, Lars},
  year = {2017},
  month = apr,
  pages = {696--723},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-54434-1_26},
  abstract = {Concurrent separation logics (CSLs) have come of age, and with age they have accumulated a great deal of complexity. Previous work on the Iris logic attempted to reduce the complex logical mechanisms of modern CSLs to two orthogonal concepts: partial commutative monoids (PCMs) and invariants. However, the realization of these concepts in Iris still bakes in several complex mechanisms\textemdash{}such as weakest preconditions and mask-changing view shifts\textemdash{}as primitive notions.In this paper, we take the Iris story to its (so to speak) logical conclusion, applying the reductionist methodology of Iris to Iris itself. Specifically, we define a small, resourceful base logic, which distills the essence of Iris: it comprises only the assertion layer of vanilla separation logic, plus a handful of simple modalities. We then show how the much fancier logical mechanisms of Iris\textemdash{}in particular, its entire program specification layer\textemdash{}can be understood as merely derived forms in our base logic. This approach helps to explain the meaning of Iris's program specifications at a much higher level of abstraction than was previously possible. We also show that the step-indexed ``later'' modality of Iris is an essential source of complexity, in that removing it leads to a logical inconsistency. All our results are fully formalized in the Coq proof assistant.},
  isbn = {978-3-662-54433-4 978-3-662-54434-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Krebbers2017Interactive,
  title = {Interactive Proofs in Higher-Order Concurrent Separation Logic},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Krebbers, Robbert and Timany, Amin and Birkedal, Lars},
  year = {2017},
  pages = {205--217},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009855},
  abstract = {When using a proof assistant to reason in an embedded logic -- like separation logic -- one cannot benefit from the proof contexts and basic tactics of the proof assistant. This results in proofs that are at a too low level of abstraction because they are cluttered with bookkeeping code related to manipulating the object logic.   In this paper, we introduce a so-called proof mode that extends the Coq proof assistant with (spatial and non-spatial) named proof contexts for the object logic. We show that thanks to these contexts we can implement high-level tactics for introduction and elimination of the connectives of the object logic, and thereby make reasoning in the embedded logic as seamless as reasoning in the meta logic of the proof assistant. We apply our method to Iris: a state of the art higher-order impredicative concurrent separation logic.   We show that our method is very general, and is not just limited to program verification. We demonstrate its generality by formalizing correctness proofs of fine-grained concurrent algorithms, derived constructs of the Iris logic, and a unary and binary logical relation for a language with concurrency, higher-order store, polymorphism, and recursive types. This is the first formalization of a binary logical relation for such an expressive language. We also show how to use the logical relation to prove contextual refinement of fine-grained concurrent algorithms.},
  isbn = {978-1-4503-4660-3},
  keywords = {Coq,Fine-grained Concurrency,Interactive Theorem Proving,Logical Relations,Separation Logic},
  series = {{{POPL}} 2017}
}

@article{Krebbers2018MoSeL,
  title = {{{MoSeL}}: {{A}} General, Extensible Modal Framework for Interactive Proofs in Separation Logic},
  shorttitle = {Mosel},
  author = {Krebbers, Robbert and Jourdan, Jacques-Henri and Jung, Ralf and Tassarotti, Joseph and Kaiser, Jan-Oliver and Timany, Amin and Chargu{\'e}raud, Arthur and Dreyer, Derek},
  year = {2018},
  month = jul,
  volume = {2},
  pages = {77:1--77:30},
  issn = {2475-1421},
  doi = {10.1145/3236772},
  abstract = {A number of tools have been developed for carrying out separation-logic proofs mechanically using an interactive proof assistant. One of the most advanced such tools is the Iris Proof Mode (IPM) for Coq, which offers a rich set of tactics for making separation-logic proofs look and feel like ordinary Coq proofs. However, IPM is tied to a particular separation logic (namely, Iris), thus limiting its applicability.   In this paper, we propose MoSeL, a general and extensible Coq framework that brings the benefits of IPM to a much larger class of separation logics. Unlike IPM, MoSeL is applicable to both affine and linear separation logics (and combinations thereof), and provides generic tactics that can be easily extended to account for the bespoke connectives of the logics with which it is instantiated. To demonstrate the effectiveness of MoSeL, we have instantiated it to provide effective tactical support for interactive and semi-automated proofs in six very different separation logics.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Coq proof assistant,interactive theorem proving,logic of bunched implications,modal logic,Separation logic},
  number = {ICFP}
}

@inproceedings{Krishnamurthi1998Synthesizing,
  title = {Synthesizing Object-Oriented and Functional Design to Promote Re-Use},
  booktitle = {{{ECOOP}}'98 \textemdash{} {{Object}}-{{Oriented Programming}}},
  author = {Krishnamurthi, Shriram and Felleisen, Matthias and Friedman, Daniel P.},
  year = {1998},
  month = jul,
  pages = {91--113},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0054088},
  abstract = {Many problems require recursively specified types of data and a collection of tools that operate on those data. Over time, these problems evolve so that the programmer must extend the toolkit or extend the types and adjust the existing tools accordingly. Ideally, this should be done without modifying existing code. Unfortunately, the prevailing program design strategies do not support both forms of extensibility: functional programming accommodates the addition of tools, while object-oriented programming supports either adding new tools or extending the data set, but not both. In this paper, we present a composite design pattern that synthesizes the best of both approaches and in the process resolves the tension between the two design strategies. We also show how this protocol suggests a new set of linguistic facilities for languages that support class systems.},
  isbn = {978-3-540-64737-9 978-3-540-69064-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Krishnamurthi2015real,
  title = {The {{Real Software Crisis}}: {{Repeatability As}} a {{Core Value}}},
  shorttitle = {The {{Real Software Crisis}}},
  author = {Krishnamurthi, Shriram and Vitek, Jan},
  year = {2015},
  month = feb,
  volume = {58},
  pages = {34--36},
  issn = {0001-0782},
  doi = {10.1145/2658987},
  abstract = {Sharing experiences running artifact evaluation committees for five major conferences.},
  journal = {Commun. ACM},
  number = {3}
}

@article{Krishnan2016Marriage,
  title = {The {{Marriage}} of {{Incremental}} and {{Approximate Computing}}},
  author = {Krishnan, Dhanya R.},
  year = {2016},
  month = nov,
  abstract = {Most data analytics systems that require low-latency execution and efficient utilization of computing resources, increasingly adopt two computational paradigms, namely, incremental and approximate computing. Incremental computation updates the output incrementally instead of re-computing everything from scratch for successive runs of a job with input changes. Approximate computation returns an approximate output for a job instead of the exact output. Both paradigms rely on computing over a subset of data items instead of computing over the entire dataset, but they differ in their means for skipping parts of the computation. Incremental computing relies on the memoization of intermediate results of sub-computations, and reusing these memoized results across jobs for sub-computations that are unaffected by the changed input. Approximate computing relies on representative sampling of the entire dataset to compute over a subset of data items. In this thesis, we make the observation that these two computing paradigms are complementary, and can be married together! The high level idea is to: design a sampling algorithm that biases the sample selection to the memoized data items from previous runs. To concretize this idea, we designed an online stratified sampling algorithm that uses self-adjusting computation to produce an incrementally updated approximate output with bounded error. We implemented our algorithm in a data analytics system called IncAppox based on Apache Spark Streaming. Our evaluation of the system shows that IncApprox achieves the benefits of both incremental and approximate computing.},
  archivePrefix = {arXiv},
  eprint = {1611.08573},
  eprinttype = {arxiv},
  journal = {arXiv:1611.08573 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@inproceedings{Krishnaswami2009focusing,
  title = {Focusing on {{Pattern Matching}}},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Krishnaswami, Neelakantan R.},
  year = {2009},
  pages = {366--378},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1480881.1480927},
  abstract = {In this paper, we show how pattern matching can be seen to arise from a proof term assignment for the focused sequent calculus. This use of the Curry-Howard correspondence allows us to give a novel coverage checking algorithm, and makes it possible to give a rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns.},
  isbn = {978-1-60558-379-2},
  keywords = {_tablet,Curry-Howard,focusing,pattern matching,type theory},
  series = {{{POPL}} '09}
}

@inproceedings{Krishnaswami2012Higherorder,
  title = {Higher-Order {{Functional Reactive Programming}} in {{Bounded Space}}},
  booktitle = {Proceedings of the 39th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Krishnaswami, Neelakantan R. and Benton, Nick and Hoffmann, Jan},
  year = {2012},
  pages = {45--58},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2103656.2103665},
  abstract = {Functional reactive programming (FRP) is an elegant and successful approach to programming reactive systems declaratively. The high levels of abstraction and expressivity that make FRP attractive as a programming model do, however, often lead to programs whose resource usage is excessive and hard to predict. In this paper, we address the problem of space leaks in discrete-time functional reactive programs. We present a functional reactive programming language that statically bounds the size of the dataflow graph a reactive program creates, while still permitting use of higher-order functions and higher-type streams such as streams of streams. We achieve this with a novel linear type theory that both controls allocation and ensures that all recursive definitions are well-founded. We also give a denotational semantics for our language by combining recent work on metric spaces for the interpretation of higher-order causal functions with length-space models of space-bounded computation. The resulting category is doubly closed and hence forms a model of the logic of bunched implications.},
  isbn = {978-1-4503-1083-3},
  keywords = {_tablet,bunched implications,dataflow,functional reactive programming,linear logic,space-bounded computation},
  series = {{{POPL}} '12}
}

@inproceedings{Krishnaswami2013internalizing,
  title = {Internalizing {{Relational Parametricity}} in the {{Extensional Calculus}} of {{Constructions}}},
  booktitle = {Computer {{Science Logic}} 2013 ({{CSL}} 2013)},
  author = {Krishnaswami, Neelakantan R. and Dreyer, Derek},
  editor = {Rocca, Simona Ronchi Della},
  year = {2013},
  volume = {23},
  pages = {432--451},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.CSL.2013.432},
  isbn = {978-3-939897-60-6},
  keywords = {_tablet},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-42125}
}

@article{Krivine2011realizability,
  title = {Realizability Algebras: A Program to Well Order {{R}}},
  shorttitle = {Realizability Algebras},
  author = {Krivine, Jean-Louis},
  year = {2011},
  month = aug,
  volume = {7},
  issn = {18605974},
  doi = {10.2168/LMCS-7(3:2)2011},
  abstract = {The theory of classical realizability is a framework in which we can develop the proof-program correspondence. Using this framework, we show how to transform into programs the proofs in classical analysis with dependent choice and the existence of a well ordering of the real line. The principal tools are: The notion of realizability algebra, which is a three-sorted variant of the well known combinatory algebra of Curry. An adaptation of the method of forcing used in set theory to prove consistency results. Here, it is used in another way, to obtain programs associated with a well ordering of R and the existence of a non trivial ultrafilter on N.},
  archivePrefix = {arXiv},
  eprint = {1005.2395},
  eprinttype = {arxiv},
  journal = {Logical Methods in Computer Science},
  keywords = {Computer Science - Logic in Computer Science,F.4.1},
  number = {3}
}

@inproceedings{Krogh-Jespersen2017relational,
  title = {A Relational Model of Types-and-Effects in Higher-Order Concurrent Separation Logic},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {{Krogh-Jespersen}, Morten and Svendsen, Kasper and Birkedal, Lars},
  year = {2017},
  pages = {218--231},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009877},
  abstract = {Recently we have seen a renewed interest in programming languages that tame the complexity of state and concurrency through refined type systems with more fine-grained control over effects. In addition to simplifying reasoning and eliminating whole classes of bugs, statically tracking effects opens the door to advanced compiler optimizations.     In this paper we present a relational model of a type-and-effect system for a higher-order, concurrent program- ming language. The model precisely captures the semantic invariants expressed by the effect annotations. We demonstrate that these invariants are strong enough to prove advanced program transformations, including automatic parallelization of expressions with suitably disjoint effects. The model also supports refinement proofs between abstract data types implementations with different internal data representations, including proofs that fine-grained concurrent algorithms refine their coarse-grained counterparts. This is the first model for such an expressive language that supports both effect-based optimizations and data abstraction.     The logical relation is defined in Iris, a state-of-the-art higher-order concurrent separation logic. This greatly simplifies proving well-definedness of the logical relation and also provides us with a powerful logic for reasoning in the model.},
  isbn = {978-1-4503-4660-3},
  keywords = {automatic parallelisation,logical rela- tions,program transformation,Separation logic,type-and-effect system},
  series = {{{POPL}} 2017}
}

@incollection{Kuan2009Engineering,
  title = {Engineering {{Higher}}-{{Order Modules}} in {{SML}}/{{NJ}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Kuan, George and MacQueen, David},
  year = {2009},
  month = sep,
  pages = {218--235},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-16478-1_13},
  abstract = {SML/NJ and other Standard ML variants extend the ML module system with higher-order functors, elevating the module language to a full functional language. In this paper, we describe the implementation of higher-order modules in SML/NJ, which is unique in providing ``true'' higher-order static behavior. This implementation is based on three key ideas: unique internal variables (entity variables) for naming static entities, factorization of the static information in both basic modules and functors into signatures and realizations, and representing the static ``effects'' and type-level mapping performed by a functor using a static lambda calculus (the entity calculus). This design conforms to MacQueen-Tofte's re-elaboration semantics without having to re-elaborate functor bodies at functor applications.},
  isbn = {978-3-642-16477-4 978-3-642-16478-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Kuci2015incremental,
  title = {Toward {{Incremental Type Checking}} for {{Java}}},
  booktitle = {Companion {{Proceedings}} of the 2015 {{ACM SIGPLAN International Conference}} on {{Systems}}, {{Programming}}, {{Languages}} and {{Applications}}: {{Software}} for {{Humanity}}},
  author = {Kuci, Edlira and Erdweg, Sebastian and Mezini, Mira},
  year = {2015},
  pages = {46--47},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814189.2817272},
  abstract = {A type system is a set of type rules and with respect to these type rules a type checker has an important role to ensure that programs exhibit a desired behavior. We consider Java type rules and extend the co-contextual formulation of type rules introduced in [1] to enable it for Java. Regarding the extension type rules result is a type, a set of context requirements and a set of class requirements. Since context and class requirements are propagated bottom-up and while traversing the syntax tree bottom-up and are merged from independent subexpression, this enables the type system to be incremental therefore the performance is increased.},
  isbn = {978-1-4503-3722-9},
  keywords = {class table,co-contextual,constraints,incremental,java,type checking},
  series = {{{SPLASH Companion}} 2015}
}

@inproceedings{Kuci2017Cocontextual,
  title = {A {{Co}}-Contextual {{Type Checker}} for {{Featherweight Java}}},
  booktitle = {31st {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2017)},
  author = {Kuci, Edlira and Erdweg, Sebastian and Bracevac, Oliver and Bejleri, Andi and Mezini, Mira},
  editor = {M{\"u}ller, Peter},
  year = {2017},
  volume = {74},
  pages = {18:1--18:26},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.ECOOP.2017.18},
  isbn = {978-3-95977-035-4},
  keywords = {class table,co-contextual,constraints,Featherweight Java,type checking},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Kuhn2015choosy,
  title = {Choosy and {{Picky}}: {{Configuration}} of {{Language Product Lines}}},
  shorttitle = {Choosy and {{Picky}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Software Product Line}}},
  author = {K{\"u}hn, Thomas and Cazzola, Walter and Olivares, Diego Mathias},
  year = {2015},
  pages = {71--80},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2791060.2791092},
  abstract = {Although most programming languages naturally share several language features, they are typically implemented as a monolithic product. Language features cannot be plugged and unplugged from a language and reused in another language. Some modular approaches to language construction do exist but composing language features requires a deep understanding of its implementation hampering their use. The choose and pick approach from software product lines provides an easy way to compose a language out of a set of language features. However, current approaches to language product lines are not sufficient enough to cope with the complexity and evolution of real world programming languages. In this work, we propose a general light-weight bottom-up approach to automatically extract a feature model from a set of tagged language components. We applied this approach to the Neverlang language development framework and developed the AiDE tool to guide language developers towards a valid language composition. The approach has been evaluated on a decomposed version of Javascript to highlight the benefits of such a language product line.},
  isbn = {978-1-4503-3613-0},
  keywords = {language composition,language product lines},
  series = {{{SPLC}} '15}
}

@article{Lafont1997Interaction,
  title = {Interaction {{Combinators}}},
  author = {Lafont, Yves},
  year = {1997},
  month = aug,
  volume = {137},
  pages = {69--101},
  issn = {0890-5401},
  doi = {10.1006/inco.1997.2643},
  abstract = {It is shown that a very simple system ofinteraction combinators, with only three symbols and six rules, is a universal model of distributed computation, in a sense that will be made precise. This paper is the continuation of the author's work oninteraction nets, inspired by Girard's proof nets forlinear logic, but no preliminary knowledge of these topics is required for its reading.},
  journal = {Information and Computation},
  number = {1}
}

@inproceedings{Lago2019Differential,
  title = {Differential {{Logical Relations}}, {{Part I}}: {{The Simply}}-{{Typed Case}}},
  shorttitle = {Differential {{Logical Relations}}, {{Part I}}},
  booktitle = {46th {{International Colloquium}} on {{Automata}}, {{Languages}}, and {{Programming}} ({{ICALP}} 2019)},
  author = {Lago, Ugo Dal and Gavazzo, Francesco and Yoshimizu, Akira},
  editor = {Baier, Christel and Chatzigiannakis, Ioannis and Flocchini, Paola and Leonardi, Stefano},
  year = {2019},
  volume = {132},
  pages = {111:1--111:14},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.ICALP.2019.111},
  isbn = {978-3-95977-109-2},
  keywords = {lambda-Calculus,Logical Relations,Program Equivalence,Semantics},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@article{Lago2019Differentiala,
  title = {Differential Logical Relations, Part {{I}}: The Simply-Typed Case (Long Version)},
  shorttitle = {Differential Logical Relations, Part i},
  author = {Lago, Ugo Dal and Gavazzo, Francesco and Yoshimizu, Akira},
  year = {2019},
  month = apr,
  abstract = {We introduce a new form of logical relation which, in the spirit of metric relations, allows us to assign each pair of programs a quantity measuring their distance, rather than a boolean value standing for their being equivalent. The novelty of differential logical relations consists in measuring the distance between terms not (necessarily) by a numerical value, but by a mathematical object which somehow reflects the interactive complexity, i.e. the type, of the compared terms. We exemplify this concept in the simply-typed lambda-calculus, and show a form of soundness theorem. We also see how ordinary logical relations and metric relations can be seen as instances of differential logical relations. Finally, we show that differential logical relations can be organised in a cartesian closed category, contrarily to metric relations, which are well-known not to have such a structure, but only that of a monoidal closed category.},
  archivePrefix = {arXiv},
  eprint = {1904.12137},
  eprinttype = {arxiv},
  journal = {arXiv:1904.12137 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@incollection{Lagorio2009Featherweight,
  title = {Featherweight {{Jigsaw}}: {{A Minimal Core Calculus}} for {{Modular Composition}} of {{Classes}}},
  shorttitle = {Featherweight {{Jigsaw}}},
  booktitle = {{{ECOOP}} 2009 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Lagorio, Giovanni and Servetto, Marco and Zucca, Elena},
  editor = {Drossopoulou, Sophia},
  year = {2009},
  month = jul,
  pages = {244--268},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-03013-0_12},
  abstract = {We present FJig, a simple calculus where basic building blocks are classes in the style of Featherweight Java, declaring fields, methods and one constructor. However, inheritance has been generalized to the much more flexible notion originally proposed in Bracha's Jigsaw framework. That is, classes play also the role of modules, that can be composed by a rich set of operators, all of which can be expressed by a minimal core. We keep the nominal approach of Java-like languages, that is, types are class names. However, a class is not necessarily a structural subtype of any class used in its defining expression. The calculus allows the encoding of a large variety of different mechanisms for software composition in class-based languages, including standard inheritance, mixin classes, traits and hiding. Hence, FJig can be used as a unifying framework for analyzing existing mechanisms and proposing new extensions. We provide two different semantics of an FJig program: flattening and direct semantics. The difference is analogous to that between two intuitive models to understand inheritance: the former where inherited methods are copied into heir classes, and the latter where member lookup is performed by ascending the inheritance chain. Here we address equivalence of these two views for a more sophisticated composition mechanism.},
  copyright = {\textcopyright{}2009 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  keywords = {Computer Communication Networks,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {5653},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Lahav2017Repairing,
  title = {Repairing {{Sequential Consistency}} in {{C}}/{{C}}++11},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Lahav, Ori and Vafeiadis, Viktor and Kang, Jeehoon and Hur, Chung-Kil and Dreyer, Derek},
  year = {2017},
  pages = {618--632},
  publisher = {{ACM}},
  address = {{Barcelona, Spain}},
  doi = {10.1145/3062341.3062352},
  abstract = {The C/C++11 memory model defines the semantics of concurrent memory accesses in C/C++, and in particular supports racy "atomic" accesses at a range of different consistency levels, from very weak consistency ("relaxed") to strong, sequential consistency ("SC"). Unfortunately, as we observe in this paper, the semantics of SC atomic accesses in C/C++11, as well as in all proposed strengthenings of the semantics, is flawed, in that (contrary to previously published results) both suggested compilation schemes to the Power architecture are unsound. We propose a model, called RC11 (for Repaired C11), with a better semantics for SC accesses that restores the soundness of the compilation schemes to Power, maintains the DRF-SC guarantee, and provides stronger, more useful, guarantees to SC fences. In addition, we formally prove, for the first time, the correctness of the proposed stronger compilation schemes to Power that preserve load-to-store ordering and avoid "out-of-thin-air" reads.},
  isbn = {978-1-4503-4988-8},
  keywords = {C++11,declarative semantics,sequential consistency,Weak memory models},
  series = {{{PLDI}} 2017}
}

@incollection{Lahiri2012symdiff,
  title = {{{SYMDIFF}}: {{A Language}}-{{Agnostic Semantic Diff Tool}} for {{Imperative Programs}}},
  shorttitle = {{{SYMDIFF}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Lahiri, Shuvendu K. and Hawblitzel, Chris and Kawaguchi, Ming and Reb{\^e}lo, Henrique},
  editor = {Madhusudan, P. and Seshia, Sanjit A.},
  year = {2012},
  month = jul,
  pages = {712--717},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-31424-7_54},
  abstract = {In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C\# and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-31423-0 978-3-642-31424-7},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Hardware,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering,Special Purpose and Application-Based Systems},
  language = {en},
  number = {7358},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Lammel2005scrap,
  title = {Scrap {{Your Boilerplate}} with {{Class}}: {{Extensible Generic Functions}}},
  shorttitle = {Scrap {{Your Boilerplate}} with {{Class}}},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {L{\"a}mmel, Ralf and Jones, Simon Peyton},
  year = {2005},
  pages = {204--215},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1086365.1086391},
  abstract = {The 'Scrap your boilerplate' approach to generic programming allows the programmer to write generic functions that can traverse arbitrary data structures, and yet have type-specific cases. However, the original approach required all the type-specific cases to be supplied at once, when the recursive knot of generic function definition is tied. Hence, generic functions were closed. In contrast, Haskell's type classes support open, or extensible, functions that can be extended with new type-specific cases as new data types are defined. In this paper, we extend the 'Scrap your boilerplate' approach to support this open style. On the way, we demonstrate the desirability of abstraction over type classes, and the usefulness of recursive dictionarie.},
  isbn = {1-59593-064-7},
  keywords = {extensibility,generic programming,recursive dictionaries,type classes,typecase},
  series = {{{ICFP}} '05}
}

@inproceedings{Lammel2006software,
  title = {Software {{Extension}} and {{Integration}} with {{Type Classes}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {L{\"a}mmel, Ralf and Ostermann, Klaus},
  year = {2006},
  pages = {161--170},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1173706.1173732},
  abstract = {The abilities to extend a software module and to integrate a software module into an existing software system without changing existing source code are fundamental challenges in software engineering and programming-language design. We reconsider these challenges at the level of language expressiveness, by using the language concept of type classes, as it is available in the functional programming language Haskell. A detailed comparison with related work shows that type classes provide a powerful framework in which solutions to known software extension and integration problems can be provided. We also pinpoint several limitations of type classes in this context.},
  isbn = {1-59593-237-2},
  keywords = {expression problem,family polymorphism,framework integration,Haskell,multiple dispatch,object adapter,software extension,software integration,type classes,tyranny of the dominant decomposition},
  series = {{{GPCE}} '06}
}

@incollection{Lammel2008expression,
  title = {The {{Expression Lemma}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {L{\"a}mmel, Ralf and Rypacek, Ondrej},
  editor = {Audebaud, Philippe and {Paulin-Mohring}, Christine},
  year = {2008},
  pages = {193--219},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Algebraic data types and catamorphisms (folds) play a central role in functional programming as they allow programmers to define recursive data structures and operations on them uniformly by structural recursion. Likewise, in object-oriented (OO) programming, recursive hierarchies of object types with virtual methods play a central role for the same reason. There is a semantical correspondence between these two situations which we reveal and formalize categorically. To this end, we assume a coalgebraic model of OO programming with functional objects. The development may be helpful in deriving refactorings that turn sufficiently disciplined functional programs into OO programs of a designated shape and vice versa.},
  copyright = {\textcopyright{}2008 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-70593-2 978-3-540-70594-9},
  keywords = {catamorphism,cofree comonad,Computation by Abstract Devices,distributive law,expression lemma,expression problem,fold,free monad,functional object,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,program calculation,Programming Languages; Compilers; Interpreters,Software Engineering,the composite design pattern},
  language = {en},
  number = {5133},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Lamping1990algorithm,
  title = {An {{Algorithm}} for {{Optimal Lambda Calculus Reduction}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lamping, John},
  year = {1990},
  pages = {16--30},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/96709.96711},
  abstract = {We present an algorithm for lambda expression reduction that avoids any copying that could later cause duplication of work. It is optimal in the sense defined by L{\'e}vy. The basis of the algorithm is a graphical representation of the kinds of commonality that can arise from substitutions; the idea can be adapted to represent other kinds of expressions besides lambda expressions. The algorithm is also well suited to parallel implementations, consisting of a fixed set of local graph rewrite rules.},
  isbn = {0-89791-343-4},
  series = {{{POPL}} '90}
}

@article{Lamport1997Composition,
  title = {Composition: {{A Way}} to {{Make Proofs Harder}}},
  shorttitle = {Composition},
  author = {Lamport, Leslie},
  year = {1997},
  month = dec,
  volume = {1536},
  abstract = {Systems are complicated. We master their complexity by building them from simpler components. This suggests that to master the complexity of reasoning about systems, we should prove properties of the separate components and then combine those properties to deduce properties of the entire system. In concurrent systems, the obvious choice of component is the process. \ldots{}},
  language = {en-US}
}

@article{Lamport2016How,
  title = {How to {{Write}} a 21st {{Century Proof}}},
  author = {Lamport, Leslie},
  year = {2016},
  month = dec,
  abstract = {I was invited to give a talk at a celebration of the 80th birthday of Richard Palais. It was at a celebration of his 60th birthday that I first gave a talk about how to write a proof\textendash{}a talk that led to [101]. So, I thought it would be fun to give the same talk, \ldots{}},
  journal = {Journal of Fixed Point Theory and Applications}
}

@article{Lamport2016Howa,
  title = {How to {{Write}} a {{Proof}}},
  author = {Lamport, Leslie},
  year = {2016},
  month = dec,
  volume = {102/7},
  abstract = {TLA gave me, for the first time, a formalism in which it was possible to write completely formal proofs without first having to add an additional layer of formal semantics. I began writing proofs the way I and all mathematicians and computer scientists had learned to write them, using a sequence of lemmas whose proofs \ldots{}},
  journal = {Microsoft Research}
}

@article{Lane1981Mathematical,
  title = {Mathematical {{Models}}: {{A Sketch}} for the {{Philosophy}} of {{Mathematics}}},
  shorttitle = {Mathematical {{Models}}},
  author = {Lane, Saunders Mac},
  year = {1981},
  volume = {88},
  pages = {462--472},
  issn = {0002-9890},
  doi = {10.2307/2321751},
  journal = {The American Mathematical Monthly},
  number = {7}
}

@article{Larsen1991Using,
  title = {Using Information Systems to Solve Recursive Domain Equations},
  author = {Larsen, Kim Guldstrand and Winskel, Glynn},
  year = {1991},
  month = apr,
  volume = {91},
  pages = {232--258},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90068-D},
  abstract = {This paper aims to make the following main contribution: to show how to use the concrete nature of Scott's information systems to advantage in solving recursive domain equations. The method is based on the substructure relation between information systems. This essentially makes a complete partial order (cpo) of information systems. Standard domain constructions like function space can be made continous on this cpo so the solution of recursive domain equations reduces to the more familiar construction of forming the least fixed-point of a continuous function.},
  journal = {Information and Computation},
  number = {2}
}

@inproceedings{Lassen2008typed,
  title = {Typed {{Normal Form Bisimulation}} for {{Parametric Polymorphism}}},
  booktitle = {23rd {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}, 2008. {{LICS}} '08},
  author = {Lassen, S.B. and Levy, P.B.},
  year = {2008},
  month = jun,
  pages = {341--352},
  doi = {10.1109/LICS.2008.26},
  abstract = {This paper presents a new bisimulation theory for parametric polymorphism which enables straight forward co-inductive proofs of program equivalences involving existential types. The theory is an instance of typed normal form bisimulation and demonstrates the power of this recent framework for modeling typed lambda calculi as labelled transition systems.We develop our theory for a continuation-passing style calculus, Jump-With-Argument, where normal form bisimulation takes a simple form. We equip the calculus with both existential and recursive types. An "ultimate pattern matching theorem" enables us to define bisimilarity and we show it to be a congruence. We apply our theory to proving program equivalences, type isomorphisms and genericity.},
  keywords = {bisimulation,bisimulation equivalence,bisimulation theory,Calculus,coinductive proofs,Computer languages,Computer science,continuation-passing style calculus,Counting circuits,equivalence classes,Filling,lambda calculi,lambda calculus,Logic,LTS,parametric polymorphism,pattern matching,Power system modeling,program equivalences,Reasoning about programs,recursive functions,recursive types,Robustness,theorem proving,type isomorphisms,type theory,typed lambda calculus,typed normal form bisimulation,ultimate pattern matching theorem}
}

@inproceedings{Le2014mucheck,
  title = {{{MuCheck}}: {{An Extensible Tool}} for {{Mutation Testing}} of {{Haskell Programs}}},
  shorttitle = {{{MuCheck}}},
  booktitle = {Proceedings of the 2014 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Le, Duc and Alipour, Mohammad Amin and Gopinath, Rahul and Groce, Alex},
  year = {2014},
  pages = {429--432},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2610384.2628052},
  abstract = {This paper presents MuCheck, a mutation testing tool for Haskell programs. MuCheck is a counterpart to the widely used QuickCheck random testing tool for functional programs, and can be used to evaluate the efficacy of QuickCheck property definitions. The tool implements mutation operators that are specifically designed for functional programs, and makes use of the type system of Haskell to achieve a more relevant set of mutants than otherwise possible. Mutation coverage is particularly valuable for functional programs due to highly compact code, referential transparency, and clean semantics; these make augmenting a test suite or specification based on surviving mutants a practical method for improved testing.},
  isbn = {978-1-4503-2645-2},
  keywords = {functional programming languages,Haskell,Mutatation Testing,Mutation Operators},
  series = {{{ISSTA}} 2014}
}

@incollection{Leather2009PullUps,
  title = {Pull-{{Ups}}, {{Push}}-{{Downs}}, and {{Passing It Around}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Leather, Sean and L{\"o}h, Andres and Jeuring, Johan},
  editor = {Moraz{\'a}n, Marco T. and Scholz, Sven-Bodo},
  year = {2009},
  month = sep,
  pages = {159--178},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-16478-1_10},
  abstract = {Programs in languages such as Haskell are often datatype-centric and make extensive use of folds on that datatype. Incrementalization of such a program can significantly improve its performance by transforming monolithic atomic folds into incremental computations. Functional incrementalization separates the recursion from the application of the algebra in order to reduce redundant computations and reuse intermediate results. In this paper, we motivate incrementalization with a simple example and present a library for transforming programs using upwards, downwards, and circular incrementalization. Our benchmarks show that incrementalized computations using the library are nearly as fast as handwritten atomic functions.},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-16477-4 978-3-642-16478-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {6041},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Leduc2017Revisiting,
  title = {Revisiting {{Visitors}} for {{Modular Extension}} of {{Executable DSMLs}}},
  booktitle = {{{ACM}}/{{IEEE}} 20th {{International Conference}} on {{Model Driven Engineering Languages}} and {{Systems}}},
  author = {Leduc, Manuel and Degueule, Thomas and Combemale, Benoit and Storm, Tijs Van Der and Barais, Olivier},
  year = {2017},
  month = sep,
  abstract = {Executable Domain-Specific Modeling Languages (xDSMLs) are typically defined by metamodels that specify their abstract syntax, and model interpreters or compilers that define their execution semantics. To face the proliferation of xDSMLs in many domains, it is important to provide language engineering facilities for opportunistic reuse, extension, and customization of existing xDSMLs to ease the definition of new ones. Current approaches to language reuse either require to anticipate reuse, make use of advanced features that are not widely available in programming languages, or are not directly applicable to metamodel-based xDSMLs. In this paper, we propose a new language implementation pattern, named REVISITOR, that enables independent extensibility of the syntax and semantics of metamodel-based xDSMLs with incremental compilation and without anticipation. We seamlessly implement our approach alongside the compilation chain of the Eclipse Modeling Framework, thereby demonstrating that it is directly and broadly applicable in various modeling environments. We show how it can be employed to incrementally extend both the syntax and semantics of the fUML language without requiring anticipation or re-compilation of existing code, and with acceptable performance penalty compared to classical handmade visitors.},
  language = {en}
}

@article{Lee2004optimizing,
  title = {Optimizing {{ML}} with {{Run}}-Time {{Code Generation}}},
  author = {Lee, Peter and Leone, Mark},
  year = {2004},
  month = apr,
  volume = {39},
  pages = {540--553},
  issn = {0362-1340},
  doi = {10.1145/989393.989448},
  abstract = {We describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Run-time code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compile-time specialization can reduce the cost of run-time code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.},
  journal = {SIGPLAN Not.},
  number = {4}
}

@inproceedings{Lee2007mechanized,
  title = {Towards a Mechanized Metatheory of {{Standard ML}}},
  booktitle = {Proceedings of the 34th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lee, Daniel K. and Crary, Karl and Harper, Robert},
  year = {2007},
  pages = {173--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1190216.1190245},
  abstract = {We present an internal language with equivalent expressive power to Standard ML, and discuss its formalization in LF and the machine-checked verification of its type safety in Twelf. The internal language is intended to serve as the target of elaboration in an elaborative semantics for Standard ML in the style of Harper and Stone. Therefore, it includes all the programming mechanisms necessary to implement Standard ML, including translucent modules, abstraction, polymorphism, higher kinds, references, exceptions, recursive types, and recursive functions. Our successful formalization of the proof involved a careful interplay between the precise formulations of the various mechanisms, and required the invention of new representation and proof techniques of general interest.},
  isbn = {1-59593-575-4},
  keywords = {language definitions,logical frameworks,mechanized metatheory,standard ML,twelf,type safety},
  series = {{{POPL}} '07}
}

@inproceedings{Leijen1999domain,
  title = {Domain {{Specific Embedded Compilers}}},
  booktitle = {Proceedings of the {{2Nd Conference}} on {{Domain}}-Specific {{Languages}}},
  author = {Leijen, Daan and Meijer, Erik},
  year = {1999},
  pages = {109--122},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/331960.331977},
  abstract = {Domain-specific embedded languages (DSELs) expressed in higher-order, typed (HOT) languages provide a composable framework for domain-specific abstractions. Such a framework is of greater utility than a collection of stand-alone domain-specific languages. Usually, embedded domain specific languages are build on top of a set of domain specific primitive functions that are ultimately implemented using some form of foreign function call. We sketch a general design pattern/or embedding client-server style services into Haskell using a domain specific embedded compiler for the server's source language. In particular we apply this idea to implement Haskell/DB, a domain specific embdedded compiler that dynamically generates of SQL queries from monad comprehensions, which are then executed on an arbitrary ODBC database server.},
  isbn = {1-58113-255-7},
  series = {{{DSL}} '99}
}

@inproceedings{Leijen2005Qualified,
  title = {Qualified {{Types}} for {{MLF}}},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Leijen, Daan and L{\"o}h, Andres},
  year = {2005},
  pages = {144--155},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1086365.1086385},
  abstract = {MLF is a type system that extends a functional language with impredicative rank-n polymorphism. Type inference remains possible and only in some clearly defined situations, a local type annotation is required. Qualified types are a general concept that can accommodate a wide range of type systems extension, for example, type classes in Haskell. We show how the theory of qualified types can be used seamlessly with the higher-ranked impredicative polymorphism of MLF, and give a solution to the non-trivial problem of evidence translation in the presence of impredicative datatypes.},
  isbn = {978-1-59593-064-4},
  keywords = {higher-rank polymorphism,impredicativity,MLF,qualified types},
  series = {{{ICFP}} '05}
}

@inproceedings{Leijen2017Type,
  title = {Type {{Directed Compilation}} of {{Row}}-Typed {{Algebraic Effects}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Leijen, Daan},
  year = {2017},
  pages = {486--499},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009872},
  abstract = {Algebraic effect handlers, introduced by Plotkin and Power in 2002,   are recently gaining in popularity as a purely functional approach to   modeling effects. In this article, we give a full overview of   practical algebraic effects in the context of a compiled   implementation in the Koka language. In particular, we show how   algebraic effects generalize over common constructs like exception   handling, state, iterators and async-await. We give an effective type   inference algorithm based on extensible effect rows using scoped   labels, and a direct operational semantics. Finally, we show an   efficient compilation scheme to common runtime platforms (like   JavaScript) using a type directed selective CPS translation.},
  isbn = {978-1-4503-4660-3},
  keywords = {Algebraic Effect Handlers,CPS transformation,type inference},
  series = {{{POPL}} 2017}
}

@article{Leinster2010informal,
  title = {An Informal Introduction to Topos Theory},
  author = {Leinster, Tom},
  year = {2010},
  month = dec,
  abstract = {This short expository text is for readers who are confident in basic category theory but know little or nothing about toposes. It is based on some impromptu talks given to a small group of category theorists.},
  archivePrefix = {arXiv},
  eprint = {1012.5647},
  eprinttype = {arxiv},
  journal = {arXiv:1012.5647 [math]},
  keywords = {Mathematics - Algebraic Geometry,Mathematics - Category Theory,Mathematics - Logic},
  primaryClass = {math}
}

@article{Leinster2012rethinking,
  title = {Rethinking Set Theory},
  author = {Leinster, Tom},
  year = {2012},
  month = dec,
  abstract = {Mathematicians manipulate sets with confidence almost every day, rarely making mistakes. Few of us, however, could accurately quote what are often referred to as "the" axioms of set theory. This suggests that we all carry around with us, perhaps subconsciously, a reliable body of operating principles for manipulating sets. What if we were to take some of those principles and adopt them as our axioms instead? The message of this article is that this can be done, in a simple, practical way (due to Lawvere). The resulting axioms are ten thoroughly mundane statements about sets. This is an expository article for a general mathematical readership.},
  archivePrefix = {arXiv},
  eprint = {1212.6543},
  eprinttype = {arxiv},
  journal = {arXiv:1212.6543 [math]},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  primaryClass = {math}
}

@article{Leinster2016Basic,
  title = {Basic {{Category Theory}}},
  author = {Leinster, Tom},
  year = {2016},
  month = dec,
  abstract = {This short introduction to category theory is for readers with relatively little mathematical background. At its heart is the concept of a universal property, important throughout mathematics. After a chapter introducing the basic definitions, separate chapters present three ways of expressing universal properties: via adjoint functors, representable functors, and limits. A final chapter ties the three together. For each new categorical concept, a generous supply of examples is provided, taken from different parts of mathematics. At points where the leap in abstraction is particularly great (such as the Yoneda lemma), the reader will find careful and extensive explanations.},
  archivePrefix = {arXiv},
  eprint = {1612.09375},
  eprinttype = {arxiv},
  journal = {arXiv:1612.09375 [math]},
  keywords = {Mathematics - Algebraic Topology,Mathematics - Category Theory,Mathematics - Logic},
  primaryClass = {math}
}

@article{Leivant1991Finitely,
  title = {Finitely Stratified Polymorphism},
  author = {Leivant, Daniel},
  year = {1991},
  month = jul,
  volume = {93},
  pages = {93--113},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90053-5},
  abstract = {We consider predicative type-abstraction disciplines based on type quantification with finitely stratified levels. These lie in the vast middle ground between quantifier-free parametric abstraction and full impredicative abstraction. Stratified polymorphism has an unproblematic set-theoretic semantics, and may lend itself to new approaches to type inference, without sacrificing useful expressive power. Our main technical result is that the functions representable in the finitely stratified polymorphic {$\lambda$}-calculus are precisely the super-elementary functions, i.e., the class {$\epsilon$}4 in Grzegorczyk's subrecursive hierarchy. This implies that there is no super-elementary bound on the length of optimal normalization sequences, and that the equality problem for finitely stratified polymorphic {$\lambda$}-expressions is not super-elementary. We also observe that finitely stratified polymorphism augmented with type recursion admits functional algorithms that are not typable in the full second order {$\lambda$}-calculus.},
  journal = {Information and Computation},
  number = {1},
  series = {Selections from 1989 {{IEEE Symposium}} on {{Logic}} in {{Computer Science}}}
}

@inproceedings{Lempsink2009Typesafe,
  title = {Type-Safe {{Diff}} for {{Families}} of {{Datatypes}}},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Lempsink, Eelco and Leather, Sean and L{\"o}h, Andres},
  year = {2009},
  pages = {61--72},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596614.1596624},
  abstract = {The UNIX diff program finds the difference between two text files using a classic algorithm for determining the longest common subsequence; however, when working with structured input (e.g. program code), we often want to find the difference between tree-like data (e.g. the abstract syntax tree). In a functional programming language such as Haskell, we can represent this data with a family of (mutually recursive) datatypes. In this paper, we describe a functional, datatype-generic implementation of diff (and the associated program patch). Our approach requires advanced type system features to preserve type safety; therefore, we present the code in Agda, a dependently-typed language well-suited to datatype-generic programming. In order to establish the usefulness of our work, we show that its efficiency can be improved with memoization and that it can also be defined in Haskell.},
  isbn = {978-1-60558-510-9},
  keywords = {Datatype-generic programming,dependent types,edit distance},
  series = {{{WGP}} '09}
}

@techreport{Leone1993deferred,
  title = {Deferred {{Compilation}}: {{The Automation}} of {{Run}}-{{Time Code Generation}}},
  shorttitle = {Deferred {{Compilation}}},
  author = {Leone, Mark and Lee, Peter},
  year = {1993},
  address = {{Pittsburgh, PA, USA}},
  institution = {{Carnegie Mellon University}},
  abstract = {This paper describes deferred compilation, an alternative and complement to compile-time program analysis and optimization. By deferring aspects of compilation to run time, exact information about programs can be exploited, leading to greater opportunities for code improvement. This is in contrast to the use of static analyses, which are inherently conservative. Deferred compilation automates the translation of ordinary programs into native machine code that performs fast optimization and native-code generation at run time. Automation is obtained through the use of a compile-time staging analysis, which determines the portions of a program that may be safely and profitably compiled at run time. Fast run-time optimization is obtained by trading space for time: compile-time specialization yields numerous run-time code generators, each customized to optimize a small portion of the source program based on run-time information. Implementation strategies developed for a prototype compiler are discussed, and the results of preliminary experiments demonstrating significant overall speedup are presented.}
}

@inproceedings{Lerner2002composing,
  title = {Composing {{Dataflow Analyses}} and {{Transformations}}},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lerner, Sorin and Grove, David and Chambers, Craig},
  year = {2002},
  pages = {270--282},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/503272.503298},
  abstract = {Dataflow analyses can have mutually beneficial interactions. Previous efforts to exploit these interactions have either (1) iteratively performed each individual analysis until no further improvements are discovered or (2) developed "super-analyses" that manually combine conceptually separate analyses. We have devised a new approach that allows analyses to be defined independently while still enabling them to be combined automatically and profitably. Our approach avoids the loss of precision associated with iterating individual analyses and the implementation difficulties of manually writing a super-analysis. The key to our approach is a novel method of implicit communication between the individual components of a super-analysis based on graph transformations. In this paper, we precisely define our approach; we demonstrate that it is sound and it terminates; finally we give experimental results showing that in practice (1) our framework produces results at least as precise as iterating the individual analyses while compiling at least 5 times faster, and (2) our framework achieves the same precision as a manually written super-analysis while incurring a compile-time overhead of less than 20\%.},
  isbn = {1-58113-450-9},
  series = {{{POPL}} '02}
}

@inproceedings{Lerner2003automatically,
  title = {Automatically {{Proving}} the {{Correctness}} of {{Compiler Optimizations}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2003 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Lerner, Sorin and Millstein, Todd and Chambers, Craig},
  year = {2003},
  pages = {220--231},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/781131.781156},
  abstract = {We describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.},
  isbn = {1-58113-662-5},
  keywords = {automated correctness proofs,compiler optimization},
  series = {{{PLDI}} '03}
}

@article{Leroy1990zinc,
  title = {The {{ZINC}} Experiment: An Economical Implementation of the {{ML}} Language},
  author = {Leroy, Xavier},
  year = {1990},
  abstract = {This report details the design and implementation of the ZINC system. This is an experimental implementation of the ML language, which has later evolved in the Caml Light system. This system is strongly oriented toward separate compilation and the production of small, standalone programs; type safety is ensured by a Modula-2-like module system. ZINC uses simple, portable techniques, such as bytecode interpretation; a sophisticated execution model helps counterbalance the interpretation overhead.},
  number = {117},
  urllocal = {http://gallium.inria.fr/ xleroy/publi/ZINC.pdf},
  xtopic = {caml}
}

@inproceedings{Leroy1995applicative,
  title = {Applicative {{Functors}} and {{Fully Transparent Higher}}-Order {{Modules}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Leroy, Xavier},
  year = {1995},
  pages = {142--153},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/199448.199476},
  abstract = {we present a variety of the Standard ML module system where parameterized abstract types (i.e. functors returning generative types) map provably equal arguments to compatible abstract types, instead of generating distinct types at each applications as in Standard ML. This extension solves the full transparency problem (how to give syntactic signatures for higher-order functors that express exactly their propagation of type equations), and also provides better support for non-closed code fragments.},
  isbn = {0-89791-692-1},
  series = {{{POPL}} '95}
}

@article{Leroy2000modular,
  title = {A Modular Module System},
  author = {Leroy, Xavier},
  year = {2000},
  month = may,
  volume = {10},
  pages = {269--303},
  issn = {1469-7653},
  doi = {null},
  abstract = {A simple implementation of an SML-like module system is presented as a module parameterized by a base language and its type-checker. This implementation is useful both as a detailed tutorial on the Harper\textendash{}Lillibridge\textendash{}Leroy module system and its implementation, and as a constructive demonstration of the applicability of that module system to a wide range of programming languages.},
  journal = {Journal of Functional Programming},
  number = {03}
}

@inproceedings{Leroy2006formal,
  title = {Formal {{Certification}} of a {{Compiler Back}}-End or: {{Programming}} a {{Compiler}} with a {{Proof Assistant}}},
  shorttitle = {Formal {{Certification}} of a {{Compiler Back}}-End Or},
  booktitle = {Conference {{Record}} of the 33rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Leroy, Xavier},
  year = {2006},
  pages = {42--54},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1111037.1111042},
  abstract = {This paper reports on the development and formal certification (proof of semantic preservation) of a compiler from Cminor (a C-like imperative language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a certified compiler is useful in the context of formal methods applied to the certification of critical software: the certification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
  isbn = {1-59593-027-2},
  keywords = {certified compilation,compiler transformations and optimizations,program proof,semantic preservation,the Coq theorem prover},
  series = {{{POPL}} '06}
}

@article{Leroy2009Coinductive,
  title = {Coinductive Big-Step Operational Semantics},
  author = {Leroy, Xavier and Grall, Herv{\'e}},
  year = {2009},
  month = feb,
  volume = {207},
  pages = {284--304},
  issn = {0890-5401},
  doi = {10.1016/j.ic.2007.12.004},
  abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.},
  journal = {Information and Computation},
  keywords = {Big-step semantics,coinduction,Compiler correctness,Mechanized proofs,Natural semantics,operational semantics,Reduction semantics,Small-step semantics,The Coq proof assistant,Type soundness},
  number = {2},
  series = {Special Issue on {{Structural Operational Semantics}} ({{SOS}})}
}

@article{Leroy2009formal,
  title = {Formal {{Verification}} of a {{Realistic Compiler}}},
  author = {Leroy, Xavier},
  year = {2009},
  month = jul,
  volume = {52},
  pages = {107--115},
  issn = {0001-0782},
  doi = {10.1145/1538788.1538814},
  abstract = {This paper reports on the development and formal verification (proof of semantic preservation) of CompCert, a compiler from Clight (a large subset of the C programming language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a verified compiler is useful in the context of critical software and its formal verification: the verification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.},
  journal = {Commun. ACM},
  number = {7}
}

@article{Letia2009crdts,
  title = {{{CRDTs}}: {{Consistency}} without Concurrency Control},
  shorttitle = {{{CRDTs}}},
  author = {Letia, Mihai and Pregui{\c c}a, Nuno and Shapiro, Marc},
  year = {2009},
  month = jul,
  abstract = {A CRDT is a data type whose operations commute when they are concurrent. Replicas of a CRDT eventually converge without any complex concurrency control. As an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer called Treedoc. We outline the design, implementation and performance of Treedoc. We discuss how the CRDT concept can be generalised, and its limitations.},
  archivePrefix = {arXiv},
  eprint = {0907.0929},
  eprinttype = {arxiv},
  journal = {arXiv:0907.0929 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@incollection{Levy1999callbypushvalue,
  title = {Call-by-{{Push}}-{{Value}}: {{A Subsuming Paradigm}}},
  shorttitle = {Call-by-{{Push}}-{{Value}}},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Levy, Paul Blain},
  editor = {Girard, Jean-Yves},
  year = {1999},
  month = jan,
  pages = {228--243},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Call-by-push-value is a new paradigm that subsumes the call-by-name and call-by-value paradigms, in the following sense: both operational and denotational semantics for those paradigms can be seen as arising, via translations that we will provide, from similar semantics for call-by-observable. To explain call-by-observable, we first discuss general operational ideas, especially the distinction between values and computations, using the principle that ``a value is, a computation does''. Using an example program, we see that the lambda-calculus primitives can be understood as push/pop commands for an operand-stack. We provide operational and denotational semantics for a range of computational effects and show their agreement. We hence obtain semantics for call-by-name and call-by-value, of which some are familiar, some are new and some were known but previously appeared mysterious.},
  copyright = {\textcopyright{}1999 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-65763-7 978-3-540-48959-7},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Techniques},
  language = {en},
  number = {1581},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Levy2003adjunction,
  title = {Adjunction {{Models For Call}}-{{By}}-{{Push}}-{{Value With Stacks}}},
  author = {Levy, Paul Blain},
  year = {2003},
  month = feb,
  volume = {69},
  pages = {248--271},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)80568-1},
  abstract = {Call-by-push-value (CBPV) is a new paradigm, which has been claimed to provide the semantic primitives from which call-by-value and call-by-name are built. We present its operational semantics in the form of a Felleisen-Friedman style CK-machine, and see how this machine suggests a new term judgement of stacks. When augmented with this judgement, CBPV has an elegant categorical semantics based on adjunctions.

We describe this categorical semantics incrementally. First, we introduce locally indexed categories and the opGrothendieck construction, and use these to give the basic structure for interpreting the 3 judgements: values, stacks and computations. Then we look at the universal property required to interpret each type constructor. We define a model to be a strong adjunction with countable coproducts, countable products and exponentials.

We justify this definition in two ways. First, we see that it has a wide range of instances: we give examples for divergence, storage, erratic choice, continuations etc., in each case decomposing Moggi's strong monad into a strong adjunction.

For the second justification, we start by giving equational laws for CBPV+stacks. This requires some additional pattern-matching constructs, but they do not affect the set of computations. We then show that the categories of theories and of models are equivalent.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CTCS}}'02, {{Category Theory}} and {{Computer Science}}}
}

@inproceedings{Lewis2000Implicit,
  title = {Implicit {{Parameters}}: {{Dynamic Scoping}} with {{Static Types}}},
  shorttitle = {Implicit {{Parameters}}},
  booktitle = {Proceedings of the 27th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lewis, Jeffrey R. and Launchbury, John and Meijer, Erik and Shields, Mark B.},
  year = {2000},
  pages = {108--118},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/325694.325708},
  abstract = {This paper introduces a language feature, called implicit parameters, that provides dynamically scoped variables within a statically-typed Hindley-Milner framework. Implicit parameters are lexically distinct from regular identifiers, and are bound by a special with construct whose scope is dynamic, rather than static as with let. Implicit parameters are treated by the type system as parameters that are not explicitly declared, but are inferred from their use.
We present implicit parameters within a small call-by-name \&lgr;-calculus. We give a type system, a type inference algorithm, and several semantics. We also explore implicit parameters in the wider settings of call-by-need languages with overloading, and call-by-value languages with effects. As a witness to the former, we have implemented implicit parameters as an extension of Haskell within the Hugs interpreter, which we use to present several motivating examples.},
  isbn = {978-1-58113-125-3},
  series = {{{POPL}} '00}
}

@inproceedings{Liang1995monad,
  title = {Monad {{Transformers}} and {{Modular Interpreters}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Liang, Sheng and Hudak, Paul and Jones, Mark},
  year = {1995},
  pages = {333--343},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/199448.199528},
  abstract = {We show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (call-by-name, call-by-value and lazy evaluation), references and assignment, nondeterminism, first-class continuations, and program tracing.The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions.Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that incudes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.},
  isbn = {0-89791-692-1},
  series = {{{POPL}} '95}
}

@incollection{Liang1996modular,
  title = {Modular Denotational Semantics for Compiler Construction},
  booktitle = {Programming {{Languages}} and {{Systems}} \textemdash{} {{ESOP}} '96},
  author = {Liang, Sheng and Hudak, Paul},
  editor = {Nielson, Hanne Riis},
  year = {1996},
  month = jan,
  pages = {219--234},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We show the benefits of applying modular monadic semantics to compiler construction. Modular monadic semantics allows us to define a language with a rich set of features from reusable building blocks, and use program transformation and equational reasoning to improve code. Compared to denotational semantics, reasoning in monadic style offers the added benefits of highly modularized proofs and more widely applicable results. To demonstrate, we present an axiomatization of environments, and use it to prove the correctness of a well-known compilation technique. The monadic approach also facilitates generating code in various target languages with different sets of built-in features.},
  copyright = {\textcopyright{}1996 Springer-Verlag},
  isbn = {978-3-540-61055-7 978-3-540-49942-8},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {1058},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@phdthesis{Liang1998modular,
  title = {Modular {{Monadic Semantics}} and {{Compilation}}},
  author = {Liang, Sheng},
  year = {1998},
  address = {{New Haven, CT, USA}},
  abstract = {Modular monadic semantics is a high-level and modular form of denotational semantics. It is capable of capturing individual programming language features and their interactions. This thesis explores the theory and applications of modular monadic semantics, including: building blocks for individual programming features, equational reasoning with laws and axioms, modular proofs, program transformation, modular interpreters, and compiler construction. We will demonstrate that the modular monadic semantics framework makes programming languages easy to
specify, reason about, and implement.},
  note = {AAI9835276},
  school = {Yale University}
}

@article{Liang2015satbased,
  title = {{{SAT}}-Based {{Analysis}} of {{Large Real}}-World {{Feature Models}} Is {{Easy}}},
  author = {Liang, Jia Hui and Ganesh, Vijay and Raman, Venkatesh and Czarnecki, Krzysztof},
  year = {2015},
  month = jun,
  abstract = {Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide efficient automatic analysis of real-world feature models (FM) of systems ranging from cars to operating systems. It is well-known that solver-based analysis of real-world FMs scale very well even though SAT instances obtained from such FMs are large, and the corresponding analysis problems are known to be NP-complete. To better understand why SAT solvers are so effective, we systematically studied many syntactic and semantic characteristics of a representative set of large real-world FMs. We discovered that a key reason why large real-world FMs are easy-to-analyze is that the vast majority of the variables in these models are unrestricted, i.e., the models are satisfiable for both true and false assignments to such variables under the current partial assignment. Given this discovery and our understanding of CDCL SAT solvers, we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size, explaining why solvers scale so well. Further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability. Additionally, we experimented with a series of well-known non-backtracking simplifications that are particularly effective in solving FMs. The remaining variables/clauses after simplifications, called the core, are so few that they are easily solved even with backtracking, further strengthening our conclusions.},
  archivePrefix = {arXiv},
  eprint = {1506.05198},
  eprinttype = {arxiv},
  journal = {arXiv:1506.05198 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  primaryClass = {cs}
}

@inproceedings{Lieberman1986Using,
  title = {Using {{Prototypical Objects}} to {{Implement Shared Behavior}} in {{Object}}-Oriented {{Systems}}},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}}},
  author = {Lieberman, Henry},
  year = {1986},
  pages = {214--223},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/28697.28718},
  abstract = {A traditional philosophical controversy between representing general concepts as abstract sets or classes and representing concepts as concrete prototypes is reflected in a controversy between two mechanisms for sharing behavior between objects in object oriented programming languages. Inheritance splits the object world into classes, which encode behavior shared among a group of instances, which represent individual members of these sets. The class/instance distinction is not needed if the alternative of using prototypes is adopted. A prototype represents the default behavior for a concept, and new objects can re-use part of the knowledge stored in the prototype by saying how the new object differs from the prototype. The prototype approach seems to hold some advantages for representing default knowledge, and incrementally and dynamically modifying concepts. Delegation is the mechanism for implementing this in object oriented languages. After checking its idiosyncratic behavior, an object can forward a message to prototypes to invoke more general knowledge. Because class objects must be created before their instances can be used, and behavior can only be associated with classes, inheritance fixes the communication patterns between objects at instance creation time. Because any object can be used as a prototype, and any messages can be forwarded at any time, delegation is the more flexible and general of the two techniques.},
  isbn = {0-89791-204-7},
  keywords = {_tablet},
  series = {{{OOPSLA}} '86}
}

@incollection{Lieberman2006feasibility,
  title = {Feasibility {{Studies}} for {{Programming}} in {{Natural Language}}},
  booktitle = {End {{User Development}}},
  author = {Lieberman, Henry and Liu, Hugo},
  editor = {Lieberman, Henry and Patern{\`o}, Fabio and Wulf, Volker},
  year = {2006},
  month = jan,
  pages = {459--473},
  publisher = {{Springer Netherlands}},
  abstract = {We think it is time to take another look at an old dream\textemdash{}that one could program a computer by speaking to it in natural language. Programming in natural language might seem impossible, because it would appear to require complete natural language understanding and dealing with the vagueness of human descriptions of programs. Butwe think that several developments might nowmake programming in natural language feasible. First, improved broad coverage natural language parsers and semantic extraction techniques permit partial understanding. Second, mixed-initiative dialogues can be used for meaning disambiguation. And finally, where direct understanding techniques fail, we hope to fall back on Programming by Example, and other techniques for specifying the program in a more fail-soft manner. To assess the feasibility of this project, as a first step, we are studying how non-programming users describe programs in unconstrained natural language.We are exploring how to design dialogs that help the user make precise their intentions for the program, while constraining them as little as possible. Key words. natural language programming, natural language processing, parsing, part-of-speech tagging, computer science education, programming languages, scripting languages, computer games.},
  copyright = {\textcopyright{}2006 Springer},
  isbn = {978-1-4020-4220-1 978-1-4020-5386-3},
  keywords = {Computer Science; general,Software Engineering,User Interfaces and Human Computer Interaction},
  language = {en},
  number = {9},
  series = {Human-{{Computer Interaction Series}}}
}

@inproceedings{Lindley2012embedding,
  title = {Embedding {{F}}},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Lindley, Sam},
  year = {2012},
  pages = {45--56},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364394.2364402},
  abstract = {This millennium has seen a great deal of research into embedded domain-specific languages. Primarily, such languages are simply-typed. Focusing on System F, we demonstrate how to embed polymorphic domain specific languages in Haskell and OCaml. We exploit recent language extensions including kind polymorphism and first-class modules.},
  isbn = {978-1-4503-1576-0},
  keywords = {domain specific languages,Haskell,higher-order abstract syntax,ocaml,polymorphism},
  series = {{{WGP}} '12}
}

@inproceedings{Lindley2013hasochism,
  title = {Hasochism: {{The Pleasure}} and {{Pain}} of {{Dependently Typed Haskell Programming}}},
  shorttitle = {Hasochism},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Lindley, Sam and McBride, Conor},
  year = {2013},
  pages = {81--92},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2503778.2503786},
  abstract = {Haskell's type system has outgrown its Hindley-Milner roots to the extent that it now stretches to the basics of dependently typed programming. In this paper, we collate and classify techniques for programming with dependent types in Haskell, and contribute some new ones. In particular, through extended examples---merge-sort and rectangular tilings---we show how to exploit Haskell's constraint solver as a theorem prover, delivering code which, as Agda programmers, we envy. We explore the compromises involved in simulating variations on the theme of the dependent function space in an attempt to help programmers put dependent types to work, and to inform the evolving language design both of Haskell and of dependently typed languages more broadly.},
  isbn = {978-1-4503-2383-3},
  keywords = {data type promotion,dependent types,invariants,proof search,singletons},
  series = {Haskell '13}
}

@inproceedings{Lindley2017be,
  title = {Do Be Do Be Do},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lindley, Sam and McBride, Conor and McLaughlin, Craig},
  year = {2017},
  pages = {500--514},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009897},
  abstract = {We explore the design and implementation of Frank, a strict functional programming language with a bidirectional effect type system designed from the ground up around a novel variant of Plotkin and Pretnar's effect handler abstraction.   Effect handlers provide an abstraction for modular effectful programming: a handler acts as an interpreter for a collection of commands whose interfaces are statically tracked by the type system. However, Frank eliminates the need for an additional effect handling construct by generalising the basic mechanism of functional abstraction itself. A function is simply the special case of a Frank operator that interprets no commands. Moreover, Frank's operators can be multihandlers which simultaneously interpret commands from several sources at once, without disturbing the direct style of functional programming with values.   Effect typing in Frank employs a novel form of effect polymorphism which avoid mentioning effect variables in source code. This is achieved by propagating an ambient ability inwards, rather than accumulating unions of potential effects outwards.   We introduce Frank by example, and then give a formal account of the Frank type system and its semantics. We introduce Core Frank by elaborating Frank operators into functions, case expressions, and unary handlers, and then give a sound small-step operational semantics for Core Frank.   Programming with effects and handlers is in its infancy. We contribute an exploration of future possibilities, particularly in combination with other forms of rich type system.},
  isbn = {978-1-4503-4660-3},
  keywords = {algebraic effects,bidirectional typing,call-by-push-value,continuations,effect handlers,effect polymorphism,pattern matching},
  series = {{{POPL}} 2017}
}

@incollection{Linger2004bindingtime,
  title = {Binding-{{Time Analysis}} for {{MetaML}} via {{Type Inference}} and {{Constraint Solving}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Linger, Nathan and Sheard, Tim},
  editor = {Jensen, Kurt and Podelski, Andreas},
  year = {2004},
  month = jan,
  pages = {266--279},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The two predominant program specialization techniques, partial evaluation and staged programming, take opposite approaches to automating binding-time analysis (BTA). Despite their common goal, there are no systems integrating both methods. Programmers must choose between the precision of manually placing staging annotations and the convenience of automating such annotation. We present an automatic BTA algorithm for a subset of MetaML. Such an algorithm provides a basis for a system integrating staged programming and partial evaluation because it allows programmers to switch between automatic and manual staging. Our algorithm is based on typing algorithm coupled with arithmetic-constraint solving. The algorithm decorates each subexpression of both a program and its type with numeric variables representing staging-annotations and then generates simple arithmetic constraints that describe the space of all possible stagings of the original program. Benefits of our approach include expressive BTA specifications in the form of stage-annotated types as well as support for polyvariance.},
  copyright = {\textcopyright{}2004 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-21299-7 978-3-540-24730-2},
  keywords = {Algorithm Analysis and Problem Complexity,Computer Communication Networks,Logics and Meanings of Programs,Software Engineering},
  language = {en},
  number = {2988},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Lippmeier2012guiding,
  title = {Guiding {{Parallel Array Fusion}} with {{Indexed Types}}},
  booktitle = {Proceedings of the 2012 {{Haskell Symposium}}},
  author = {Lippmeier, Ben and Chakravarty, Manuel and Keller, Gabriele and Peyton Jones, Simon},
  year = {2012},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364506.2364511},
  abstract = {We present a refined approach to parallel array fusion that uses indexed types to specify the internal representation of each array. Our approach aids the client programmer in reasoning about the performance of their program in terms of the source code. It also makes the intermediate code easier to transform at compile-time, resulting in faster compilation and more reliable runtimes. We demonstrate how our new approach improves both the clarity and performance of several end-user written programs, including a fluid flow solver and an interpolator for volumetric data.},
  isbn = {978-1-4503-1574-6},
  keywords = {arrays,data parallelism,Haskell},
  series = {Haskell '12}
}

@article{Lipton2018Troubling,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  author = {Lipton, Zachary C. and Steinhardt, Jacob},
  year = {2018},
  month = jul,
  abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
  archivePrefix = {arXiv},
  eprint = {1807.03341},
  eprinttype = {arxiv},
  journal = {arXiv:1807.03341 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{Liu2013intel,
  title = {The {{Intel Labs Haskell Research Compiler}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Liu, Hai and Glew, Neal and Petersen, Leaf and Anderson, Todd A.},
  year = {2013},
  pages = {105--116},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2503778.2503779},
  abstract = {The Glasgow Haskell Compiler (GHC) is a well supported optimizing compiler for the Haskell programming language, along with its own extensions to the language and libraries. Haskell's lazy semantics imposes a runtime model which is in general difficult to implement efficiently. GHC achieves good performance across a wide variety of programs via aggressive optimization taking advantage of the lack of side effects, and by targeting a carefully tuned virtual machine. The Intel Labs Haskell Research Compiler uses GHC as a frontend, but provides a new whole-program optimizing backend by compiling the GHC intermediate representation to a relatively generic functional language compilation platform. We found that GHC's external Core language was relatively easy to use, but reusing GHC's libraries and achieving full compatibility were harder. For certain classes of programs, our platform provides substantial performance benefits over GHC alone, performing 2x faster than GHC with the LLVM backend on selected modern performance-oriented benchmarks; for other classes of programs, the benefits of GHC's tuned virtual machine continue to outweigh the benefits of more aggressive whole program optimization. Overall we achieve parity with GHC with the LLVM backend. In this paper, we describe our Haskell compiler stack, its implementation and optimization approach, and present benchmark results comparing it to GHC.},
  isbn = {978-1-4503-2383-3},
  keywords = {compiler optimization,functional language compiler,Haskell},
  series = {Haskell '13}
}

@article{Liu2015demanddriven,
  title = {Demand-{{Driven Incremental Object Queries}}},
  author = {Liu, Yanhong A. and Brandvein, Jon and Stoller, Scott D. and Lin, Bo},
  year = {2015},
  month = nov,
  abstract = {Object queries are significantly easier to write, understand, and maintain than efficient low-level programs. However, a query may involve any number and combination of objects and sets, which can be arbitrarily nested and aliased. The objects and sets involved, starting from the given demand---the given parameter values of interest---can change arbitrarily. How to generate efficient implementations automatically, and furthermore to provide complexity guarantees? This paper describes such an automatic method. The method allows the queries to be written completely declaratively. It transforms demand into relations, based on the same basic idea for transforming objects and sets into relations in a prior work. Most importantly, it defines and incrementally maintains invariants for not only the query results, but also all auxiliary values about the objects and sets involved, starting from the demand. Implementation and experiments with problems from a variety of application areas, including distributed algorithms, confirm the analyzed complexities, trade-offs, and significant improvements over prior works.},
  archivePrefix = {arXiv},
  eprint = {1511.04583},
  eprinttype = {arxiv},
  journal = {arXiv:1511.04583 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Liu2016generic,
  title = {A Generic Algorithm for Checking Exhaustivity of Pattern Matching},
  author = {Liu, Fengyun},
  year = {2016},
  doi = {10.1145/2998392.2998401},
  abstract = {Algebraic data types and pattern matching are key features of functional programming languages. Exhaustivity checking of pattern matching is a safety belt that defends against unmatched exceptions at runtime and boosts type safety. However, the presence of language features like inheritance, typecase, traits, GADTs, path-dependent types and union types makes the checking difficult and the algorithm complex. In this paper we propose a generic algorithm that decouples the checking algorithm from specific type theories. The decoupling makes the algorithm simple and enables easy customization for specific type systems. Liu, Fengyun},
  journal = {SCALA 2016 Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
  language = {en}
}

@inproceedings{Liu2018Initialization,
  title = {Initialization Patterns in {{Dotty}}},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Liu, Fengyun and Biboudis, Aggelos and Odersky, Martin},
  year = {2018},
  pages = {51--55},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3241653.3241662},
  abstract = {Safe object initialization is important to avoid a category of runtime errors in programming languages. In this paper, we provide a case study of the initialization patterns on the Dotty compiler. In particular, we find that calling dynamic-dispatching methods, the usage of closures and instantiating nested classes are important for initialization of Scala objects. Based on the study, we conclude that existing proposals for safe initialization are inadequate for Scala.},
  isbn = {978-1-4503-5836-1},
  keywords = {Object initilization,Scala},
  series = {Scala 2018}
}

@article{Longley1999matching,
  title = {Matching Typed and Untyped Realizability},
  author = {Longley, John},
  year = {1999},
  volume = {23},
  pages = {74--100},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00105-7},
  abstract = {Realizability interpretations of logics are given by saying what it means for computational objects of some kind to realize logical formulae. The computational objects in question might be drawn from an untyped universe of computation, such as a partial combinatory algebra, or they might be typed objects such as terms of a PCF-style programming language. In some instances, one can show that a particular untyped realizability interpretation matches a particular typed one, in the sense that they give the same set of realizable formulae. In this case, we have a very good fit indeed between the typed language and the untyped realizability model \textemdash{} we refer to this condition as (constructive) logical full abstraction.

We give some examples of this situation for a variety of extensions of PCF. Of particular interest are some models that are logically fully abstract for typed languages including non-functional features. Our results establish connections between what is computable in various programming languages and what is true inside various realizability toposes. We consider some examples of logical formulae to illustrate these ideas, in particular their application to exact real-number computability.},
  journal = {Electronic Notes in Theoretical Computer Science},
  number = {1},
  series = {Tutorial {{Workshop}} on {{Realizability Semantics}} and {{Applications}} (Associated to {{FLoC}}'99, the 1999 {{Federated Logic Conference}})}
}

@inproceedings{Longley1999when,
  title = {When Is a {{Functional Program Not}} a {{Functional Program}}?},
  booktitle = {Proceedings of the {{Fourth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Longley, John},
  year = {1999},
  pages = {1--7},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/317636.317775},
  abstract = {In an impure functional language, there are programs whose behaviour is completely functional (in that they behave extensionally on inputs), but the functions they compute cannot be written in the purely functional fragment of the language. That is, the class of programs with functional behaviour is more expressive than the usual class of pure functional programs. In this paper we introduce this extended class of "functional" programs by means of examples in Standard ML, and explore what they might have to offer to programmers and language implementors.After reviewing some theoretical background, we present some examples of functions of the above kind, and discuss how they may be implemented. We then consider two possible programming applications for these functions: the implementation of a search algorithm, and an algorithm for exact real-number integration. We discuss the advantages and limitations of this style of programming relative to other approaches. We also consider the increased scope for compiler optimizations that these functions would offer.},
  isbn = {1-58113-111-9},
  series = {{{ICFP}} '99}
}

@inproceedings{Lucassen1988Polymorphic,
  title = {Polymorphic Effect Systems},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Lucassen, J. M. and Gifford, D. K.},
  year = {1988},
  pages = {47--57},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/73560.73564},
  abstract = {We present a new approach to programming languages for parallel computers that uses an effect system to discover expression scheduling constraints. This effect system is part of a 'kinded' type system with three base kinds: types, which describe the value that an expression may return; effects, which describe the side-effects that an expression may have; and regions, which describe the area of the store in which side-effects may occur. Types, effects and regions are collectively called descriptions.
Expressions can be abstracted over any kind of description variable -- this permits type, effect and region polymorphism. Unobservable side-effects can be masked by the effect system; an effect soundness property guarantees that the effects computed statically by the effect system are a conservative approximation of the actual side-effects that a given expression may have.
The effect system we describe performs certain kinds of side-effect analysis that were not previously feasible. Experimental data from the programming language FX indicate that an effect system can be used effectively to compile programs for parallel computers.},
  isbn = {978-0-89791-252-5},
  series = {{{POPL}} '88}
}

@inproceedings{Luo1989ECC,
  title = {{{ECC}}, an Extended Calculus of Constructions},
  booktitle = {[1989] {{Proceedings}}. {{Fourth Annual Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Luo, Z.},
  year = {1989},
  month = jun,
  pages = {386--395},
  doi = {10.1109/LICS.1989.39193},
  abstract = {A higher-order calculus ECC (extended calculus of constructions) is presented which can be seen as an extension of the calculus of constructions by adding strong sum types and a fully cumulative type hierarchy. ECC turns out to be rather expressive so that mathematical theories can be abstractly described and abstract mathematics may be adequately formalized. It is shown that ECC is strongly normalizing and has other nice proof-theoretic properties. An {$\omega$}-set (realizability) model is described to show how the essential properties of the calculus can be captured set-theoretically},
  keywords = {Abstract algebra,abstract mathematics,Buildings,Calculus,Computer languages,Computer science,extended calculus of constructions,formal languages,formal logic,fully cumulative type hierarchy,Functional programming,higher-order calculus ECC,Inference algorithms,Mathematical model,Mathematics,proof-theoretic properties,realizability,set theory,strong sum types,strongly normalizing,ω-set}
}

@misc{LuoProblem,
  title = {A {{Problem}} of {{Adequacy}}: Conservativity of Calculus of Constructions over Higher-Order Logic},
  shorttitle = {A {{Problem}} of {{Adequacy}}},
  author = {Luo, Zhaohui},
  abstract = {This paper discusses a problem of adequacy in formalization of mathematical notions in type theories. Based on a point of view that there should be a clear distinction between logical formulae and data types, we believe that, in an impredicative type theory like the calculus of constructions, arbitrary sets should be formalized as non-propositional types rather than propositional types which may be formed impredicatively.
We show that the calculus of constructions with type constants is a conservative extension of the intuitionistic higher-order logic, provided that the object set of the higher-order logic is interpreted as a non-propositional type constant. This result is proved by giving a formulae-as-types formulation of higher-order logic and using a projection technique developed by Berardi and Mohring. Comparing with the non-conservativity result of the (pure) calculus of constructions over higher-order logic [Ber89][Geu89], this provides an evidence to support the above view of adequate formalization and gives a better understanding of the formulae-as-types principle for higher-order logic.
We briefly discuss how abstract mathematics (e.g., abstract algebras) can be adequately formalized in an impredicative type theoy with predicative universes to support abstract reasoning.},
  howpublished = {http://www.lfcs.inf.ed.ac.uk/reports/90/ECS-LFCS-90-121/}
}

@article{Lupei2014incremental,
  title = {Incremental {{View Maintenance}} for {{Nested}}-{{Relational Databases}}},
  author = {Lupei, Daniel and Koch, Christoph and Tannen, Val},
  year = {2014},
  month = dec,
  abstract = {Incremental view maintenance is an essential tool for speeding up the processing of large, locally changing workloads. Its fundamental challenge is to ensure that changes are propagated from input to output more efficiently than via recomputation. We formalize this requirement for positive nested relational algebra (NRA+) on bags and we propose a transformation deriving deltas for any expression in the language. The main difficulty in maintaining nested queries lies in the inability to express within NRA+ the efficient updating of inner bags, i.e., without completely replacing the tuples that contain them. To address this problem, we first show how to efficiently incrementalize IncNRA+, a large fragment of NRA+ whose deltas never generate inner bag updates. We then provide a semantics-preserving transformation that takes any nested query into a collection of IncNRA+ queries. This constitutes the first static solution for the efficient incremental processing of languages with nested collections. Furthermore, we show that the state-of-the-art technique of recursive IVM, originally developed for positive relational algebra with aggregation, also extends to nested queries. Finally, we generalize our static approach for the efficient incrementalization of NRA+ to a family of simply-typed lambda calculi, given that its primitives are themselves efficiently incrementalizable.},
  archivePrefix = {arXiv},
  eprint = {1412.4320},
  eprinttype = {arxiv},
  journal = {arXiv:1412.4320 [cs]},
  keywords = {Computer Science - Databases},
  primaryClass = {cs}
}

@article{Lupei2017SLeNDer,
  title = {{{SLeNDer}}: {{Query Compilation}} for {{Agile Collection Processing}}},
  shorttitle = {{{SLeNDer}}},
  author = {Lupei, Daniel and Nikolic, Milos and Koch, Christoph},
  year = {2017}
}

@inproceedings{Luth2002composing,
  title = {Composing {{Monads Using Coproducts}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {L{\"u}th, Christoph and Ghani, Neil},
  year = {2002},
  pages = {133--144},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/581478.581492},
  abstract = {Monads are a useful abstraction of computation, as they model diverse computational effects such as stateful computations, exceptions and I/O in a uniform manner. Their potential to provide both a modular semantics and a modular programming style was soon recognised. However, in general, monads proved difficult to compose and so research focused on special mechanisms for their composition such as distributive monads and monad transformers.We present a new approach to this problem which is general in that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning monads and computationally expressive in supporting a canonical recursion operator. In a nutshell, we propose that two monads should be composed by taking their coproduct. Although abstractly this is a simple idea, the actual construction of the coproduct of two monads is non-trivial. We outline this construction, show how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We also discuss its relationship with other ways of combining monads, in particular distributive laws for monads and monad transformers.},
  isbn = {1-58113-487-8},
  series = {{{ICFP}} '02}
}

@inproceedings{Ma2007investigating,
  title = {Investigating the {{Viability}} of {{Mental Models Held}} by {{Novice Programmers}}},
  booktitle = {Proceedings of the 38th {{SIGCSE Technical Symposium}} on {{Computer Science Education}}},
  author = {Ma, Linxiao and Ferguson, John and Roper, Marc and Wood, Murray},
  year = {2007},
  pages = {499--503},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1227310.1227481},
  abstract = {This paper describes an investigation into the viability of mental models used by novice programmers at the end of a first year Java programming course. The qualitative findings identify the range of mental models of value and reference assignment held by the participants. The quantitative analysis reveals that approximately one third of students held non-viable mental models of value assignment and only 17\% of students held a viable mental model of reference assignment. Further, in terms of a comparison between the participants' mental models and their performance in in-course assessments and final examination, it was found that students with viable mental models performed significantly better than those with non-viable models. These findings are used to propose a more "constructivist" approach to teaching programming based on the integration of "cognitive conflict" and program visualisation.},
  isbn = {1-59593-361-1},
  keywords = {CS1,mental models,novice,programming},
  series = {{{SIGCSE}} '07}
}

@inproceedings{MacQueen1984ideal,
  title = {An {{Ideal Model}} for {{Recursive Polymorphic Types}}},
  booktitle = {Proceedings of the 11th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {MacQueen, David and Plotkin, Gordon and Sethi, Ravi},
  year = {1984},
  pages = {165--174},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800017.800528},
  isbn = {0-89791-125-3},
  series = {{{POPL}} '84}
}

@article{MacQueen1986ideal,
  title = {An Ideal Model for Recursive Polymorphic Types},
  author = {MacQueen, David and Plotkin, Gordon and Sethi, Ravi},
  year = {1986},
  month = oct,
  volume = {71},
  pages = {95--130},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(86)80019-5},
  journal = {Information and Control},
  number = {1\textendash{}2}
}

@inproceedings{MacQueen1986Using,
  title = {Using {{Dependent Types}} to {{Express Modular Structure}}},
  booktitle = {Proceedings of the 13th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {MacQueen, David B.},
  year = {1986},
  pages = {277--286},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/512644.512670},
  abstract = {Writing any large program poses difficult problems of organization. In many modern programming languages these problems are addressed by special linguistic constructs, variously known as modules, packages, or clusters, which provide for partitioning programs into manageable components and for securely combining these components to form complete programs. Some general purpose components are able to take on a life of their own, being separately compiled and stored in libraries of generic, reusable program units. Usually modularity constructs also support some form of information hiding, such as "abstract data types." "Programming in the large" is concerned with using such constructs to impose structure on large programs, in contrast to "programming in the small", which deals with the detailed implementation of algorithms in terms of data structures and control constructs. Our goal here is to examine some of the proposed linguistic notions with respect to how they meet the pragmatic requirements of programming in the large.},
  series = {{{POPL}} '86}
}

@article{Madni2012elegant,
  title = {Elegant Systems Design: {{Creative}} Fusion of Simplicity and Power},
  shorttitle = {Elegant Systems Design},
  author = {Madni, Azad M.},
  year = {2012},
  month = sep,
  volume = {15},
  pages = {347--354},
  issn = {1520-6858},
  doi = {10.1002/sys.21209},
  abstract = {Elegance is a term frequently associated with aesthetics in design. It typically connotes simplicity, beauty, and grace. When it comes to complex systems, it also connotes predictable behavior, power, and creative functionality. Elegance is what separates the merely functional from the engaging. An elegant design usually has a thematic vision that drives its creation. It engages both designers and users and supports creative exploration on their part. The process of elegant design is an iterative, creative process that exploits systems thinking, probing questioning, and appropriate analogies and metaphors to gain insights that can be transformed novel solutions. This paper provides key insights into elegant systems design and characteristics of elegant systems designers. It offers a heuristics-driven elegant systems design process along with metrics for assessing elegance. \textcopyright{} 2012 Wiley Periodicals, Inc. Syst Eng 15},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc.},
  journal = {Systems Engineering},
  keywords = {complexity,creativity,design thinking,elegant design,stakeholders,systems thinking},
  language = {en},
  number = {3}
}

@inproceedings{Maes1987concepts,
  title = {Concepts and {{Experiments}} in {{Computational Reflection}}},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}}},
  author = {Maes, Pattie},
  year = {1987},
  pages = {147--155},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/38765.38821},
  abstract = {This paper brings some perspective to various concepts in computational reflection. A definition of computational reflection is presented, the importance of computational reflection is discussed and the architecture of languages that support reflection is studied. Further, this paper presents a survey of some experiments in reflection which have been performed. Examples of existing procedural, logic-based and rule-based languages with an architecture for reflection are briefly presented. The main part of the paper describes an original experiment to introduce a reflective architecture in an object-oriented language. It stresses the contributions of this language to the field of object-oriented programming and illustrates the new programming style made possible. The examples show that a lot of programming problems that were previously handled on an ad hoc basis, can in a reflective architecture be solved more elegantly.},
  isbn = {0-89791-247-0},
  series = {{{OOPSLA}} '87}
}

@inproceedings{Maher2005Herbrand,
  title = {Herbrand Constraint Abduction},
  booktitle = {20th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}} ({{LICS}}' 05)},
  author = {Maher, M.},
  year = {2005},
  month = jun,
  pages = {397--406},
  doi = {10.1109/LICS.2005.21},
  abstract = {In this paper we explore abduction over the Herbrand domain - equations on the algebra of finite terms (or finite trees) - which is a central element of logic programming and first-order automated reasoning. This paper is a case study of constraint abduction in the Herbrand domain. The direct relationship between Herbrand constraint abduction and type inference outlined above should make it easy to interpret the results of this paper in the context of type inference.},
  keywords = {Algebra,Australia,automated reasoning,Concrete,Databases,Equations,Herbrand domain,logic programming,Logic programming,process algebra,Query processing,type inference,type theory}
}

@inproceedings{Mainland2013exploiting,
  title = {Exploiting {{Vector Instructions}} with {{Generalized Stream Fusion}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Mainland, Geoffrey and Leshchinskiy, Roman and Peyton Jones, Simon},
  year = {2013},
  pages = {37--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500601},
  abstract = {Stream fusion is a powerful technique for automatically transforming high-level sequence-processing functions into efficient implementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all.  In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together multiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are implemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C.},
  isbn = {978-1-4503-2326-0},
  keywords = {Haskell,simd,stream fusion,vectorization},
  series = {{{ICFP}} '13}
}

@inproceedings{Malecha2016Extensible,
  title = {Extensible and {{Efficient Automation Through Reflective Tactics}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Malecha, Gregory and Bengtson, Jesper},
  year = {2016},
  month = apr,
  pages = {532--559},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-49498-1_21},
  abstract = {Foundational proof assistants simultaneously offer both expressive logics and strong guarantees. The price they pay for this flexibility is often the need to build and check explicit proof objects which can be expensive. In this work we develop a collection of techniques for building reflective automation, where proofs are witnessed by verified decision procedures rather than verbose proof objects. Our techniques center around a verified domain specific language for proving, tacRtac\textbackslash{}mathcal \{R\}\_\{tac\}, written in Gallina, Coq's logic. The design of tactics makes it easy to combine them into higher-level automation that can be proved sound in a mostly automated way. Furthermore, unlike traditional uses of reflection, tacRtac\textbackslash{}mathcal \{R\}\_\{tac\} tactics are independent of the underlying problem domain, which allows them to be re-tasked to automate new problems with very little effort. We demonstrate the usability of tacRtac\textbackslash{}mathcal \{R\}\_\{tac\} through several case studies demonstrating orders of magnitude speedups for relatively little engineering work.},
  isbn = {978-3-662-49497-4 978-3-662-49498-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{Malekievaluation,
  title = {An {{Evaluation}} of {{Vectorizing Compilers}}},
  author = {Maleki, Saeed and Gao, Yaoqing and Garzar{\'a}n, Mar{\'i}a J. and Wong, Tommy and Padua, David A.},
  abstract = {Abstract\textemdash{}Most of today's processors include vector units that have been designed to speedup single threaded programs. Although vector instructions can deliver high performance, writing vector code in assembly language or using intrinsics in high level languages is a time consuming and error-prone task. The alternative is to automate the process of vectorization by using vectorizing compilers. This paper evaluates how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II. We evaluated three compilers:}
}

@article{Maraist1995callbyname,
  title = {Call-by-Name, {{Call}}-by-Value, {{Call}}-by-Need, and the {{Linear Lambda Calculus}}},
  author = {Maraist, John and Odersky, Martin and Turner, David N. and Wadler, Philip},
  year = {1995},
  volume = {1},
  pages = {370--392},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00022-2},
  abstract = {Girard described two translations of intuitionistic logic into linear logic, one where A \textrightarrow{} B maps to (!A) \textendash{$\bigcirc$} B, and another where it maps to !(A \textendash{$\bigcirc$} B). We detail the action of these translations on terms, and show that the first corresponds to a call-by-name calculus, while the second corresponds to call-by-value. We further show that if the target of the translation is taken to be an affine calculus, where ! controls contraction but weakening is allowed everywhere, then the second translation corresponds to a call-by-need calculus, as recently defined by Ariola, Felleisen, Maraist, Odersky and Wadler. Thus the different calling mechanisms can be explained in terms of logical translations, bringing them into the scope of the Curry-Howard isomorphism.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{MFPS XI}}, {{Mathematical Foundations}} of {{Programming Semantics}}, {{Eleventh Annual Conference}}}
}

@article{Maraist1998callbyneed,
  title = {The Call-by-Need Lambda Calculus},
  author = {Maraist, John and Odersky, Martin and Wadler, Philip},
  year = {1998},
  month = may,
  volume = {8},
  pages = {275--317},
  issn = {1469-7653},
  doi = {null},
  abstract = {We present a calculus that captures the operational semantics of call-by-need. The call-by-need lambda calculus is confluent, has a notion of standard reduction, and entails the same observational equivalence relation as the call-by-name calculus. The system can be formulated with or without explicit let bindings, admits useful notions of marking and developments, and has a straightforward operational interpretation.},
  journal = {Journal of Functional Programming},
  number = {03}
}

@inproceedings{Marlow2004Making,
  title = {Making a {{Fast Curry}}: {{Push}}/{{Enter}} vs. {{Eval}}/{{Apply}} for {{Higher}}-Order {{Languages}}},
  shorttitle = {Making a {{Fast Curry}}},
  booktitle = {Proceedings of the {{Ninth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Marlow, Simon and Jones, Simon Peyton},
  year = {2004},
  pages = {4--15},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1016850.1016856},
  abstract = {Higher-order languages that encourage currying are implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other.Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell.Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  isbn = {1-58113-905-5},
  series = {{{ICFP}} '04}
}

@article{Marlow2006Making,
  title = {Making a Fast Curry: Push/Enter vs. Eval/Apply for Higher-Order Languages},
  shorttitle = {Making a Fast Curry},
  author = {Marlow, Simon and Jones, Simon Peyton},
  year = {2006},
  volume = {16},
  pages = {415--449},
  issn = {1469-7653},
  doi = {10.1017/S0956796806005995},
  abstract = {Higher-order languages that encourage currying are typically implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other. Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell. Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  journal = {Journal of Functional Programming},
  number = {4-5}
}

@inproceedings{Marsik2016Introducing,
  title = {Introducing a {{Calculus}} of {{Effects}} and {{Handlers}} for {{Natural Language Semantics}}},
  booktitle = {Formal {{Grammar}} 2016},
  author = {Mar{\v s}{\'i}k, Jirka and Amblard, Maxime},
  year = {2016},
  month = aug,
  volume = {9804},
  pages = {257--272},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-662-53042-9},
  abstract = {In compositional model-theoretic semantics, researchers assemble truth-conditions or other kinds of denotations using the lambda calculus. It was previously observed that the lambda terms and/or the denotations studied tend to follow the same pattern: they are instances of a monad. In this paper, we present an extension of the simply-typed lambda calculus that exploits this uniformity using the recently discovered technique of effect handlers. We prove that our calculus exhibits some of the key formal properties of the lambda calculus and we use it to construct a modular semantics for a small fragment that involves multiple distinct semantic phenomena.},
  language = {en}
}

@article{Mason1991Equivalence,
  title = {Equivalence in Functional Languages with Effects},
  author = {Mason, Ian and Talcott, Carolyn},
  year = {1991},
  month = jul,
  volume = {1},
  pages = {287--327},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796800000125},
  abstract = {AbstractTraditionally the view has been that direct expression of control and store mechanisms and clear mathematical semantics are incompatible requirements. This paper shows that adding objects with memory to the call-by-value lambda calculus results in a language with a rich equational theory, satisfying many of the usual laws. Combined with other recent work, this provides evidence that expressive, mathematically clean programming languages are indeed possible.},
  journal = {Journal of Functional Programming},
  number = {3}
}

@inproceedings{Matthews2008Parametric,
  title = {Parametric {{Polymorphism}} through {{Run}}-{{Time Sealing}} or, {{Theorems}} for {{Low}}, {{Low Prices}}!},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Matthews, Jacob and Ahmed, Amal},
  year = {2008},
  month = mar,
  pages = {16--31},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78739-6_2},
  abstract = {We show how to extend System F's parametricity guarantee to a Matthews-Findler-style multi-language system that combines System F with an untyped language by use of dynamic sealing. While the use of sealing for this purpose has been suggested before, it has never been proven to preserve parametricity. In this paper we prove that it does using step-indexed logical relations. Using this result we show a scheme for implementing parametric higher-order contracts in an untyped setting which corresponds to a translation given by Sumii and Pierce. These contracts satisfy rich enough guarantees that we can extract analogues to Wadler's free theorems that rely on run-time enforcement of dynamic seals.},
  language = {en}
}

@article{McBRIDE2002Faking,
  title = {Faking It {{Simulating}} Dependent Types in {{Haskell}}},
  author = {McBRIDE, Conor},
  year = {2002},
  month = jul,
  volume = {12},
  pages = {375--392},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796802004355},
  abstract = {Dependent types reflect the fact that validity of data is often a relative notion by allowing
prior data to affect the types of subsequent data. Not only does this make for a precise
type system, but also a highly generic one: both the type and the program for each instance
of a family of operations can be computed from the data which codes for that instance.
Recent experimental extensions to the Haskell type class mechanism give us strong tools to
relativize types to other types. We may simulate some aspects of dependent typing by making
counterfeit type-level copies of data, with type constructors simulating data constructors and
type classes simulating datatypes. This paper gives examples of the technique and discusses
its potential.},
  journal = {Journal of Functional Programming},
  number = {4-5}
}

@article{Mcbride2003firstorder,
  title = {First-Order Unification by Structural Recursion},
  author = {Mcbride, Conor},
  year = {2003},
  volume = {13},
  pages = {1061--1075},
  issn = {1469-7653},
  doi = {10.1017/S0956796803004957},
  abstract = {First-order unification algorithms (Robinson, 1965) are traditionally implemented via general recursion, with separate proofs for partial correctness and termination. The latter tends to involve counting the number of unsolved variables and showing that this total decreases each time a substitution enlarges the terms. There are many such proofs in the literature (Manna \& Waldinger, 1981; Paulson, 1985; Coen, 1992; Rouyer, 1992; Jaume, 1997; Bove, 1999). This paper shows how a dependent type can relate terms to the set of variables over which they are constructed. As a consequence, first-order unification becomes a structurally recursive program, and a termination proof is no longer required. Both the program and its correctness proof have been checked using the proof assistant LEGO (Luo \& Pollack, 1992; McBride, 1999).},
  journal = {Journal of Functional Programming},
  number = {06}
}

@inproceedings{McBride2004Functional,
  title = {Functional {{Pearl}}: {{I Am Not}} a {{Number}}\textendash{}i {{Am}} a {{Free Variable}}},
  shorttitle = {Functional {{Pearl}}},
  booktitle = {Proceedings of the 2004 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  author = {McBride, Conor and McKinna, James},
  year = {2004},
  pages = {1--9},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1017472.1017477},
  abstract = {In this paper, we show how to manipulate syntax with binding using a mixed representation of names for free variables (with respect to the task in hand) and de Bruijn indices [5] for bound variables. By doing so, we retain the advantages of both representations: naming supports easy, arithmetic-free manipulation of terms; de Bruijn indices eliminate the need for {$\alpha$}-conversion. Further, we have ensured that not only the user but also the implementation need never deal with de Bruijn indices, except within key basic operations.Moreover, we give a hierarchical representation for names which naturally reflects the structure of the operations we implement. Name choice is safe and straightforward. Our technology combines easily with an approach to syntax manipulation inspired by Huet's 'zippers'[10].Without the ideas in this paper, we would have struggled to implement EPIGRAM [19]. Our example-constructing inductive elimination operators for datatype families-is but one of many where it proves invaluable.},
  isbn = {978-1-58113-850-4},
  keywords = {abstract syntax,bound variables,de Bruijn representation,free variables,fresh names,haskell,implementing epigram,induction principles},
  series = {Haskell '04}
}

@book{Mcbride2005Typepreserving,
  title = {Type-Preserving Renaming and Substitution},
  author = {Mcbride, Conor},
  year = {2005},
  abstract = {I present a substitution algorithm for the simply-typed {$\lambda$}-calculus, represented in the style of Altenkirch and Reus (1999) which is statically guaranteed to respect scope and type. Moreover, I use a single traversal function, instantiated first to renaming, then to substitution. The program is written in Epigram (McBride \& McKinna, 2004). 1}
}

@inproceedings{McBride2008clowns,
  title = {Clowns to the {{Left}} of {{Me}}, {{Jokers}} to the {{Right}} ({{Pearl}}): {{Dissecting Data Structures}}},
  shorttitle = {Clowns to the {{Left}} of {{Me}}, {{Jokers}} to the {{Right}} ({{Pearl}})},
  booktitle = {Proceedings of the 35th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {McBride, Conor},
  year = {2008},
  pages = {287--295},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1328438.1328474},
  abstract = {This paper introduces a small but useful generalisation to the 'derivative' operation on datatypes underlying Huet's notion of 'zipper', giving a concrete representation to one-hole contexts in data which is undergoing transformation. This operator, 'dissection', turns a container-like functor into a bifunctor representing a one-hole context in which elements to the left of the hole are distinguished in type from elements to its right. I present dissection here as a generic program, albeit for polynomial functors only. The notion is certainly applicable more widely, but here I prefer to concentrate on its diverse applications. For a start, map-like operations over the functor and fold-like operations over the recursive data structure it induces can be expressed by tail recursion alone. Further, the derivative is readily recovered from the dissection. Indeed, it is the dissection structure which delivers Huet's operations for navigating zippers. The original motivation for dissection was to define 'division', capturing the notion of leftmost hole, canonically distinguishing values with no elements from those with at least one. Division gives rise to an isomorphism corresponding to the remainder theorem in algebra. By way of a larger example, division and dissection are exploited to give a relatively efficient generic algorithm for abstracting all occurrences of one term from another in a first-order syntax. The source code for the paper is available online and compiles with recent extensions to the Glasgow Haskell Compiler.},
  isbn = {978-1-59593-689-9},
  keywords = {datatype,differentiation,dissection,division,generic programming,iteration,polynomial,stack,tail recursion,traversal,zipper},
  series = {{{POPL}} '08}
}

@incollection{McBride2009lets,
  title = {Let's {{See How Things Unfold}}: {{Reconciling}} the {{Infinite}} with the {{Intensional}} ({{Extended Abstract}})},
  shorttitle = {Let's {{See How Things Unfold}}},
  booktitle = {Algebra and {{Coalgebra}} in {{Computer Science}}},
  author = {McBride, Conor},
  editor = {Kurz, Alexander and Lenisa, Marina and Tarlecki, Andrzej},
  year = {2009},
  pages = {113--126},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Coinductive types model infinite structures unfolded on demand, like politicians' excuses: for each attack, there is a defence but no likelihood of resolution. Representing such evolving processes coinductively is often more attractive than representing them as functions from a set of permitted observations, such as projections or finite approximants, as it can be tricky to ensure that observations are meaningful and consistent. As programmers and reasoners, we need coinductive definitions in our toolbox, equipped with appropriate computational and logical machinery. Lazy functional languages like Haskell [18] exploit call-by-need computation to over-approximate the programming toolkit for coinductive data: in a sense, all data is coinductive and delivered on demand, or not at all if the programmer has failed to ensure the productivity of a program.},
  copyright = {\textcopyright{}2009 Springer Berlin Heidelberg},
  isbn = {978-3-642-03740-5 978-3-642-03741-2},
  keywords = {Computation by Abstract Devices,Discrete Mathematics in Computer Science,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Models and Principles,Symbolic and Algebraic Manipulation},
  language = {en},
  number = {5728},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{McBride2010outrageous,
  title = {Outrageous but {{Meaningful Coincidences}}: {{Dependent Type}}-Safe {{Syntax}} and {{Evaluation}}},
  shorttitle = {Outrageous but {{Meaningful Coincidences}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {McBride, Conor},
  year = {2010},
  pages = {1--12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863495.1863497},
  abstract = {Tagless interpreters for well-typed terms in some object language are a standard example of the power and benefit of precise indexing in types, whether with dependent types, or generalized algebraic datatypes. The key is to reflect object language types as indices (however they may be constituted) for the term datatype in the host language, so that host type coincidence ensures object type coincidence. Whilst this technique is widespread for simply typed object languages, dependent types have proved a tougher nut with nontrivial computation in type equality. In their type-safe representations, Danielsson [2006] and Chapman [2009] succeed in capturing the equality rules, but at the cost of representing equality derivations explicitly within terms. This article constructs a type-safe representation for a dependently typed object language, dubbed KIPLING, whose computational type equality just appropriates that of its host, Agda. The KIPLING interpreter example is not merely de rigeur - it is key to the construction. At the heart of the technique is that key component of generic programming, the universe.},
  isbn = {978-1-4503-0251-7},
  keywords = {dependent types,generic programming},
  series = {{{WGP}} '10}
}

@inproceedings{McBride2015TuringCompleteness,
  title = {Turing-{{Completeness Totally Free}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {McBride, Conor},
  year = {2015},
  month = jun,
  pages = {257--275},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-19797-5_13},
  abstract = {In this paper, I show that general recursive definitions can be represented in the free monad which supports the `effect' of making a recursive call, without saying how these calls should be executed. Diverse semantics can be given within a total framework by suitable monad morphisms. The Bove-Capretta construction of the domain of a general recursive function can be presented datatype-generically as an instance of this technique. The paper is literate Agda, but its key ideas are more broadly transferable.},
  isbn = {978-3-319-19796-8 978-3-319-19797-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{McCarthy1981history,
  title = {History of {{LISP}}},
  booktitle = {History of {{Programming Languages I}}},
  author = {McCarthy, John},
  editor = {Wexelblat, Richard L.},
  year = {1981},
  pages = {173--185},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  isbn = {0-12-745040-8}
}

@inproceedings{McCreight2010certified,
  title = {A {{Certified Framework}} for {{Compiling}} and {{Executing Garbage}}-Collected {{Languages}}},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {McCreight, Andrew and Chevalier, Tim and Tolmach, Andrew},
  year = {2010},
  pages = {273--284},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863543.1863584},
  abstract = {We describe the design, implementation, and use of a machine-certified framework for correct compilation and execution of programs in garbage-collected languages. Our framework extends Leroy's Coq-certified Compcert compiler and Cminor intermediate language. We add: (i) a new intermediate language, GCminor, that includes primitives for allocating memory in a garbage-collected heap and for specifying GC roots; (ii) a precise, low-level specification for a Cminor library for garbage collection; and (iii) a proven semantics-preserving translation from GCminor to Cminor plus the GC library. GCminor neatly encapsulates the interface between mutator and collector code, while remaining simple and flexible enough to be used with a wide variety of source languages and collector styles. Front ends targeting GCminor can be implemented using any compiler technology and any desired degree of verification, including full semantics preservation, type preservation, or informal trust. As an example application of our framework, we describe a compiler for Haskell that translates the Glasgow Haskell Compiler's Core intermediate language to GCminor. To support a simple but useful memory safety argument for this compiler, the front end uses a novel combination of type preservation and runtime checks, which is of independent interest.},
  isbn = {978-1-60558-794-3},
  keywords = {certified compilation,garbage collection,haskell compilation,program proof,the Coq theorem prover},
  series = {{{ICFP}} '10}
}

@inproceedings{McDirmid2011coding,
  title = {Coding at the {{Speed}} of {{Touch}}},
  booktitle = {Proceedings of the 10th {{SIGPLAN Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {McDirmid, Sean},
  year = {2011},
  pages = {61--76},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2048237.2048246},
  abstract = {Although programming is one of the most creative things that one can do with a computer, there is currently no way to make programs on an increasingly popular class of tablet computers. Tablets appear unable to support capable (proficient) programming experiences because of their small form factor and touch-centric input method. This paper demonstrates how co-design of a programming language, YinYang, and its environment can overcome these challenges to enable do-it-yourself game creation on tablets. YinYang's programming model is based on tile and behavior constructs that simplify program structure for effective display and input on tablets, and also supports the definition and safe reuse of new abstractions to be competitive with capable programming languages. This paper details YinYang's design and evaluates our initial experience through a prototype that runs on current tablet hardware.},
  isbn = {978-1-4503-0941-7},
  keywords = {behavior,tiles,touch},
  series = {Onward! 2011}
}

@incollection{McKinna1993pure,
  title = {Pure Type Systems Formalized},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {McKinna, James and Pollack, Robert},
  editor = {Bezem, Marc and Groote, Jan Friso},
  year = {1993},
  month = jan,
  pages = {289--305},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In doing this work of formalizing a well known body of mathematics, we spent a large amount of time solving mathematical problems, e.g. the Thinning Lemma. Another big problem was maintaining and organizing the formal knowledge, e.g. allowing two people to extend different parts of the data base at the same time, and finding the right lemma in the mass of checked material. We feel that better understanding of mathematical issues of formalization (e.g. names/namefree, intentional/extentional), and organization of formal development are the most useful areas to work on now for the long-term goal of formal mathematics. Finally, it is not so easy to understand the relationship between some informal mathematics and a claimed formalization of it. Are you satisfied with our definition of reduction? It might be more satisfying if we also defined de Bruijn terms and their reduction, and proved a correspondence between the two representations, but this only changes the degree of the problem, not its nature. What about the choice between the typing rules Lda and Lda'? There may be no ``right'' answer, as we may have different ideas in mind informally. There is no such thing as certain truth, and formalization does not change this state of affairs.},
  copyright = {\textcopyright{}1993 Springer-Verlag},
  isbn = {978-3-540-56517-8 978-3-540-47586-6},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Techniques},
  language = {en},
  number = {664},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{McLaughlin2008imogen,
  title = {Imogen: {{Focusing}} the {{Polarized Inverse Method}} for {{Intuitionistic Propositional Logic}}},
  shorttitle = {Imogen},
  booktitle = {Logic for {{Programming}}, {{Artificial Intelligence}}, and {{Reasoning}}},
  author = {McLaughlin, Sean and Pfenning, Frank},
  editor = {Cervesato, Iliano and Veith, Helmut and Voronkov, Andrei},
  year = {2008},
  month = jan,
  pages = {174--181},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In this paper we describe Imogen, a theorem prover for intuitionistic propositional logic using the focused inverse method. We represent fine-grained control of the search behavior by polarizing the input formula. In manipulating the polarity of atoms and subformulas, we can often improve the search time by several orders of magnitude. We tested our method against seven other systems on the propositional fragment of the ILTP library. We found that our prover outperforms all other provers on a substantial subset of the library.},
  copyright = {\textcopyright{}2008 Springer Berlin Heidelberg},
  isbn = {978-3-540-89438-4 978-3-540-89439-1},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Techniques,Software Engineering},
  language = {en},
  number = {5330},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Meertens1996calculate,
  title = {Calculate Polytypically!},
  booktitle = {Programming {{Languages}}: {{Implementations}}, {{Logics}}, and {{Programs}}},
  author = {Meertens, Lambert},
  editor = {Kuchen, Herbert and Swierstra, S. Doaitse},
  year = {1996},
  pages = {1--16},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A polytypic function definition is a function definition that is parametrised with a datatype. It embraces a class of algorithms. As an example we define a simple polytypic ``crush'' combinator that can be used to calculate polytypically. The ability to define functions polytypically adds another level of flexibility in the reusability of programming idioms and in the design of libraries of interoperable components.},
  copyright = {\textcopyright{}1996 Springer-Verlag},
  isbn = {978-3-540-61756-3 978-3-540-70654-0},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {1140},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Mellies1995Typed,
  title = {Typed {$\lambda$}-Calculi with Explicit Substitutions May Not Terminate},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Mellies, Paul-Andr{\'e}},
  year = {1995},
  month = apr,
  pages = {328--334},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0014062},
  abstract = {We present a simply typed {$\lambda$}-term whose computation in the {$\lambda\sigma$}-calculus does not always terminate.},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Mellies2005Recursive,
  title = {Recursive Polymorphic Types and Parametricity in an Operational Framework},
  booktitle = {20th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}} ({{LICS}}' 05)},
  author = {Mellies, P. A. and Vouillon, J.},
  year = {2005},
  month = jun,
  pages = {82--91},
  doi = {10.1109/LICS.2005.42},
  abstract = {We construct a realizability model of recursive polymorphic types, starting from an untyped language of terms and contexts. An orthogonality relation e{$\perp\pi$} indicates when a term e and a context {$\pi$} may be safely combined in the language. Types are interpreted as sets of terms closed by biorthogonality. Our main result states that recursive types are approximated by converging sequences of interval types. Our proof is based on a "type-directed" approximation technique, which departs from the "language-directed" approximation technique developed by MacQueen, Plotkin and Sethi in the ideal model. We thus keep the language elementary (a call-by-name {$\lambda$}-calculus) and unstratified (no typecase, no reduction labels). We also include a short account of parametricity, based on an orthogonality relation between quadruples of terms and contexts.},
  keywords = {call-by-name lambda-calculus,Computer science,Context modeling,Equations,H infinity control,lambda calculus,Lattices,Layout,Logic,orthogonality relation,pi calculus,realizability model,recursive functions,recursive polymorphic types,Terminology,type theory,type-directed approximation,untyped language}
}

@article{Mens2002stateoftheart,
  title = {A State-of-the-Art Survey on Software Merging},
  author = {Mens, T.},
  year = {2002},
  month = may,
  volume = {28},
  pages = {449--462},
  issn = {0098-5589},
  doi = {10.1109/TSE.2002.1000449},
  abstract = {Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {configuration management,conflict detection,conflict resolution,large-scale software systems,merge approaches,merging,software life-cycle,software maintenance,software merging,textual merging},
  number = {5}
}

@incollection{Meyer1985continuation,
  title = {Continuation Semantics in Typed Lambda-Calculi},
  booktitle = {Logics of {{Programs}}},
  author = {Meyer, Albert R. and Wand, Mitchell},
  editor = {Parikh, Rohit},
  year = {1985},
  pages = {219--224},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper reports preliminary work on the semantics of the continuation transform. Previous work on the semantics of continuations has concentrated on untyped lambda-calculi and has used primarily the mechanism of inclusive predicates. Such predicates are easy to understand on atomic values, but they become obscure on functional values. In the case of the typed lambda-calculus, we show that such predicates can be replaced by retractions. The main theorem states that the meaning of a closed term is a retraction of the meaning of the corresponding continuationized term.},
  copyright = {\textcopyright{}1985 Springer-Verlag},
  isbn = {978-3-540-15648-2 978-3-540-39527-0},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  language = {en},
  number = {193},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Meyer1986type,
  title = {"{{Type}}" Is {{Not}} a {{Type}}},
  booktitle = {Proceedings of the 13th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Meyer, Albert R. and Reinhold, Mark B.},
  year = {1986},
  pages = {287--295},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/512644.512671},
  abstract = {A function has a \emph{dependent type} when the type of its result depends upon the value of its argument. Dependent types originated in the type theory of intuitionistic mathematics and have reappeared independently in programming languages such as CLU, Pebble, and Russell. Some of these languages make the assumption that there exists a \emph{type-of-all-types} which is its own type as well as the type of all other types. Girard proved that this approach is inconsistent from the perspective of intuitionistic logic. We apply Girard's techniques to establish that the type-of-all-types assumption creates serious pathologies from a programming perspective: a system using this assumption is inherently not normalizing, term equality is undecidable, and the resulting theory fails to be a conservative extension of the theory of the underlying base types. The failure of conservative extension means that classical reasoning about programs in such a system is not sound.},
  series = {{{POPL}} '86}
}

@inproceedings{Meyerovich2009flapjax,
  title = {Flapjax: {{A Programming Language}} for {{Ajax Applications}}},
  shorttitle = {Flapjax},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Meyerovich, Leo A. and Guha, Arjun and Baskin, Jacob and Cooper, Gregory H. and Greenberg, Michael and Bromfield, Aleks and Krishnamurthi, Shriram},
  year = {2009},
  pages = {1--20},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1640089.1640091},
  abstract = {This paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces. Flapjax provides two key features that simplify writing these applications. First, it provides event streams, a uniform abstraction for communication within a program as well as with external Web services. Second, the language itself is reactive: it automatically tracks data dependencies and propagates updates along those dataflows. This allows developers to write reactive interfaces in a declarative and compositional style. Flapjax is built on top of JavaScript. It runs on unmodified browsers and readily interoperates with existing JavaScript code. It is usable as either a programming language (that is compiled to JavaScript) or as a JavaScript library, and is designed for both uses. This paper presents the language, its design decisions, and illustrative examples drawn from several working Flapjax applications.},
  isbn = {978-1-60558-766-0},
  keywords = {functional reactive programming,javascript,web programming},
  series = {{{OOPSLA}} '09}
}

@inproceedings{Mezini2003conquering,
  title = {Conquering {{Aspects}} with {{Caesar}}},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Aspect}}-Oriented {{Software Development}}},
  author = {Mezini, Mira and Ostermann, Klaus},
  year = {2003},
  pages = {90--99},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/643603.643613},
  abstract = {Join point interception (JPI), is considered an important cornerstone of aspect-oriented languages. However, we claim that JPI alone does not suffice for a modular structuring of aspects. We propose CAESAR, a model for aspect-oriented programming with a higher-level module concept on top of JPI, which enables reuse and componentization of aspects, allows us to use aspects polymorphically, and introduces a novel concept for dynamic aspect deployment.},
  isbn = {1-58113-660-9},
  series = {{{AOSD}} '03}
}

@article{Michie1968memo,
  title = {``{{Memo}}'' {{Functions}} and {{Machine Learning}}},
  author = {Michie, Donald},
  year = {1968},
  month = apr,
  volume = {218},
  pages = {19--22},
  doi = {10.1038/218019a0},
  abstract = {It would be useful if computers could learn from experience and thus automatically improve the efficiency of their own programs during execution. A simple but effective rote-learning facility can be provided within the framework of a suitable programming language.},
  copyright = {\textcopyright{} 1968 Nature Publishing Group},
  journal = {Nature},
  keywords = {read},
  language = {en},
  number = {5136}
}

@inproceedings{Miller2013instant,
  title = {Instant {{Pickles}}: {{Generating Object}}-Oriented {{Pickler Combinators}} for {{Fast}} and {{Extensible Serialization}}},
  shorttitle = {Instant {{Pickles}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN International Conference}} on {{Object Oriented Programming Systems Languages}} \&\#38; {{Applications}}},
  author = {Miller, Heather and Haller, Philipp and Burmako, Eugene and Odersky, Martin},
  year = {2013},
  pages = {183--202},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2509136.2509547},
  abstract = {As more applications migrate to the cloud, and as "big data" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often under-considered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to object-oriented programming due to qualities like open class hierarchies and subtyping polymorphism. Furthermore, both functional pickler combinators and popular, Java-based serialization frameworks tend to be tied to a specific pickle format, leaving programmers with no choice of how their data is persisted. In this paper, we present object-oriented pickler combinators and a framework for generating them at compile-time, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In addition to high performance and the need for little to no boilerplate, our framework is extensible: using the type class pattern, users can provide both (1) custom, easily interchangeable pickle formats and (2) custom picklers, to override the default behavior of the pickling framework. In benchmarks, we compare scala/pickling with other popular industrial frameworks, and present results on time, memory usage, and size when pickling/unpickling a number of data types used in real-world, large-scale distributed applications and frameworks.},
  isbn = {978-1-4503-2374-1},
  keywords = {distributed programming,meta-programming,pickling,Scala,serialization},
  series = {{{OOPSLA}} '13}
}

@article{Miller2014selfassembly,
  title = {Self-{{Assembly}}: {{Lightweight Language Extension}} and {{Datatype Generic Programming}}, {{All}}-in-{{One}}!},
  shorttitle = {Self-{{Assembly}}},
  author = {Miller, Heather and Haller, Philipp and Oliveira, d S. and Bruno, C.},
  year = {2014},
  keywords = {datatype generic programming,generative programming,language extension,meta-programming}
}

@inproceedings{Miller2014spores,
  title = {Spores: {{A Type}}-{{Based Foundation}} for {{Closures}} in the {{Age}} of {{Concurrency}} and {{Distribution}}},
  shorttitle = {Spores},
  booktitle = {European {{Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}}"14)},
  author = {Miller, Heather and Haller, Philipp and Odersky, Martin},
  year = {2014},
  keywords = {closures,concurrent programming,distributed programming,functions,type systems}
}

@inproceedings{Millstein2002modular,
  title = {Modular {{Typechecking}} for {{Hierarchically Extensible Datatypes}} and {{Functions}}},
  booktitle = {Proceedings of the {{Seventh ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Millstein, Todd and Bleckner, Colin and Chambers, Craig},
  year = {2002},
  pages = {110--122},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/581478.581489},
  abstract = {One promising approach for adding object-oriented (OO) facilities to functional languages like ML is to generalize the existing datatype and function constructs to be hierarchical and extensible, so that datatype variants simulate classes and function cases simulate methods. This approach allows existing datatypes to be easily extended with both new operations and new variants, resolving a long-standing conflict between the functional and OO styles. However, previous designs based on this approach have been forced to give up modular typechecking, requiring whole-program checks to ensure type safety. We describe Extensible ML (eml), an ML-like language that supports hierarchical, extensible datatypes and functions while preserving purely modular typechecking. To achieve this result, eml's type system imposes a few requirements on datatype and function extensibility, but eml is still able to express both traditional functional and OO idioms. We have formalized a core version of eml and proven the associated type system sound, and we have developed a prototype interpreter for the language.},
  isbn = {1-58113-487-8},
  keywords = {extensible datatypes,extensible functions,modular typechecking},
  series = {{{ICFP}} '02}
}

@article{Millstein2009Expressive,
  title = {Expressive and Modular Predicate Dispatch for {{Java}}},
  author = {Millstein, Todd and Frost, Christopher and Ryder, Jason and Warth, Alessandro},
  year = {2009},
  month = feb,
  volume = {31},
  pages = {7:1--7:54},
  issn = {0164-0925},
  doi = {10.1145/1462166.1462168},
  abstract = {Predicate dispatch is an object-oriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functional-style pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit the practical utility of this language feature. We describe JPred, a backward-compatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included special-purpose algorithms for reasoning about predicates, JPred employs general-purpose, off-the-shelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language informally, we present an extension to Featherweight Java that formalizes the language and its modular type system, which we have proven sound. Finally, we discuss two case studies that illustrate the practical utility of JPred, including its use in the detection of several errors.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {dynamic dispatch,modular typechecking,Predicate dispatch},
  number = {2}
}

@inproceedings{Minamide1996typed,
  title = {Typed {{Closure Conversion}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Minamide, Yasuhiko and Morrisett, Greg and Harper, Robert},
  year = {1996},
  pages = {271--283},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/237721.237791},
  abstract = {Closure conversion is a program transformation used by compilers to separate code from data. Previous accounts of closure conversion use only untyped target languages. Recent studies show that translating to typed target languages is a useful methodology for building compilers, because a compiler can use the types to implement efficient data representations, calling conventions, and tag-free garbage collection. Furthermore, type-based translations facilitate security and debugging through automatic type checking, as well as correctness arguments through the method of logical relations.We present closure conversion as a type-directed, and type-preserving translation for both the simply-typed and the polymorphic \textquestiondown{}-calculus. Our translations are based on a simple "closures as objects" principle: higher-order functions are viewed as objects consisting of a single method (the code) and a single instance variable (the environment). In the simply-typed case, the Pierce-Turner model of object typing where objects are packages of existential type suffices. In the polymorphic case, more careful tracking of type sharing is required. We exploit a variant of the Harper-Lillibridge "translucent type" formalism to characterize the types of polymorphic closures.},
  isbn = {0-89791-769-3},
  series = {{{POPL}} '96}
}

@article{Miranda-Perea2009selective,
  title = {Selective {{Memoization}} with {{Box Types}}},
  author = {{Miranda-Perea}, Favio Ezequiel and {Gonz{\'a}lez-Huesca}, Lourdes Del Carmen},
  year = {2009},
  month = dec,
  volume = {256},
  pages = {67--85},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2009.11.006},
  abstract = {Memoization is a useful technique to eliminate computational redundancy. A memo function remembers all the arguments to which it has been applied, together with their corresponding results, by storing them in a table. This table is consulted before each functional call to determine if the particular argument is in it. If so, the call is skipped and the stored result is returned; otherwise the call is performed and its result added to the table. Acar, Belloch and Harper present a framework to apply memoization selectively, that is, enabling the programmer to determine precisely the dependences between the input and the result of a function. This framework is efficient and yields programs whose performance can be analyzed using standard techniques. The language, implemented as an SML library, is based on a modal type system which allows the programmer to reveal the true data input/output dependences in a program. However, the modality seems to be an ad-hoc choice for the implementation. In this paper we develop selective memoization, using instead box types, corresponding to the necessitation modality {$\square$}. We also include non-memoized functions, and provide full proofs of type safeness and soundness of the dynamic semantics with respect to an effect-free system which is later translated into the very well-known language PCF.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {adaptive computation,box types,functional programming,modal types,selective memoization,type safeness},
  series = {Proceedings of the {{Fourth Workshop}} on {{Logical}} and {{Semantic Frameworks}}, with {{Applications}} ({{LSFA}} 2009)}
}

@incollection{Mishra-Linger2008erasure,
  title = {Erasure and {{Polymorphism}} in {{Pure Type Systems}}},
  booktitle = {Foundations of {{Software Science}} and {{Computational Structures}}},
  author = {{Mishra-Linger}, Nathan and Sheard, Tim},
  editor = {Amadio, Roberto},
  year = {2008},
  pages = {350--364},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We introduce Erasure Pure Type Systems, an extension to Pure Type Systems with an erasure semantics centered around a type constructor {$\forall$} indicating parametric polymorphism. The erasure phase is guided by lightweight program annotations. The typing rules guarantee that well-typed programs obey a phase distinction between erasable (compile-time) and non-erasable (run-time) terms. The erasability of an expression depends only on how its value is used in the rest of the program. Despite this simple observation, most languages treat erasability as an intrinsic property of expressions, leading to code duplication problems. Our approach overcomes this deficiency by treating erasability extrinsically. Because the execution model of EPTS generalizes the familiar notions of type erasure and parametric polymorphism, we believe functional programmers will find it quite natural to program in such a setting.},
  copyright = {\textcopyright{}2008 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-78497-5 978-3-540-78499-9},
  keywords = {Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {4962},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Mitchell1985Secondorder,
  title = {Second-Order Logical Relations},
  booktitle = {Logics of {{Programs}}},
  author = {Mitchell, John C. and Meyer, Albert R.},
  year = {1985},
  month = jun,
  pages = {225--236},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-15648-8_18},
  abstract = {Logical relations are a generalization of homomorphisms between models of typed lambda calculus. We define logical relations for second-order typed lambda calculus and use these relations to give a semantic characterization of second-order lambda definability. Logical relations are also used to state and prove a general representation independence theorem. Representation independence implies that the meanings of expressions do not depend on whether true is represented by 1 and false by 0, as long as all the functions that manipulate truth values are represented correctly.},
  language = {en}
}

@inproceedings{Mitchell1986Typeinference,
  title = {A {{Type}}-Inference {{Approach}} to {{Reduction Properties}} and {{Semantics}} of {{Polymorphic Expressions}} ({{Summary}})},
  booktitle = {Proceedings of the 1986 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  author = {Mitchell, John C.},
  year = {1986},
  pages = {308--319},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/319838.319872},
  isbn = {0-89791-200-4},
  series = {{{LFP}} '86}
}

@article{Mitchell1991kripkestyle,
  title = {Kripke-Style Models for Typed Lambda Calculus},
  author = {Mitchell, John C. and Moggi, Eugenio},
  year = {1991},
  month = mar,
  volume = {51},
  pages = {99--124},
  issn = {0168-0072},
  doi = {10.1016/0168-0072(91)90067-V},
  abstract = {Mitchell, J.C. and E. Moggi, Kripke-style models for typed lambda calculus, Annals of Pure and Applied Logic 51 (1991) 99\textendash{}124.

The semantics of typed lambda calculus is usually described using Henkin models, consisting of functions over some collection of sets, or concrete cartesian closed categories, which are essentially equivalent. We describe a more general class of Kripke-style models. In categorical terms, our Kripke lambda models are cartesian closed subcategories of the presheaves over a poset. To those familiar with Kripke models of modal or intuitionistic logics, Kripke lambda models are likely to seem adequately `semantic'. However, when viewed as cartesian closed categories, they do not have the property variously referred to as concreteness, well- pointedness or having enough points. While the traditional lambda calculus proof system is not complete for Henkin models that may have empty types, we prove strong completeness for Kripke models. In fact, every set of equations that is closed under implication is the theory of a single Kripke model. We also develop some properties of logical relations over Kripke structures, showing that every theory is the theory of a model determined by a Kripke equivalence relation over a Henkin model. We discuss cartesian closed categories but present the main definitions and results without the use of category theory.},
  journal = {Annals of Pure and Applied Logic},
  number = {1\textendash{}2}
}

@inproceedings{Mitchell2009Losing,
  title = {Losing {{Functions Without Gaining Data}}: {{Another Look}} at {{Defunctionalisation}}},
  shorttitle = {Losing {{Functions Without Gaining Data}}},
  booktitle = {Proceedings of the {{2Nd ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Mitchell, Neil and Runciman, Colin},
  year = {2009},
  pages = {13--24},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596638.1596641},
  abstract = {We describe a transformation which takes a higher-order program, and produces an equivalent first-order program. Unlike Reynolds-style defunctionalisation, it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. We can use our method to improve the results of existing analysis operations, including strictness analysis, pattern-match safety and termination checking. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method cannot always succeed in removing all functional values, but in practice is remarkably successful.},
  isbn = {978-1-60558-508-6},
  keywords = {defunctionalisation,firstification,Haskell},
  series = {Haskell '09}
}

@inproceedings{Mitschke2014i3ql,
  title = {{{I3QL}}: {{Language}}-Integrated {{Live Data Views}}},
  shorttitle = {{{I3QL}}},
  booktitle = {Proceedings of the 2014 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} \&\#38; {{Applications}}},
  author = {Mitschke, Ralf and Erdweg, Sebastian and K{\"o}hler, Mirko and Mezini, Mira and Salvaneschi, Guido},
  year = {2014},
  pages = {417--432},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2660193.2660242},
  abstract = {An incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) real-time systems. This paper presents i3QL, a general-purpose programming language for specifying incremental computations. i3QL provides a declarative SQL-like syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for non-incremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes user-defined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.},
  isbn = {978-1-4503-2585-1},
  keywords = {incremental computation,reactive programming,Scala},
  series = {{{OOPSLA}} '14}
}

@phdthesis{Mitschke2014scalable,
  title = {Scalable {{Automated Incrementalization}} for {{Real}}-{{Time Static Analyses}}},
  author = {Mitschke, Ralf},
  year = {2014},
  month = may,
  abstract = {This thesis proposes a framework for easy development of static analyses, whose results are incrementalized to provide instantaneous feedback in an integrated development environment (IDE).

Today, IDEs feature many tools that have static analyses as their foundation to assess software quality and catch correctness problems. 
Yet, these tools often fail to provide instantaneous feedback and are thus restricted to nightly build processes. This precludes developers from fixing issues at their inception time, i.e., when the problem and the developed solution are both still fresh in mind.

In order to provide instantaneous feedback, incrementalization is a well-known technique that utilizes the fact that developers make only small changes to the code and, hence, analysis results can be re-computed fast based on these changes. Yet, incrementalization requires carefully crafted static analyses. Thus, a manual approach to incrementalization is unattractive. Automated incrementalization can alleviate these problems and allows analyses writers to formulate their analyses as queries with the full data set in mind, without worrying over the semantics of incremental changes.

Existing approaches to automated incrementalization utilize standard technologies, such as deductive databases, that provide declarative query languages, yet also require to materialize the full dataset in main-memory, i.e., the memory is permanently blocked by the data required for the analyses. Other standard technologies such as relational databases offer better scalability due to persistence, yet require large transaction times for data. Both technologies are not a perfect match for integrating static analyses into an IDE, since the underlying data, i.e., the code base, is already persisted and managed by the IDE. Hence, transitioning the data into a database is redundant work.

In this thesis a novel approach is proposed that provides a declarative query language and automated incrementalization, yet retains in memory only a necessary minimum of data, i.e., only the data that is required for the incrementalization. The approach allows to declare static analyses as incrementally maintained views, where the underlying formalism for incrementalization is the relational algebra with extensions for object-orientation and recursion. The algebra allows to deduce which data is the necessary minimum for incremental maintenance and indeed shows that many views are self-maintainable, i.e., do not require to materialize memory at all. In addition an optimization for the algebra is proposed that allows to widen the range of self-maintainable views, based on domain knowledge of the underlying data. The optimization works similar to declaring primary keys for databases, i.e., the optimization is declared on the schema of the data, and defines which data is incrementally maintained in the same scope. The scope makes all analyses (views) that correlate only data within the boundaries of the scope self-maintainable.

The approach is implemented as an embedded domain specific language in a general-purpose programming language. The implementation can be understood as a database-like engine with an SQL-style query language and the execution semantics of the relational algebra. As such the system is a general purpose database-like query engine and can be used to incrementalize other domains than static analyses. To evaluate the approach a large variety of static analyses were sampled from real-world tools and formulated as incrementally maintained views in the implemented engine.},
  school = {TU Darmstadt},
  type = {Ph.{{D}}. {{Thesis}}}
}

@article{Mogelberg2009relational,
  title = {Relational Parametricity for Computational Effects},
  author = {M{\o}gelberg, Rasmus and Simpson, Alex},
  editor = {Tennent, Robert},
  year = {2009},
  month = aug,
  volume = {5},
  issn = {18605974},
  doi = {10.2168/LMCS-5(3:7)2009},
  journal = {Logical Methods in Computer Science},
  keywords = {_tablet},
  language = {en},
  number = {3}
}

@article{Mogelberg2018Bisimulation,
  title = {Bisimulation as Path Type for Guarded Recursive Types},
  author = {M{\o}gelberg, Rasmus Ejlers and Veltri, Niccol{\`o}},
  year = {2018},
  month = oct,
  abstract = {In type theory, coinductive types are used to represent processes, and are thus crucial for the formal verification of non-terminating reactive programs in proof assistants based on type theory, such as Coq and Agda. Currently, programming and reasoning about coinductive types is difficult for two reasons: The need for recursive definitions to be productive, and the lack of coincidence of the built-in identity types and the important notion of bisimilarity. Guarded recursion in the sense of Nakano has recently been suggested as a possible approach to dealing with the problem of productivity, allowing this to be encoded in types. Indeed, coinductive types can be encoded using a combination of guarded recursion and universal quantification over clocks. This paper studies the notion of bisimilarity for guarded recursive types in Ticked Cubical Type Theory, an extension of Cubical Type Theory with guarded recursion. We prove that, for any functor, an abstract, category theoretic notion of bisimilarity for the final guarded coalgebra is equivalent (in the sense of homotopy type theory) to path equality (the primitive notion of equality in cubical type theory). As a worked example we study a guarded notion of labelled transition systems, and show that, as a special case of the general theorem, path equality coincides with an adaptation of the usual notion of bisimulation for processes. In particular, this implies that guarded recursion can be used to give simple equational reasoning proofs of bisimilarity. This work should be seen as a step towards obtaining bisimilarity as path equality for coinductive types using the encodings mentioned above.},
  archivePrefix = {arXiv},
  eprint = {1810.13261},
  eprinttype = {arxiv},
  journal = {arXiv:1810.13261 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@article{Mogelberg2018Denotational,
  title = {Denotational Semantics of Recursive Types in Synthetic Guarded Domain Theory},
  author = {M{\o}gelberg, Rasmus E. and Paviotti, Marco},
  year = {2018},
  month = may,
  abstract = {Just like any other branch of mathematics, denotational semantics of programming languages should be formalised in type theory, but adapting traditional domain theoretic semantics, as originally formulated in classical set theory to type theory has proven challenging. This paper is part of a project on formulating denotational semantics in type theories with guarded recursion. This should have the benefit of not only giving simpler semantics and proofs of properties such as adequacy, but also hopefully in the future to scale to languages with advanced features, such as general references, outside the reach of traditional domain theoretic techniques. Working in Guarded Dependent Type Theory (GDTT), we develop denotational semantics for FPC, the simply typed lambda calculus extended with recursive types, modelling the recursive types of FPC using the guarded recursive types of GDTT. We prove soundness and computational adequacy of the model in GDTT using a logical relation between syntax and semantics constructed also using guarded recursive types. The denotational semantics is intensional in the sense that it counts the number of unfold-fold reductions needed to compute the value of a term, but we construct a relation relating the denotations of extensionally equal terms, i.e., pairs of terms that compute the same value in a different number of steps. Finally we show how the denotational semantics of terms can be executed inside type theory and prove that executing the denotation of a boolean term computes the same value as the operational semantics of FPC.},
  archivePrefix = {arXiv},
  eprint = {1805.00289},
  eprinttype = {arxiv},
  journal = {arXiv:1805.00289 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Moggi1989computational,
  title = {Computational Lambda-Calculus and Monads},
  booktitle = {, {{Fourth Annual Symposium}} on {{Logic}} in {{Computer Science}}, 1989. {{LICS}} '89, {{Proceedings}}},
  author = {Moggi, E.},
  year = {1989},
  month = jun,
  pages = {14--23},
  doi = {10.1109/LICS.1989.39155},
  abstract = {The {$\lambda$}-calculus is considered a useful mathematical tool in the study of programming languages. However, if one uses {$\beta\eta$}-conversion to prove equivalence of programs, then a gross simplification is introduced. The author gives a calculus based on a categorical semantics for computations, which provides a correct basis for proving equivalence of programs, independent from any specific computational model},
  keywords = {Calculus,categorical semantics,computational lambda-calculus,Computer languages,Computer science,Contracts,equivalence of programs,formal languages,formal logic,Logic programming,Mathematical model,Mathematical programming,mathematical tool,Monads,programming languages,prove,Reasoning about programs,βη-conversion,λ-Calculus}
}

@article{Moggi1991notions,
  title = {Notions of Computation and Monads},
  author = {Moggi, Eugenio},
  year = {1991},
  month = jul,
  volume = {93},
  pages = {55--92},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90052-4},
  abstract = {The {$\lambda$}-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with {$\lambda$}-terms. However, if one goes further and uses {$\beta\eta$}-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations, that provide a correct basis for proving equivalence of programs for a wide range of notions of computation.},
  journal = {Information and Computation},
  number = {1},
  series = {Selections from 1989 {{IEEE Symposium}} on {{Logic}} in {{Computer Science}}}
}

@article{Moggi1999monads,
  title = {Monads, {{Shapely Functors}} and {{Traversals}}},
  author = {Moggi, E. and Bell{\`e}, G. and Jay, C. B.},
  year = {1999},
  volume = {29},
  pages = {187--208},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(05)80316-0},
  abstract = {This paper demonstrates the potential for combining the polytypic and monadic programming styles, by introducing a new kind of combinator, called a traversal. The natural setting for defining traversals is the class of shapely data types. This result reinforces the view that shapely data types form a natural domain for polytypism: they include most of the data types of interest, while to exceed them would sacrifice a very smooth interaction between polytypic and monadic programming.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {functional/monadic/polytypic programming,shape theory},
  series = {{{CTCS}} '99, {{Conference}} on {{Category Theory}} and {{Computer Science}}}
}

@inproceedings{Monin2010Proof,
  title = {Proof {{Trick}}: {{Small Inversions}}},
  shorttitle = {Proof {{Trick}}},
  booktitle = {Second {{Coq Workshop}}},
  author = {Monin, Jean-Fran{\c c}ois},
  editor = {Bertot, Yves},
  year = {2010},
  month = jul,
  publisher = {{Yves Bertot}},
  address = {{Edinburgh, United Kingdom}},
  abstract = {We show how an inductive hypothesis can be inverted with small proof terms, using just dependent elimination with a diagonal predicate. The technique works without any auxiliary type such as True, False, eq. It can also be used to discriminate, in some sense, the constructors of an inductive type of sort Prop in Coq.}
}

@inproceedings{Montagu2009Modeling,
  title = {Modeling {{Abstract Types}} in {{Modules}} with {{Open Existential Types}}},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Montagu, Beno{\^i}t and R{\'e}my, Didier},
  year = {2009},
  pages = {354--365},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1480881.1480926},
  abstract = {We propose F-zip, a calculus of open existential types that is an extension of System F obtained by decomposing the introduction and elimination of existential types into more atomic constructs. Open existential types model modular type abstraction as done in module systems. The static semantics of F-zip adapts standard techniques to deal with linearity of typing contexts, its dynamic semantics is a small-step reduction semantics that performs extrusion of type abstraction as needed during reduction, and the two are related by subject reduction and progress lemmas. Applying the Curry-Howard isomorphism, F-zip can be also read back as a logic with the same expressive power as second-order logic but with more modular ways of assembling partial proofs. We also extend the core calculus to handle the double vision problem as well as type-level and term-level recursion. The resulting language turns out to be a new formalization of (a minor variant of) Dreyer's internal language for recursive and mixin modules.},
  isbn = {978-1-60558-379-2},
  keywords = {abstract types,existential types,generativity,lambda-calculus,linear type systems,modularity,modules,type systems},
  series = {{{POPL}} '09}
}

@incollection{Morihata2016incremental,
  title = {Incremental {{Computing}} with {{Abstract Data Structures}}},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Morihata, Akimasa},
  editor = {Kiselyov, Oleg and King, Andy},
  year = {2016},
  month = mar,
  pages = {215--231},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-29604-3_14},
  abstract = {Incremental computing is a method of keeping consistency between an input and an output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for abstract data structures such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an incremental computing method, which can deal with complex modifications and therefore is suitable for abstract data structures. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring's incremental computing method, which can deal with algebraic data structures, so as to deal with abstract data structures. Our method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  copyright = {\textcopyright{}2016 Springer International Publishing Switzerland},
  isbn = {978-3-319-29603-6 978-3-319-29604-3},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {9613},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{MorihataIncremental,
  title = {Incremental Computing with Data Structures},
  author = {Morihata, Akimasa},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2017.04.001},
  abstract = {Incremental computing is a method of maintaining consistency between an input and output. If only a small portion of the input is modified, it is natural to expect that the corresponding output can be obtained more efficiently than full re-computation. However, for nontrivial data structures, such as self-balancing binary search trees, even the most primitive modifications may lead to drastic change of the underlying structure. In this paper, we develop an method of incremental computing on data structures that may consist of complex modifications. The key idea is to use shortcut fusion in order to decompose a complex modification to a series of simple ones. Based on this idea, we extend Jeuring's incremental computing method on algebraic data structures to one on more complex data structures. The method is purely functional and does not rely on any run-time support. Its correctness is straightforward from parametricity. Moreover, its cost is often proportional to that of the corresponding modification.},
  journal = {Science of Computer Programming},
  keywords = {Data Structures,Datatype-generic programming,Incremental computing,Shortcut fusion}
}

@inproceedings{Morris1973types,
  title = {Types {{Are Not Sets}}},
  booktitle = {Proceedings of the 1st {{Annual ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Morris, Jr., James H.},
  year = {1973},
  pages = {120--124},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/512927.512938},
  abstract = {The title is not a statement of fact, of course, but an opinion about how language designers should think about types. There has been a natural tendency to look to mathematics for a consistent, precise notion of what types are. The point of view there is extensional: a type is a subset of the universe of values. While this approach may have served its purpose quite adequately in mathematics, defining programming language types in this way ignores some vital ideas. Some interesting developments following the extensional approach are the ALGOL-68 type system [vW], Scott's theory [S], and Reynolds' system [R]. While each of these lend valuable insight to programming languages, I feel they miss an important aspect of types.Rather than worry about what types are I shall focus on the role of type checking. Type checking seems to serve two distinct purposes: authentication and secrecy. Both are useful when a programmer undertakes to implement a class of abstract objects to be used by many other programmers. He usually proceeds by choosing a representation for the objects in terms of other objects and then writes the required operations to manipulate them.},
  series = {{{POPL}} '73}
}

@inproceedings{Morris2016Best,
  title = {The {{Best}} of {{Both Worlds}}: {{Linear Functional Programming Without Compromise}}},
  shorttitle = {The {{Best}} of {{Both Worlds}}},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Morris, J. Garrett},
  year = {2016},
  pages = {448--461},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2951913.2951925},
  abstract = {We present a linear functional calculus with both the safety guarantees expressible with linear types and the rich language of combinators and composition provided by functional programming. Unlike previous combinations of linear typing and functional programming, we compromise neither the linear side (for example, our linear values are first-class citizens of the language) nor the functional side (for example, we do not require duplicate definitions of compositions for linear and unrestricted functions). To do so, we must generalize abstraction and application to encompass both linear and unrestricted functions. We capture the typing of the generalized constructs with a novel use of qualified types. Our system maintains the metatheoretic properties of the theory of qualified types, including principal types and decidable type inference. Finally, we give a formal basis for our claims of expressiveness, by showing that evaluation respects linearity, and that our language is a conservative extension of existing functional calculi.},
  isbn = {978-1-4503-4219-3},
  keywords = {linear types,qualified types,substructural types},
  series = {{{ICFP}} 2016}
}

@article{Morris2017Constrained,
  title = {Constrained {{Type Families}}},
  author = {Morris, J. Garrett and Eisenberg, Richard A.},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {42:1--42:28},
  issn = {2475-1421},
  doi = {10.1145/3110286},
  abstract = {We present an approach to support partiality in type-level computation without compromising expressiveness or type safety. Existing frameworks for type-level computation either require totality or implicitly assume it. For example, type families in Haskell provide a powerful, modular means of defining type-level computation. However, their current design implicitly assumes that type families are total, introducing nonsensical types and significantly complicating the metatheory of type families and their extensions. We propose an alternative design, using qualified types to pair type-level computations with predicates that capture their domains. Our approach naturally captures the intuitive partiality of type families, simplifying their metatheory. As evidence, we present the first complete proof of consistency for a language with closed type families.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Haskell,type classes,type families,type-level computation},
  number = {ICFP}
}

@article{Morrisett1998typed,
  title = {Typed {{Closure Conversion}} for {{Recursively}}-{{Defined Functions}}: {{Extended Abstract}}},
  shorttitle = {Typed {{Closure Conversion}} for {{Recursively}}-{{Defined Functions}}},
  author = {Morrisett, Greg and Harper, Robert},
  year = {1998},
  volume = {10},
  pages = {230--241},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(05)80702-9},
  abstract = {Much recent work on the compilation of statically typed languages such as ML relies on the propagation of type information from source to object code in order to increase the reliability and maintainabilty of the compiler itself and to improve the efficiency and verifiability of generated code. To achieve this the program transformations performed by a compiler must be cast as type-preserving translations between typed intermediate languages. In earlier work with Minamide we studied one important compiler transformation, closure conversion, for the case of pure simply-typed and polymorphic {$\lambda$}-calculus. Here we extend the treatment of simply-typed closure conversion to account for recursively-defined functions such as are found in ML. We consider three main approaches, one based on a recursive code construct, one based on a self-referential data structure, and one based on recursive types. We discuss their relative advantages and disadvantages, and sketch correctness proofs for these transformations based on the method of logical relations.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{HOOTS II}}, {{Second Workshop}} on {{Higher}}-{{Order Operational Techniques}} in {{Semantics}}}
}

@inproceedings{Mulligan2014lem,
  title = {Lem: {{Reusable Engineering}} of {{Real}}-World {{Semantics}}},
  shorttitle = {Lem},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Mulligan, Dominic P. and Owens, Scott and Gray, Kathryn E. and Ridge, Tom and Sewell, Peter},
  year = {2014},
  pages = {175--188},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628143},
  abstract = {Recent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of real-world processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many person-years effort and maintained over an extended period, cannot be used by those familiar with another. We introduce Lem, a language for engineering reusable large-scale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations - akin to compilation, but subject to the constraint of producing usable and human-readable code for each target. Lem's effectiveness is demonstrated by its use in practice.},
  isbn = {978-1-4503-2873-9},
  keywords = {lem,proof assistants,real-world semantics,specification languages},
  series = {{{ICFP}} '14}
}

@article{Muralidharan2016Designing,
  title = {Designing a {{Tunable Nested Data}}-{{Parallel Programming System}}},
  author = {Muralidharan, Saurav and Garland, Michael and Sidelnik, Albert and Hall, Mary},
  year = {2016},
  month = dec,
  volume = {13},
  pages = {47:1--47:24},
  issn = {1544-3566},
  doi = {10.1145/3012011},
  abstract = {This article describes Surge, a nested data-parallel programming system designed to simplify the porting and tuning of parallel applications to multiple target architectures. Surge decouples high-level specification of computations, expressed using a C++ programming interface, from low-level implementation details using two first-class constructs: schedules and policies. Schedules describe the valid ways in which data-parallel operators may be implemented, while policies encapsulate a set of parameters that govern platform-specific code generation. These two mechanisms are used to implement a code generation system that analyzes computations and automatically generates a search space of valid platform-specific implementations. An input and architecture-adaptive autotuning system then explores this search space to find optimized implementations. We express in Surge five real-world benchmarks from domains such as machine learning and sparse linear algebra and from the high-level specifications, Surge automatically generates CPU and GPU implementations that perform on par with or better than manually optimized versions.},
  journal = {ACM Trans. Archit. Code Optim.},
  keywords = {autotuning,nested data parallelism,performance portability},
  number = {4}
}

@inproceedings{Murray2011steno,
  title = {Steno: {{Automatic Optimization}} of {{Declarative Queries}}},
  shorttitle = {Steno},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Murray, Derek Gordon and Isard, Michael and Yu, Yuan},
  year = {2011},
  pages = {121--131},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1993498.1993513},
  abstract = {Declarative queries enable programmers to write data manipulation code without being aware of the underlying data structure implementation. By increasing the level of abstraction over imperative code, they improve program readability and, crucially, create opportunities for automatic parallelization and optimization. For example, the Language Integrated Query (LINQ) extensions to C\# allow the same declarative query to process in-memory collections, and datasets that are distributed across a compute cluster. However, our experiments show that the serial performance of declarative code is several times slower than the equivalent hand-optimized code, because it is implemented using run-time abstractions---such as iterators---that incur overhead due to virtual function calls and superfluous instructions. To address this problem, we have developed Steno, which uses a combination of novel and well-known techniques to generate code for declarative queries that is almost as efficient as hand-optimized code. Steno translates a declarative LINQ query into type-specialized, inlined and loop-based imperative code. It eliminates chains of iterators from query execution, and optimizes nested queries. We have implemented Steno for uniprocessor, multiprocessor and distributed computing platforms, and show that, for a real-world distributed job, it can almost double the speed of end-to-end execution.},
  isbn = {978-1-4503-0663-8},
  keywords = {abstract machines,query optimization},
  series = {{{PLDI}} '11}
}

@inproceedings{Mycroft1984polymorphic,
  title = {Polymorphic {{Type Schemes}} and {{Recursive Definitions}}},
  booktitle = {Proceedings of the 6th {{Colloquium}} on {{International Symposium}} on {{Programming}}},
  author = {Mycroft, Alan},
  year = {1984},
  pages = {217--228},
  publisher = {{Springer-Verlag}},
  address = {{London, UK, UK}},
  isbn = {3-540-12925-1}
}

@article{Najd2016Embedding,
  title = {Embedding by {{Normalisation}}},
  author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
  year = {2016},
  month = mar,
  abstract = {This paper presents the insight that practical embedding techniques, commonly used for implementing Domain-Specific Languages, correspond to theoretical Normalisation-By-Evaluation (NBE) techniques, commonly used for deriving canonical form of terms with respect to an equational theory. NBE constitutes of four components: a syntactic domain, a semantic domain, and a pair of translations between the two. Embedding also often constitutes of four components: an object language, a host language, encoding of object terms in the host, and extraction of object code from the host. The correspondence is deep in that all four components in embedding and NBE correspond to each other. Based on this correspondence, this paper introduces Embedding-By-Normalisation (EBN) as a principled approach to study and structure embedding. The correspondence is useful in that solutions from NBE can be borrowed to solve problems in embedding. In particular, based on NBE techniques, such as Type-Directed Partial Evaluation, this paper presents a solution to the problem of extracting object code from embedded programs involving sum types, such as conditional expressions, and primitives, such as literals and operations on them.},
  archivePrefix = {arXiv},
  eprint = {1603.05197},
  eprinttype = {arxiv},
  journal = {arXiv:1603.05197 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Najd2016Everything,
  title = {Everything {{Old}} Is {{New Again}}: {{Quoted Domain}}-Specific {{Languages}}},
  shorttitle = {Everything {{Old}} Is {{New Again}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
  year = {2016},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2847538.2847541},
  abstract = {We describe a new approach to implementing Domain-Specific Languages(DSLs), called Quoted DSLs (QDSLs), that is inspired by two old ideas:quasi-quotation, from McCarthy's Lisp of 1960, and the subformula principle of normal proofs, from Gentzen's natural deduction of 1935. QDSLs reuse facilities provided for the host language, since host and quoted terms share the same syntax, type system, and normalisation rules. QDSL terms are normalised to a canonical form, inspired by the subformula principle, which guarantees that one can use higher-order types in the source while guaranteeing first-order types in the target, and enables using types to guide fusion. We test our ideas by re-implementing Feldspar, which was originally implemented as an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants. The two variants produce identical code.},
  isbn = {978-1-4503-4097-7},
  keywords = {domain-specific language,DSL,EDSL,embedded language,normalisation,QDSL,quotation,subformula principle},
  series = {{{PEPM}} 2016}
}

@article{Najd2017Trees,
  title = {Trees That {{Grow}}},
  author = {Najd, Shayan and Peyton Jones, Simon},
  year = {2017},
  abstract = {We study the notion of extensibility in functional data types, as a new approach to the problem of decorating abstract syntax trees with additional information. We observed the need for such extensibility while redesigning the data types representing Haskell abstract syntax inside Glasgow Haskell Compiler (GHC).

Specifically, we describe a programming idiom that exploits type-level functions to allow a particular form of extensibility. The approach scales to support existentials and generalised algebraic data types, and we can use pattern synonyms to make it convenient in practice.}
}

@inproceedings{Nakata2006Recursive,
  title = {Recursive {{Modules}} for {{Programming}}},
  booktitle = {Proceedings of the {{Eleventh ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Nakata, Keiko and Garrigue, Jacques},
  year = {2006},
  pages = {74--86},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1159803.1159813},
  abstract = {TheML module system is useful for building large-scale programs. The programmer can factor programs into nested and parameterized modules, and can control abstraction with signatures. Yet ML prohibits recursion between modules. As a result of this constraint, the programmer may have to consolidate conceptually separate components into a single module, intruding on modular programming. Introducing recursive modules is a natural way out of this predicament. Existing proposals, however, vary in expressiveness and verbosity. In this paper, we propose a type system for recursive modules, which can infer their signatures. Opaque signatures can also be given explicitly, to provide type abstraction either inside or outside the recursion. The type system is decidable, and is sound for a call-by-value semantics. We also present a solution to the expression problem, in support of our design choices.},
  isbn = {978-1-59593-309-6},
  keywords = {applicative functors,recursive modules,the expression problem,type inference,type systems},
  series = {{{ICFP}} '06}
}

@inproceedings{Narasimhan2017Interactive,
  title = {Interactive {{Data Representation Migration}}: {{Exploiting Program Dependence}} to {{Aid Program Transformation}}},
  shorttitle = {Interactive {{Data Representation Migration}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Narasimhan, Krishna and Reichenbach, Christoph and Lawall, Julia},
  year = {2017},
  pages = {47--58},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3018882.3018890},
  abstract = {Data representation migration is a program transformation that involves changing the type of a particular data structure, and then updating all of the operations that somehow depend on that data structure according to the new type. Changing the data representation can provide benefits such as improving efficiency and improving the quality of the computed results. Performing such a transformation is challenging, because it requires applying data-type specific changes to code fragments that may be widely scattered throughout the source code, connected by dataflow dependencies. Refactoring systems are typically sensitive to dataflow dependencies, but are not programmable with respect to the features of particular data types. Existing program transformation languages provide the needed flexibility, but do not concisely support reasoning about dataflow dependencies. To address the needs of data representation migration, we propose a new approach to program transformation that relies on a notion of semantic dependency: every transformation step propagates the transformation process onward to code that somehow depends on the transformed code. Our approach provides a declarative transformation-specification language, for expressing typespecific transformation rules. We further provide scoped rules, a mechanism for guiding rule application, and tags, a device for simple program analysis within our framework, to enable more powerful program transformations. We have implemented a prototype transformation system based on these ideas for C and C++ code and evaluate it against three example specifications, including vectorization, transformation of integers to big integers, and transformation of array-of-structures data types to structure-of-arrays format. Our evaluation shows that our approach can improve program performance and the precision of the computed results, and that it scales to programs of up to 3700 lines.},
  isbn = {978-1-4503-4721-1},
  keywords = {DSL,program transformation,static analysis},
  series = {{{PEPM}} 2017}
}

@inproceedings{Neis2015pilsner,
  title = {Pilsner: {{A Compositionally Verified Compiler}} for a {{Higher}}-Order {{Imperative Language}}},
  shorttitle = {Pilsner},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Neis, Georg and Hur, Chung-Kil and Kaiser, Jan-Oliver and McLaughlin, Craig and Dreyer, Derek and Vafeiadis, Viktor},
  year = {2015},
  pages = {166--178},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784764},
  abstract = {Compiler verification is essential for the construction of fully verified software, but most prior work (such as CompCert) has focused on verifying whole-program compilers. To support separate compilation and to enable linking of results from different verified compilers, it is important to develop a compositional notion of compiler correctness that is modular (preserved under linking), transitive (supports multi-pass compilation), and flexible (applicable to compilers that use different intermediate languages or employ non-standard program transformations). In this paper, building on prior work of Hur et al., we develop a novel approach to compositional compiler verification based on parametric inter-language simulations (PILS). PILS are modular: they enable compiler verification in a manner that supports separate compilation. PILS are transitive: we use them to verify Pilsner, a simple (but non-trivial) multi-pass optimizing compiler (programmed in Coq) from an ML-like source language S to an assembly-like target language T, going through a CPS-based intermediate language. Pilsner is the first multi-pass compiler for a higher-order imperative language to be compositionally verified. Lastly, PILS are flexible: we use them to additionally verify (1) Zwickel, a direct non-optimizing compiler for S, and (2) a hand-coded self-modifying T module, proven correct w.r.t. an S-level specification. The output of Zwickel and the self-modifying T module can then be safely linked together with the output of Pilsner. All together, this has been a significant undertaking, involving several person-years of work and over 55,000 lines of Coq.},
  isbn = {978-1-4503-3669-7},
  keywords = {abstract types,Compositional compiler verification,higher-order state,parametric simulations,recursive types,transitivity},
  series = {{{ICFP}} 2015}
}

@article{Nguyen2015higherorder,
  title = {Higher-Order Symbolic Execution for Contract Verification and Refutation},
  author = {Nguyen, Phuc C. and {Tobin-Hochstadt}, Sam and Van Horn, David},
  year = {2015},
  month = jul,
  abstract = {We present a new approach to automated reasoning about higher-order programs by endowing symbolic execution with a notion of higher-order, symbolic values. Our approach is sound and relatively complete with respect to a first-order solver for base type values. Therefore, it can form the basis of automated verification and bug-finding tools for higher-order programs. To validate our approach, we use it to develop and evaluate a system for verifying and refuting behavioral software contracts of components in a functional language, which we call soft contract verification. In doing so, we discover a mutually beneficial relation between behavioral contracts and higher-order symbolic execution. Our system uses higher-order symbolic execution, leveraging contracts as a source of symbolic values including unknown behavioral values, and employs an updatable heap of contract invariants to reason about flow-sensitive facts. Whenever a contract is refuted, it reports a concrete counterexample reproducing the error, which may involve solving for an unknown function. The approach is able to analyze first-class contracts, recursive data structures, unknown functions, and control-flow-sensitive refinements of values, which are all idiomatic in dynamic languages. It makes effective use of an off-the-shelf solver to decide problems without heavy encodings. The approach is competitive with a wide range of existing tools---including type systems, flow analyzers, and model checkers---on their own benchmarks. We have built a tool which analyzes programs written in Racket, and report on its effectiveness in verifying and refuting contracts.},
  archivePrefix = {arXiv},
  eprint = {1507.04817},
  eprinttype = {arxiv},
  journal = {arXiv:1507.04817 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Nieto2017Algorithmic,
  title = {Towards {{Algorithmic Typing}} for {{DOT}} ({{Short Paper}})},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Nieto, Abel},
  year = {2017},
  pages = {2--7},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3136000.3136003},
  abstract = {The Dependent Object Types (DOT) calculus formalizes key features of Scala. The D{$<$}: calculus is the core of DOT. To date, presentations of D{$<$}: have used declarative, as opposed to algorithmic, typing and subtyping rules. Unfortunately, algorithmic typing for full D{$<$}: is known to be an undecidable problem. We explore the design space for a restricted version of D{$<$}: that has decidable typechecking. Even in this simplified D{$<$}:, algorithmic typing and subtyping are tricky, due to the {\^a}bad bounds{\^a} problem. The Scala compiler bypasses bad bounds at the cost of a loss in expressiveness in its type system. Based on the approach taken in the Scala compiler, we present the Step Typing and Step Subtyping relations for D{$<$}:. These relations are sound and decidable. They are not complete with respect to the original D{$<$}: typing rules.},
  isbn = {978-1-4503-5529-2},
  keywords = {algorithmic typing,dependent object types,DOT calculus,Scala},
  note = {00000},
  series = {{{SCALA}} 2017}
}

@article{Nikolic2014LINVIEW,
  title = {{{LINVIEW}}: {{Incremental View Maintenance}} for {{Complex Analytical Queries}}},
  shorttitle = {{{LINVIEW}}},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  year = {2014},
  month = mar,
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1403.6968},
  eprinttype = {arxiv},
  journal = {arXiv:1403.6968 [cs]},
  keywords = {Computer Science - Databases,Computer Science - Numerical Analysis},
  primaryClass = {cs}
}

@inproceedings{Nikolic2014linview,
  title = {{{LINVIEW}}: {{Incremental View Maintenance}} for {{Complex Analytical Queries}}},
  shorttitle = {{{LINVIEW}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  year = {2014},
  pages = {253--264},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2588555.2610519},
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  isbn = {978-1-4503-2376-5},
  keywords = {Compilation,Incremental view maintenance,linear algebra,machine learning,spark},
  series = {{{SIGMOD}} '14}
}

@inproceedings{Nipkow1998Javalight,
  title = {Javalight Is {{Type}}-Safe\textemdash{{Definitely}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Nipkow, Tobias and {von Oheimb}, David},
  year = {1998},
  pages = {161--170},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/268946.268960},
  abstract = {Javalight is a large sequential sublanguage of Java. We formalize its abstract syntax, type system, well-formedness conditions, and an operational evaluation semantics. Based on this formalization, we can express and prove type soundness. All definitions and proofs have been done formally in the theorem prover Isabelle/HOL. Thus this paper demonstrates that machine-checking the design of non-trivial programming languages has become a reality.},
  isbn = {978-0-89791-979-1},
  series = {{{POPL}} '98}
}

@phdthesis{Norell2007practical,
  title = {Towards a Practical Programming Language Based on Dependent Type Theory},
  author = {Norell, Ulf},
  year = {2007},
  month = sep,
  address = {{SE-412 96 G{\"o}teborg, Sweden}},
  abstract = {Dependent type theories have a long history of being used for theorem proving. One aspect of type theory which makes it very powerful as a proof language is that it mixes deduction with computation. This also makes type theory a good candidate for programming---the strength of the type system allows properties of programs to be stated and established, and the computational properties provide semantics for the programs.

This thesis is concerned with bridging the gap between the theoretical presentations of type theory and the requirements on a practical programming language. Although there are many challenging research problems left to solve before we have an industrial scale programming language based on type theory, this thesis takes us a good step along the way.

In functional programming languages pattern matching provides a concise notation for defining functions. In dependent type theory, pattern matching becomes even more powerful, in that inspecting the value of a particular term can reveal information about the types and values of other terms. In this thesis we give a type checking algorithm for definitions by pattern matching in type theory, supporting overlapping patterns, and pattern matching on intermediate results using the with rule.

Traditional presentations of type theory suffers from rather verbose notation, cluttering programs and proofs with, for instance, explicit type information. One solution to this problem is to allow terms that can be inferred automatically to be omitted. This is usually implemented by inserting metavariables in place of the omitted terms and using unification to solve these metavariables during type checking. We present a type checking algorithm for a theory with metavariables and prove its soundness independent of whether the metavariables are solved or not.

In any programming language it is important to be able to structure large programs into separate units or modules and limit the interaction between these modules. In this thesis we present a simple, but powerful module system for a dependently typed language. The main focus of the module system is to manage the name space of a program, and an important characteristic is a clear separation between the module system and the type checker, making it largely independent of the underlying language.

As a side track, not directly related to the use of type theory for programming, we present a connection between type theory and a first-order logic theorem prover. This connection saves the user the burden of proving simple, but tedious first-order theorems by leaving them for the prover. We use a transparent translation to first-order logic which makes the proofs constructed by the theorem prover human readable. The soundness of the connection is established by a general metatheorem.

Finally we put our work into practise in the implementation of a programming language, Agda, based on type theory. As an illustrating example we show how to program a simple certfied prover for equations in a commutative monoid, which can be used internally in Agda. Much more impressive examples have been done by others, showing that the ideas developed in this thesis are viable in practise.},
  school = {Department of Computer Science and Engineering, Chalmers University of Technology}
}

@article{Nuyts2017Parametric,
  title = {Parametric {{Quantifiers}} for {{Dependent Type Theory}}},
  author = {Nuyts, Andreas and Vezzosi, Andrea and Devriese, Dominique},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {32:1--32:29},
  issn = {2475-1421},
  doi = {10.1145/3110276},
  abstract = {Polymorphic type systems such as System F enjoy the parametricity property: polymorphic functions cannot inspect their type argument and will therefore apply the same algorithm to any type they are instantiated on. This idea is formalized mathematically in Reynolds{\^a}s theory of relational parametricity, which allows the metatheoretical derivation of parametricity theorems about all values of a given type. Although predicative System F embeds into dependent type systems such as Martin-L{\~A}\textparagraph{}f Type Theory (MLTT), parametricity does not carry over as easily. The identity extension lemma, which is crucial if we want to prove theorems involving equality, has only been shown to hold for small types, excluding the universe. We attribute this to the fact that MLTT uses a single type former {\^I} to generalize both the parametric quantifier {\^a} and the type former {\^a} which is non-parametric in the sense that its elements may use their argument as a value. We equip MLTT with parametric quantifiers {\^a} and {\^a} alongside the existing {\^I} and {\^I}\textsterling, and provide relation type formers for proving parametricity theorems internally. We show internally the existence of initial algebras and final co-algebras of indexed functors both by Church encoding and, for a large class of functors, by using sized types. We prove soundness of our type system by enhancing existing iterated reflexive graph (cubical set) models of dependently typed parametricity by distinguishing between edges that express relatedness of objects (bridges) and edges that express equality (paths). The parametric functions are those that map bridges to paths. We implement an extension to the Agda proof assistant that type-checks proofs in our type system.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Agda,cubical type theory,parametricity,presheaf semantics,sized types},
  number = {ICFP}
}

@incollection{OConnor2005essential,
  title = {Essential {{Incompleteness}} of {{Arithmetic Verified}} by {{Coq}}},
  booktitle = {Theorem {{Proving}} in {{Higher Order Logics}}},
  author = {O'Connor, Russell},
  editor = {Hurd, Joe and Melham, Tom},
  year = {2005},
  month = jan,
  pages = {245--260},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A constructive proof of the G{\"o}del-Rosser incompleteness theorem [9] has been completed using Coq proof assistant. Some theory of classical first-order logic over an arbitrary language is formalized. A development of primitive recursive functions is given, and all primitive recursive functions are proved to be representable in a weak axiom system. Formulas and proofs are encoded as natural numbers, and functions operating on these codes are proved to be primitive recursive. The weak axiom system is proved to be essentially incomplete. In particular, Peano arithmetic is proved to be consistent in Coq's type theory and therefore is incomplete.},
  copyright = {\textcopyright{}2005 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-28372-0 978-3-540-31820-0},
  keywords = {Artificial Intelligence (incl. Robotics),Logic Design,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Software Engineering},
  language = {en},
  number = {3603},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{OConnor2011functor,
  title = {Functor Is to {{Lens}} as {{Applicative}} Is to {{Biplate}}: {{Introducing Multiplate}}},
  shorttitle = {Functor Is to {{Lens}} as {{Applicative}} Is to {{Biplate}}},
  author = {O'Connor, Russell},
  year = {2011},
  month = mar,
  abstract = {This paper gives two new categorical characterisations of lenses: one as a coalgebra of the store comonad, and the other as a monoidal natural transformation on a category of a certain class of coalgebras. The store comonad of the first characterisation can be generalized to a Cartesian store comonad, and the coalgebras of this Cartesian store comonad turn out to be exactly the Biplates of the Uniplate generic programming library. On the other hand, the monoidal natural transformations on functors can be generalized to work on a category of more specific coalgebras. This generalization turns out to be the type of compos from the Compos generic programming library. A theorem, originally conjectured by van Laarhoven, proves that these two generalizations are isomorphic, thus the core data types of the Uniplate and Compos libraries supporting generic program on single recursive types are the same. Both the Uniplate and Compos libraries generalize this core functionality to support mutually recursive types in different ways. This paper proposes a third extension to support mutually recursive data types that is as powerful as Compos and as easy to use as Uniplate. This proposal, called Multiplate, only requires rank 3 polymorphism in addition to the normal type class mechanism of Haskell.},
  archivePrefix = {arXiv},
  eprint = {1103.2841},
  eprinttype = {arxiv},
  journal = {arXiv:1103.2841 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Odersky1997pizza,
  title = {Pizza into {{Java}}: {{Translating Theory}} into {{Practice}}},
  shorttitle = {Pizza into {{Java}}},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Odersky, Martin and Wadler, Philip},
  year = {1997},
  pages = {146--159},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/263699.263715},
  isbn = {0-89791-853-3},
  series = {{{POPL}} '97}
}

@article{Odersky1999Type,
  title = {Type {{Inference}} with {{Constrained Types}}},
  author = {Odersky, Martin and Sulzmann, Martin and Wehr, Martin},
  year = {1999},
  month = jan,
  volume = {5},
  pages = {35--55},
  issn = {1074-3227},
  doi = {10.1002/(SICI)1096-9942(199901/03)5:1<35::AID-TAPO4>3.0.CO;2-4},
  journal = {Theor. Pract. Object Syst.},
  number = {1}
}

@article{Odersky2002Nominal,
  title = {A Nominal Theory of Objects with Dependent Types},
  author = {Odersky, Martin and Cremet, Vincent and R{\"o}ckl, Christine and Zenger, Matthias},
  year = {2002},
  abstract = {We design and study newObj, a calculus and dependent type system for objects and classes which can have types as members. Type members can be aliases, abstract types, or new types. The type system can model the essential concepts of Java's inner classes as well as virtual types and family polymorphism found in BETA or gbeta. It can also model most concepts of SML-style module systems, including sharing constraints and higher-order functors, but excluding applicative functors. The type system can thus be used as a basis for unifying concepts that so far existed in parallel in advanced object systems and in module systems. The technical report presents results on confluence of the calculus, soundness of the type system, and undecidability of type checking. Odersky, Martin; Cremet, Vincent; R{\"o}ckl, Christine; Zenger, Matthias},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{Odersky2003Nominal,
  title = {A Nominal Theory of Objects with Dependent Types},
  booktitle = {{{ECOOP}} 2003 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Odersky, Martin and Cremet, Vincent and R{\"o}ckl, Christine and Zenger, Matthias},
  year = {2003},
  month = jul,
  pages = {201--224},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45070-2_10},
  abstract = {We design and study vObj, a calculus and dependent type system for objects and classes which can have types as members. Type members can be aliases, abstract types, or new types. The type system can model the essential concepts of JAVA's inner classes as well as virtual types and family polymorphism found in BETA or GBETA. It can also model most concepts of SML-style module systems, including sharing constraints and higher-order functors, but excluding applicative functors. The type system can thus be used as a basis for unifying concepts that so far existed in parallel in advanced object systems and in module systems. The paper presents results on confluence of the calculus, soundness of the type system, and undecidability of type checking.},
  isbn = {978-3-540-40531-3 978-3-540-45070-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Odersky2016Implementing,
  title = {Implementing Higher-Kinded Types in {{Dotty}}},
  booktitle = {Proceedings of the 2016 7th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Odersky, Martin and Martres, Guillaume and Petrashko, Dmitry},
  year = {2016},
  month = oct,
  pages = {51--60},
  publisher = {{ACM}},
  doi = {10.1145/2998392.2998400},
  isbn = {978-1-4503-4648-1}
}

@article{Odersky2017Foundations,
  title = {Foundations of {{Implicit Function Types}}},
  author = {Odersky, Martin and Biboudis, Aggelos and Liu, Fengyun and Blanvillain, Olivier},
  year = {2017}
}

@article{Odersky2017Simplicitly,
  title = {Simplicitly: {{Foundations}} and {{Applications}} of {{Implicit Function Types}}},
  shorttitle = {Simplicitly},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {42:1--42:29},
  issn = {2475-1421},
  doi = {10.1145/3158130},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Dotty,implicit parameters,Scala},
  number = {POPL}
}

@article{Odersky2017Simplicitlya,
  title = {Simplicitly},
  author = {Odersky, Martin and Biboudis, Aggelos and Liu, Fengyun and Blanvillain, Olivier and Miller, Heather},
  year = {2017},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over. This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler. To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.}
}

@inproceedings{Ofenbeck2013Spiral,
  title = {Spiral in {{Scala}}: {{Towards}} the {{Systematic Construction}} of {{Generators}} for {{Performance Libraries}}},
  shorttitle = {Spiral in {{Scala}}},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Generative Programming}}: {{Concepts}} \& {{Experiences}}},
  author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P{\"u}schel, Markus},
  year = {2013},
  pages = {125--134},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2517208.2517228},
  abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.},
  isbn = {978-1-4503-2373-4},
  keywords = {abstraction over staging,data representation,scalar replacement,selective precomputation,synthesis},
  series = {{{GPCE}} '13}
}

@article{OHearn1995parametricity,
  title = {Parametricity and {{Local Variables}}},
  author = {O'Hearn, P. W. and Tennent, R. D.},
  year = {1995},
  month = may,
  volume = {42},
  pages = {658--709},
  issn = {0004-5411},
  doi = {10.1145/210346.210425},
  abstract = {We propose that the phenomenon of local state may be understood in terms of Strachey's concept of parametric (i.e., uniform) polymorphism. The intuitive basis for our proposal is the following analogy: a non-local procedure is independent of locally-declared variables in the same way that a parametrically polymorphic function is independent of types to which it is instantiated.A connection between parametricity and representational abstraction was first suggested by J.C. Reynolds. Reynolds used logical relations to formalize this connection in languages with type variables and user-defined types. We use relational parametricity to construct a model for an Algol-like language in which interactions between local and non-local entities satisfy certain relational criteria.  Reasoning about local variables essentially involved proving properties of polymorphic functions. The new model supports straightforward validations of all the test equivalences that have been proposed in the literature for local-variable semantics, and encompasses standard methods of reasoning about data representations. It is not known whether our techniques yield fully abstract semantics. A model based on partial equivalence relations on the natural numbers is also briefly examined.},
  journal = {J. ACM},
  keywords = {algol-like languages,local state,logical relations,parametric polymorphism},
  number = {3}
}

@article{OHearn1995Syntactic,
  title = {Syntactic {{Control}} of {{Interference Revisited}}},
  author = {O'Hearn, P. W. and Power, A. J. and Takeyama, M. and Tennent, R. D.},
  year = {1995},
  month = jan,
  volume = {1},
  pages = {447--486},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00026-X},
  abstract = {In ``Syntactic Control of Interference'' (POPL, 1978), J. C. Reynolds proposes three design principles intended to constrain the scope of imperative state effects in Algol-like languages. The resulting linguistic framework seems to be a very satisfactory way of combining functional and imperative concepts, having the desirable attributes of both purely functional languages (such as PCF) and simple imperative languages (such as the language of while programs). However, Reynolds points out that an ``obvious'' syntax for interference control has the unfortunate property that {$\beta$}-reductions do not always preserve typings. Reynolds has subsequently presented a solution to this problem (ICALP, 1989), but it is fairly complicated and requires intersection types in the type system. Here, we present a much simpler solution which does not require intersection types. We first describe a new type system inspired in part by linear logic and verify that reductions preserve typings. We then define a class of ``bireflective'' models, which are shown to provide a sound interpretation of the type system; a companion paper, ``Bireflectivity'', in this volume provides a categorical analysis of these models. Finally, we describe a concrete model for an illustrative programming language based on the new type system; this improves on earlier such efforts in that states are not assumed to be structured using locations.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{MFPS XI}}, {{Mathematical Foundations}} of {{Programming Semantics}}, {{Eleventh Annual Conference}}}
}

@article{OHearn1999syntactic,
  title = {Syntactic Control of Interference Revisited},
  author = {O'Hearn, P. W. and Power, A. J. and Takeyama, M. and Tennent, R. D.},
  year = {1999},
  month = oct,
  volume = {228},
  pages = {211--252},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(98)00359-4},
  abstract = {In ``syntactic control of interference'' (POPL, 1978), J.C. Reynolds proposes three design principles intended to constrain the scope of imperative state effects in Algol-like languages. The resulting linguistic framework seems to be a very satisfactory way of combining functional and imperative concepts, having the desirable attributes of both purely functional languages (such as PCF) and simple imperative languages (such as the language of while programs). However, Reynolds points out that the ``obvious'' syntax for interference control has the unfortunate property that {$\beta$}-reductions do not always preserve typings. Reynolds has subsequently presented a solution to this problem (ICALP, 1989), but it is fairly complicated and requires intersection types in the type system. Here, we present a much simpler solution which does not require intersection types. We first describe a new type system inspired in part by linear logic and verify that reductions preserve typings. We then define a class of ``bireflective'' models, which provide a categorical analysis of structure underlying the new typing rules; a companion paper ``Bireflectivity'', in this volume, exposes wider ramifications of this structure. Finally, we describe a concrete model for an illustrative programming language based on the new type system; this improves on earlier such efforts in that states are not assumed to be structured using locations.},
  journal = {Theoretical Computer Science},
  keywords = {Aliasing,Denotational semantics,Interference,type systems},
  number = {1\textendash{}2}
}

@article{OHearn2000algol,
  title = {From {{Algol}} to {{Polymorphic Linear Lambda}}-Calculus},
  author = {O'Hearn, Peter W. and Reynolds, John C.},
  year = {2000},
  month = jan,
  volume = {47},
  pages = {167--223},
  issn = {0004-5411},
  doi = {10.1145/331605.331611},
  abstract = {In a linearly-typed functional language, one can define functions that consume their arguments in the process of computing their results. This is reminiscent of state transformations in imperative languages, where execition of an assignment statement alters the contents of the store. We explore this connection by translating two variations on Algol 60 into a purely functional language with polymorphic linear types. On the one hand, the translations lead to a semantic analysis of Algol-like programs, in terms of a model of the linear language. On the other hand, they demonstrate that a linearly-typed functional language can be at least as expressive as Algol.},
  journal = {J. ACM},
  keywords = {linear logic,local state,logical relations,parametric polymorphism},
  number = {1}
}

@incollection{OHearn2001local,
  title = {Local {{Reasoning}} about {{Programs}} That {{Alter Data Structures}}},
  booktitle = {Computer {{Science Logic}}},
  author = {O'Hearn, Peter and Reynolds, John and Yang, Hongseok},
  editor = {Fribourg, Laurent},
  year = {2001},
  month = jan,
  pages = {1--19},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We describe an extension of Hoare's logic for reasoning about programs that alter data structures. We consider a low-level storage model based on a heap with associated lookup, update, allocation and deallocation operations, and unrestricted address arithmetic. The assertion language is based on a possible worlds model of the logic of bunched implications, and includes spatial conjunction and implication connectives alongside those of classical logic. Heap operations are axiomatized using what we call the ``small axioms'', each of which mentions only those cells accessed by a particular command. Through these and a number of examples we show that the formalism supports local reasoning: A specification and proof can concentrate on only those cells in memory that a program accesses. This paper builds on earlier work by Burstall, Reynolds, Ishtiaq and O'Hearn on reasoning about data structures.},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-42554-0 978-3-540-44802-0},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  language = {en},
  number = {2142},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{OHearn2004Separation,
  title = {Separation and {{Information Hiding}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {O'Hearn, Peter W. and Yang, Hongseok and Reynolds, John C.},
  year = {2004},
  pages = {268--280},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/964001.964024},
  abstract = {We investigate proof rules for information hiding, using the recent formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased.},
  isbn = {978-1-58113-729-3},
  keywords = {modularity,resource protection,separation logic},
  series = {{{POPL}} '04}
}

@article{OHearn2007Resources,
  title = {Resources, Concurrency, and Local Reasoning},
  author = {O'Hearn, Peter W.},
  year = {2007},
  month = may,
  volume = {375},
  pages = {271--307},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.12.035},
  abstract = {In this paper we show how a resource-oriented logic, separation logic, can be used to reason about the usage of resources in concurrent programs.},
  journal = {Theoretical Computer Science},
  keywords = {Concurrency,Logics of programs,Separation logic},
  number = {1},
  series = {Festschrift for {{John C}}. {{Reynolds}}'s 70th Birthday}
}

@article{OHearn2009Separation,
  title = {Separation and {{Information Hiding}}},
  author = {O'Hearn, Peter W. and Yang, Hongseok and Reynolds, John C.},
  year = {2009},
  month = apr,
  volume = {31},
  pages = {11:1--11:50},
  issn = {0164-0925},
  doi = {10.1145/1498926.1498929},
  abstract = {We investigate proof rules for information hiding, using the formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {modularity,resource protection,Separation logic},
  number = {3}
}

@book{OHearn2013algollike,
  title = {Algol-like {{Languages}}},
  author = {O'Hearn, Peter and Tennent, Robert},
  year = {2013},
  publisher = {{Birkh\&\#228;user Basel}},
  abstract = {In 1959 John Backus presented a paper on a proposed international algebraic language which evolved into ALGOL 60. This set of two volumes aims to review the attempts over recent years to use programming languages based on ALGOL 60, using Backus' original document as an introduction.},
  isbn = {1-4757-3853-6 978-1-4757-3853-7}
}

@misc{OHearnSeparation,
  title = {Separation {{Logic}}},
  author = {O'Hearn, Peter},
  abstract = {Separation logic is a key development in formal reasoning about programs, opening up new lines of attack on longstanding problems.},
  howpublished = {https://cacm.acm.org/magazines/2019/2/234356-separation-logic/fulltext?fbclid=IwAR092kLW98DpDEXKTQvQKGTXIjVqUWNckdi6Toi9uvr\_fgDaSrDhHtS39Mo},
  language = {en}
}

@inproceedings{Ohori2007Lightweight,
  title = {Lightweight {{Fusion}} by {{Fixed Point Promotion}}},
  booktitle = {Proceedings of the 34th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ohori, Atsushi and Sasano, Isao},
  year = {2007},
  pages = {143--154},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1190216.1190241},
  abstract = {This paper proposes a lightweight fusion method for general recursive function definitions. Compared with existing proposals, our method has several significant practical features: it works for general recursive functions on general algebraic data types; it does not produce extra runtime overhea (except for possible code size increase due to the success of fusion); and it is readily incorporated in standard inlining optimization. This is achieved by extending the ordinary inlining process with a new fusion law that transforms a term of the form f o (fixg{$\lambda$}x.E) to a new fixed point term fixh{$\lambda$}x.E{${'}$} by promoting the function f through the fixed point operator. This is a sound syntactic transformation rule that is not sensitive to the types of f and g. This property makes our method applicable to wide range of functions including those with multi-parameters in both curried and uncurried forms. Although this method does not guarantee any form of completeness, it fuses typical examples discussed in the literature and others that involve accumulating parameters, either in the tt foldl-like specific forms or in general recursive forms, without any additional machinery. In order to substantiate our claim, we have implemented our method in a compiler. Although it is preliminary, it demonstrates practical feasibility of this method.},
  isbn = {1-59593-575-4},
  keywords = {fixed point,fusion,inlining},
  series = {{{POPL}} '07}
}

@article{Ohori2018Finitary,
  title = {Finitary {{Polymorphism}} for {{Optimizing Type}}-Directed {{Compilation}}},
  author = {Ohori, Atsushi and Ueno, Katsuhiro and Mima, Hisayuki},
  year = {2018},
  month = jul,
  volume = {2},
  pages = {81:1--81:29},
  issn = {2475-1421},
  doi = {10.1145/3236776},
  abstract = {We develop a type-theoretical method for optimizing type directed compilation of polymorphic languages, implement the method in SML\#, which is a full-scale compiler of Standard ML extended with several advanced features that require type-passing operational semantics, and report its effectiveness through performance evaluation. For this purpose, we first define a predicative second-order lambda calculus with finitary polymorphism, where each type abstraction is explicitly constrained to a finite type universe, and establishes the type soundness with respect to a type-passing operational semantics. Different from a calculus with stratified type universes, type universes of the calculus are terms that represent a finite set of instance types. We then develop a universe reconstruction algorithm that takes a term of the standard second-order lambda calculus, checks if the term is typable with finitary polymorphism, and, if typable, constructs a term in the calculus of finitary polymorphism. Based on these results, we present a type-based optimization method for polymorphic functions. Since our formalism is based on the second-order lambda calculus, it can be used to optimize various polymorphic languages. We implement the optimization method for native (tag-free) data representation and record polymorphism, and evaluate its effectiveness through benchmarks. The evaluation shows that 83.79\% of type passing abstractions are eliminated, and achieves the average of 15.28\% speed-up of compiled code.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Compiler Optimization,Finitary Polymorphism,Second-Order Lambda Calculus,SML\#,Type-Directed Compilation},
  number = {ICFP}
}

@inproceedings{Ohshima2013future,
  title = {Toward the {{Future}} of {{Personal Computing System Construction}}},
  booktitle = {Proceedings of the 2013 {{Workshop}} on {{Programming Based}} on {{Actors}}, {{Agents}}, and {{Decentralized Control}}},
  author = {Ohshima, Yoshiki},
  year = {2013},
  pages = {1--2},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2541329.2541346},
  abstract = {The software for today's personal computing environments has become so complex that no single person can understand an entire system. Our group's early experiences with personal computing led us to understand that the essential model of personal computing can be expressed much more compactly. Our group engaged in a project (called the STEPS project) to materialize that vision over the last six years. Several interesting results came out of the STEPS project. There are various meta-language implementations. A new stream-processing language called Nile was invented. The syntax of Nile allows a fully-featured vector graphics engine called Gezira to be written in a clean, mathematical manner in less than 500 lines of code. We also wrote a dynamic, yet declarative, graphical user interface framework in which we built a universal document editor. From the experience with the STEPS project, we are exploring the next steps; one of the directions is to incorporate the idea of using multiple constraint solvers to build a system. Solvers can help stronger "negotiation" between objects and languages, and help the system to be written in a more declarative manner. Another direction is to take the idea of loose-coupling to the next level; objects should not know about other objects directly but should always negotiate and "find" other objects. As the basic concept for this next stage, we are using the "Internet all the way down" analogy. The Internet has proven itself to be a great engineering feat: it scaled from just four nodes to billions of nodes without ever needing a full shutdown for maintenance. It is also likely that we can take out a random 10\% of the Internet's nodes and it will still keep running. By building a personal computing environment that consists of such nodes, or objects, perhaps we can make it never need to be restarted. Interestingly, J. C. R. Licklider already foresaw the need for program components to discover each other on a huge network of computers. From that viewpoint, what we are trying to do is to carry the vision forward.},
  isbn = {978-1-4503-2602-5},
  keywords = {exploratory programming,the internet all the way down model},
  series = {{{AGERE}}! '13}
}

@article{Okasaki1994callbyneed,
  title = {Call-by-Need and Continuation-Passing Style},
  author = {Okasaki, Chris and Lee, Peter and Tarditi, David},
  year = {1994},
  month = jan,
  volume = {7},
  pages = {57--81},
  issn = {0892-4635, 1573-0557},
  doi = {10.1007/BF01019945},
  abstract = {This paper examines the transformation of call-by-need {$\lambda$} terms into continuation-passing style (CPS). It begins by presenting a simple transformation of call-by-need {$\lambda$} terms into program graphs and a reducer for such graphs. From this, an informal derivation is carried out, resulting in a translation from {$\lambda$} terms into self-reducing program graphs, where the graphs are represented as CPS terms involving storage operations. Though informal, the derivation proceeds in simple steps, and the resulting translation is taken to be our canonical CPS transformation for call-by-need {$\lambda$} terms. In order to define the CPS transformation more formally, two alternative presentations are given. The first takes the form of a continuation semantics for the call-by-need language. The second presentation follows Danvy and Hatcliff's two-stage decomposition of the call-by-name CPS transformation, resulting in a similar two-stage CPS transformation for call-by-need. Finally, a number of practical matters are considered, including an improvement to eliminate the so-called administrative redexes, as well as to avoid unnecessary memoization and take advantage of strictness information. These improvements make it feasible to consider potential applications in compilers for call-by-need programming languages.},
  journal = {LISP and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),Call-by-need,Continuation-passing Style,continuations,functional programming,Lazy Evaluation,Numeric Computing,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {1}
}

@inproceedings{Okasaki1995Purely,
  title = {Purely {{Functional Random}}-Access {{Lists}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Functional Programming Languages}} and {{Computer Architecture}}},
  author = {Okasaki, Chris},
  year = {1995},
  pages = {86--95},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/224164.224187},
  isbn = {978-0-89791-719-3},
  series = {{{FPCA}} '95}
}

@inproceedings{Oliveira2008scala,
  title = {Scala for {{Generic Programmers}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Oliveira, Bruno C.d.S. and Gibbons, Jeremy},
  year = {2008},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1411318.1411323},
  abstract = {Datatype-generic programming involves parametrization by the shape of data, in the form of type constructors such as "list of". Most approaches to datatype-generic programming are developed in the lazy functional programming language Haskell. We argue that the functional object-oriented language Scala is in many ways a better setting. Not only does Scala provide equivalents of all the necessary functional programming features (such parametric polymorphism, higher-order functions, higher-kinded type operations, and type- and constructor-classes), but it also provides the most useful features of object-oriented languages (such as subtyping, overriding, traditional single inheritance, and multiple inheritance in the form of traits). We show how this combination of features benefits datatype-generic programming, using three different approaches as illustrations.},
  isbn = {978-1-60558-060-9},
  keywords = {Datatype-generic programming,polytypic programming,Scala},
  series = {{{WGP}} '08}
}

@inproceedings{Oliveira2008visitor,
  title = {The {{Visitor Pattern As}} a {{Reusable}}, {{Generic}}, {{Type}}-Safe {{Component}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems Languages}} and {{Applications}}},
  author = {Oliveira, Bruno C.d.S. and Wang, Meng and Gibbons, Jeremy},
  year = {2008},
  pages = {439--456},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1449764.1449799},
  abstract = {The VISITOR design pattern shows how to separate the structure of an object hierarchy from the behaviour of traversals over that hierarchy. The pattern is very flexible; this very flexibility makes it difficult to capture the pattern as anything more formal than prose, pictures and prototypes. We show how to capture the essence of the VISITOR pattern as a reusable software library, by using advanced type system features appearing in modern object-oriented languages such as Scala. We preserve type-safety statically and modularly: no reflection or similar mechanisms are used and modules can be independently compiled. The library is generic, in two senses: not only is it parametrised by both the return type and the shape of the object hierarchy, but also it allows a number of implementation choices (internal versus external control, imperative versus functional behaviour, orthogonal aspects such as tracing and memoisation) to be specified by parameters rather than fixed in early design decisions. Finally, we propose a generalised datatype-like notation,on top of our visitor library: this provides a convenient functional decomposition style in object-oriented languages.},
  isbn = {978-1-60558-215-3},
  keywords = {algebraic datatypes,design patterns,program extensibility,software components,traversal,visitor pattern},
  series = {{{OOPSLA}} '08}
}

@incollection{Oliveira2009modular,
  title = {Modular {{Visitor Components}}},
  booktitle = {{{ECOOP}} 2009 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Oliveira, Bruno C. d S.},
  editor = {Drossopoulou, Sophia},
  year = {2009},
  month = jan,
  pages = {269--293},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The expression families problem can be defined as the problem of achieving reusability and composability across the components involved in a family of related datatypes and corresponding operations over those datatypes. Like the traditional expression problem, adding new components (either variants or operations) should be possible while preserving modular and static type-safety. Moreover, different combinations of components should have different type identities and the subtyping relationships between components should be preserved. By generalizing previous work exploring the connection between type-theoretic encodings of datatypes and visitors, we propose two solutions for this problem in Scala using modular visitor components. These components can be grouped into features that can be easily composed in a feature-oriented programming style to obtain customized datatypes and operations.},
  copyright = {\textcopyright{}2009 Springer Berlin Heidelberg},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  keywords = {Computer Communication Networks,Logics and Meanings of Programs,Management of Computing and Information Systems,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {5653},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Oliveira2012extensibility,
  title = {Extensibility for the {{Masses}}},
  booktitle = {{{ECOOP}} 2012 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Oliveira, Bruno C. d S. and Cook, William R.},
  editor = {Noble, James},
  year = {2012},
  pages = {2--27},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper presents a new solution to the expression problem (EP) that works in OO languages with simple generics (including Java or C\#). A key novelty of this solution is that advanced typing features, including F-bounded quantification, wildcards and variance annotations, are not needed. The solution is based on object algebras, which are an abstraction closely related to algebraic datatypes and Church encodings. Object algebras also have much in common with the traditional forms of the Visitor pattern, but without many of its drawbacks: they are extensible, remove the need for accept methods, and do not compromise encapsulation. We show applications of object algebras that go beyond toy examples usually presented in solutions for the expression problem. In the paper we develop an increasingly more complex set of features for a mini-imperative language, and we discuss a real-world application of object algebras in an implementation of remote batches. We believe that object algebras bring extensibility to the masses: object algebras work in mainstream OO languages, and they significantly reduce the conceptual overhead by using only features that are used by everyday programmers.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  keywords = {Computer Communication Networks,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {7313},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Oliveira2012functional,
  title = {Functional {{Programming}} with {{Structured Graphs}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Oliveira, Bruno C.d.S. and Cook, William R.},
  year = {2012},
  pages = {77--88},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364527.2364541},
  abstract = {This paper presents a new functional programming model for graph structures called structured graphs. Structured graphs extend conventional algebraic datatypes with explicit definition and manipulation of cycles and/or sharing, and offer a practical and convenient way to program graphs in functional programming languages like Haskell. The representation of sharing and cycles (edges) employs recursive binders and uses an encoding inspired by parametric higher-order abstract syntax. Unlike traditional approaches based on mutable references or node/edge lists, well-formedness of the graph structure is ensured statically and reasoning can be done with standard functional programming techniques. Since the binding structure is generic, we can define many useful generic combinators for manipulating structured graphs. We give applications and show how to reason about structured graphs.},
  isbn = {978-1-4503-1054-3},
  keywords = {graphs,Haskell,parametric hoas},
  series = {{{ICFP}} '12}
}

@incollection{Omar2014safely,
  title = {Safely {{Composable Type}}-{{Specific Languages}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  year = {2014},
  month = jan,
  pages = {105--130},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  copyright = {\textcopyright{}2014 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  keywords = {bidirectional typechecking,Computer Communication Networks,extensible languages,hygiene,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,parsing,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {8586},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Omar2016Hazelnut,
  title = {Hazelnut: {{A Bidirectionally Typed Structure Editor Calculus}}},
  shorttitle = {Hazelnut},
  author = {Omar, Cyrus and Voysey, Ian and Hilton, Michael and Aldrich, Jonathan and Hammer, Matthew A.},
  year = {2016},
  month = jul,
  abstract = {Structure editors allow programmers to edit the tree structure of a program directly. This can have cognitive benefits, particularly for novice and end-user programmers (as evidenced by the popularity of structure editors like Scratch.) It also simplifies matters for tool designers, because they do not need to contend with malformed program text. This paper defines Hazelnut, a structure editor based on a small bidirectionally typed lambda calculus extended with holes and a cursor (a la Huet's zipper.) Hazelnut goes one step beyond syntactic well-formedness: it's edit actions operate over statically meaningful (i.e. well-typed) terms. Naively, this prohibition on ill-typed edit states would force the programmer to construct terms in a rigid "outside-in" manner. To avoid this problem, the action semantics automatically places terms assigned a type that is inconsistent with the expected type inside a hole. This safely defers the type consistency check until the term inside the hole is finished. Hazelnut is a foundational type-theoretic account of typed structure editing, rather than an end-user tool itself. To that end, we describe how Hazelnut's rich metatheory, which we have mechanized in Agda, guides the definition of an extension to the calculus. We also discuss various plausible evaluation strategies for terms with holes, and in so doing reveal connections with gradual typing and contextual modal type theory (the Curry-Howard interpretation of contextual modal logic.) Finally, we discuss how Hazelnut's semantics lends itself to implementation as a functional reactive program. Our reference implementation is written using js\_of\_ocaml.},
  archivePrefix = {arXiv},
  eprint = {1607.04180},
  eprinttype = {arxiv},
  journal = {arXiv:1607.04180 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Omar2018Live,
  title = {Live {{Functional Programming}} with {{Typed Holes}}},
  author = {Omar, Cyrus and Voysey, Ian and Chugh, Ravi and Hammer, Matthew A.},
  year = {2018},
  month = apr,
  abstract = {This paper develops a dynamic semantics for incomplete functional programs, starting from the static semantics developed in recent work on Hazelnut. We model incomplete functional programs as expressions with holes, with empty holes standing for missing expressions or types, and non-empty holes operating as membranes around static and dynamic type inconsistencies. Rather than aborting when evaluation encounters any of these holes as in some existing systems, evaluation proceeds around holes, tracking the closure around each hole instance as it flows through the remainder of the program. Editor services can use the information in these hole closures to help the programmer develop and confirm their mental model of the behavior of the complete portions of the program as they decide how to fill the remaining holes. Hole closures also enable a fill-and-resume operation that avoids the need to restart evaluation after edits that amount to hole filling. Formally, the semantics borrows machinery from both gradual type theory (which supplies the basis for handling unfilled type holes) and contextual modal type theory (which supplies a logical basis for hole closures), combining these and developing additional machinery necessary to continue evaluation past holes while maintaining type safety. We have mechanized the metatheory of the core calculus, called Hazelnut Live, using the Agda proof assistant. We have also implemented these ideas into the Hazel programming environment. The implementation inserts holes automatically, following the Hazelnut edit action calculus, to guarantee that every editor state has some (possibly incomplete) type. Taken together with this paper's type safety property, the result is a proof-of-concept live programming environment where rich dynamic feedback is truly available without gaps, i.e. for every reachable editor state.},
  archivePrefix = {arXiv},
  eprint = {1805.00155},
  eprinttype = {arxiv},
  journal = {arXiv:1805.00155 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Orchard2014semantic,
  title = {The Semantic Marriage of Monads and Effects},
  author = {Orchard, Dominic and Petricek, Tomas and Mycroft, Alan},
  year = {2014},
  month = jan,
  abstract = {Wadler and Thiemann unified type-and-effect systems with monadic semantics via a syntactic correspondence and soundness results with respect to an operational semantics. They conjecture that a general, "coherent" denotational semantics can be given to unify effect systems with a monadic-style semantics. We provide such a semantics based on the novel structure of an indexed monad, which we introduce. We redefine the semantics of Moggi's computational lambda-calculus in terms of (strong) indexed monads which gives a one-to-one correspondence between indices of the denotations and the effect annotations of traditional effect systems. Dually, this approach yields indexed comonads which gives a unified semantics and effect system to contextual notions of effect (called coeffects), which we have previously described.},
  archivePrefix = {arXiv},
  eprint = {1401.5391},
  eprinttype = {arxiv},
  journal = {arXiv:1401.5391 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Orchard2016Effects,
  title = {Effects {{As Sessions}}, {{Sessions As Effects}}},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Orchard, Dominic and Yoshida, Nobuko},
  year = {2016},
  pages = {568--581},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2837614.2837634},
  abstract = {Effect and session type systems are two expressive behavioural type systems. The former is usually developed in the context of the lambda-calculus and its variants, the latter for the pi-calculus. In this paper we explore their relative expressive power. Firstly, we give an embedding from PCF, augmented with a parameterised effect system, into a session-typed pi-calculus (session calculus), showing that session types are powerful enough to express effects. Secondly, we give a reverse embedding, from the session calculus back into PCF, by instantiating PCF with concurrency primitives and its effect system with a session-like effect algebra; effect systems are powerful enough to express sessions. The embedding of session types into an effect system is leveraged to give a new implementation of session types in Haskell, via an effect system encoding. The correctness of this implementation follows from the second embedding result. We also discuss various extensions to our embeddings.},
  isbn = {978-1-4503-3549-2},
  keywords = {Concurrent Haskell,effect systems,encoding,PCF,session types,type systems},
  series = {{{POPL}} '16}
}

@inproceedings{Orton2017Models,
  title = {Models of {{Type Theory Based}} on {{Moore Paths}}},
  booktitle = {2nd {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}} ({{FSCD}} 2017)},
  author = {Orton, Ian and Pitts, Andrew M.},
  editor = {Miller, Dale},
  year = {2017},
  volume = {84},
  pages = {28:1--28:16},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.FSCD.2017.28},
  isbn = {978-3-95977-047-7},
  keywords = {Dependent type theory,homotopy theory,Moore path,topos},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Ostermann2001Objectoriented,
  title = {Object-Oriented {{Composition Untangled}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Ostermann, Klaus and Mezini, Mira},
  year = {2001},
  pages = {283--299},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/504282.504303},
  abstract = {Object-oriented languages come with pre-defined composition mechansims, such as inheritance, object composition, or delegation, each characterized by a certain set of composition properties, which do not themselves individually exist as abstractions at the language level. However, often non-standard composition semantics is needed, with a mixture of composition mechanisms. Such non-standard semantics are simulated by complicated architectures that are sensitive to requirement changes and cannot easily be adapted without invalidating existing clients. In this paper, we propose compound references, a new abstraction for object references, that allows us to provide explicit linguistic means for expressing and combining individual composition properties on-demand. The model is statically typed and allows the programmer to express a seamless spectrum of composition semantics in the interval between object composition and inheritance. The resulting programs are better understandable, due to explicity expressed design decisions, and less sensitive to requirement changes.},
  isbn = {1-58113-335-9},
  keywords = {_tablet},
  series = {{{OOPSLA}} '01}
}

@inproceedings{Ostermann2018Dualizing,
  title = {Dualizing {{Generalized Algebraic Data Types}} by {{Matrix Transposition}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Ostermann, Klaus and Jabs, Julian},
  year = {2018},
  month = apr,
  pages = {60--85},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-89884-1_3},
  abstract = {We characterize the relation between generalized algebraic datatypes (GADTs) with pattern matching on their constructors one hand, and generalized algebraic co-datatypes (GAcoDTs) with copattern matching on their destructors on the other hand: GADTs can be converted mechanically to GAcoDTs by refunctionalization, GAcoDTs can be converted mechanically to GADTs by defunctionalization, and both defunctionalization and refunctionalization correspond to a transposition of the matrix in which the equations for each constructor/destructor pair of the (co-)datatype are organized. We have defined a calculus, GADTTGADTTGADT\^T, which unifies GADTs and GAcoDTs in such a way that GADTs and GAcoDTs are merely different ways to partition the program.We have formalized the type system and operational semantics of GADTTGADTTGADT\^T in the Coq proof assistant and have mechanically verified the following results: (1) The type system of GADTTGADTTGADT\^T is sound, (2) defunctionalization and refunctionalization can translate GADTs to GAcoDTs and back, (3) both transformations are type- and semantics-preserving and are inverses of each other, (4) (co-)datatypes can be represented by matrices in such a way the aforementioned transformations correspond to matrix transposition, (5) GADTs are extensible in an exactly dual way to GAcoDTs; we thereby clarify folklore knowledge about the ``expression problem''.We believe that the identification of this relationship can guide future language design of ``dual features'' for data and codata.},
  isbn = {978-3-319-89883-4 978-3-319-89884-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Osvald2016Gentrification,
  title = {Gentrification {{Gone Too Far}}? {{Affordable 2Nd}}-Class {{Values}} for {{Fun}} and ({{Co}}-){{Effect}}},
  shorttitle = {Gentrification {{Gone Too Far}}?},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Osvald, Leo and Essertel, Gr{\'e}gory and Wu, Xilun and Alay{\'o}n, Lilliam I. Gonz{\'a}lez and Rompf, Tiark},
  year = {2016},
  pages = {234--251},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2983990.2984009},
  abstract = {First-class functions dramatically increase expressiveness, at the expense of static guarantees. In ALGOL or PASCAL, functions could be passed as arguments but never escape their defining scope. Therefore, function arguments could serve as temporary access tokens or capabilities, enabling callees to perform some action, but only for the duration of the call. In modern languages, such programming patterns are no longer available. The central thrust of this paper is to re-introduce second-class functions and other values alongside first-class entities in modern languages. We formalize second-class values with stack-bounded lifetimes as an extension to simply-typed {\^I}\guillemotright{} calculus, and for richer type systems such as F{$<$}: and systems with path-dependent types. We generalize the binary first- vs second-class distinction to arbitrary privilege lattices, with the underlying type lattice as a special case. In this setting, abstract types naturally enable privilege parametricity. We prove type soundness and lifetime properties in Coq. We implement our system as an extension of Scala, and present several case studies. First, we modify the Scala Collections library and add privilege annotations to all higher-order functions. Privilege parametricity is key to retain the high degree of code-reuse between sequential and parallel as well as lazy and eager collections. Second, we use scoped capabilities to introduce a model of checked exceptions in the Scala library, with only few changes to the code. Third, we employ second-class capabilities for memory safety in a region-based off-heap memory library.},
  isbn = {978-1-4503-4444-9},
  keywords = {capabilities,effects,first-class,object lifetimes,second-class,types},
  series = {{{OOPSLA}} 2016}
}

@inproceedings{Owens2016Functional,
  title = {Functional {{Big}}-{{Step Semantics}}},
  booktitle = {Proceedings of the 25th {{European Symposium}} on {{Programming Languages}} and {{Systems}} - {{Volume}} 9632},
  author = {Owens, Scott and Myreen, Magnus O. and Kumar, Ramana and Tan, Yong Kiam},
  year = {2016},
  pages = {589--615},
  publisher = {{Springer-Verlag New York, Inc.}},
  address = {{New York, NY, USA}},
  doi = {10.1007/978-3-662-49498-1_23},
  abstract = {When doing an interactive proof about a piece of software, it is important that the underlying programming language's semantics does not make the proof unnecessarily difficult or unwieldy. Both small-step and big-step semantics are commonly used, and the latter is typically given by an inductively defined relation. In this paper, we consider an alternative: using a recursive function akin{\"i}\textthreequarters\textquestiondown{}to an interpreter for the language. The advantages include a better induction theorem, less duplication, accessibility to ordinary functional programmers, and the ease of doing symbolic simulation in proofs via rewriting. We believe that this style of semantics is well suited for compiler verification, including proofs of divergence preservation. We do not claim the invention of this style of semantics: our contribution here is to clarify its value, and to explain how it supports several language features that might appear to require a relational or small-step approach. We illustrate the technique on a simple imperative language with C-like for-loops and a break statement, and compare it to a variety of other approaches. We also provide ML and lambda-calculus based examples to illustrate its generality.},
  isbn = {978-3-662-49497-4}
}

@article{Palem2014inexactness,
  title = {Inexactness and a Future of Computing},
  author = {Palem, Krishna V.},
  year = {2014},
  month = jun,
  volume = {372},
  pages = {20130281},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2013.0281},
  abstract = {As pressures, notably from energy consumption, start impeding the growth and scale of computing systems, inevitably, designers and users are increasingly considering the prospect of trading accuracy or exactness. This paper is a perspective on the progress in embracing this somewhat unusual philosophy of innovating computing systems that are designed to be inexact or approximate, in the interests of realizing extreme efficiencies. With our own experience in designing inexact physical systems including hardware as a backdrop, we speculate on the rich potential for considering inexactness as a broad emerging theme if not an entire domain for investigation for exciting research and innovation. If this emerging trend to pursuing inexactness persists and grows, then we anticipate an increasing need to consider system co-design where application domain characteristics and technology features interplay in an active manner. A noteworthy early example of this approach is our own excursion into tailoring and hence co-designing floating point arithmetic units guided by the needs of stochastic climate models. This approach requires a unified effort between software and hardware designers that does away with the normal clean abstraction layers between the two.},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  keywords = {approximation algorithms,heuristics,inexact computing,information theory,probabilistic CMOS,randomized algorithms},
  language = {en},
  number = {2018},
  pmid = {24842028}
}

@article{Palmgren2005internalising,
  title = {Internalising Modified Realisability in Constructive Type Theory},
  author = {Palmgren, Erik},
  editor = {Setzer, Anton},
  year = {2005},
  month = oct,
  volume = {1},
  issn = {18605974},
  doi = {10.2168/LMCS-1(2:2)2005},
  journal = {Logical Methods in Computer Science},
  language = {en},
  number = {2}
}

@article{Park2014Mechanizing,
  title = {Mechanizing {{Metatheory Without Typing Contexts}}},
  author = {Park, Jonghyun and Seo, Jeongbong and Park, Sungwoo and Lee, Gyesik},
  year = {2014},
  month = feb,
  volume = {52},
  pages = {215--239},
  issn = {1573-0670},
  doi = {10.1007/s10817-013-9287-4},
  abstract = {When mechanizing the metatheory of a programming language, one usually needs many lemmas proving structural properties of typing judgments, such as permutation and weakening. Such structural lemmas are sometimes unnecessary if we eliminate typing contexts by expanding typing judgments into their original hypothetical proofs. This technique of eliminating typing contexts, which has been around since Church (J Symb Log 5(2):56\textendash{}68, 1940), is based on the view that entailment relations, such as typing judgments, are just syntactic tools for displaying only the hypotheses and conclusion of a hypothetical proof while hiding its internal structure. In this paper, we apply this technique to parts 1A/2A of the textscPoplMark challenge (Aydemir et al. 2005) and experimentally evaluate its efficiency by formalizing System F {$<$} : in the Coq proof assistant in a number of different ways. An analysis of our Coq developments shows that eliminating typing contexts produces a more significant reduction in both the number of lemmas and the count of tactics than the cofinite quantification, one of the most effective ways of simplifying the mechanization involving binders. Our experiment with System F {$<$} : suggests three guidelines to follow when applying the technique of eliminating typing contexts.},
  journal = {Journal of Automated Reasoning},
  keywords = {PoplMark,System F < :,Typing context},
  language = {en},
  number = {2}
}

@inproceedings{Parkinson2005Separation,
  title = {Separation {{Logic}} and {{Abstraction}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Parkinson, Matthew and Bierman, Gavin},
  year = {2005},
  pages = {247--258},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1040305.1040326},
  abstract = {In this paper we address the problem of writing specifications for programs that use various forms of modularity, including procedures and Java-like classes. We build on the formalism of separation logic and introduce the new notion of an abstract predicate and, more generally, abstract predicate families. This provides a flexible mechanism for reasoning about the different forms of abstraction found in modern programming languages, such as abstract datatypes and objects. As well as demonstrating the soundness of our proof system, we illustrate its utility with a series of examples.},
  isbn = {978-1-58113-830-6},
  keywords = {abstract data types,classes,modularity,resources,separation logic},
  series = {{{POPL}} '05}
}

@article{Parnas1972criteria,
  title = {On the {{Criteria}} to {{Be Used}} in {{Decomposing Systems}} into {{Modules}}},
  author = {Parnas, D. L.},
  year = {1972},
  month = dec,
  volume = {15},
  pages = {1053--1058},
  issn = {0001-0782},
  doi = {10.1145/361598.361623},
  abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a ``modularization'' is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
  journal = {Commun. ACM},
  keywords = {KWIC index,modularity,modules,software,software design,Software Engineering},
  number = {12}
}

@incollection{Parnas2002secret,
  title = {The {{Secret History}} of {{Information Hiding}}},
  booktitle = {Software {{Pioneers}}},
  author = {Parnas, David L.},
  editor = {Broy, Prof Dr Manfred and Denert, Prof Dr Ernst},
  year = {2002},
  pages = {398--409},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The concept of ``information-hiding'' as a software design principle is widely accepted in academic circles. Many successful designs can be seen as successful applications of abstraction or information hiding. On the other hand, most industrial software developers do not apply the idea and many consider it unrealistic. This paper describes how the idea developed, discusses difficulties in its application, and speculates on why it has not been completely successful.},
  copyright = {\textcopyright{}2002 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-63970-8 978-3-642-59412-0},
  keywords = {History of Computing,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Software Engineering/Programming and Operating Systems},
  language = {en}
}

@article{Parreaux2017Quoted,
  title = {Quoted {{Staged Rewriting}}: {{A Practical Approach}} to {{Library}}-{{Defined Optimizations}}},
  shorttitle = {Quoted {{Staged Rewriting}}},
  author = {Parreaux, Lionel Emile Vincent and Shaikhha, Amir and Koch, Christoph},
  year = {2017},
  doi = {10.1145/3136040.3136043},
  journal = {Proceedings of 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences (GPCE'17)}
}

@inproceedings{Parreaux2019improved,
  title = {Towards Improved {{GADT}} Reasoning in {{Scala}}},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Parreaux, Lionel and {Boruch-Gruszecki}, Aleksander and Giarrusso, Paolo G.},
  year = {2019},
  pages = {12--16},
  publisher = {{ACM}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3337932.3338813},
  abstract = {Generalized algebraic data types (GADT) have been notoriously difficult to implement correctly in Scala. Both major Scala compilers, Scalac and Dotty, are currently known to have type soundness holes related to them. In particular, covariant GADTs have exposed paradoxes due to Scala's inheritance model. We informally explore foundations for GADTs within Scala's core type system, to guide a principled understanding and implementation of GADTs in Scala.},
  isbn = {978-1-4503-6824-7},
  keywords = {DOT,generalized algebraic data types,Scala},
  series = {Scala '19}
}

@incollection{Partush2013abstract,
  title = {Abstract {{Semantic Differencing}} for {{Numerical Programs}}},
  booktitle = {Static {{Analysis}}},
  author = {Partush, Nimrod and Yahav, Eran},
  editor = {Logozzo, Francesco and F{\"a}hndrich, Manuel},
  year = {2013},
  pages = {238--258},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists. We focus on computing semantic differences in numerical programs where the values of variables have no a-priori bounds, and use abstract interpretation to compute an over-approximation of program differences. Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, we first construct a correlating program in which these relationships can be tracked, and then use a correlating abstract domain to compute a sound approximation of these relationships. To better establish equivalence between correlated variables and precisely capture differences, our domain has to represent non-convex information using a partially-disjunctive abstract domain. To balance precision and cost of this representation, our domain over-approximates numerical information while preserving equivalence between correlated variables by dynamically partitioning the disjunctive state according to equivalence criteria. We have implemented our approach in a tool called DIZY, and applied it to a number of real-world examples, including programs from the GNU core utilities, Mozilla Firefox and the Linux Kernel. Our evaluation shows that DIZY often manages to establish equivalence, describes precise approximation of semantic differences when difference exists, and reports only a few false differences.},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-38855-2 978-3-642-38856-9},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {7935},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Paulin-Mohring1989extracting,
  title = {Extracting \&{{Ohgr}};'s {{Programs}} from {{Proofs}} in the {{Calculus}} of {{Constructions}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {{Paulin-Mohring}, C.},
  year = {1989},
  pages = {89--104},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/75277.75285},
  abstract = {We define in this paper a notion of realizability for the Calculus of Constructions. The extracted programs are terms of the Calculus that do not contain dependent types. We introduce a distinction between informative and non-informative propositions. This distinction allows the removal of the ``logical'' part in the development of a program. We show also how to use our notion of realizability in order to interpret various axioms like the axiom of choice or the induction on integers. A practical example of development of program is given in the appendix.},
  isbn = {0-89791-294-2},
  series = {{{POPL}} '89}
}

@inproceedings{Paulin-Mohring1993Inductive,
  title = {Inductive Definitions in the System {{Coq}} Rules and Properties},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {{Paulin-Mohring}, Christine},
  year = {1993},
  month = mar,
  pages = {328--345},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/BFb0037116},
  abstract = {In the pure Calculus of Constructions, it is possible to represent data structures and predicates using higher-order quantification. However, this representation is not satisfactory, from the point of view of both the efficiency of the underlying programs and the power of the logical system. For these reasons, the calculus was extended with a primitive notion of inductive definitions [8]. This paper describes the rules for inductive definitions in the system Coq. They are general enough to be seen as one formulation of adding inductive definitions to a typed lambda-calculus. We prove strong normalization for a subsystem of Coq corresponding to the pure Calculus of Constructions plus Inductive Definitions with only weak eliminations.},
  isbn = {978-3-540-56517-8 978-3-540-47586-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Pavlinovic2015practical,
  title = {Practical {{SMT}}-Based {{Type Error Localization}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas},
  year = {2015},
  pages = {412--423},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784765},
  abstract = {Compilers for statically typed functional programming languages are notorious for generating confusing type error messages. When the compiler detects a type error, it typically reports the program location where the type checking failed as the source of the error. Since other error sources are not even considered, the actual root cause is often missed. A more adequate approach is to consider all possible error sources and report the most useful one subject to some usefulness criterion. In our previous work, we showed that this approach can be formulated as an optimization problem related to satisfiability modulo theories (SMT). This formulation cleanly separates the heuristic nature of usefulness criteria from the underlying search problem. Unfortunately, algorithms that search for an optimal error source cannot directly use principal types which are crucial for dealing with the exponential-time complexity of the decision problem of polymorphic type checking. In this paper, we present a new algorithm that efficiently finds an optimal error source in a given ill-typed program. Our algorithm uses an improved SMT encoding to cope with the high complexity of polymorphic typing by iteratively expanding the typing constraints from which principal types are derived. The algorithm preserves the clean separation between the heuristics and the actual search. We have implemented our algorithm for OCaml. In our experimental evaluation, we found that the algorithm reduces the running times for optimal type error localization from minutes to seconds and scales better than previous localization algorithms.},
  isbn = {978-1-4503-3669-7},
  keywords = {polymorphic types,Satisfiability Modulo Theories,Type Error Localization},
  series = {{{ICFP}} 2015}
}

@inproceedings{Pearce2017Rewriting,
  title = {Rewriting for Sound and Complete Union, Intersection and Negation Types},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Pearce, David J.},
  year = {2017},
  pages = {117--130},
  publisher = {{ACM}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/3136040.3136042},
  abstract = {Implementing the type system of a programming language is a critical task that is often done in an ad-hoc fashion. Whilst this makes it hard to ensure the system is sound, it also makes it difficult to extend as the language evolves. We are interested in describing type systems using declarative rewrite rules from which an implementation can be automatically generated. Whilst not all type systems are easily expressed in this manner, those involving unions, intersections and negations are well-suited for this.   In this paper, we consider a relatively complex type system involving unions, intersections and negations developed previously. This system was not developed with rewriting in mind, though clear parallels are immediately apparent from the original presentation. For example, the system presented required types be first converted into a variation on Disjunctive Normal Form. We identify that the original system can, for the most part, be reworked to enable a natural expression using declarative rewrite rules. We present an implementation of our rewrite rules in the Whiley Rewrite Language (WyRL), and report performance results compared with a hand-coded solution.},
  isbn = {978-1-4503-5524-7},
  keywords = {Rewrite Systems,Type Systems,Type Theory},
  series = {{{GPCE}} 2017}
}

@inproceedings{Perera2012functional,
  title = {Functional {{Programs That Explain Their Work}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Perera, Roly and Acar, Umut A. and Cheney, James and Levy, Paul Blain},
  year = {2012},
  pages = {365--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364527.2364579},
  abstract = {We present techniques that enable higher-order functional computations to "explain" their work by answering questions about how parts of their output were calculated. As explanations, we consider the traditional notion of program slices, which we show can be inadequate, and propose a new notion: trace slices. We present techniques for specifying flexible and rich slicing criteria based on partial expressions, parts of which have been replaced by holes. We characterise program slices in an algorithm-independent fashion and show that a least slice for a given criterion exists. We then present an algorithm, called unevaluation, for computing least program slices from computations reified as traces. Observing a limitation of program slices, we develop a notion of trace slice as another form of explanation and present an algorithm for computing them. The unevaluation algorithm can be applied to any subtrace of a trace slice to compute a program slice whose evaluation generates that subtrace. This close correspondence between programs, traces, and their slices can enable the programmer to understand a computation interactively, in terms of the programming language in which the computation is expressed. We present an implementation in the form of a tool, discuss some important practical implementation concerns and present some techniques for addressing them.},
  isbn = {978-1-4503-1054-3},
  keywords = {debugging,program slicing,provenance},
  series = {{{ICFP}} '12}
}

@inproceedings{Person2008Differential,
  title = {Differential Symbolic Execution},
  booktitle = {Proceedings of the 16th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Person, Suzette and Dwyer, Matthew B. and Elbaum, Sebastian and P{\v a}s{\v a}reanu, Corina S.},
  year = {2008},
  pages = {226--237},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1453101.1453131},
  abstract = {Detecting and characterizing the effects of software changes is a fundamental component of software maintenance. Version differencing information can be used to perform version merging, infer change characteristics, produce program documentation, and guide program re-validation. Existing techniques for characterizing code changes, however, are imprecise leading to unnecessary maintenance efforts. In this paper, we introduce a novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change. This technique, which we call differential symbolic execution (DSE), exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results. We define the foundational concepts of DSE, describe cost-effective tool support for DSE, and illustrate its potential benefit through an exploratory study that considers version histories of two Java code bases.},
  isbn = {978-1-59593-995-1},
  keywords = {program differencing,software evolution,symbolic execution},
  series = {{{SIGSOFT}} '08/{{FSE}}-16}
}

@article{Petrashko2017Design,
  title = {Design and Implementation of an Optimizing Type-Centric Compiler for a High-Level Language},
  author = {Petrashko, Dmytro},
  year = {2017},
  doi = {10.5075/epfl-thesis-7979},
  abstract = {Production compilers for programming languages face multiple requirements. They should be correct, as we rely on them to produce code. They should be fast, in order to provide a good developer experience. They should also be easy to maintain and evolve. This thesis shows how an expressive high level type system can be used to simplify the development of a compiler and demonstrates this on a compiler for Scala. First, it shows how expressive types of high level languages can be used to build internal data structures that provide a statically checked API, ensuring that important properties hold at compile time. Second, we also show how high level language features can be used to abstract the components of a compiler. We demonstrate this by introducing a type-safe layer on top of the bytecode emission phase. This makes it possible to abstract away the implementation details of the compiler frontend and run the same bytecode emission phase in two different Scala compilers. Third, it presents MiniPhases, a novel way to organize transformation passes in a compiler. MiniPhases impose constraints on the organization of passes that are beneficial for maintainability, performance, and testability. We include a detailed performance evaluation of MiniPhases which indicates that their speedup is due to improved cache friendliness and to a lower rate of promotions of objects into the old generations of garbage collectors. Finally, we demonstrate how the expressive type system of the language being compiled can be used for static analysis. We present a novel call graph construction algorithm which uses the typing context for context sensitivity. The resulting algorithm is both substantially faster and more precise than existing alternatives. We demonstrate the applicability of this analysis by extending common subexpression elimination to idempotent expression elimination. Petrashko, Dmytro},
  language = {en}
}

@inproceedings{Petrashko2017Miniphases,
  title = {Miniphases: {{Compilation}} Using {{Modular}} and {{Efficient Tree Transformations}}},
  shorttitle = {Miniphases},
  booktitle = {{{PLDI}}},
  author = {Petrashko, Dmytro and Lhot{\'a}k, Ondrej and Odersky, Martin},
  year = {2017},
  keywords = {cache locality,compiler performance,tree traversal fusion}
}

@inproceedings{Petri2015cooking,
  title = {Cooking the {{Books}}: {{Formalizing JMM Implementation Recipes}}},
  booktitle = {29th {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2015)},
  author = {Petri, Gustavo and Vitek, Jan and Jagannathan, Suresh},
  editor = {Boyland, John Tang},
  year = {2015},
  volume = {37},
  pages = {445--469},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.ECOOP.2015.445},
  isbn = {978-3-939897-86-6},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-52334}
}

@inproceedings{PeytonJones1993imperative,
  title = {Imperative {{Functional Programming}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Peyton Jones, Simon L. and Wadler, Philip},
  year = {1993},
  pages = {71--84},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/158511.158524},
  abstract = {We present a new model, based on monads, for performing input/output in a non-strict, purely functional language. It is composable, extensible, efficient, requires no extensions to the type system, and extends smoothly to incorporate mixed-language working and in-place array updates.},
  isbn = {0-89791-560-7},
  series = {{{POPL}} '93}
}

@inproceedings{PeytonJones1998bridging,
  title = {Bridging the {{Gulf}}: {{A Common Intermediate Language}} for {{ML}} and {{Haskell}}},
  shorttitle = {Bridging the {{Gulf}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Peyton Jones, Simon and Shields, Mark and Launchbury, John and Tolmach, Andrew},
  year = {1998},
  pages = {49--61},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/268946.268951},
  isbn = {0-89791-979-3},
  series = {{{POPL}} '98}
}

@article{Pfenning2001judgmental,
  title = {A Judgmental Reconstruction of Modal Logic},
  author = {Pfenning, Frank and Davies, Rowan},
  year = {2001},
  month = aug,
  volume = {11},
  pages = {511--540},
  issn = {1469-8072},
  doi = {10.1017/S0960129501003322},
  abstract = {We reconsider the foundations of modal logic, following Martin-L{\"o}f's methodology of distinguishing judgments from propositions. We give constructive meaning explanations for necessity and possibility, which yields a simple and uniform system of natural deduction for intuitionistic modal logic that does not exhibit anomalies found in other proposals. We also give a new presentation of lax logic and find that the lax modality is already expressible using possibility and necessity. Through a computational interpretation of proofs in modal logic we further obtain a new formulation of Moggi's monadic metalanguage.},
  journal = {Mathematical Structures in Computer Science},
  number = {04}
}

@incollection{Pfenning2002logical,
  title = {Logical {{Frameworks}}\textemdash{{A Brief Introduction}}},
  booktitle = {Proof and {{System}}-{{Reliability}}},
  author = {Pfenning, Frank},
  editor = {Schwichtenberg, Helmut and Steinbr{\"u}ggen, Ralf},
  year = {2002},
  month = jan,
  pages = {137--166},
  publisher = {{Springer Netherlands}},
  abstract = {A logical framework is a meta-language for the formalization of deductive systems. We provide a brief introduction to logical frameworks and their methodology, concentrating on LF. We use first-order logic as the running example to illustrate the representations of syntax, natural deductions, and proof transformations. We also sketch a recent formulation of LF centered on the notion of canonical form, and show how it affects proofs of adequacy of encodings.},
  copyright = {\textcopyright{}2002 Kluwer Academic Publishers},
  isbn = {978-1-4020-0608-1 978-94-010-0413-8},
  keywords = {Computing Methodologies,Logic,logical frameworks,Mathematical Logic and Foundations,Theory of Computation,type theory},
  language = {en},
  number = {62},
  series = {{{NATO Science Series}}}
}

@inproceedings{Pickering2016Pattern,
  title = {Pattern Synonyms},
  booktitle = {Proceedings of the 9th {{International Symposium}} on {{Haskell}}},
  author = {Pickering, Matthew and {\'E}rdi, Gerg{\H o} and Peyton Jones, Simon and Eisenberg, Richard A.},
  year = {2016},
  pages = {80--91},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976002.2976013},
  abstract = {Pattern matching has proven to be a convenient, expressive way of inspecting data. Yet this language feature, in its traditional form, is limited: patterns must be data constructors of concrete data types. No computation or abstraction is allowed. The data type in question must be concrete, with no ability to enforce any invariants. Any change in this data type requires all clients to update their code.   This paper introduces pattern synonyms, which allow programmers to abstract over patterns, painting over all the shortcomings listed above. Pattern synonyms are assigned types, enabling a compiler to check the validity of a synonym independent of its definition. These types are intricate; detailing how to assign a type to a pattern synonym is a key contribution of this work. We have implemented pattern synonyms in the Glasgow Haskell Compiler, where they have enjoyed immediate popularity, but we believe this feature could easily be exported to other languages that support pattern matching.},
  isbn = {978-1-4503-4434-0},
  keywords = {functional programming,Haskell,pattern matching},
  series = {Haskell 2016}
}

@inproceedings{Pirog2019Typed,
  title = {Typed Equivalence of Effect Handlers and Delimited Control},
  booktitle = {4th {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}} ({{FSCD}} 2019)},
  author = {Pir{\'o}g, Maciej and Polesiuk, Piotr and Sieczkowski, Filip},
  editor = {Geuvers, Herman},
  year = {2019},
  volume = {131},
  pages = {30:1--30:16},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.FSCD.2019.30},
  isbn = {978-3-95977-107-8},
  keywords = {algebraic effects,delimited control,macro expressibility,type-and-effect systems},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@inproceedings{Pistone2017Dinaturality,
  title = {On {{Dinaturality}}, {{Typability}} and Beta-Eta-{{Stable Models}}},
  booktitle = {2nd {{International Conference}} on {{Formal Structures}} for {{Computation}} and {{Deduction}} ({{FSCD}} 2017)},
  author = {Pistone, Paolo},
  editor = {Miller, Dale},
  year = {2017},
  volume = {84},
  pages = {29:1--29:17},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.FSCD.2017.29},
  isbn = {978-3-95977-047-7},
  keywords = {beta-eta-stable semantics,Completeness,Dinaturality,simply-typed lambda-calculus},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@incollection{Pitts1987polymorphism,
  title = {Polymorphism Is Set Theoretic, Constructively},
  booktitle = {Category {{Theory}} and {{Computer Science}}},
  author = {Pitts, A. M.},
  editor = {Pitt, David H. and Poign{\'e}, Axel and Rydeheard, David E.},
  year = {1987},
  month = jan,
  pages = {12--39},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}1987 Springer-Verlag},
  isbn = {978-3-540-18508-6 978-3-540-48006-8},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations},
  language = {en},
  number = {283},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Pitts1997operationallybased,
  title = {Operationally-Based Theories of Program Equivalence},
  booktitle = {Semantics and {{Logics}} of {{Computation}}},
  author = {Pitts, Andrew},
  year = {1997},
  pages = {241--298},
  publisher = {{Cambridge University Press}},
  abstract = {null}
}

@article{Pitts2000Parametric,
  title = {Parametric Polymorphism and Operational Equivalence},
  author = {Pitts, Andrew M.},
  year = {2000},
  month = jun,
  volume = {10},
  pages = {321--359},
  issn = {1469-8072},
  doi = {null},
  abstract = {Studies of the mathematical properties of impredicative polymorphic types have for the most part focused on the polymorphic lambda calculus of Girard\textendash{}Reynolds, which is a calculus of total polymorphic functions. This paper considers polymorphic types from a functional programming perspective, where the partialness arising from the presence of fixpoint recursion complicates the nature of potentially infinite (`lazy') data types. An approach to Reynolds' notion of relational parametricity is developed that works directly on the syntax of a programming language, using a novel closure operator to relate operational behaviour to parametricity properties of types. Working with an extension of Plotkin's PCF with {$\forall$}-types, lazy lists and existential types, we show by example how the resulting logical relation can be used to prove properties of polymorphic types up to operational equivalence.},
  journal = {Mathematical Structures in Computer Science},
  number = {03}
}

@inproceedings{Piumarta2011open,
  title = {Open, {{Extensible Composition Models}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Free Composition}}},
  author = {Piumarta, Ian},
  year = {2011},
  pages = {2:1--2:5},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2068776.2068778},
  abstract = {Simple functional languages like LISP are useful for exploring novel semantics and composition mechanisms. That usefulness can be limited by the assumptions built into the evaluator about the structure of data and the meaning of expressions. These assumptions create difficulties when a program introduces a composition mechanism that differs substantially from the built-in mechanism of function application. We explore how an evaluator can be constructed to eliminate most built-in assumptions about meaning, and show how new composition mechanisms can be introduced easily and seamlessly into the language it evaluates.},
  isbn = {978-1-4503-0892-2},
  series = {{{FREECO}} '11}
}

@inproceedings{Plociniczak2013Scalad,
  title = {Scalad: {{An Interactive Type}}-Level {{Debugger}}},
  shorttitle = {Scalad},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Scala}}},
  author = {Plociniczak, Hubert},
  year = {2013},
  pages = {8:1--8:4},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2489837.2489845},
  abstract = {Dealing with statically-typed languages and their complex type systems is problematic for programmers of all levels. Resolving confusing type errors is a time consuming and not always successful process. In this tool demonstration we give an overview of Scalad, an interactive tool that can explain decisions made by the existing typechecker of a multi-paradigm programming language by visualizing the whole process in the form of a proof tree. The tool works for both type correct and incorrect programs, making it suitable for educational purposes as well as debugging. We provide examples on how the tool can be used to understand typing puzzles. The debugger comes with an expandable search mechanism that can precisely guide users in finding answers to the typechecking problems and improve exploration time.},
  isbn = {978-1-4503-2064-1},
  keywords = {debugging,errors,type system,visualization},
  series = {{{SCALA}} '13}
}

@inproceedings{Ploeg2014reflection,
  title = {Reflection {{Without Remorse}}: {{Revealing}} a {{Hidden Sequence}} to {{Speed Up Monadic Reflection}}},
  shorttitle = {Reflection {{Without Remorse}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {van der Ploeg, Atze and Kiselyov, Oleg},
  year = {2014},
  pages = {133--144},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2633357.2633360},
  abstract = {A series of list appends or monadic binds for many monads performs algorithmically worse when left-associated. Continuation-passing style (CPS) is well-known to cure this severe dependence of performance on the association pattern. The advantage of CPS dwindles or disappears if we have to examine or modify the intermediate result of a series of appends or binds, before continuing the series. Such examination is frequently needed, for example, to control search in non-determinism monads. We present an alternative approach that is just as general as CPS but more robust: it makes series of binds and other such operations efficient regardless of the association pattern-- and also provides efficient access to intermediate results. The key is to represent such a conceptual sequence as an efficient sequence data structure. Efficient sequence data structures from the literature are homogeneous and cannot be applied as they are in a type-safe way to series of monadic binds. We generalize them to type aligned sequences and show how to construct their (assuredly order-preserving) implementations. We demonstrate that our solution solves previously undocumented, severe performance problems in iteratees, LogicT transformers, free monads and extensible effects.},
  isbn = {978-1-4503-3041-1},
  keywords = {Data Structures,Monads,performance,reflection},
  series = {Haskell '14}
}

@inproceedings{Ploeg2015practical,
  title = {Practical {{Principled FRP}}: {{Forget}} the {{Past}}, {{Change}} the {{Future}}, {{FRPNow}}!},
  shorttitle = {Practical {{Principled FRP}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {van der Ploeg, Atze and Claessen, Koen},
  year = {2015},
  pages = {302--314},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784752},
  abstract = {We present a new interface for practical Functional Reactive Programming (FRP) that (1) is close in spirit to the original FRP ideas, (2) does not have the original space-leak problems, without using arrows or advanced types, and (3) provides a simple and expressive way for performing IO actions from FRP code. We also provide a denotational semantics for this new interface, and a technique (using Kripke logical relations) for reasoning about which FRP functions may "forget their past", i.e. which functions do not have an inherent space-leak. Finally, we show how we have implemented this interface as a Haskell library called FRPNow.},
  isbn = {978-1-4503-3669-7},
  keywords = {functional reactive programming,Kripke Logical Relations,Purely Functional IO,Space-leak},
  series = {{{ICFP}} 2015}
}

@inproceedings{Ploeg2016Key,
  title = {The {{Key Monad}}: {{Type}}-Safe {{Unconstrained Dynamic Typing}}},
  shorttitle = {The {{Key Monad}}},
  booktitle = {Proceedings of the 9th {{International Symposium}} on {{Haskell}}},
  author = {van der Ploeg, Atze and Claessen, Koen and Buiras, Pablo},
  year = {2016},
  pages = {146--157},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976002.2976008},
  abstract = {We present a small extension to Haskell called the Key monad. With the Key monad, unique keys of different types can be created and can be tested for equality. When two keys are equal, we also obtain a concrete proof that their types are equal. This gives us a form of dynamic typing, without the need for Typeable constraints. We show that our extension allows us to safely do things we could not otherwise do: it allows us to implement the ST monad (inefficiently), to implement an embedded form of arrow notation, and to translate parametric HOAS to typed de Bruijn indices, among others. Although strongly related to the ST monad, the Key monad is simpler and might be easier to prove safe. We do not provide such a proof of the safety of the Key monad, but we note that, surprisingly, a full proof of the safety of the ST monad also remains elusive to this day. Hence, another reason for studying the Key monad is that a safety proof for it might be a stepping stone towards a safety proof of the ST monad.},
  isbn = {978-1-4503-4434-0},
  keywords = {Arrow notation,functional programming,Haskell,higher-order state,parametric hoas,ST monad},
  series = {Haskell 2016}
}

@incollection{Plotkin2009handlers,
  title = {Handlers of {{Algebraic Effects}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Plotkin, Gordon and Pretnar, Matija},
  editor = {Castagna, Giuseppe},
  year = {2009},
  month = jan,
  pages = {80--94},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We present an algebraic treatment of exception handlers and, more generally, introduce handlers for other computational effects representable by an algebraic theory. These include nondeterminism, interactive input/output, concurrency, state, time, and their combinations; in all cases the computation monad is the free-model monad of the theory. Each such handler corresponds to a model of the theory for the effects at hand. The handling construct, which applies a handler to a computation, is based on the one introduced by Benton and Kennedy, and is interpreted using the homomorphism induced by the universal property of the free model. This general construct can be used to describe previously unrelated concepts from both theory and practice.},
  copyright = {\textcopyright{}2009 Springer Berlin Heidelberg},
  isbn = {978-3-642-00589-3 978-3-642-00590-9},
  keywords = {Algorithm Analysis and Problem Complexity,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {5502},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Polakow2015embedding,
  title = {Embedding a {{Full Linear Lambda Calculus}} in {{Haskell}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Polakow, Jeff},
  year = {2015},
  pages = {177--188},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2804302.2804309},
  abstract = {We present an encoding of full linear lambda calculus in Haskell using higher order abstract syntax. By making use of promoted data kinds, multi-parameter type classes and functional dependencies, the encoding allows Haskell to do both linear type checking and linear type inference.},
  isbn = {978-1-4503-3808-0},
  keywords = {domain specific language,higher-order abstract syntax,linear lambda calculus},
  series = {Haskell '15}
}

@article{Polesiuk2018Logical,
  title = {Logical Relations for Coherence of Effect Subtyping},
  author = {Polesiuk, Piotr and Biernacki, Dariusz},
  year = {2018},
  month = jan,
  volume = {Volume 14, Issue 1},
  abstract = {A coercion semantics of a programming language with subtyping is typically defined on typing derivations rather than on typing judgments. To avoid semantic ambiguity, such a semantics is expected to be coherent, i.e., independent of the typing derivation for a given typing judgment. In this article we present heterogeneous, biorthogonal, step-indexed logical relations for establishing the coherence of coercion semantics of programming languages with subtyping. To illustrate the effectiveness of the proof method, we develop a proof of coherence of a type-directed, selective CPS translation from a typed call-by-value lambda calculus with delimited continuations and control-effect subtyping. The article is accompanied by a Coq formalization that relies on a novel shallow embedding of a logic for reasoning about step-indexing.},
  journal = {Logical Methods in Computer Science},
  language = {en}
}

@article{Poll2000coalgebraic,
  title = {A {{Coalgebraic Semantics}} of {{Subtyping}}},
  author = {Poll, Erik},
  year = {2000},
  volume = {33},
  pages = {276--293},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(05)80352-4},
  abstract = {Subtyping is a central notion in object-oriented programming. In this paper we investigate how the coalgebraic semantics of objects accounts for subtyping. We show that different characterisations of so-called behavioural subtyping found in the literature can conveniently be expressed in coalgebraic terms. We define subtyping between coalgebras and subtyping between coalgebraic specifications, and show that the latter is sound and complete w.r.t. the former. We also illustrate the subtle difference between the notions of subtyping and refinement.},
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CMCS}}'2000, {{Coalgebraic Methods}} in {{Computer Science}}}
}

@article{Poll2001coalgebraic,
  title = {A {{Coalgebraic Semantics}} of {{Subtyping}}},
  author = {Poll, Erik},
  year = {2001},
  volume = {35},
  pages = {61--81},
  doi = {10.1051/ita:2001109},
  abstract = {Coalgebras have been proposed as formal basis for the semantics of objects in the sense of object-oriented programming. This paper shows that this semantics provides a smooth interpretation for subtyping, a central notion in object-oriented programming. We show that different characterisations of behavioural subtyping found in the literature can conveniently be expressed in coalgebraic terms. We also investigate the subtle difference between behavioural subtyping and refinement.},
  journal = {RAI},
  number = {01}
}

@techreport{Pollack1997How,
  title = {How to {{Believe}} a {{Machine}}-{{Checked Proof}}},
  author = {Pollack, Robert},
  year = {1997},
  abstract = {Suppose I say ``Here is a machine-checked proof of Fermat's last theorem (FLT)''. How can you use my putative machine-checked proof as evidence for belief in FLT? This paper presents a technological approach for reducing the problem of believing a formal proof to the same psychological and philosophical issues as believing a conventional proof in a mathematics journal. I split the question of belief into two parts; asking whether the claimed proof is actually a derivation in the claimed formal system, and then whether what it proves is the claimed theorem. The first question is addressed by independent checking of formal proofs. I discuss how this approach extends the informal notion of proof as surveyable by human readers, and how surveyability of proofs reappears as the issue of feasibility of proof-checking. The second question has no technical answer; I discuss how it appears in verification of computer systems.}
}

@article{Pombrio2017Inferring,
  title = {Inferring {{Scope Through Syntactic Sugar}}},
  author = {Pombrio, Justin and Krishnamurthi, Shriram and Wand, Mitchell},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {44:1--44:28},
  issn = {2475-1421},
  doi = {10.1145/3110288},
  abstract = {Many languages use syntactic sugar to define parts of their surface language in terms of a smaller core. Thus some properties of the surface language, like its scoping rules, are not immediately evident. Nevertheless, IDEs, refactorers, and other tools that traffic in source code depend on these rules to present information to users and to soundly perform their operations. In this paper, we show how to lift scoping rules defined on a core language to rules on the surface, a process of scope inference. In the process we introduce a new representation of binding structure---scope as a preorder---and present a theoretical advance: proving that a desugaring system preserves {\^I}{$\pm$}-equivalence even though scoping rules have been provided only for the core language. We have also implemented the system presented in this paper.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {binding,Scope,syntactic sugar},
  number = {ICFP}
}

@inproceedings{Pottier2004polymorphic,
  title = {Polymorphic {{Typed Defunctionalization}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Pottier, Fran{\c c}ois and Gauthier, Nadji},
  year = {2004},
  pages = {89--98},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/964001.964009},
  abstract = {Defunctionalization is a program transformation that aims to turn a higher-order functional program into a first-order one, that is, to eliminate the use of functions as first-class values. Its purpose is thus identical to that of closure conversion. It differs from closure conversion, however, by storing a tag, instead of a code pointer, within every closure. Defunctionalization has been used both as a reasoning tool and as a compilation technique.Defunctionalization is commonly defined and studied in the setting of a simply-typed {$\lambda$}-calculus, where it is shown that semantics and well-typedness are preserved. It has been observed that, in the setting of a polymorphic type system, such as ML or System F, defunctionalization is not type-preserving. In this paper, we show that extending System F with guarded algebraic data types allows recovering type preservation. This result allows adding defunctionalization to the toolbox of type-preserving compiler writers.},
  isbn = {1-58113-729-X},
  keywords = {closure conversion,defunctionalization,polymorphism,type preservation},
  series = {{{POPL}} '04}
}

@article{Pottier2006Polymorphic,
  title = {Polymorphic Typed Defunctionalization and Concretization},
  author = {Pottier, Fran{\c c}ois and Gauthier, Nadji},
  year = {2006},
  month = mar,
  volume = {19},
  pages = {125--162},
  issn = {1388-3690, 1573-0557},
  doi = {10.1007/s10990-006-8611-7},
  abstract = {Defunctionalization is a program transformation that eliminates functions as first-class values. We show that defunctionalization can be viewed as a type-preserving transformation of an extension of F with guarded algebraic data types into itself. We also suggest that defunctionalization is an instance of concretization, a more general technique that allows eliminating constructs other than functions. We illustrate this point by presenting two new type-preserving transformations that can be viewed as instances of concretization. One eliminates R{\'e}my-style polymorphic records; the other eliminates the dictionary records introduced by the standard compilation scheme for Haskell's type classes.},
  journal = {Higher-Order and Symbolic Computation},
  language = {en},
  number = {1}
}

@inproceedings{Pottier2014hindleymilner,
  title = {Hindley-Milner {{Elaboration}} in {{Applicative Style}}: {{Functional Pearl}}},
  shorttitle = {Hindley-Milner {{Elaboration}} in {{Applicative Style}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Pottier, Fran{\c c}ois},
  year = {2014},
  pages = {203--212},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628145},
  abstract = {Type inference - the problem of determining whether a program is well-typed - is well-understood. In contrast, elaboration - the task of constructing an explicitly-typed representation of the program - seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.},
  isbn = {978-1-4503-2873-9},
  keywords = {constraints,elaboration,polymorphism,type inference},
  series = {{{ICFP}} '14}
}

@inproceedings{Premont2015Referential,
  title = {Referential {{Integrity}} with {{Scala Types}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Pr{\'e}mont, Patrick},
  year = {2015},
  pages = {30--34},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2774975.2774979},
  abstract = {Referential integrity constraints are critical elements of relational data models, and have found widespread use in industry. However, their benefits in terms of data integrity do not fully extend to program correctness. Constraint violations are identified at run-time and must then be handled appropriately by programs. We show how Scala can be used to build data models and programs where referential integrity is enforced at compile-time. Scala's type system, with its variance annotations and path-dependent types, is especially suited to express these constraints and proofs in a natural manner. We also explore potential compiler improvements that could enhance support for type-checked referential integrity.},
  isbn = {978-1-4503-3626-0},
  keywords = {Data structures,Dependent types,Referential integrity,Scala,Variance},
  series = {{{SCALA}} 2015}
}

@inproceedings{Puech2012safe,
  title = {Safe {{Incremental Type Checking}}},
  booktitle = {{{TLDI}} 2012 - {{Seventh ACM SIGPLAN Workshop}} on {{Types}} in {{Language Design}} and {{Implementation}}},
  author = {Puech, Matthias and {R{\'e}gis-Gianas}, Yann},
  year = {2012},
  month = jan,
  address = {{Philadelphia, United States}},
  abstract = {We study the problem of verifying the well-typing of terms, not in a batch fashion, as it is usually the case for typed languages, but incrementally, that is by sequentially modifying a term, and re- verifying each time only a smaller amount of information than the whole term, still ensuring that it is well-typed.},
  keywords = {incrementality,logical framework,type checking,ver- sion control},
  language = {English},
  note = {2 pages}
}

@inproceedings{Pugh1989incremental,
  title = {Incremental {{Computation}} via {{Function Caching}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Pugh, W. and Teitelbaum, T.},
  year = {1989},
  pages = {315--328},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/75277.75305},
  isbn = {0-89791-294-2},
  series = {{{POPL}} '89}
}

@article{Putnam1980Models,
  title = {Models and Reality},
  author = {Putnam, Hilary},
  year = {1980},
  month = sep,
  volume = {45},
  pages = {464--482},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2273415},
  abstract = {In 1922 Skolem delivered an address before the Fifth Congress of Scandinavian Mathematicians in which he pointed out what he called a ``relativity of set-theoretic notions''. This ``relativity'' has frequently been regarded as paradoxical; but today, although one hears the expression ``the L{\"o}wenheim-Skolem Paradox'', it seems to be thought of as only an apparent paradox, something the cognoscenti enjoy but are not seriously troubled by. Thus van Heijenoort writes, ``The existence of such a `relativity' is sometimes referred to as the L{\"o}wenheim-Skolem Paradox. But, of course, it is not a paradox in the sense of an antinomy; it is a novel and unexpected feature of formal systems.'' In this address I want to take up Skolem's arguments, not with the aim of refuting them but with the aim of extending them in somewhat the direction he seemed to be indicating. It is not my claim that the ``L{\"o}wenheim-Skolem Paradox'' is an antinomy in formal logic; but I shall argue that it is an antinomy, or something close to it, in philosophy of language. Moreover, I shall argue that the resolution of the antinomy\textemdash{}the only resolution that I myself can see as making sense\textemdash{}has profound implications for the great metaphysical dispute about realism which has always been the central dispute in the philosophy of language.The structure of my argument will be as follows: I shall point out that in many different areas there are three main positions on reference and truth: there is the extreme Platonist position, which posits nonnatural mental powers of directly ``grasping'' forms (it is characteristic of this position that ``understanding'' or ``grasping'' is itself an irreducible and unexplicated notion); there is the verificationist position which replaces the classical notion of truth with the notion of verification or proof, at least when it comes to describing how the language is understood; and there is the moderate realist position which seeks to preserve the centrality of the classical notions of truth and reference without postulating nonnatural mental powers.},
  journal = {The Journal of Symbolic Logic},
  language = {en},
  number = {3}
}

@article{Qi2009implementation,
  title = {An {{Implementation}} of the {{Language Lambda Prolog Organized}} around {{Higher}}-{{Order Pattern Unification}}},
  author = {Qi, Xiaochu},
  year = {2009},
  month = nov,
  abstract = {This thesis concerns the implementation of Lambda Prolog, a higher-order logic programming language that supports the lambda-tree syntax approach to representing and manipulating formal syntactic objects. Lambda Prolog achieves its functionality by extending a Prolog-like language by using typed lambda terms as data structures that it then manipulates via higher-order unification and some new program-level abstraction mechanisms. These additional features raise new implementation questions that must be adequately addressed for Lambda Prolog to be an effective programming tool. We consider these questions here, providing eventually a virtual machine and compilation based realization. A key idea is the orientation of the computation model of Lambda Prolog around a restricted version of higher-order unification with nice algorithmic properties and appearing to encompass most interesting applications. Our virtual machine embeds a treatment of this form of unification within the structure of the Warren Abstract Machine that is used in traditional Prolog implementations. Along the way, we treat various auxiliary issues such as the low-level representation of lambda terms, the implementation of reduction on such terms and the optimized processing of types in computation. We also develop an actual implementation of Lambda Prolog called Teyjus Version 2. A characteristic of this system is that it realizes an emulator for the virtual machine in the C language a compiler in the OCaml language. We present a treatment of the software issues that arise from this kind of mixing of languages within one system and we discuss issues relevant to the portability of our virtual machine emulator across arbitrary architectures. Finally, we assess the the efficacy of our various design ideas through experiments carried out using the system.},
  archivePrefix = {arXiv},
  eprint = {0911.5203},
  eprinttype = {arxiv},
  journal = {arXiv:0911.5203 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Qi2009Masked,
  title = {Masked {{Types}} for {{Sound Object Initialization}}},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Qi, Xin and Myers, Andrew C.},
  year = {2009},
  pages = {53--65},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1480881.1480890},
  abstract = {This paper presents a type-based solution to the long-standing problem of object initialization. Constructors, the conventional mechanism for object initialization, have semantics that are surprising to programmers and that lead to bugs. They also contribute to the problem of null-pointer exceptions, which make software less reliable. Masked types are a new type-state mechanism that explicitly tracks the initialization state of objects and prevents reading from uninitialized fields. In the resulting language, constructors are ordinary methods that operate on uninitialized objects, and no special default value (null) is needed in the language. Initialization of cyclic data structures is achieved with the use of conditionally masked types. Masked types are modular and compatible with data abstraction. The type system is presented in a simplified object calculus and is proved to soundly prevent reading from uninitialized fields. Masked types have been implemented as an extension to Java, in which compilation simply erases extra type information. Experience using the extended language suggests that masked types work well on real code.},
  isbn = {978-1-60558-379-2},
  keywords = {conditional masks,cyclic data structures,data abstraction,invariants,null pointer exceptions},
  series = {{{POPL}} '09}
}

@inproceedings{Qi2009sharing,
  title = {Sharing {{Classes Between Families}}},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Qi, Xin and Myers, Andrew C.},
  year = {2009},
  pages = {281--292},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1542476.1542508},
  abstract = {Class sharing is a new language mechanism for building extensible software systems. Recent work has separately explored two different kinds of extensibility: first, family inheritance, in which an entire family of related classes can be inherited, and second, adaptation, in which existing objects are extended in place with new behavior and state. Class sharing integrates these two kinds of extensibility mechanisms. With little programmer effort, objects of one family can be used as members of another, while preserving relationships among objects. Therefore, a family of classes can be adapted in place with new functionality spanning multiple classes. Object graphs can evolve from one family to another, adding or removing functionality even at run time. Several new mechanisms support this flexibility while ensuring type safety. Class sharing has been implemented as an extension to Java, and its utility for evolving and extending software is demonstrated with realistic systems.},
  isbn = {978-1-60558-392-1},
  keywords = {family inheritance,masked types,views},
  series = {{{PLDI}} '09}
}

@inproceedings{Qi2010homogeneous,
  title = {Homogeneous {{Family Sharing}}},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Qi, Xin and Myers, Andrew C.},
  year = {2010},
  pages = {520--538},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1869459.1869502},
  abstract = {Recent work has introduced class sharing as a mechanism for adapting a family of related classes with new functionality. This paper introduces homogeneous family sharing, implemented in the J\&h language, in which the sharing mechanism is lifted from class-level sharing to true family-level sharing. Compared to the original (heterogeneous) class sharing mechanism, homogeneous family sharing provides useful new functionality and substantially reduces the annotation burden on programmers by eliminating the need for masked types and sharing declarations. This is achieved through a new mechanism, shadow classes, which permit homogeneous sharing of all related classes in shared families. The new sharing mechanism has a straightforward semantics, which is formalized in the J\&h calculus. The soundness of the J\&h type system is proved. The J\&h language is implemented as an extension to the J\& language. To demonstrate the effectiveness of family sharing, the Polyglot compiler framework is ported to J\&h.},
  isbn = {978-1-4503-0203-6},
  keywords = {family inheritance,shadow classes,views},
  series = {{{OOPSLA}} '10}
}

@article{Rabe2013Logical,
  title = {Logical {{Relations}} for a {{Logical Framework}}},
  author = {Rabe, Florian and Sojakova, Kristina},
  year = {2013},
  month = nov,
  volume = {14},
  pages = {32:1--32:34},
  issn = {1529-3785},
  doi = {10.1145/2536740.2536741},
  abstract = {Logical relations are a central concept used to study various higher-order type theories and occur frequently in the proofs of a wide variety of meta-theorems. Besides extending the logical relation principle to more general languages, an important research question has been how to represent and thus verify logical relation arguments in logical frameworks. We formulate a theory of logical relations for Dependent Type Theory (DTT) with {$\beta$} {$\eta$}-equality which guarantees that any valid logical relation satisfies the Basic Lemma. Our definition is syntactic and reflective in the sense that a relation at a type is represented as a DTT type family but also permits expressing certain semantic definitions. We use the Edinburgh Logical Framework (LF) incarnation of DTT and implement our notion of logical relations in the type-checker Twelf. This enables us to formalize and mechanically decide the validity of logical relation arguments. Furthermore, our implementation includes a module system so that logical relations can be built modularly. We validate our approach by formalizing and verifying several syntactic and semantic meta-theorems in Twelf. Moreover, we show how object languages encoded in DTT can inherit a notion of logical relation from the logical framework.},
  journal = {ACM Trans. Comput. Logic},
  keywords = {Dependent type theory,LF,logical framework,logical relation,module system,parametricity,twelf},
  number = {4}
}

@article{Rahli2019Bar,
  title = {Bar {{Induction}} Is {{Compatible}} with {{Constructive Type Theory}}},
  author = {Rahli, Vincent and Bickford, Mark and Cohen, Liron and Constable, Robert L.},
  year = {2019},
  month = apr,
  volume = {66},
  pages = {13},
  issn = {0004-5411},
  doi = {10.1145/3305261},
  journal = {Journal of the ACM (JACM)},
  number = {2}
}

@article{Ramos2014sequences,
  title = {Sequences of {{Rewrites}}: {{A Categorical Interpretation}}},
  shorttitle = {Sequences of {{Rewrites}}},
  author = {Ramos, Arthur and {de Queiroz}, Ruy J. G. B. and {de Oliveira}, Anjolina G.},
  year = {2014},
  month = dec,
  abstract = {In Martin-L\textbackslash{}"of's Intensional Type Theory, identity type is a heavily used and studied concept. The reason for that is the fact that it's responsible for the recently discovered connection between Type Theory and Homotopy Theory. The main problem with identity types, as originally formulated, is that they are complex to understand and use. Using that fact as motivation, a much simpler formulation for the identity type was proposed by Anjolina de Oliveira and Ruy de Queiroz. In this formulation, an element of an identity type is seen as a sequence of rewrites (or computational paths). Together with the logical rules of this new entity, there exists a system of reduction rules between sequence of rewrites called LND\_EQS-RWS. This system is constructed using the labelled natural deduction (i.e. Prawitz' Natural Deduction plus derivations-as-terms) and is responsible for establishing how a sequence of rewrites can be rewritten, resulting in a new sequence of rewrites. In this context, we propose a categorical interpretation for this new entity, using the types as objects and the rules of rewrites as morphisms. Moreover, we show that our interpretation is in accordance with some known results, like that types have a groupoidal structure. We also interpret more complicated structures, like the one formed by a rewrite of a sequence of rewrites.},
  archivePrefix = {arXiv},
  eprint = {1412.2105},
  eprinttype = {arxiv},
  journal = {arXiv:1412.2105 [cs, math]},
  keywords = {Computer Science - Logic in Computer Science,Mathematics - Category Theory},
  primaryClass = {cs, math}
}

@inproceedings{Ramsey1996relocating,
  title = {Relocating {{Machine Instructions}} by {{Currying}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1996 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Ramsey, Norman},
  year = {1996},
  pages = {226--236},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/231379.231429},
  abstract = {Relocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machine-dependent object-file format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machine-dependence.The New Jersey Machine-Code Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instruction-set specification, the toolkit generates the bit-manipulating code. Instructions are described by constructors, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as "relocatable," meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machine-independent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machine-independent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machine-independent method of relocation.Currying rewrites a \&lambda;-term into two nested \&lambda;-terms. The standard implementation has the first \&lambda; allocate a closure and store therein its operands and a pointer to the second \&lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner \&lambda;-terms---one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second \&lambda; into the first makes many of the second \&lambda;s identical---a handful are enough for a whole instruction set. This optimization reduces the size of machine-dependent assembly and linking code by 15--20\% for the MIPS, SPARC, and PowerPC, and by about 30\% for the Pentium. It also makes the second \&lambda;s equivalent to relocating transformations named in standard object-file formats.},
  isbn = {0-89791-795-2},
  series = {{{PLDI}} '96}
}

@inproceedings{Ramsey2001algebraic,
  title = {An {{Algebraic Approach}} to {{File Synchronization}}},
  booktitle = {Proceedings of the 8th {{European Software Engineering Conference Held Jointly}} with 9th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Ramsey, Norman and Csirmaz, El\$\^''\$od},
  year = {2001},
  pages = {175--185},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/503209.503233},
  abstract = {A file synchronizer restores consistency after multiple replicas of a filesystem have been changed independently. We present an algebra for reasoning about operations on filesystems and show that it is sound and complete with respect to a simple model. The algebra enables us to specify a file-synchronization algorithm that can be combined with several different conflict-resolution policies. By contrast, previous work builds the conflict-resolution policy into the specification, or worse, does not specify the synchronizer's behavior precisely. We classify synchronizers by asking whether conflicts can be resolved at a single disconnected replica and whether all replicas are identical after synchronization. We also discuss timestamps and argue that there is no good way to propagate timestamps when there is severe clock skew between replicas.},
  isbn = {1-58113-390-1},
  series = {{{ESEC}}/{{FSE}}-9}
}

@inproceedings{Ramsey2002stochastic,
  title = {Stochastic {{Lambda Calculus}} and {{Monads}} of {{Probability Distributions}}},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ramsey, Norman and Pfeffer, Avi},
  year = {2002},
  pages = {154--165},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/503272.503288},
  abstract = {Probability distributions are useful for expressing the meanings of probabilistic languages, which support formal modeling of and reasoning about uncertainty. Probability distributions form a monad, and the monadic definition leads to a simple, natural semantics for a stochastic lambda calculus, as well as simple, clean implementations of common queries. But the monadic implementation of the expectation query can be much less efficient than current best practices in probabilistic modeling. We therefore present a language of measure terms, which can not only denote discrete probability distributions but can also support the best known modeling techniques. We give a translation of stochastic lambda calculus into measure terms. Whether one translates into the probability monad or into measure terms, the results of the translations denote the same probability distribution.},
  isbn = {1-58113-450-9},
  series = {{{POPL}} '02}
}

@article{Ramsey2006ml,
  title = {{{ML Module Mania}}: {{A Type}}-{{Safe}}, {{Separately Compiled}}, {{Extensible Interpreter}}},
  shorttitle = {{{ML Module Mania}}},
  author = {Ramsey, Norman},
  year = {2006},
  month = mar,
  volume = {148},
  pages = {181--209},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2005.11.045},
  abstract = {To illustrate the utility of a powerful modules language, this paper presents the embedded interpreter Lua-ML. The interpreter combines extensibility and separate compilation without compromising type safety. Its types are extended by applying a sum constructor to built-in types and to extensions, then tying a recursive knot using a two-level type; the sum constructor is written using an ML functor. The initial basis is extended by composing initialization functions from individual extensions, also using ML functors.},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {embedded interpreters,extensible interpreters,higher-order functors,ML modules,scripting languages},
  number = {2},
  series = {Proceedings of the {{ACM}}-{{SIGPLAN Workshop}} on {{ML}} ({{ML}} 2005) {{ACM}}-{{SIGPLAN Workshop}} on {{ML}} 2005}
}

@inproceedings{Ramsey2010hoopl,
  title = {Hoopl: {{A Modular}}, {{Reusable Library}} for {{Dataflow Analysis}} and {{Transformation}}},
  shorttitle = {Hoopl},
  booktitle = {Proceedings of the {{Third ACM Haskell Symposium}} on {{Haskell}}},
  author = {Ramsey, Norman and Dias, Jo{\~a}o and Peyton Jones, Simon},
  year = {2010},
  pages = {121--134},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1863523.1863539},
  abstract = {Dataflow analysis and transformation of control-flow graphs is pervasive in optimizing compilers, but it is typically entangled with the details of a particular compiler. We describe Hoopl, a reusable library that makes it unusually easy to define new analyses and transformations for any compiler written in Haskell. Hoopl's interface is modular and polymorphic, and it offers unusually strong static guarantees. The implementation encapsulates state-of-the-art algorithms (interleaved analysis and rewriting, dynamic error isolation), and it cleanly separates their tricky elements so that they can be understood independently.},
  isbn = {978-1-4503-0252-4},
  keywords = {dataflow},
  series = {Haskell '10}
}

@article{Rapoport2016Mutable,
  title = {Mutable {{WadlerFest DOT}}},
  author = {Rapoport, Marianna and Lhot{\'a}k, Ond{\v r}ej},
  year = {2016},
  month = nov,
  abstract = {The Dependent Object Types (DOT) calculus aims to model the essence of Scala, with a focus on abstract type members, path-dependent types, and subtyping. Other Scala features could be defined by translation to DOT. Mutation is a fundamental feature of Scala currently missing in DOT. Mutation in DOT is needed not only to model effectful computation and mutation in Scala programs, but even to precisely specify how Scala initializes immutable variables and fields (vals). We present an extension to DOT that adds typed mutable reference cells. We have proven the extension sound with a mechanized proof in Coq. We present the key features of our extended calculus and its soundness proof, and discuss the challenges that we encountered in our search for a sound design and the alternative solutions that we considered.},
  archivePrefix = {arXiv},
  eprint = {1611.07610},
  eprinttype = {arxiv},
  journal = {arXiv:1611.07610 [cs]},
  keywords = {_tablet,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Rapoport2017Simple,
  title = {A Simple Soundness Proof for Dependent Object Types},
  author = {Rapoport, Marianna and Kabir, Ifaz and He, Paul and Lhot{\'a}k, Ond{\v r}ej},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {46:1--46:27},
  issn = {2475-1421},
  doi = {10.1145/3133870},
  abstract = {Dependent Object Types (DOT) is intended to be a core calculus for modelling Scala. Its distinguishing feature is abstract type members, fields in objects that hold types rather than values. Proving soundness of DOT has been surprisingly challenging, and existing proofs are complicated, and reason about multiple concepts at the same time (e.g. types, values, evaluation). To serve as a core calculus for Scala, DOT should be easy to experiment with and extend, and therefore its soundness proof needs to be easy to modify.   This paper presents a simple and modular proof strategy for reasoning in DOT. The strategy separates reasoning about types from other concerns. It is centred around a theorem that connects the full DOT type system to a restricted variant in which the challenges and paradoxes caused by abstract type members are eliminated. Almost all reasoning in the proof is done in the intuitive world of this restricted type system. Once we have the necessary results about types, we observe that the other aspects of DOT are mostly standard and can be incorporated into a soundness proof using familiar techniques known from other calculi.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {_tablet,Computer Science - Programming Languages,dependent object types,DOT calculus,Scala,type safety},
  number = {OOPSLA}
}

@article{Rapoport2019path,
  title = {A Path to {{DOT}}: {{Formalizing}} Fully Path-Dependent Types},
  shorttitle = {A Path to Dot},
  author = {Rapoport, Marianna and Lhot{\'a}k, Ond{\v r}ej},
  year = {2019},
  month = apr,
  abstract = {The Dependent Object Types (DOT) calculus aims to formalize the Scala programming language with a focus on path-dependent types \$-\$ types such as \$x.a\_1\textbackslash{}dots a\_n.T\$ that depend on the runtime value of a path \$x.a\_1\textbackslash{}dots a\_n\$ to an object. Unfortunately, existing formulations of DOT can model only types of the form \$x.A\$ which depend on variables rather than general paths. This restriction makes it impossible to model nested module dependencies. Nesting small components inside larger ones is a necessary ingredient of a modular, scalable language. DOT's variable restriction thus undermines its ability to fully formalize a variety of programming-language features including Scala's module system, family polymorphism, and covariant specialization. This paper presents the pDOT calculus, which generalizes DOT to support types that depend on paths of arbitrary length, as well as singleton types to track path equality. We show that naive approaches to add paths to DOT make it inherently unsound, and present necessary conditions for such a calculus to be sound. We discuss the key changes necessary to adapt the techniques of the DOT soundness proofs so that they can be applied to pDOT. Our paper comes with a Coq-mechanized type-safety proof of pDOT. With support for paths of arbitrary length, pDOT can realize DOT's full potential for formalizing Scala-like calculi.},
  archivePrefix = {arXiv},
  eprint = {1904.07298},
  eprinttype = {arxiv},
  journal = {arXiv:1904.07298 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@misc{ratto,
  title = {Il Ratto Di {{Europa}} by {{Scuola Superiore}} Di {{Catania}}},
  abstract = {Watch Scuola Superiore di Catania's Il ratto di Europa on Livestream.com. Il seminario, tenuto da Andrea Scavo, ex allievo SSC, {\`e} organizzato dall'associazione Alumni della Scuola Superiore di Catania.}
}

@article{Reddy1996global,
  title = {Global State Considered Unnecessary: {{An}} Introduction to Object-Based Semantics},
  shorttitle = {Global State Considered Unnecessary},
  author = {Reddy, Uday S.},
  year = {1996},
  month = feb,
  volume = {9},
  pages = {7--76},
  issn = {0892-4635, 1573-0557},
  doi = {10.1007/BF01806032},
  abstract = {Semantics of imperative programming languages is traditionally described in terms of functions on global states. We propose here a novel approach based on a notion ofobjects and characterize them in terms of their observable behavior. States are regarded as part of the internal structure of objects and play no role in the observable behavior. It is shown that this leads to considerable accuracy in the semantic modelling of locality and single-threadedness properties of objects.},
  journal = {LISP and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),Denotational semantics,Imperative programs,Numeric Computing,Objects,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems,Syntactic control of interference},
  language = {en},
  number = {1}
}

@article{Reynolds1974relation,
  title = {On the {{Relation Between Direct}} and {{Continuation Semantics}}},
  author = {Reynolds, John},
  year = {1974},
  month = jan,
  journal = {Computer Science Department}
}

@incollection{Reynolds1974theory,
  title = {Towards a Theory of Type Structure},
  booktitle = {Programming {{Symposium}}},
  author = {Reynolds, John C.},
  editor = {Robinet, Prof B.},
  year = {1974},
  pages = {408--425},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}1974 Springer-Verlag},
  isbn = {978-3-540-06859-4 978-3-540-37819-8},
  keywords = {Computer Science; general},
  language = {en},
  number = {19},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Reynolds1978syntactic,
  title = {Syntactic {{Control}} of {{Interference}}},
  booktitle = {Proceedings of the 5th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Reynolds, John C.},
  year = {1978},
  pages = {39--46},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/512760.512766},
  abstract = {In programming languages which permit both assignment and procedures, distinct identifiers can represent data structures which share storage or procedures with interfering side effects. In addition to being a direct source of programming errors, this phenomenon, which we call interference can impact type structure and parallelism. We show how to eliminate these difficulties by imposing syntactic restrictions, without prohibiting the kind of constructive interference which occurs with higher-order procedures or SIMULA classes. The basic idea is to prohibit interference between identifiers, but to permit interference among components of collections named by single identifiers.},
  series = {{{POPL}} '78}
}

@incollection{Reynolds1984polymorphism,
  title = {Polymorphism Is Not Set-Theoretic},
  booktitle = {Semantics of {{Data Types}}},
  author = {Reynolds, John C.},
  editor = {Kahn, Gilles and MacQueen, David B. and Plotkin, Gordon},
  year = {1984},
  month = jan,
  pages = {145--156},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The polymorphic, or second-order, typed lambda calculus is an extension of the typed lambda calculus in which polymorphic functions can be defined. In this paper, we will prove that the standard set-theoretic model of the ordinary typed lambda calculus cannot be extended to model this language extension.},
  copyright = {\textcopyright{}1984 Springer-Verlag},
  isbn = {978-3-540-13346-9 978-3-540-38891-3},
  keywords = {Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters},
  number = {173},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Reynolds1985three,
  title = {Three Approaches to Type Structure},
  booktitle = {Mathematical {{Foundations}} of {{Software Development}}},
  author = {Reynolds, John C.},
  editor = {Ehrig, Hartmut and Floyd, Christiane and Nivat, Maurice and Thatcher, James},
  year = {1985},
  pages = {97--138},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We examine three disparate views of the type structure of programming languages: Milner's type deduction system and polymorphic let construct, the theory of subtypes and generic operators, and the polymorphic or second-order typed lambda calculus. These approaches are illustrated with a functional language including product, sum and list constructors. The syntactic behavior of types is formalized with type inference rules, but their semantics is treated intuitively.},
  copyright = {\textcopyright{}1985 Springer-Verlag},
  isbn = {978-3-540-15198-2 978-3-540-39302-3},
  keywords = {Combinatorics,Logics and Meanings of Programs,Software Engineering},
  language = {en},
  number = {185},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Reynolds1989syntactic,
  title = {Syntactic Control of Interference {{Part}} 2},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Reynolds, John C.},
  editor = {Ausiello, Giorgio and {Dezani-Ciancaglini}, Mariangiola and Rocca, Simonetta Ronchi Della},
  year = {1989},
  month = jan,
  pages = {704--722},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In 1978, we proposed that Algol-like languages should be constrained so that aliasing between variables and, more generally, interference between commands or procedures would be syntactically detectable in a fail-safe manner. In particular, we proposed syntactic restrictions that prohibited interference between distinct identifiers, while permitting interference between qualifications of the same identifier. However, these restrictions had the unfortunate property that syntactic correctness was not preserved by beta reduction. In the present paper, we show how this difficulty can be avoided by the use of a variant of conjunctive types. We also give an algorithm for typechecking explicitly typed programs.},
  copyright = {\textcopyright{}1989 Springer-Verlag},
  isbn = {978-3-540-51371-1 978-3-540-46201-9},
  keywords = {Algorithm Analysis and Problem Complexity,Combinatorics,Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Processor Architectures},
  language = {en},
  number = {372},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Reynolds1993functors,
  title = {On {{Functors Expressible}} in the {{Polymorphic Typed Lambda Calculus}}},
  author = {Reynolds, J. C. and Plotkin, G. D.},
  year = {1993},
  month = jul,
  volume = {105},
  pages = {1--29},
  issn = {0890-5401},
  doi = {10.1006/inco.1993.1037},
  abstract = {Given a model of the polymorphic typed lambda calculus based upon a Cartesian closed category K, there will be functors from K to K whose action on objects can be expressed by type expressions and whose action on morphisms can be expressed by ordinary expressions. We show that if T is such a functor then there is a weak initial T-algebra and if, in addition, K possesses equalizers of all subsets of its morphism sets, then there is an initial T-algebra. These results are used to establish the impossibility of certain models, including those in which types denote sets and S \textrightarrow{} S{${'}$} denotes the set of all functions from S to S{${'}$}.},
  journal = {Information and Computation},
  number = {1}
}

@inproceedings{Reynolds2000intuitionistic,
  title = {Intuitionistic {{Reasoning}} about {{Shared Mutable Data Structure}}},
  booktitle = {Millennial {{Perspectives}} in {{Computer Science}}},
  author = {Reynolds, John C.},
  year = {2000},
  pages = {303--321},
  publisher = {{Palgrave}},
  abstract = {Drawing upon early work by Burstall, we extend Hoare's approach  to proving the correctness of imperative programs, to deal with programs  that perform destructive updates to data structures containing  more than one pointer to the same location. The key concept is an  "independent conjunction" P \& Q that holds only when P and Q  are both true and depend upon distinct areas of storage. To make  this concept precise we use an intuitionistic logic of assertions, with a  Kripke semantics whose possible worlds are heaps (mapping locations  into tuples of values).}
}

@article{Reynolds2000meaning,
  title = {The {{Meaning}} of {{Types From Intrinsic}} to {{Extrinsic Semantics}}},
  author = {Reynolds, John},
  year = {2000},
  month = dec,
  journal = {BRICS Report Series}
}

@inproceedings{Reynolds2002Separation,
  title = {Separation Logic: A Logic for Shared Mutable Data Structures},
  shorttitle = {Separation Logic},
  booktitle = {Proceedings 17th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Reynolds, J. C.},
  year = {2002},
  month = jul,
  pages = {55--74},
  doi = {10.1109/LICS.2002.1029817},
  abstract = {In joint work with Peter O'Hearn and others, based on early ideas of Burstall, we have developed an extension of Hoare logic that permits reasoning about low-level imperative programs that use shared mutable data structure. The simple imperative programming language is extended with commands (not expressions) for accessing and modifying shared structures, and for explicit allocation and deallocation of storage. Assertions are extended by introducing a "separating conjunction" that asserts that its subformulas hold for disjoint parts of the heap, and a closely related "separating implication". Coupled with the inductive definition of predicates on abstract data structures, this extension permits the concise and flexible description of structures with controlled sharing. In this paper, we survey the current development of this program logic, including extensions that permit unrestricted address arithmetic, dynamically allocated arrays, and recursive procedures. We also discuss promising future directions.},
  keywords = {address arithmetic,Arithmetic,Artificial intelligence,Bibliographies,computational complexity,Computer languages,Computer science,data structures,Data structures,formal logic,heap,Hoare logic,imperative programming language,Logic arrays,Logic programming,low-level imperative programs,program logic,Programmable logic arrays,reasoning,recursive procedures,Reflection,separation logic,shared mutable data structures}
}

@incollection{Reynolds2003what,
  title = {What Do Types Mean? \textemdash{} {{From}} Intrinsic to Extrinsic Semantics},
  shorttitle = {What Do Types Mean?},
  booktitle = {Programming {{Methodology}}},
  author = {Reynolds, John C.},
  editor = {McIver, Annabelle and Morgan, Carroll},
  year = {2003},
  month = jan,
  pages = {309--327},
  publisher = {{Springer New York}},
  abstract = {A definition of a typed language is said to be ``intrinsic'' if it assigns meanings to typings rather than arbitrary phrases, so that ill-typed phrases are meaningless. In contrast, a definition is said to be ``extrinsic'' if all phrases have meanings that are independent of their typings, while typings represent properties of these meanings. For a simply typed lambda calculus, extended with integers, recursion, and conditional expressions, we give an intrinsic denotational semantics and a denotational semantics of the underlying untyped language. We then estab?lish a logical relations theorem between these two semantics, and show that the logical relations can be ``bracketed'' by retractions between the domains of the two semantics. From these results, we derive an extrinsic semantics that uses partial equivalence relations.},
  copyright = {\textcopyright{}2003 Springer-Verlag New York},
  isbn = {978-1-4419-2964-8 978-0-387-21798-7},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  series = {Monographs in {{Computer Science}}}
}

@incollection{Reynolds2009using,
  title = {Using {{Category Theory}} to {{Design Programming Languages}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Reynolds, John C.},
  editor = {Castagna, Giuseppe},
  year = {2009},
  month = jan,
  pages = {62--63},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In a 1980 paper entitled ``Using Category Theory to Design Conversions and Generic Operators'', the author showed how the concepts of category theory can guide the design of a programming language to avoid anomalies in the interaction of implicit conversions and generic operators.},
  copyright = {\textcopyright{}2009 Springer Berlin Heidelberg},
  isbn = {978-3-642-00589-3 978-3-642-00590-9},
  keywords = {Algorithm Analysis and Problem Complexity,Logics and Meanings of Programs,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {5502},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Richard2007OOMatch,
  title = {{{OOMatch}}: {{Pattern}} Matching as Dispatch in {{Java}}},
  shorttitle = {{{OOMatch}}},
  booktitle = {Companion to the {{22Nd ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}} and {{Applications Companion}}},
  author = {Richard, Adam and Lhotak, Ondrej},
  year = {2007},
  pages = {771--772},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1297846.1297880},
  abstract = {We present an extension to Java, dubbed OOMatch. It allows method parameters to be specified as patterns, which are matched against the arguments to the method call. When matches occur, the method applies; if multiple methods apply, the method with the more specific pattern overrides the others.},
  isbn = {978-1-59593-865-7},
  keywords = {dynamic dispatch,Java,multimethods,pattern matching,predicate dispatch},
  series = {{{OOPSLA}} '07}
}

@inproceedings{Ricketts2014automating,
  title = {Automating {{Formal Proofs}} for {{Reactive Systems}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Ricketts, Daniel and Robert, Valentin and Jang, Dongseok and Tatlock, Zachary and Lerner, Sorin},
  year = {2014},
  pages = {452--462},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594338},
  abstract = {Implementing systems in proof assistants like Coq and proving their correctness in full formal detail has consistently demonstrated promise for making extremely strong guarantees about critical software, ranging from compilers and operating systems to databases and web browsers. Unfortunately, these verifications demand such heroic manual proof effort, even for a single system, that the approach has not been widely adopted. We demonstrate a technique to eliminate the manual proof burden for verifying many properties within an entire class of applications, in our case reactive systems, while only expending effort comparable to the manual verification of a single system. A crucial insight of our approach is simultaneously designing both (1) a domain-specific language (DSL) for expressing reactive systems and their correctness properties and (2) proof automation which exploits the constrained language of both programs and properties to enable fully automatic, pushbutton verification. We apply this insight in a deeply embedded Coq DSL, dubbed Reflex, and illustrate Reflex's expressiveness by implementing and automatically verifying realistic systems including a modern web browser, an SSH server, and a web server. Using Reflex radically reduced the proof burden: in previous, similar versions of our benchmarks written in Coq by experts, proofs accounted for over 80\% of the code base; our versions require no manual proofs.},
  isbn = {978-1-4503-2784-8},
  keywords = {dependent types,domain-specific languages,interactive proof assistants,proof automation,reactive systems},
  series = {{{PLDI}} '14}
}

@inproceedings{Robinson1994reflexive,
  title = {Reflexive Graphs and Parametric Polymorphism},
  booktitle = {, {{Symposium}} on {{Logic}} in {{Computer Science}}, 1994. {{LICS}} '94. {{Proceedings}}},
  author = {Robinson, E.P. and Rosolini, G.},
  year = {1994},
  month = jul,
  pages = {364--371},
  doi = {10.1109/LICS.1994.316053},
  abstract = {The pioneering work on relational parametricity for the second order lambda calculus was done by Reynolds (1983) under the assumption of the existence of set-based models, and subsequently reformulated by him, in conjunction with his student Ma, using the technology of PL-categories. The aim of this paper is to use the different technology of internal category theory to re-examine Ma and Reynolds' definitions. Apart from clarifying some of their constructions, this view enables us to prove that if we start with a non-parametric model which is left exact and which satisfies a completeness condition corresponding to Ma and Reynolds ``suitability for polymorphism'', then we can recover a parametric model with the same category of closed types. This implies, for example, that any suitably complete model (such as the PER model) has a parametric counterpart},
  keywords = {Calculus,category theory,closed types,completeness condition,formal logic,Indexing,internal category theory,lambda calculus,nonparametric model,parametric model,parametric polymorphism,PER model,polymorphism,reflexive graphs,relational parametricity,second order lambda calculus,set theory,set-based models,Space technology,type theory}
}

@article{Rodin2007categorical,
  title = {On {{Categorical Theory}}-{{Building}}: {{Beyond}} the {{Formal}}},
  shorttitle = {On {{Categorical Theory}}-{{Building}}},
  author = {Rodin, Andrei},
  year = {2007},
  month = jul,
  abstract = {I propose a notion of theory motivated by Category theory.},
  archivePrefix = {arXiv},
  eprint = {0707.3745},
  eprinttype = {arxiv},
  journal = {arXiv:0707.3745 [math]},
  keywords = {00A30,Mathematics - Category Theory,Mathematics - History and Overview},
  primaryClass = {math}
}

@inproceedings{Rompf2010lightweight,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2010},
  pages = {127--136},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1868294.1868314},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  isbn = {978-1-4503-0154-1},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  series = {{{GPCE}} '10}
}

@article{Rompf2012lightweight,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  shorttitle = {Lightweight {{Modular Staging}}},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2012},
  month = jun,
  volume = {55},
  pages = {121--130},
  issn = {0001-0782},
  doi = {10.1145/2184319.2184345},
  abstract = {Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources. Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize. We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework. LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
  journal = {Commun. ACM},
  number = {6}
}

@inproceedings{Rompf2014surgical,
  title = {Surgical {{Precision JIT Compilers}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
  year = {2014},
  pages = {41--52},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594316},
  abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable. In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks. We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction. In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
  isbn = {978-1-4503-2784-8},
  keywords = {JIT compilation,program generation,staging},
  series = {{{PLDI}} '14}
}

@article{Rompf2015DOT,
  title = {From {{F}} to {{DOT}}: {{Type}} Soundness Proofs with Definitional Interpreters},
  shorttitle = {From f to Dot},
  author = {Rompf, Tiark and Amin, Nada},
  year = {2015},
  month = oct,
  abstract = {Scala's type system unifies ML modules, object-oriented, and functional programming. The Dependent Object Types (DOT) family of calculi has been proposed as a new foundation for Scala and similar languages. Unfortunately, it is not clear how DOT relates to any well-known type systems, and type soundness has only been established for very restricted subsets. In fact, important Scala features are known to break at least one key metatheoretic property such as environment narrowing or subtyping transitivity, which are usually required for a type soundness proof. First, and, perhaps surprisingly, we show how rich DOT calculi can still be proved sound. The key insight is that narrowing and subtyping transitivity only need to hold for runtime objects, but not for code that is never executed. Alas, the dominant method of proving type soundness, Wright and Felleisen's syntactic approach, is based on term rewriting, which does not a priori make a distinction between runtime and type assignment time. Second, we demonstrate how type soundness can be proved for advanced, polymorphic, type systems with respect to high-level, definitional interpreters, implemented in Coq. We present the first mechanized soundness proof in this style for System F{$<$}: and several extensions, including mutable references. Our proofs use only simple induction: another surprising result, as the combination of big-step semantics, mutable references, and polymorphism is commonly believed to require co-inductive proof techniques. Third, we show how DOT-like calculi emerge as generalizations of F{$<$}:, exposing a rich design space of calculi with path-dependent types which we collectively call System D. Armed with insights from the definitional interpreter semantics, we also show how equivalent small-step semantics and soundness proofs in Wright-Felleisen-style can be derived for these systems.},
  archivePrefix = {arXiv},
  eprint = {1510.05216},
  eprinttype = {arxiv},
  journal = {arXiv:1510.05216 [cs]},
  keywords = {_tablet,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Rompf2015functional,
  title = {Functional {{Pearl}}: {{A SQL}} to {{C Compiler}} in 500 {{Lines}} of {{Code}}},
  shorttitle = {Functional {{Pearl}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Rompf, Tiark and Amin, Nada},
  year = {2015},
  pages = {2--9},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784760},
  abstract = {We present the design and implementation of a SQL query processor that outperforms existing database systems and is written in just about 500 lines of Scala code -- a convincing case study that high-level functional programming can handily beat C for systems-level programming where the last drop of performance matters. The key enabler is a shift in perspective towards generative programming. The core of the query engine is an interpreter for relational algebra operations, written in Scala. Using the open-source LMS Framework (Lightweight Modular Staging), we turn this interpreter into a query compiler with very low effort. To do so, we capitalize on an old and widely known result from partial evaluation known as Futamura projections, which state that a program that can specialize an interpreter to any given input program is equivalent to a compiler. In this pearl, we discuss LMS programming patterns such as mixed-stage data structures (e.g. data records with static schema and dynamic field components) and techniques to generate low-level C code, including specialized data structures and data loading primitives.},
  isbn = {978-1-4503-3669-7},
  keywords = {Futamura projections,generative programming,Query Compilation,sql,staging},
  series = {{{ICFP}} 2015}
}

@inproceedings{Rompf2015go,
  title = {Go {{Meta}}! {{A Case}} for {{Generative Programming}} and {{DSLs}} in {{Performance Critical Systems}}},
  booktitle = {1st {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2015)},
  author = {Rompf, Tiark and Brown, Kevin J. and Lee, HyoukJoong and Sujeeth, Arvind K. and Jonnalagedda, Manohar and Amin, Nada and Ofenbeck, Georg and Stojanov, Alen and Klonatos, Yannis and Dashti, Mohammad and Koch, Christoph and P{\"u}schel, Markus and Olukotun, Kunle},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  year = {2015},
  volume = {32},
  pages = {238--261},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.238},
  isbn = {978-3-939897-80-4},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-50295}
}

@incollection{Rompf2016Essence,
  title = {The {{Essence}} of {{Multi}}-Stage {{Evaluation}} in {{LMS}}},
  booktitle = {A {{List}} of {{Successes That Can Change}} the {{World}}},
  author = {Rompf, Tiark},
  editor = {Lindley, Sam and McBride, Conor and Trinder, Phil and Sannella, Don},
  year = {2016},
  pages = {318--335},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-30936-1_17},
  abstract = {Embedded domain-specific languages (DSLs) are the subject of wide-spread interest, and a variety of implementation techniques exist. Some of them have been invented, and some of them discovered. Many are based on a form of generative or multi-stage programming, where the host language program builds up DSL terms during its evaluation. In this paper, we examine the execution model of LMS (Lightweight Modular Staging), a framework for embedded DSLs in Scala, and link it to evaluation in a two-stage lambda calculus. This clarifies the semantics of certain ad-hoc implementation choices, and provides guidance for implementing similar multi-stage evaluation facilities in other languages.},
  copyright = {\textcopyright{}2016 Springer International Publishing Switzerland},
  isbn = {978-3-319-30935-4 978-3-319-30936-1},
  keywords = {domain-specific languages,LMS (Lightweight Modular Staging),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,multi-stage programming,partial evaluation,Programming Languages; Compilers; Interpreters,Programming Techniques,Scala,Software Engineering},
  language = {en},
  number = {9600},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Rompf2016Type,
  title = {Type Soundness for Dependent Object Types ({{DOT}})},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Rompf, Tiark and Amin, Nada},
  year = {2016},
  pages = {624--641},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2983990.2984008},
  abstract = {Scala's type system unifies aspects of ML modules, object- oriented, and functional programming. The Dependent Object Types (DOT) family of calculi has been proposed as a new theoretic foundation for Scala and similar expressive languages. Unfortunately, type soundness has only been established for restricted subsets of DOT. In fact, it has been shown that important Scala features such as type refinement or a subtyping relation with lattice structure break at least one key metatheoretic property such as environment narrowing or invertible subtyping transitivity, which are usually required for a type soundness proof. The main contribution of this paper is to demonstrate how, perhaps surprisingly, even though these properties are lost in their full generality, a rich DOT calculus that includes recursive type refinement and a subtyping lattice with intersection types can still be proved sound. The key insight is that subtyping transitivity only needs to be invertible in code paths executed at runtime, with contexts consisting entirely of valid runtime objects, whereas inconsistent subtyping contexts can be permitted for code that is never executed.},
  isbn = {978-1-4503-4444-9},
  keywords = {_tablet,dependent object types,DOT,Scala,soundness},
  series = {{{OOPSLA}} 2016}
}

@inproceedings{Rossberg2010fing,
  title = {F-Ing {{Modules}}},
  booktitle = {Proceedings of the 5th {{ACM SIGPLAN Workshop}} on {{Types}} in {{Language Design}} and {{Implementation}}},
  author = {Rossberg, Andreas and Russo, Claudio V. and Dreyer, Derek},
  year = {2010},
  pages = {89--102},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1708016.1708028},
  abstract = {ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being "complex" and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we give a very simple elaboration semantics for a full-featured, higher-order ML-like module language. Our elaboration defines the meaning of module expressions by a straightforward, compositional translation into vanilla System F-{$\omega$} (the higher-order polymorphic {$\lambda$}-calculus), under plain F-{$\omega$} typing environments. We thereby show that ML modules are merely a particular mode of use of System F-{$\omega$}. Our module language supports the usual second-class modules with Standard ML-style generative functors and local module definitions. To demonstrate the versatility of our approach, we further extend the language with the ability to package modules as first-class values---a very simple extension, as it turns out. Our approach also scales to handle OCaml-style applicative functor semantics, but the details are significantly more subtle, so we leave their presentation to a future, expanded version of this paper. Lastly, we report on our experience using the "locally nameless" approach in order to mechanize the soundness of our elaboration semantics in Coq.},
  isbn = {978-1-60558-891-9},
  keywords = {abstract data types,elaboration,existential types,first-class modules,ML modules,system f,type systems},
  series = {{{TLDI}} '10}
}

@article{Rossberg2013mixin,
  title = {Mixin' up the {{ML}} Module System},
  author = {Rossberg, Andreas and Dreyer, Derek},
  year = {2013},
  month = apr,
  volume = {35},
  pages = {2:1--2:84},
  issn = {0164-0925},
  doi = {10.1145/2450136.2450137},
  abstract = {ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent ML-style data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role. We provide a declarative type system for MixML, including two important extensions: higher-order modules, and modules as first-class values. We also present a sound and complete, three-pass type-checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG---namely, a polymorphic lambda calculus with single-assignment references and recursive type generativity---which employs a linear type and kind system to track definedness of term and type imports.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {abstract data types,hierarchical composability,mixin modules,ML modules,recursive modules,type systems},
  number = {1}
}

@article{Rossberg2014fing,
  title = {F-Ing Modules},
  author = {Rossberg, Andreas and Russo, Claudio and Dreyer, Derek},
  year = {2014},
  volume = {24},
  pages = {529--607},
  issn = {1469-7653},
  doi = {10.1017/S0956796814000264},
  abstract = {ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being ``complex'' and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we present a novel formalization of ML modules, which defines their semantics directly by a compositional ``elaboration'' translation into plain System F{$\omega$} (the higher-order polymorphic {$\lambda$}-calculus). To demonstrate the scalability of our ``F-ing'' semantics, we use it to define a representative, higher-order ML-style module language, encompassing all the major features of existing ML module dialects (except for recursive modules). We thereby show that ML modules are merely a particular mode of use of System F{$\omega$}. To streamline the exposition, we present the semantics of our module language in stages. We begin by defining a subset of the language supporting a Standard ML-like language with second-class modules and generative functors. We then extend this sublanguage with the ability to package modules as first-class values (a very simple extension, as it turns out) and OCaml-style applicative functors (somewhat harder). Unlike previous work combining both generative and applicative functors, we do not require two distinct forms of functor or signature sealing. Instead, whether a functor is applicative or not depends only on the computational purity of its body. In fact, we argue that applicative/generative is rather incidental terminology for pure versus impure functors. This approach results in a semantics that we feel is simpler and more natural than previous accounts, and moreover prohibits breaches of abstraction safety that were possible under them.},
  journal = {Journal of Functional Programming},
  number = {05}
}

@inproceedings{Rossberg20151ml,
  title = {{{1ML}} \textendash{} {{Core}} and Modules United ({{F}}-Ing First-Class Modules)},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Rossberg, Andreas},
  year = {2015},
  pages = {35--47},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784738},
  abstract = {ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it reduces expressiveness. For example, selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent, are syntactically cumbersome, and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values, and core and module layer are unified into one language. In this "1ML", functions, functors, and even type constructors are one and the same construct; likewise, no distinction is made between structures, records, or tuples. Or viewed the other way round, everything is just ("a mode of use of") modules. Yet, 1ML does not require dependent types, and its type structure is expressible in terms of plain System F{$\omega$}, in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML, and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System F{$\omega$} that allows combining term and type abstraction in a more compositional manner than the bare calculus.},
  isbn = {978-1-4503-3669-7},
  keywords = {abstract data types,elaboration,existential types,first-class modules,ML modules,system f,type systems},
  series = {{{ICFP}} 2015}
}

@article{Rossberg20181ML,
  title = {{{1ML}} \textemdash{} {{Core}} and Modules United},
  author = {Rossberg, Andreas},
  year = {2018/ed},
  volume = {28},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796818000205},
  abstract = {ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures, and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it imposes seemingly unnecessary limits on expressiveness because it makes modules second-class citizens of the language. For example, selecting one among several possible modules implementing a given interface cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent and tend to be even more syntactically heavyweight than using second-class modules alone. We propose a redesign of ML in which modules are truly first-class values, and core and module layers are unified into one language. In this ``1ML'', functions, functors, and even type constructors are one and the same construct; likewise, no distinction is needed between structures, records, or tuples. Or viewed the other way round, everything is just (``a mode of use of'') modules. Yet, 1ML does not require dependent types: its type structure is expressible in terms of plain System F{$\omega$}, with a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML and an extension with Damas\textendash{}Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML.},
  journal = {Journal of Functional Programming},
  language = {en}
}

@inproceedings{Rowe2019Characterising,
  title = {Characterising {{Renaming}} within {{OCaml}}'s {{Module System}}: {{Theory}} and {{Implementation}}},
  shorttitle = {Characterising {{Renaming}} within {{OCaml}}'s {{Module System}}},
  booktitle = {{{PLDI}} '19: {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Rowe, Reuben and F{\'e}r{\'e}e, Hugo and Thompson, Simon and Owens, Scott},
  year = {2019},
  month = apr,
  address = {{Phoenix, Arizona, United States}},
  doi = {Rowe, Reuben and F\%00e9r\%00e9e, Hugo and Thompson, Simon and Owens, Scott  (2019)   Characterising Renaming within OCaml\%0019s Module System: Theory and Implementation.      In:   PLDI '19: ACM SIGPLAN Conference on Programming Language Design and Implementation Proceedings.          (In press)   (doi:https://doi.org/10.1145/3314221.3314600 <https://doi.org/10.1145/3314221.3314600>)  (Full text available)},
  abstract = {We present an abstract, set-theoretic denotational semantics
for a significant subset of OCaml and its module system
in order to reason about the correctness of renaming value
bindings. Our abstract semantics captures information about
the binding structure of programs. Crucially for renaming, it
also captures information about the relatedness of different
declarations that is induced by the use of various different
language constructs (e.g. functors, module types and module constraints). Correct renamings are precisely those that
preserve this structure. We demonstrate that our semantics
allows us to prove various high-level, intuitive properties
of renamings. We also show that it is sound with respect to
a (domain-theoretic) denotational model of the operational
behaviour of programs. This formal framework has been
implemented in a prototype refactoring tool for OCaml that
performs renaming},
  language = {en}
}

@article{Rowe2019ROTOR,
  title = {{{ROTOR}}: {{A Tool}} for {{Renaming Values}} in {{OCaml}}'s {{Module System}}},
  author = {Rowe, Reuben N S and Feree, Hugo and Thompson, Simon J and Owens, Scott},
  year = {2019},
  pages = {4},
  abstract = {The functional programming paradigm presents its own unique challenges to refactoring. For the OCaml language in particular, the expressiveness of its module system makes this a highly non-trivial task and there is currently no automated support for large-scale refactoring in the OCaml language.},
  language = {en}
}

@inproceedings{Royce1987managing,
  title = {Managing the {{Development}} of {{Large Software Systems}}: {{Concepts}} and {{Techniques}}},
  shorttitle = {Managing the {{Development}} of {{Large Software Systems}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Software Engineering}}},
  author = {Royce, W. W.},
  year = {1987},
  pages = {328--338},
  publisher = {{IEEE Computer Society Press}},
  address = {{Los Alamitos, CA, USA}},
  isbn = {0-89791-216-0},
  series = {{{ICSE}} '87}
}

@article{Rummelhoff2004Polynat,
  title = {Polynat in {{PER}} Models},
  author = {Rummelhoff, Ivar},
  year = {2004},
  month = may,
  volume = {316},
  pages = {215--224},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2004.01.031},
  abstract = {The polymorphic lambda-calculus can be modelled using PERs on a partial combinatory algebra. We say that the type of natural numbers (polynat) is polymorphically standard in such a model if the interpretation of the type only contains (the interpretations of) the Church numerals. We show that this is not always the case by constructing an explicit counterexample. On the other hand, when the PCA has either (strong) equality or weak equality plus a form of continuity, we show polynat is standard.},
  journal = {Theoretical Computer Science},
  keywords = {PER,polymorphism,semantics,type theory},
  number = {1},
  series = {Recent {{Developments}} in {{Domain Theory}}: {{A}} Collection of Papers in Honour of {{Dana S}}. {{Scott}}}
}

@incollection{Rytz2012lightweight,
  title = {Lightweight {{Polymorphic Effects}}},
  booktitle = {{{ECOOP}} 2012 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Rytz, Lukas and Odersky, Martin and Haller, Philipp},
  editor = {Noble, James},
  year = {2012},
  month = jan,
  pages = {258--282},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Type-and-effect systems are a well-studied approach for reasoning about the computational behavior of programs. Nevertheless, there is only one example of an effect system that has been adopted in a wide-spread industrial language: Java's checked exceptions. We believe that the main obstacle to using effect systems in day-to-day programming is their verbosity, especially when writing functions that are polymorphic in the effect of their argument. To overcome this issue, we propose a new syntactically lightweight technique for writing effect-polymorphic functions. We show its independence from a specific kind of side-effect by embedding it into a generic and extensible framework for checking effects of multiple domains. Finally, we verify the expressiveness and practicality of the system by implementing it for the Scala programming language.},
  copyright = {\textcopyright{}2012 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  keywords = {Computer Communication Networks,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {7313},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Rytz2013flowinsensitive,
  title = {A {{Flow}}-Insensitive, {{Modular Effect System}} for {{Purity}}},
  booktitle = {Proceedings of the 15th {{Workshop}} on {{Formal Techniques}} for {{Java}}-like {{Programs}}},
  author = {Rytz, Lukas and Amin, Nada and Odersky, Martin},
  year = {2013},
  pages = {4:1--4:7},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2489804.2489808},
  abstract = {This article presents a modular, flow-insensitive type-and-effect system for purity with lightweight annotations. It does not enforce a global programming discipline and allows arbitrary effects to occur in impure parts of the program. The system is designed to support higher-order languages that mix functional and imperative code like Scala or C\#. We show that it can express purity of non-local programming patterns which involve mutable state such as those used in the Scala collections library. We formalize the type system using a functional language with mutable records and define type and effect soundness.},
  isbn = {978-1-4503-2042-9},
  keywords = {purity,type-and-effect systems},
  series = {{{FTfJP}} '13}
}

@inproceedings{Ryu2010Adding,
  title = {Adding Pattern Matching to Existing Object-Oriented Languages},
  booktitle = {{{FOOL}}},
  author = {Ryu, Sukyoung and Park, Changhee and Steele, Guy L.},
  year = {2010},
  abstract = {While object-oriented languages are designed for information hiding and data encapsulation, intensive data manipulation often calls for pattern matching, one of the main features of functional programming languages. Pattern matching provides a concise way to describe specific structures or conditions of objects so that programmers can clearly identify and easily access the corresponding objects. The need for both object information hiding and pattern matching over objects, which are seemingly conflicting features, shows up very well in the problem of language manipulation. In the compiler development of Fortress, a new object-oriented language with extensive support for functional programming, we originally implemented the Fortress type checker in Java. However, because Java does not provide pattern matching, we reimplemented the type checker in Scala. Compared to the pattern matching mechanism in Scala, Java's visitor patterns are verbose, make it hard to capture the high-level specification of objects, and are nontrivial to maintain. In this paper, we introduce a new pattern-matching mechanism which can be added to existing object-oriented languages. We present the mechanism currently being implemented as an addition to the Fortress programming language. The mechanism provides a concise way of describing and using patterns in the presence of runtime manipulation of types using typecase and multimethods, unlike Scala which has type-erasure semantics. Even though it is premature to discuss the performance and scalability of the presented mechanism, we believe that the same optimization techniques used in Scala will apply to our mechanism.}
}

@article{Sabry1997reflection,
  title = {A {{Reflection}} on {{Call}}-by-Value},
  author = {Sabry, Amr and Wadler, Philip},
  year = {1997},
  month = nov,
  volume = {19},
  pages = {916--941},
  issn = {0164-0925},
  doi = {10.1145/267959.269968},
  abstract = {One way to model a sound and complete translation from a source calculus into a target calculus is with an adjoint or a Galois connection. In the special case of a reflection, one also has that the target calculus is isomorphic to a subset of the source. We show that three widely studied translations form reflections. We use as our source language Moggi's computational lambda calculus, which is an extension of Plotkin's call-by-value calculus. We show that Plotkin's CPS translation, Moggi's monad translation, and Girard's translation to linear logic can all be regarded as reflections form this source language, and we put forward the computational lambda calculus as a model of call-by-value computation that improves on the traditional  call-by-value calculus. Our work strengthens Plotkin's and Moggi's original results and improves on recent work based on equational correspondence, which uses equations rather than reductions.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {_tablet,category theory,compiling,continuations,Galois connections},
  number = {6}
}

@inproceedings{Salvaneschi2013Reactive,
  title = {Reactive {{Behavior}} in {{Object}}-Oriented {{Applications}}: {{An Analysis}} and a {{Research Roadmap}}},
  shorttitle = {Reactive {{Behavior}} in {{Object}}-Oriented {{Applications}}},
  booktitle = {Proceedings of the 12th {{Annual International Conference}} on {{Aspect}}-Oriented {{Software Development}}},
  author = {Salvaneschi, Guido and Mezini, Mira},
  year = {2013},
  pages = {37--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2451436.2451442},
  abstract = {Reactive applications are difficult to implement. Traditional solutions based on event systems and the Observer pattern have a number of inconveniences, but programmers bear them in return for the benefits of OO design. On the other hand, reactive approaches based on automatic updates of dependencies - like functional reactive programming and dataflow languages - provide undoubted advantages but do not fit well with mutable objects. In this paper, we provide a research roadmap to overcome the limitations of the current approaches and to support reactive applications in the OO setting. To establish a solid background for our investigation, we propose a conceptual framework to model the design space of reactive applications and we study the flaws of the existing solutions. Then we highlight how reactive languages have the potential to address those issues and we formulate our research plan.},
  isbn = {978-1-4503-1766-5},
  keywords = {functional-reactive programming,incremental computation,object-oriented programming,reactive programming},
  series = {{{AOSD}} '13}
}

@inproceedings{Sansom1995time,
  title = {Time and {{Space Profiling}} for {{Non}}-Strict, {{Higher}}-Order {{Functional Languages}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Sansom, Patrick M. and Peyton Jones, Simon L.},
  year = {1995},
  pages = {355--366},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/199448.199531},
  abstract = {We present the first profiler for a compiled, non-strict, higher-order, purely functional language capable of measuring time as well as space usage. Our profiler is implemented in a production-quality optimising compiler for Haskell, has low overheads, and can successfully profile large applications.A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centres. This specification enables us to discuss our design decisions in a precise framework. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementation-oriented operational semantics, and prove it equivalent to the specification.},
  isbn = {0-89791-692-1},
  series = {{{POPL}} '95}
}

@inproceedings{Santo2015curryhoward,
  title = {Curry-{{Howard}} for {{Sequent Calculus}} at {{Last}}!},
  booktitle = {13th {{International Conference}} on {{Typed Lambda Calculi}} and {{Applications}} ({{TLCA}} 2015)},
  author = {Santo, Jos{\'e} Espirito},
  editor = {Altenkirch, Thorsten},
  year = {2015},
  volume = {38},
  pages = {165--179},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.TLCA.2015.165},
  isbn = {978-3-939897-87-3},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-51626}
}

@inproceedings{Sarkar2004nanopass,
  title = {A {{Nanopass Infrastructure}} for {{Compiler Education}}},
  booktitle = {Proceedings of the {{Ninth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Sarkar, Dipanwita and Waddell, Oscar and Dybvig, R. Kent},
  year = {2004},
  pages = {201--212},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1016850.1016878},
  abstract = {Compilers structured as a small number of monolithic passes are difficult to understand and difficult to maintain. Adding new optimizations often requires major restructuring of existing passes that cannot be understood in isolation. The steep learning curve is daunting, and even experienced developers find it hard to modify existing passes without introducing subtle and tenacious bugs. These problems are especially frustrating when the developer is a student in a compiler class.An attractive alternative is to structure a compiler as a collection of many small passes, each of which performs a single task. This "micropass" structure aligns the actual implementation of a compiler with its logical organization, simplifying development, testing, and debugging. Unfortunately, writing many small passes duplicates code for traversing and rewriting abstract syntax trees and can obscure the meaningful transformations performed by individual passes.To address these problems, we have developed a methodology and associated tools that simplify the task of building compilers composed of many fine-grained passes. We describe these compilers as "nanopass" compilers to indicate both the intended granularity of the passes and the amount of source code required to implement each pass. This paper describes the methodology and tools comprising the nanopass framework.},
  isbn = {1-58113-905-5},
  keywords = {compiler writing tools,domain-specific languages,nanopass compilers,syntactic abstraction},
  series = {{{ICFP}} '04}
}

@inproceedings{Schafer2015Autosubst,
  title = {Autosubst: {{Reasoning}} with de {{Bruijn Terms}} and {{Parallel Substitutions}}},
  shorttitle = {Autosubst},
  booktitle = {Interactive {{Theorem Proving}}},
  author = {Sch{\"a}fer, Steven and Tebbi, Tobias and Smolka, Gert},
  year = {2015},
  month = aug,
  pages = {359--374},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-22102-1_24},
  abstract = {Reasoning about syntax with binders plays an essential role in the formalization of the metatheory of programming languages. While the intricacies of binders can be ignored in paper proofs, formalizations involving binders tend to be heavyweight. We present a discipline for syntax with binders based on de Bruijn terms and parallel substitutions, with a decision procedure covering all assumption-free equational substitution lemmas. The approach is implemented in the Coq library Autosubst, which additionally derives substitution operations and proofs of substitution lemmas for custom term types. We demonstrate the effectiveness of the approach with several case studies, including part A of the POPLmark challenge.},
  keywords = {Decision Procedure,Identity Substitution,Parallel Substitution,Strong Normalization,Term Language},
  language = {en}
}

@inproceedings{Schafer2015Completeness,
  title = {Completeness and {{Decidability}} of {{De Bruijn Substitution Algebra}} in {{Coq}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Certified Programs}} and {{Proofs}}},
  author = {Sch{\"a}fer, Steven and Smolka, Gert and Tebbi, Tobias},
  year = {2015},
  pages = {67--73},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2676724.2693163},
  abstract = {We consider a two-sorted algebra over de Bruijn terms and de Bruijn substitutions equipped with the constants and operations from Abadi et al.'s sigma-calculus. We consider expressions with term variables and substitution variables and show that the semantic equivalence obtained with the algebra coincides with the axiomatic equivalence obtained with finitely many axioms based on the sigma-calculus. We prove this result with an informative decision algorithm for axiomatic equivalence, which in the negative case returns a variable assignment separating the given expressions in the algebra. The entire development is formalized in Coq.},
  isbn = {978-1-4503-3296-5},
  keywords = {algebra,completeness,coq,de bruijn terms,decision procedures,explicit substitutions,finite axiomatization},
  series = {{{CPP}} '15}
}

@article{Scherer2013GADTs,
  title = {{{GADTs}} Meet Subtyping},
  author = {Scherer, Gabriel and R{\'e}my, Didier},
  year = {2013},
  month = jan,
  abstract = {While generalized algebraic datatypes (\textbackslash{}GADTs) are now considered well-understood, adding them to a language with a notion of subtyping comes with a few surprises. What does it mean for a \textbackslash{}GADT parameter to be covariant? The answer turns out to be quite subtle. It involves fine-grained properties of the subtyping relation that raise interesting design questions. We allow variance annotations in \textbackslash{}GADT definitions, study their soundness, and present a sound and complete algorithm to check them. Our work may be applied to real-world ML-like languages with explicit subtyping such as OCaml, or to languages with general subtyping constraints.},
  archivePrefix = {arXiv},
  eprint = {1301.2903},
  eprinttype = {arxiv},
  journal = {arXiv:1301.2903 [cs]},
  keywords = {Computer Science - Programming Languages},
  note = {22nd European Symposium on Programming (ESOP), Rome : Italy (2013)},
  primaryClass = {cs}
}

@inproceedings{Scherer2015which,
  title = {Which {{Simple Types Have}} a {{Unique Inhabitant}}?},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Scherer, Gabriel and R{\'e}my, Didier},
  year = {2015},
  pages = {243--255},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784757},
  abstract = {We study the question of whether a given type has a unique inhabitant modulo program equivalence. In the setting of simply-typed lambda-calculus with sums, equipped with the strong --equivalence, we show that uniqueness is decidable. We present a saturating focused logic that introduces irreducible cuts on positive types ``as soon as possible''. Backward search in this logic gives an effective algorithm that returns either zero, one or two distinct inhabitants for any given type. Preliminary application studies show that such a feature can be useful in strongly-typed programs, inferring the code of highly-polymorphic library functions, or ``glue code'' inside more complex terms.},
  isbn = {978-1-4503-3669-7},
  keywords = {canonicity,code inference,focusing,proof search,saturation,simply-typed lambda-calculus,sums,Unique inhabitants},
  series = {{{ICFP}} 2015}
}

@inproceedings{Scherer2017Deciding,
  title = {Deciding {{Equivalence}} with {{Sums}} and the {{Empty Type}}},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Scherer, Gabriel},
  year = {2017},
  pages = {374--386},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3009837.3009901},
  abstract = {The logical technique of focusing can be applied to the {\^I}\guillemotright{}-calculus; in a simple type system with atomic types and negative type formers (functions, products, the unit type), its normal forms coincide with {\^I}{$^2$}{\^I}{$\cdot$}-normal forms. Introducing a saturation phase gives a notion of quasi-normal forms in presence of positive types (sum types and the empty type). This rich structure let us prove the decidability of {\^I}{$^2$}{\^I}{$\cdot$}-equivalence in presence of the empty type, the fact that it coincides with contextual equivalence, and with set-theoretic equality in all finite models.},
  isbn = {978-1-4503-4660-3},
  keywords = {canonicity,empty type,equivalence,focusing,saturation,simply-typed lambda-calculus,sums},
  series = {{{POPL}} 2017}
}

@inproceedings{Scherer2017Search,
  title = {Search for {{Program Structure}}},
  booktitle = {2nd {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2017)},
  author = {Scherer, Gabriel},
  editor = {Lerner, Benjamin S. and Bod{\'i}k, Rastislav and Krishnamurthi, Shriram},
  year = {2017},
  volume = {71},
  pages = {15:1--15:14},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.SNAPL.2017.15},
  isbn = {978-3-95977-032-3},
  keywords = {canonicity,focusing,programs,proofs},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@incollection{Scherr2014implicit,
  title = {Implicit {{Staging}} of {{EDSL Expressions}}: {{A Bridge}} between {{Shallow}} and {{Deep Embedding}}},
  shorttitle = {Implicit {{Staging}} of {{EDSL Expressions}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Scherr, Maximilian and Chiba, Shigeru},
  editor = {Jones, Richard},
  year = {2014},
  month = jan,
  pages = {385--410},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Common implementation approaches for embedding DSLs in general-purpose host languages force developers to choose between a shallow (single-staged) embedding which offers seamless usage, but limits DSL developers, or a deep (multi-staged) embedding which offers freedom to optimize at will, but is less seamless to use and incurs additional runtime overhead. We propose a metaprogrammatic approach for extracting domain-specific programs from user programs for custom processing. This allows for similar optimization options as deep embedding, while still allowing for seamless embedded usage. We have implemented a simplified instance of this approach in a prototype framework for Java-embedded EDSL expressions, which relies on load-time reflection for improved deployability and usability.},
  copyright = {\textcopyright{}2014 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  keywords = {Computer Communication Networks,DSL,java,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,metaprogramming,programming languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {8586},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Scherr2015almost,
  title = {Almost {{First}}-Class {{Language Embedding}}: {{Taming Staged Embedded DSLs}}},
  shorttitle = {Almost {{First}}-Class {{Language Embedding}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Scherr, Maximilian and Chiba, Shigeru},
  year = {2015},
  pages = {21--30},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814204.2814217},
  abstract = {Embedded domain-specific languages (EDSLs), inheriting a general-purpose language's features as well as look-and-feel, have traditionally been second-class or rather non-citizens in terms of host-language design. This makes sense when one regards them to be on the same level as traditional, non-EDSL library interfaces. However, this equivalence only applies to the simplest of EDSLs. In this paper we illustrate why this is detrimental when moving on to EDSLs that employ staging, i.e. program reification, by example of various issues that affect authors and users alike. We believe that if EDSLs are to be considered a reliable, language-like interface abstraction, they require exceptional attention and design scrutiny. Instead of unenforceable conventions, we advocate the acceptance of EDSLs as proper, i.e. almost first-class, citizens while retaining most advantages of pure embeddings. As a small step towards this goal, we present a pragmatic framework prototype for Java. It is based on annotations that explicate and document membership to explicit EDSL entities. In a nutshell, our framework identifies (annotated) method calls and field accesses as EDSL terms and dynamically constructs an abstract-syntax representation, which is eventually passed to a semantics-defining back end implemented by the EDSL author.},
  isbn = {978-1-4503-3687-1},
  keywords = {design,embedded DSLs,implementation,java,metaprogramming,program transformation,programming languages,staging},
  series = {{{GPCE}} 2015}
}

@inproceedings{Schmid2016SMTbased,
  title = {{{SMT}}-Based {{Checking}} of {{Predicate}}-Qualified {{Types}} for {{Scala}}},
  booktitle = {Proceedings of the 2016 7th {{ACM SIGPLAN Symposium}} on {{Scala}}},
  author = {Schmid, Georg Stefan and Kuncak, Viktor},
  year = {2016},
  pages = {31--40},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2998392.2998398},
  abstract = {We present *qualified types* for Scala, a form of refinement types adapted to the Scala language. Qualified types allow users to refine base types and classes using predicate expressions. We implemented a type checker for qualified types that is embedded in Scala's next-generation compiler Dotty and delegates constraint checking to an SMT solver. Our system supports many of Scala's functional as well as its object-oriented constructs. To propagate user-provided qualifier ascriptions we utilize both Scala's own type system and an incomplete, but effective qualifier inference algorithm. Our evaluation shows that for a series of examples exerting various of Scala's language features, the additional compile-time overhead is manageable. By combining these features we show that one can verify essential safety properties such as static bounds-checks while retaining several of Scala's advanced features.},
  isbn = {978-1-4503-4648-1},
  keywords = {refinement types,Scala},
  series = {{{SCALA}} 2016}
}

@article{Schmidhuber2008driven,
  title = {Driven by {{Compression Progress}}: {{A Simple Principle Explains Essential Aspects}} of {{Subjective Beauty}}, {{Novelty}}, {{Surprise}}, {{Interestingness}}, {{Attention}}, {{Curiosity}}, {{Creativity}}, {{Art}}, {{Science}}, {{Music}}, {{Jokes}}},
  shorttitle = {Driven by {{Compression Progress}}},
  author = {Schmidhuber, Juergen},
  year = {2008},
  month = dec,
  abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.},
  archivePrefix = {arXiv},
  eprint = {0812.4360},
  eprinttype = {arxiv},
  journal = {arXiv:0812.4360 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{Schmidt-Schauss2015Observational,
  title = {Observational Program Calculi and the Correctness of Translations},
  author = {{Schmidt-Schau{\ss}}, Manfred and Sabel, David and Niehren, Joachim and Schwinghammer, Jan},
  year = {2015},
  month = apr,
  volume = {577},
  pages = {98--124},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.02.027},
  abstract = {For the issue of translations between programming languages with observational semantics, this paper clarifies the notions, the relevant questions, and the methods; it constructs a general framework, and provides several tools for proving various correctness properties of translations like adequacy and full abstractness, with a special emphasis on observational correctness. We will demonstrate that a wide range of programming languages and programming calculi and their translations can make advantageous use of our framework for focusing the analysis of their correctness.},
  journal = {Theoretical Computer Science},
  keywords = {contextual equivalence,Correctness,semantics,Translations},
  note = {00000},
  number = {Supplement C}
}

@article{Schmidt-SchaussImprovements,
  title = {Improvements in a Call-by-Need Functional Core Language: {{Common}} Subexpression Elimination and Resource Preserving Translations},
  shorttitle = {Improvements in a Call-by-Need Functional Core Language},
  author = {{Schmidt-Schau{\ss}}, Manfred and Sabel, David},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2017.01.001},
  abstract = {An improvement is a correct program transformation that optimizes the program, where the criterion is that the number of computation steps until a value is obtained is not strictly increased in any context. This paper investigates improvements in both \textendash{} an untyped and a polymorphically typed variant \textendash{} of a call-by-need lambda calculus with letrec, case, constructors and seq. Besides showing that several local transformations are optimizations, a main result of this paper is a proof that common subexpression elimination is correct and an improvement, which proves a conjecture and thus closes a gap in the improvement theory of Moran and Sands. The improvement relation used in this paper is generic in which essential computation steps are counted and thus the obtained results apply for several notions of improvement. Besides the small-step operational semantics, also an abstract machine semantics is considered for counting computation steps. We show for several length measures that the call-by-need calculus of Moran and Sands and our calculus are equivalent.},
  journal = {Science of Computer Programming},
  keywords = {functional programming,Improvement,lambda calculus,Lazy Evaluation,semantics}
}

@book{Schmidt1987denotational,
  title = {Denotational {{Semantics}}: {{A Methodology}} for {{Language Development}}},
  shorttitle = {Denotational {{Semantics}}},
  author = {Schmidt, David A.},
  year = {1987},
  publisher = {{McGraw-Hill Professional}},
  isbn = {0-205-08974-7}
}

@inproceedings{Schmidt1998data,
  title = {Data {{Flow Analysis}} Is {{Model Checking}} of {{Abstract Interpretations}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Schmidt, David A.},
  year = {1998},
  pages = {38--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/268946.268950},
  abstract = {This expository paper simplifies and clarifies Steifen's depiction of data flow analysis (d.f.a.) as model checking: By employing abstract interpretation (a.i.) to generate program traces and by utilizing Kozen's modal mu-calculus to express trace properties, we express in simplest possible terms that a d.f.a. is a model check of a program's a.i. trace. In particular, the classic flow equations for bit-vector-based d.f.a.s reformat trivially into modal mu-calculus formulas. A surprising consequence is that two of the classical d.f.a.s are exposed as unsound; this problem is analyzed and simply repaired. In the process of making the above discoveries, we cIarify the relationship between a.i. and d.f.a. in terms of the often-misunderstood notion of collecting semantics and we highlight how the research areas of flow analysis, abstract interpretation, and model checking have grown together.},
  isbn = {0-89791-979-3},
  series = {{{POPL}} '98}
}

@inproceedings{Schoepe2014selinq,
  title = {{{SeLINQ}}: {{Tracking Information Across Application}}-Database {{Boundaries}}},
  shorttitle = {{{SeLINQ}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Schoepe, Daniel and Hedin, Daniel and Sabelfeld, Andrei},
  year = {2014},
  pages = {25--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2628136.2628151},
  abstract = {The root cause for confidentiality and integrity attacks against computing systems is insecure information flow. The complexity of modern systems poses a major challenge to secure end-to-end information flow, ensuring that the insecurity of a single component does not render the entire system insecure. While information flow in a variety of languages and settings has been thoroughly studied in isolation, the problem of tracking information across component boundaries has been largely out of reach of the work so far. This is unsatisfactory because tracking information across component boundaries is necessary for end-to-end security. This paper proposes a framework for uniform tracking of information flow through both the application and the underlying database. Key enabler of the uniform treatment is recent work by Cheney et al., which studies database manipulation via an embedded language-integrated query language (with Microsoft's LINQ on the backend). Because both the host language and the embedded query languages are functional F\#-like languages, we are able to leverage information-flow enforcement for functional languages to obtain information-flow control for databases "for free", synergize it with information-flow control for applications and thus guarantee security across application-database boundaries. We develop the formal results in the form of a security type system that includes a treatment of algebraic data types and pattern matching, and establish its soundness. On the practical side, we implement the framework and demonstrate its usefulness in a case study with a realistic movie rental database.},
  isbn = {978-1-4503-2873-9},
  keywords = {end-to-end security,information flow,language-integrated queries,static analysis},
  series = {{{ICFP}} '14}
}

@inproceedings{Schrijvers2008type,
  title = {Type {{Checking}} with {{Open Type Functions}}},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Schrijvers, Tom and Peyton Jones, Simon and Chakravarty, Manuel and Sulzmann, Martin},
  year = {2008},
  pages = {51--62},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1411204.1411215},
  abstract = {We report on an extension of Haskell with open type-level functions and equality constraints that unifies earlier work on GADTs, functional dependencies, and associated types. The contribution of the paper is that we identify and characterise the key technical challenge of entailment checking; and we give a novel, decidable, sound, and complete algorithm to solve it, together with some practically-important variants. Our system is implemented in GHC, and is already in active use.},
  isbn = {978-1-59593-919-7},
  keywords = {Haskell,type checking,type families,type functions},
  series = {{{ICFP}} '08}
}

@inproceedings{Schrijvers2009Complete,
  title = {Complete and Decidable Type Inference for {{GADTs}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Schrijvers, Tom and Peyton Jones, Simon and Sulzmann, Martin and Vytiniotis, Dimitrios},
  year = {2009},
  pages = {341--352},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596550.1596599},
  abstract = {GADTs have proven to be an invaluable language extension, for ensuring data invariants and program correctness among others. Unfortunately, they pose a tough problem for type inference: we lose the principal-type property, which is necessary for modular type inference. We present a novel and simplified type inference approach for local type assumptions from GADT pattern matches. Our approach is complete and decidable, while more liberal than previous such approaches.},
  isbn = {978-1-60558-332-7},
  keywords = {_tablet,GADTs,Haskell,type inference},
  series = {{{ICFP}} '09}
}

@article{Schrijvers2017Cochis,
  title = {Cochis: {{Deterministic}} and Coherent Implicits},
  shorttitle = {Cochis},
  author = {Schrijvers, Tom and {C. d. S. Oliveira}, Bruno and Wadler, Philip},
  year = {2017},
  month = may,
  language = {en}
}

@incollection{Schroeder-Heister1994definitional,
  title = {Definitional Reflection and the Completion},
  booktitle = {Extensions of {{Logic Programming}}},
  author = {{Schroeder-Heister}, Peter},
  editor = {Dyckhoff, Roy},
  year = {1994},
  pages = {333--347},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The logic of definitional reflection is extended with a theory of free equality. Based on this equality theory a sequent-style notion of the completion of a definition is motivated. Definitional reflection with free equality turns out to be equivalent to the completion in this sense.},
  copyright = {\textcopyright{}1994 Springer-Verlag},
  isbn = {978-3-540-58025-6 978-3-540-48417-2},
  keywords = {Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages,Programming Techniques},
  language = {en},
  number = {798},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Schroeder-Heister2014prooftheoretic,
  title = {Proof-{{Theoretic Semantics}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {{Schroeder-Heister}, Peter},
  editor = {Zalta, Edward N.},
  year = {2014},
  edition = {Summer 2014},
  abstract = {Proof-theoretic semantics is an alternative to truth-conditionsemantics. It is based on the fundamental assumption that the centralnotion in terms of which meanings are assigned to certain expressionsof our language, in particular to logical constants, is that ofproof rather than truth. In this senseproof-theoretic semantics is semantics in terms of proof .Proof-theoretic semantics also means the semantics of proofs,i.e., the semantics of entities which describe how we arrive at certainassertions given certain assumptions. Both aspects of proof-theoreticsemantics can be intertwined, i.e. the semantics of proofs is itselfoften given in terms of proofs., Proof-theoretic semantics has several roots, the most specific onebeing Gentzen's remarks that the introduction rules in hiscalculus of natural deduction define the meanings of logical constants,while the elimination rules can be obtained as a consequence of thisdefinition (see section 2.2.1). Morebroadly, it belongs to what Prawitz called general prooftheory (see section 1.1). Even morebroadly, it is part of the tradition according to which the meaning ofa term should be explained by reference to the way it is usedin our language., Within philosophy, proof-theoretic semantics has mostly figuredunder the heading ``theory of meaning''. This terminologyfollows Dummett, who claimed that the theory of meaning is the basis oftheoretical philosophy, a view which he attributed to Frege. The term``proof-theoretic semantics'' was proposed bySchroeder-Heister (1991; used already in 1987 lectures in Stockholm) in order not to leave the term``semantics'' to denotationalism alone\textemdash{}after all,``semantics'' is the standard term for investigations dealingwith the meaning of linguistic expressions. Furthermore, unlike``theory of meaning'', the term ``proof-theoreticsemantics'' covers philosophical and technical aspects likewise.In 1999, the first conference with this title took place inT{\"u}bingen.},
  keywords = {category theory,connectives: sentence connectives in formal logic,Curry's paradox,Hilbert; David: program in the foundations of mathematics,logic; history of: intuitionistic logic,logic: classical,logic: intuitionistic,logic: linear,logic: substructural,logical constants,mathematics; philosophy of: intuitionism,paradoxes: and contemporary logic,proof theory: development of,realism: challenges to metaphysical,Russell's paradox,self-reference,truth: revision theory of,type theory}
}

@inproceedings{Schurmann2008Structural,
  title = {Structural {{Logical Relations}}},
  booktitle = {2008 23rd {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Sch{\"u}rmann, C. and Sarnat, J.},
  year = {2008},
  month = jun,
  pages = {69--80},
  doi = {10.1109/LICS.2008.44},
  abstract = {Tait's method (a.k.a. proof by logical relations) is a powerful proof technique frequently used for showing foundational properties of languages based on typed lambda-calculi. Historically, these proofs have been extremely difficult to formalize in proof assistants with weak meta-logics, such as Twelf, and yet they are often straightforward in proof assistants with stronger meta-logics. In this paper, we propose structural logical relations as a technique for conducting these proofs in systems with limited meta-logical strength by explicitly representing and reasoning about an auxiliary logic. In support of our claims, we give a Twelf-checked proof of the completeness of an algorithm for checking equality of simply typed lambda-terms.},
  keywords = {auxiliary logic,Computer science,Cut-Elimination,Encoding,inference mechanisms,lambda calculus,Logic,Logical Frameworks,Logical Relations,meta-logics,Normalization,proof assistants,structural logical relations,Turning,Twelf,Twelf-checked proof,typed lambda-calculi,USA Councils}
}

@inproceedings{Schuster2018Typing,
  title = {Typing, Representing, and Abstracting Control: Functional Pearl},
  shorttitle = {Typing, Representing, and Abstracting Control},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on {{Type}}-{{Driven Development}}},
  author = {Schuster, Philipp and Brachth{\"a}user, Jonathan Immanuel},
  year = {2018},
  pages = {14--24},
  publisher = {{ACM}},
  address = {{St. Louis, MO, USA}},
  doi = {10.1145/3240719.3241788},
  abstract = {A well known technique to implement programming languages with delimited control operators shift and reset is to translate programs into continuation passing style (CPS). We can iterate the CPS translation to obtain the CPS hierarchy and to implement a family of control operators shifti and reseti. This functional pearl retells the story of a family of delimited control operators and their translation to lambda calculus via the CPS hierarchy. Prior work on the CPS hierarchy fixes a level of n control operators for the entire program upfront, but we allow different parts of the program to live at different levels. It turns out that taking shift0 rather than shift as the basis for the family of control operators is essential for this. Our source language is a typed embedding in the dependently typed language Idris. Our target language is a HOAS embedding in Idris. The translation avoids administrative beta- and eta-redexes at all levels of the CPS hierarchy, by iterating well-known techniques for the non-iterated CPS translation.},
  isbn = {978-1-4503-5825-5},
  keywords = {Compilation,Continua- tion Passing Style,Control Effects,CPS Hierarchy,Delimited Control},
  series = {{{TyDe}} 2018}
}

@inproceedings{Schwaab2013Modular,
  title = {Modular {{Type}}-Safety {{Proofs}} in {{Agda}}},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{Programming Languages Meets Program Verification}}},
  author = {Schwaab, Christopher and Siek, Jeremy G.},
  year = {2013},
  pages = {3--12},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2428116.2428120},
  abstract = {Methods for reusing code are widespread and well researched, but methods for reusing proofs are still emerging. We consider the use of dependent types for this purpose, introducing a modular approach for composing mechanized proofs. We show that common techniques for abstracting algorithms over data structures naturally translate to abstractions over proofs. We introduce a language composed of a series of smaller language components, each defined as functors, and tie them together by taking the fixed point of their sum [Malcom, 1990]. We then give proofs of type preservation for each language component and show how to compose these proofs into a proof for the entire language, again by taking the fixed point of a sum of functors.},
  isbn = {978-1-4503-1860-0},
  keywords = {Agda,meta-theory,modularity},
  series = {{{PLPV}} '13}
}

@article{Schwinghammer2009Stepindexed,
  title = {A {{Step}}-Indexed {{Semantics}} of {{Imperative Objects}}},
  author = {Schwinghammer, Jan and Hritcu, Catalin},
  year = {2009},
  month = dec,
  volume = {Volume 5, Issue 4},
  abstract = {Step-indexed semantic interpretations of types were proposed as an alternative to purely syntactic proofs of type safety using subject reduction. The types are interpreted as sets of values indexed by the number of computation steps for which these values are guaranteed to behave like proper elements of the type. Building on work by Ahmed, Appel and others, we introduce a step-indexed semantics for the imperative object calculus of Abadi and Cardelli. Providing a semantic account of this calculus using more `traditional', domain-theoretic approaches has proved challenging due to the combination of dynamically allocated objects, higher-order store, and an expressive type system. Here we show that, using step-indexing, one can interpret a rich type discipline with object types, subtyping, recursive and bounded quantified types in the presence of state.},
  journal = {Logical Methods in Computer Science},
  keywords = {_tablet},
  language = {en}
}

@inproceedings{Scibior2015practical,
  title = {Practical {{Probabilistic Programming}} with {{Monads}}},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {{\'S}cibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.},
  year = {2015},
  pages = {165--176},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2804302.2804317},
  abstract = {The machine learning community has recently shown a lot of interest in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probability distributions, but their use is often limited to very simple inference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a probability distribution and apply Sequential Monte Carlo-based methods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementation that achieves performance comparable with Anglican, a state-of-the-art probabilistic programming system.},
  isbn = {978-1-4503-3808-0},
  keywords = {Bayesian statistics,Haskell,Monads,Monte Carlo,probabilistic programming},
  series = {Haskell 2015}
}

@article{Scott1976data,
  title = {Data {{Types}} as {{Lattices}}},
  author = {Scott, D.},
  year = {1976},
  month = sep,
  volume = {5},
  pages = {522--587},
  issn = {0097-5397},
  doi = {10.1137/0205037},
  abstract = {The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of ``partial'' objects. In this report these spaces are modeled in one universal domain \$\{\textbackslash{}bf P\} \textbackslash{}omega  \$, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.,  The meaning of many kinds of expressions in programming languages can be taken as elements of certain spaces of ``partial'' objects. In this report these spaces are modeled in one universal domain \$\{\textbackslash{}bf P\} \textbackslash{}omega  \$, the set of all subsets of the integers. This domain renders the connection of this semantic theory with the ordinary theory of number theoretic (especially general recursive) functions clear and straightforward.},
  journal = {SIAM Journal on Computing},
  number = {3}
}

@inproceedings{Sculthorpe2013Constrainedmonad,
  title = {The {{Constrained}}-Monad {{Problem}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Sculthorpe, Neil and Bracker, Jan and Giorgidze, George and Gill, Andy},
  year = {2013},
  pages = {287--298},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500602},
  abstract = {In Haskell, there are many data types that would form monads were it not for the presence of type-class constraints on the operations on that data type. This is a frustrating problem in practice, because there is a considerable amount of support and infrastructure for monads that these data types cannot use. Using several examples, we show that a monadic computation can be restructured into a normal form such that the standard monad class can be used. The technique is not specific to monads, and we show how it can also be applied to other structures, such as applicative functors. One significant use case for this technique is domain-specific languages, where it is often desirable to compile a deep embedding of a computation to some other language, which requires restricting the types that can appear in that computation.},
  isbn = {978-1-4503-2326-0},
  keywords = {class constraints,deep embeddings,Haskell,Monads},
  series = {{{ICFP}} '13}
}

@incollection{Sculthorpe2013hermit,
  title = {The {{HERMIT}} in the {{Tree}}},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  author = {Sculthorpe, Neil and Farmer, Andrew and Gill, Andy},
  editor = {Hinze, Ralf},
  year = {2013},
  month = jan,
  pages = {86--103},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {This paper describes our experience using the HERMIT toolkit to apply well-known transformations to the internal core language of the Glasgow Haskell Compiler. HERMIT provides several mechanisms to support writing general-purpose transformations: a domain-specific language for strategic programming specialized to GHC's core language, a library of primitive rewrites, and a shell-style\textendash{}based scripting language for interactive and batch usage. There are many program transformation techniques that have been described in the literature but have not been mechanized and made available inside GHC \textemdash{} either because they are too specialized to include in a general-purpose compiler, or because the developers' interest is in theory rather than implementation. The mechanization process can often reveal pragmatic obstacles that are glossed over in pen-and-paper proofs; understanding and removing these obstacles is our concern. Using HERMIT, we implement eleven examples of three program transformations, report on our experience, and describe improvements made in the process.},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-41581-4 978-3-642-41582-1},
  keywords = {ghc,Information Systems Applications (incl. Internet),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mechanization,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering,Transformation,Worker/wrapper},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Sculthorpe2014kansas,
  title = {The {{Kansas University Rewrite Engine}}: {{A Haskell}}-{{Embedded Strategic Programming Language}} with {{Custom Closed Universes}}},
  author = {Sculthorpe, Neil and Frisby, Nicolas and Gill, Andy},
  year = {2014},
  journal = {Journal of Functional Programming},
  xurl = {http://www.ittc.ku.edu/csdl/fpg/software/kure.html}
}

@article{Seely1984locally,
  title = {Locally Cartesian Closed Categories and Type Theory},
  author = {Seely, R. a. G.},
  year = {1984},
  month = jan,
  volume = {95},
  pages = {33--48},
  issn = {1469-8064},
  doi = {10.1017/S0305004100061284},
  abstract = {It is well known that for much of the mathematics of topos theory, it is in fact sufficient to use a category C whose slice categories C/A are cartesian closed. In such a category, the notion of a `generalized set', for example an `A-indexed set', is represented by a morphism B \textrightarrow{} A of C, i.e. by an object of C/A. The point about such a category C is that C is a C-indexed category, and more, is a hyper-doctrine, so that it has a full first order logic associated with it. This logic has some peculiar aspects. For instance, the types are the objects of C and the terms are the morphisms of C. For a given type A, the predicates with a free variable of type A are morphisms into A, and `proofs' are morphisms over A. We see here a certain `ambiguity' between the notions of type, predicate, and term, of object and proof: a term of type A is a morphism into A, which is a predicate over A; a morphism 1 \textrightarrow{} A can be viewed either as an object of type A or as a proof of the proposition A.},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  number = {01}
}

@article{Seely1987categorical,
  title = {Categorical Semantics for Higher Order Polymorphic Lambda Calculus},
  author = {Seely, R. a. G.},
  year = {1987},
  month = dec,
  volume = {52},
  pages = {969--989},
  issn = {1943-5886},
  doi = {10.2307/2273831},
  abstract = {A categorical structure suitable for interpreting polymorphic lambda calculus (PLC) is defined, providing an algebraic semantics for PLC which is sound and complete. In fact, there is an equivalence between the theories and the categories. Also presented is a definitional extension of PLC including ``subtypes'', for example, equality subtypes, together with a construction providing models of the extended language, and a context for Girard's extension of the Dialectica interpretation.},
  journal = {Journal of Symbolic Logic},
  number = {04}
}

@article{Seidel2014type,
  title = {Type {{Targeted Testing}}},
  author = {Seidel, Eric L. and Vazou, Niki and Jhala, Ranjit},
  year = {2014},
  month = oct,
  abstract = {We present a new technique called type targeted testing, which translates precise refinement types into comprehensive test-suites. The key insight behind our approach is that through the lens of SMT solvers, refinement types can also be viewed as a high-level, declarative, test generation technique, wherein types are converted to SMT queries whose models can be decoded into concrete program inputs. Our approach enables the systematic and exhaustive testing of implementations from high-level declarative specifications, and furthermore, provides a gradual path from testing to full verification. We have implemented our approach as a Haskell testing tool called TARGET, and present an evaluation that shows how TARGET can be used to test a wide variety of properties and how it compares against state-of-the-art testing approaches.},
  archivePrefix = {arXiv},
  eprint = {1410.5370},
  eprinttype = {arxiv},
  journal = {arXiv:1410.5370 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Sergey2014Modular,
  title = {Modular, {{Higher}}-Order {{Cardinality Analysis}} in {{Theory}} and {{Practice}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  year = {2014},
  pages = {335--347},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2535838.2535861},
  abstract = {Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.},
  isbn = {978-1-4503-2544-8},
  keywords = {cardinality analysis,Compilers,functional programming languages,Haskell,Lazy Evaluation,operational semantics,Program optimisation,static analysis,thunks,types and effects},
  series = {{{POPL}} '14}
}

@inproceedings{Serrano2018Guarded,
  title = {Guarded Impredicative Polymorphism},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Serrano, Alejandro and Hage, Jurriaan and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  year = {2018},
  pages = {783--796},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3192366.3192389},
  abstract = {The design space for type systems that support impredicative instantiation is extremely complicated. One needs to strike a balance between expressiveness, simplicity for both the end programmer and the type system implementor, and how easily the system can be integrated with other advanced type system concepts. In this paper, we propose a new point in the design space, which we call guarded impredicativity. Its key idea is that impredicative instantiation in an application is allowed for type variables that occur under a type constructor. The resulting type system has a clean declarative specification \textemdash{} making it easy for programmers to predict what will type and what will not \textemdash, allows for a smooth integration with GHC's OutsideIn(X) constraint solving framework, while giving up very little in terms of expressiveness compared to systems like HMF, HML, FPH and MLF. We give a sound and complete inference algorithm, and prove a principal type property for our system.},
  isbn = {978-1-4503-5698-5},
  keywords = {constraint-based inference,impredicative polymorphism,Type systems},
  series = {{{PLDI}} 2018}
}

@inproceedings{Servetto2013BillionDollar,
  title = {The {{Billion}}-{{Dollar Fix}}},
  booktitle = {{{ECOOP}} 2013 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Servetto, Marco and Mackay, Julian and Potanin, Alex and Noble, James},
  editor = {Castagna, Giuseppe},
  year = {2013},
  pages = {205--229},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Programmers often need to initialise circular structures of objects. Initialisation should be safe (so that programs can never suffer null pointer exceptions or otherwise observe uninitialised values) and modular (so that each part of the circular structure can be written and compiled separately). Unfortunately, existing languages do not support modular circular initialisation: programmers in practical languages resort to Tony Hoare's ``Billion Dollar Mistake'': initialising variables with nulls, and then hoping to fix them up afterward. While recent research languages have offered some solutions, none fully support safe modular circular initialisation.We present placeholders, a straightforward extension to object-oriented languages that describes circular structures simply, directly, and modularly. In typed languages, placeholders can be described by placeholder types that ensure placeholders are used safely. We define an operational semantics for placeholders, a type system for placeholder types, and prove soundness. Incorporating placeholders into object-oriented languages should make programs simultaneously simpler to write, and easier to write correctly.},
  isbn = {978-3-642-39038-8},
  keywords = {Actual Object,Factory Method,Null Pointer,Object Type,Type System},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Setzer2014unnesting,
  title = {Unnesting of {{Copatterns}}},
  booktitle = {Rewriting and {{Typed Lambda Calculi}}},
  author = {Setzer, Anton and Abel, Andreas and Pientka, Brigitte and Thibodeau, David},
  editor = {Dowek, Gilles},
  year = {2014},
  month = jul,
  pages = {31--45},
  publisher = {{Springer International Publishing}},
  abstract = {Inductive data such as finite lists and trees can elegantly be defined by constructors which allow programmers to analyze and manipulate finite data via pattern matching. Dually, coinductive data such as streams can be defined by observations such as head and tail and programmers can synthesize infinite data via copattern matching. This leads to a symmetric language where finite and infinite data can be nested. In this paper, we compile nested pattern and copattern matching into a core language which only supports simple non-nested (co)pattern matching. This core language may serve as an intermediate language of a compiler. We show that this translation is conservative, i.e. the multi-step reduction relation in both languages coincides for terms of the original language. Furthermore, we show that the translation preserves strong and weak normalisation: a term of the original language is strongly/weakly normalising in one language if and only if it is so in the other. In the proof we develop more general criteria which guarantee that extensions of abstract reduction systems are conservative and preserve strong or weak normalisation.},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-08917-1 978-3-319-08918-8},
  keywords = {abstract reduction system,algebraic data types,ARS,coalgebras,codata,Computing Methodologies,conservative extension,copattern matching,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Mathematics of Computing,pattern matching,strong normalisation,Symbolic and Algebraic Manipulation,weak normalisation},
  language = {en},
  number = {8560},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Severi2017Two,
  title = {Two {{Light Modalities}} for {{Recursion}}},
  author = {Severi, Paula},
  year = {2017},
  month = dec,
  abstract = {We investigate the interplay between two modalities for controlling the behaviour of recursive functional programs on infinite structures which are completely silent in the syntax. The latter means that programs do not contain "marks" showing the application of the introduction and elimination rules for the modalities. This shifts the burden of controlling recursion from the programmer to the compiler. To do this, we introduce a typed lambda calculus a la Curry with two silent modalities and guarded recursive types. The first modality delays the data while the second one removes the delays. The typing discipline guarantees normalisation and can be transformed into an algorithm which infers the type of a program.},
  archivePrefix = {arXiv},
  eprint = {1801.00285},
  eprinttype = {arxiv},
  journal = {arXiv:1801.00285 [cs]},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@inproceedings{Shaikhha2016How,
  title = {How to {{Architect}} a {{Query Compiler}}},
  booktitle = {{{SIGMOD}} 2016},
  author = {Shaikhha, Amir and Klonatos, Ioannis and Parreaux, Lionel Emile Vincent and Brown, Lewis and Dashti Rahmat Abadi, Mohammad and Koch, Christoph},
  year = {2016},
  doi = {10.1145/2882903.2915244}
}

@inproceedings{Shaikhha2017Destinationpassing,
  title = {Destination-Passing {{Style}} for {{Efficient Memory Management}}},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN International Workshop}} on {{Functional High}}-{{Performance Computing}}},
  author = {Shaikhha, Amir and Fitzgibbon, Andrew and Peyton Jones, Simon and Vytiniotis, Dimitrios},
  year = {2017},
  pages = {12--23},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3122948.3122949},
  abstract = {We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination-passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.},
  isbn = {978-1-4503-5181-2},
  keywords = {Array Programming,Destination-Passing Style},
  series = {{{FHPC}} 2017}
}

@article{Shaikhha2018CompilerCompiler,
  title = {A {{Compiler}}-{{Compiler}} for {{DSL Embedding}}},
  author = {Shaikhha, Amir and Jovanovic, Vojin and Koch, Christoph},
  year = {2018},
  month = aug,
  abstract = {In this paper, we present a framework to generate compilers for embedded domain-specific languages (EDSLs). This framework provides facilities to automatically generate the boilerplate code required for building DSL compilers on top of extensible optimizing compilers. We evaluate the practicality of our framework by demonstrating several use-cases successfully built with it.},
  archivePrefix = {arXiv},
  eprint = {1808.01344},
  eprinttype = {arxiv},
  journal = {arXiv:1808.01344 [cs]},
  keywords = {Computer Science - Programming Languages},
  primaryClass = {cs}
}

@article{Shaikhha2018Efficient,
  title = {Efficient {{Differentiable Programming}} in a {{Functional Array}}-{{Processing Language}}},
  author = {Shaikhha, Amir and Fitzgibbon, Andrew and Vytiniotis, Dimitrios and Jones, Simon Peyton and Koch, Christoph},
  year = {2018},
  month = jun,
  abstract = {We present a system for the automatic differentiation of a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source automatic differentiation and global optimizations such as loop transformations. Thanks to this feature, we demonstrate how for some real-world machine learning and computer vision benchmarks, the system outperforms the state-of-the-art automatic differentiation tools.},
  archivePrefix = {arXiv},
  eprint = {1806.02136},
  eprinttype = {arxiv},
  journal = {arXiv:1806.02136 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Programming Languages,Computer Science - Symbolic Computation,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{Shankar2005runtime,
  title = {Runtime {{Specialization}} with {{Optimistic Heap Analysis}}},
  booktitle = {Proceedings of the 20th {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Shankar, Ajeet and Sastry, S. Subramanya and Bod{\'i}k, Rastislav and Smith, James E.},
  year = {2005},
  pages = {327--343},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1094811.1094837},
  abstract = {We describe a highly practical program specializer for Java programs. The specializer is powerful, because it specializes optimistically, using (potentially transient) constants in the heap; it is precise, because it specializes using data structures that are only partially invariant; it is deployable, because it is hidden in a JIT compiler and does not require any user annotations or offline preprocessing; it is simple, because it uses existing JIT compiler ingredients; and it is fast, because it specializes programs in under 1s.These properties are the result of (1) a new algorithm for selecting specializable code fragments, based on a notion of influence; (2) a precise store profile for identifying constant heap locations; and (3) an efficient invalidation mechanism for monitoring optimistic assumptions about heap constants. Our implementation of the specializer in the Jikes RVM has low overhead, selects specialization points that would be chosen manually, and produces speedups ranging from a factor of 1.2 to 6.4, comparable with annotation-guided specializers.},
  isbn = {1-59593-031-0},
  keywords = {dynamic optimization,partial evaluation,program analysis,specialization},
  series = {{{OOPSLA}} '05}
}

@inproceedings{Shao1998implementing,
  title = {Implementing {{Typed Intermediate Languages}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Shao, Zhong and League, Christopher and Monnier, Stefan},
  year = {1998},
  pages = {313--323},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289460},
  abstract = {Recent advances in compiler technology have demonstrated the benefits of using strongly typed intermediate languages to compile richly typed source languages (e.g., ML). A type-preserving compiler can use types to guide advanced optimizations and to help generate provably secure mobile code. Types, unfortunately, are very hard to represent and manipulate efficiently; a naive implementation can easily add exponential overhead to the compilation and execution of a program. This paper describes our experience with implementing the FLINT typed intermediate language in the SML/NJ production compiler. We observe that a type-preserving compiler will not scale to handle large types unless all of its type-preserving stages preserve the asymptotic time and space usage in representing and manipulating types. We present a series of novel techniques for achieving this property and give empirical evidence of their effectiveness.},
  isbn = {1-58113-024-4},
  series = {{{ICFP}} '98}
}

@article{Shao2000Efficient,
  title = {Efficient and {{Safe}}-for-Space {{Closure Conversion}}},
  author = {Shao, Zhong and Appel, Andrew W.},
  year = {2000},
  month = jan,
  volume = {22},
  pages = {129--161},
  issn = {0164-0925},
  doi = {10.1145/345099.345125},
  abstract = {Modern compilers often implement function calls (or returns) in two steps: first, a ``closure'' environment is properly installed to provide access for free variables in the target program fragment; second, the control is transferred to the target by a ``jump with arguments (for results).'' Closure conversion\textemdash{}which decides where and how to represent closures at runtime\textemdash{}is a crucial step in the compilation of functional languages. This paper presents a new algorithm that exploits the use of compile-time control and data-flow information to optimize funtion calls. By extensive closure sharing and allocation by 36\% and memory fetches for local and global variables by 43\%; and improves the already efficient code generated by an earlier version of the Standard ML of New Jersey compiler by about 17\% on a DECstation 5000. Moreover, unlike most other approaches, our new closure-allocation scheme the strong safe-for-space-complexity rule, thus achieving good asymptotic space usage.},
  journal = {ACM Trans. Program. Lang. Syst.},
  keywords = {callee-save registers,closure conversion,closure representation,compiler optimization,flow analysis,heap-based compilation,space safety},
  number = {1}
}

@inproceedings{Sheard2001generic,
  title = {Generic {{Unification}} via {{Two}}-Level {{Types}} and {{Parameterized Modules}}},
  booktitle = {Proceedings of the {{Sixth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Sheard, Tim},
  year = {2001},
  pages = {86--97},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/507635.507648},
  abstract = {As a functional pearl, we describe an efficient, modularized
implementation of unification using the state of mutable reference
cells to encode substitutions. We abstract our algorithms along two
dimensions, first abstracting away from the structure of the terms
to be unified, and second over the monad in which the mutable state
is encapsulated. We choose this example to illustrate two important
techniques that we believe many functional programmers would find
useful. The first of these is the definition of recursive data
types using two levels: a structure defining level, and a recursive
knot-tying level. The second is the use of rank-2 polymorphism
inside Haskell's record types to implement a form of type
parameterized modules.},
  isbn = {1-58113-415-0},
  keywords = {generic programs,parameterized modules,unification},
  series = {{{ICFP}} '01}
}

@inproceedings{Sheard2002template,
  title = {Template {{Meta}}-Programming for {{Haskell}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  author = {Sheard, Tim and Jones, Simon Peyton},
  year = {2002},
  pages = {1--16},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/581690.581691},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  isbn = {1-58113-605-6},
  keywords = {meta programming,templates},
  series = {Haskell '02}
}

@inproceedings{Sheeran2015functional,
  title = {Functional {{Programming}} and {{Hardware Design}}: {{Still Interesting After All These Years}}},
  shorttitle = {Functional {{Programming}} and {{Hardware Design}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Sheeran, Mary},
  year = {2015},
  pages = {165--165},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2789053},
  abstract = {Higher order functions provide an elegant way to express algorithms designed for implementation in hardware. By showing examples of both classic and new algorithms, I will explain why higher order functions deserve to be studied. Next, I will consider the extent to which ideas from functional programming, and associated formal verification methods, have influenced hardware design in practice. What can we learn from looking back? You might ask "Why are methods of hardware design still important to our community?". Maybe we should just give up? One reason for not giving up is that hardware design is really a form of parallel programming. And here there is still a lot to do! Inspired by Blelloch's wonderful invited talk at ICFP 2010, I still believe that functional programming has much to offer in the central question of how to program the parallel machines of today, and, more particularly, of the future. I will briefly present some of the areas where I think that we are poised to make great contributions. But maybe we need to work harder on getting our act together?},
  isbn = {978-1-4503-3669-7},
  keywords = {functional programming,Hardware design,higher order functions,parallel algorithms,parallel programming},
  series = {{{ICFP}} 2015}
}

@incollection{Shivers1996universal,
  title = {A Universal Scripting Framework or {{Lambda}}: {{The}} Ultimate ``Little Language''},
  shorttitle = {A Universal Scripting Framework or {{Lambda}}},
  booktitle = {Concurrency and {{Parallelism}}, {{Programming}}, {{Networking}}, and {{Security}}},
  author = {Shivers, Olin},
  editor = {Jaffar, Joxan and Yap, Roland H. C.},
  year = {1996},
  month = jan,
  pages = {254--265},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The ``little languages'' approach to systems programming is flawed: inefficient, fragile, error-prone, inexpressive, and difficult to compose. A better solution is to embed task-specific sublanguages within a powerful, syntactically extensible, universal language, such as Scheme. I demonstrate two such embeddings that have been implemented in scsh, a Scheme programming environment for Unix systems programming. The first embedded language is a highlevel process-control notation; the second provides for Awk-like processing. Embedding systems in this way is a powerful technique: for example, although the embedded Awk system was implemented with 7\% of the code required for the standard C-based Awk, it is significantly more expressive than its C counterpart.},
  copyright = {\textcopyright{}1996 Springer-Verlag},
  isbn = {978-3-540-62031-0 978-3-540-49626-7},
  keywords = {Algorithm Analysis and Problem Complexity,Computer Communication Networks,Logics and Meanings of Programs,Programming Techniques},
  language = {en},
  number = {1179},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Shivers2004higherorder,
  title = {Higher-Order {{Control}}-Flow {{Analysis}} in {{Retrospect}}: {{Lessons Learned}}, {{Lessons Abandoned}}},
  shorttitle = {Higher-Order {{Control}}-Flow {{Analysis}} in {{Retrospect}}},
  author = {Shivers, Olin},
  year = {2004},
  month = apr,
  volume = {39},
  pages = {257--269},
  issn = {0362-1340},
  doi = {10.1145/989393.989421},
  abstract = {Traditional flow analysis techniques, such as the ones typically employed by optimising Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique --- control flow analysis --- which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.The techniques presented in this paper are backed up by working code. They are applicable not only to Scheme, but also to related languages, such as Common Lisp and ML.},
  journal = {SIGPLAN Not.},
  number = {4}
}

@article{Shulman2008set,
  title = {Set Theory for Category Theory},
  author = {Shulman, Michael A.},
  year = {2008},
  month = oct,
  abstract = {Questions of set-theoretic size play an essential role in category theory, especially the distinction between sets and proper classes (or small sets and large sets). There are many different ways to formalize this, and which choice is made can have noticeable effects on what categorical constructions are permissible. In this expository paper we summarize and compare a number of such "set-theoretic foundations for category theory," and describe their implications for the everyday use of category theory. We assume the reader has some basic knowledge of category theory, but little or no prior experience with formal logic or set theory.},
  archivePrefix = {arXiv},
  eprint = {0810.1279},
  eprinttype = {arxiv},
  journal = {arXiv:0810.1279 [math]},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  primaryClass = {math}
}

@article{Shulman2017Homotopy,
  title = {Homotopy Type Theory: The Logic of Space},
  shorttitle = {Homotopy Type Theory},
  author = {Shulman, Michael},
  year = {2017},
  month = mar,
  abstract = {This is an introduction to type theory, synthetic topology, and homotopy type theory from a category-theoretic and topological point of view, written as a chapter for the book "New Spaces for Mathematics and Physics" (ed. Gabriel Catren and Mathieu Anel).},
  archivePrefix = {arXiv},
  eprint = {1703.03007},
  eprinttype = {arxiv},
  journal = {arXiv:1703.03007 [math]},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  primaryClass = {math}
}

@inproceedings{Sieczkowski2015ModuRes,
  title = {{{ModuRes}}: {{A Coq Library}} for {{Modular Reasoning About Concurrent Higher}}-{{Order Imperative Programming Languages}}},
  shorttitle = {{{ModuRes}}},
  booktitle = {Interactive {{Theorem Proving}}},
  author = {Sieczkowski, Filip and Bizjak, Ale{\v s} and Birkedal, Lars},
  editor = {Urban, Christian and Zhang, Xingyuan},
  year = {2015},
  pages = {375--390},
  publisher = {{Springer International Publishing}},
  abstract = {It is well-known that it is challenging to build semantic models of type systems or logics for reasoning about concurrent higher-order imperative programming languages. One of the key challenges is that such semantic models often involve constructing solutions to certain kinds of recursive domain equations, which in practice has been a barrier to formalization efforts. Here we present the ModuRes Coq library, which provides an easy way to solve such equations. We show how the library can be used to construct models of type systems and logics for reasoning about concurrent higher-order imperative programming languages.},
  isbn = {978-3-319-22102-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Simpson1995categorical,
  title = {Categorical Completeness Results for the Simply-Typed Lambda-Calculus},
  booktitle = {Typed {{Lambda Calculi}} and {{Applications}}},
  author = {Simpson, Alex K.},
  editor = {{Dezani-Ciancaglini}, Mariangiola and Plotkin, Gordon},
  year = {1995},
  month = jan,
  pages = {414--427},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We investigate, in a categorical setting, some completeness properties of beta-eta conversion between closed terms of the simplytyped lambda calculus. A cartesian-closed category is said to be complete if, for any two unconvertible terms, there is some interpretation of the calculus in the category that distinguishes them. It is said to have a complete interpretation if there is some interpretation that equates only interconvertible terms. We give simple necessary and sufficient conditions on the category for each of the two forms of completeness to hold. The classic completeness results of, e.g., Friedman and Plotkin are immediate consequences. As another application, we derive a syntactic theorem of Statman characterizing beta-eta conversion as a maximum consistent congruence relation satisfying a property known as typical ambiguity.},
  copyright = {\textcopyright{}1995 Springer-Verlag},
  isbn = {978-3-540-59048-4 978-3-540-49178-1},
  keywords = {Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Programming Techniques},
  language = {en},
  number = {902},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Sjoberg2012Irrelevance,
  title = {Irrelevance, {{Heterogeneous Equality}}, and {{Call}}-by-Value {{Dependent Type Systems}}},
  author = {Sj{\"o}berg, Vilhelm and Casinghino, Chris and Ahn, Ki Yung and Collins, Nathan and Eades III, Harley D. and Fu, Peng and Kimmell, Garrin and Sheard, Tim and Stump, Aaron and Weirich, Stephanie},
  year = {2012},
  month = feb,
  doi = {10.4204/EPTCS.76.9},
  abstract = {We present a full-spectrum dependently typed core language which includes both nontermination and computational irrelevance (a.k.a. erasure), a combination which has not been studied before. The two features interact: to protect type safety we must be careful to only erase terminating expressions. Our language design is strongly influenced by the choice of CBV evaluation, and by our novel treatment of propositional equality which has a heterogeneous, completely erased elimination form.},
  language = {en}
}

@inproceedings{Sjoberg2015programming,
  title = {Programming {{Up}} to {{Congruence}}},
  booktitle = {Proceedings of the {{42Nd Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Sj{\"o}berg, Vilhelm and Weirich, Stephanie},
  year = {2015},
  pages = {369--382},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2676726.2676974},
  abstract = {This paper presents the design of Zombie, a dependently-typed programming language that uses an adaptation of a congruence closure algorithm for proof and type inference. This algorithm allows the type checker to automatically use equality assumptions from the context when reasoning about equality. Most dependently-typed languages automatically use equalities that follow from beta-reduction during type checking; however, such reasoning is incompatible with congruence closure. In contrast, Zombie does not use automatic beta-reduction because types may contain potentially diverging terms. Therefore Zombie provides a unique opportunity to explore an alternative definition of equivalence in dependently-typed language design. Our work includes the specification of the language via a bidirectional type system, which works "up-to-congruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system, and always produces well typed terms in the core language. This algorithm has been implemented in the Zombie language, which includes general recursion, irrelevant arguments, heterogeneous equality and datatypes.},
  isbn = {978-1-4503-3300-9},
  keywords = {congruence closure,dependent types},
  series = {{{POPL}} '15}
}

@phdthesis{Sjobergdependently,
  title = {A Dependently Typed Language with Nontermination},
  author = {Sj{\"o}berg, Vilhelm},
  abstract = {We propose a full-spectrum dependently typed programming language, Zombie, which supports general recursion natively. The Zombie implementation is an elaborating typechecker. We prove type safety for a large subset of the Zombie core language, including features such as computational irrelevance, CBV-reduction, and propositional equality with a heterogeneous, completely erased elimination form. Zombie does not automatically {$\beta$}-reduce expressions, but instead uses congruence closure for proof and type inference. We give a specification of a subset of the surface language via a bidirectional type system, which works ``up-to-congruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system. Zombie also features an optional termination-checker, allowing nonterminating programs returning proofs as well as external proofs about programs.},
  language = {en}
}

@inproceedings{Slesarenko2014firstclass,
  title = {First-Class {{Isomorphic Specialization}} by {{Staged Evaluation}}},
  booktitle = {Proceedings of the 10th {{ACM SIGPLAN Workshop}} on {{Generic Programming}}},
  author = {Slesarenko, Alexander and Filippov, Alexander and Romanov, Alexey},
  year = {2014},
  pages = {35--46},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2633628.2633632},
  abstract = {The state of the art approach for reducing complexity in software development is to use abstraction mechanisms of programming languages such as modules, types, higher-order functions etc. and develop high-level frameworks and domain-specific abstractions. Abstraction mechanisms, however, along with simplicity, introduce also execution overhead and often lead to significant performance degradation. Avoiding abstractions in favor of performance, on the other hand, increases code complexity and cost of maintenance. We develop a systematic approach and formalized framework for implementing software components with a first-class specialization capability. We show how to extend a higher-order functional language with abstraction mechanisms carefully designed to provide automatic and guaranteed elimination of abstraction overhead. We propose staged evaluation as a new method of program staging and show how it can be implemented as zipper-based traversal of program terms where one-hole contexts are generically constructed from the abstract syntax of the language. We show how generic programming techniques together with staged evaluation lead to a very simple yet powerful method of isomorphic specialization which utilizes first-class definitions of isomorphisms between data types to provide guarantee of abstraction elimination. We give a formalized description of the isomorphic specialization algorithm and show how it can be implemented as a set of term rewriting rules using active patterns and staged evaluation. We implemented our approach as a generic programming framework with first-class staging, term rewriting and isomorphic specialization and show in our evaluation that the proposed capabilities give rise to a new paradigm to develop domain-specific software components without abstraction penalty.},
  isbn = {978-1-4503-3042-8},
  keywords = {domain-specific languages,DSL,generic programming,isomorphisms,multi-stage programming,polytypic programming,specialization,staging},
  series = {{{WGP}} '14}
}

@inproceedings{Smolka2015fast,
  title = {A {{Fast Compiler}} for {{NetKAT}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Smolka, Steffen and Eliopoulos, Spiridon and Foster, Nate and Guha, Arjun},
  year = {2015},
  pages = {328--341},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784761},
  abstract = {High-level programming languages play a key role in a growing number of networking platforms, streamlining application development and enabling precise formal reasoning about network behavior. Unfortunately, current compilers only handle "local" programs that specify behavior in terms of hop-by-hop forwarding behavior, or modest extensions such as simple paths. To encode richer "global" behaviors, programmers must add extra state -- something that is tricky to get right and makes programs harder to write and maintain. Making matters worse, existing compilers can take tens of minutes to generate the forwarding state for the network, even on relatively small inputs. This forces programmers to waste time working around performance issues or even revert to using hardware-level APIs. This paper presents a new compiler for the NetKAT language that handles rich features including regular paths and virtual networks, and yet is several orders of magnitude faster than previous compilers. The compiler uses symbolic automata to calculate the extra state needed to implement "global" programs, and an intermediate representation based on binary decision diagrams to dramatically improve performance. We describe the design and implementation of three essential compiler stages: from virtual programs (which specify behavior in terms of virtual topologies) to global programs (which specify network-wide behavior in terms of physical topologies), from global programs to local programs (which specify behavior in terms of single-switch behavior), and from local programs to hardware-level forwarding tables. We present results from experiments on real-world benchmarks that quantify performance in terms of compilation time and forwarding table size.},
  isbn = {978-1-4503-3669-7},
  keywords = {binary decision diagrams,domain-specific languages,Frenetic,Kleene Algebra with tests,NetKAT,Software-defined networking,virtualization},
  series = {{{ICFP}} 2015}
}

@article{Smyth1982categorytheoretic,
  title = {The {{Category}}-{{Theoretic Solution}} of {{Recursive Domain Equations}}},
  author = {Smyth, M. and Plotkin, G.},
  year = {1982},
  month = nov,
  volume = {11},
  pages = {761--783},
  issn = {0097-5397},
  doi = {10.1137/0211062},
  abstract = {Recursive specifications of domains plays a crucial role in denotational semantics as developed by Scott and Strachey and their followers. The purpose of the present paper is to set up a categorical framework in which the known techniques for solving these equations find a natural place. The idea is to follow the well-known analogy between partial orders and categories, generalizing from least fixed-points of continuous functions over cpos to initial ones of continuous functors over \$\textbackslash{}omega \$-categories. To apply these general ideas we introduce Wand's \$\{\textbackslash{}bf O\}\$-categories where the morphism-sets have a partial order structure and which include almost all the categories occurring in semantics. The idea is to find solutions in a derived category of embeddings and we give order-theoretic conditions which are easy to verify and which imply the needed categorical ones. The main tool is a very general form of the limit-colimit coincidence remarked by Scott. In the concluding section we outline how compatibility considerations are to be included in the framework. A future paper will show how Scott's universal domain method can be included too.,  Recursive specifications of domains plays a crucial role in denotational semantics as developed by Scott and Strachey and their followers. The purpose of the present paper is to set up a categorical framework in which the known techniques for solving these equations find a natural place. The idea is to follow the well-known analogy between partial orders and categories, generalizing from least fixed-points of continuous functions over cpos to initial ones of continuous functors over \$\textbackslash{}omega \$-categories. To apply these general ideas we introduce Wand's \$\{\textbackslash{}bf O\}\$-categories where the morphism-sets have a partial order structure and which include almost all the categories occurring in semantics. The idea is to find solutions in a derived category of embeddings and we give order-theoretic conditions which are easy to verify and which imply the needed categorical ones. The main tool is a very general form of the limit-colimit coincidence remarked by Scott. In the concluding section we outline how compatibility considerations are to be included in the framework. A future paper will show how Scott's universal domain method can be included too.},
  journal = {SIAM Journal on Computing},
  number = {4}
}

@inproceedings{Solodkyy2012open,
  title = {Open and {{Efficient Type Switch}} for {{C}}++},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Solodkyy, Yuriy and Dos Reis, Gabriel and Stroustrup, Bjarne},
  year = {2012},
  pages = {963--982},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2384616.2384686},
  abstract = {Selecting operations based on the run-time type of an object is key to many object-oriented and functional programming techniques. We present a technique for implementing open and efficient type switching on hierarchical extensible data types. The technique is general and copes well with C++ multiple inheritance. To simplify experimentation and gain realistic performance using production-quality compilers and tool chains, we implement a type switch construct as an ISO C++11 library, called Mach7. This library-only implementation provides concise notation and outperforms the visitor design pattern, commonly used for case analysis on types in object-oriented programming. For closed sets of types, its performance roughly equals equivalent code in functional languages, such as OCaml and Haskell. The type-switching code is easier to use and is more expressive than hand-coded visitors are. The library is non-intrusive and circumvents most of the extensibility restrictions typical of the visitor design pattern. It was motivated by applications involving large, typed, abstract syntax trees.},
  isbn = {978-1-4503-1561-6},
  keywords = {C++,memoization,type switch,typecase,visitor design pattern},
  series = {{{OOPSLA}} '12}
}

@article{Soule2015river,
  title = {River: An Intermediate Language for Stream Processing},
  shorttitle = {River},
  author = {Soul{\'e}, Robert and Hirzel, Martin and Gedik, Bu{\u g}ra and Grimm, Robert},
  year = {2015},
  month = jun,
  pages = {n/a-n/a},
  issn = {1097-024X},
  doi = {10.1002/spe.2338},
  abstract = {This paper presents both a calculus for stream processing, named Brooklet, and its realization as an intermediate language, named River. Because River is based on Brooklet, it has a formal semantics that enables reasoning about the correctness of source translations and optimizations. River builds on Brooklet by addressing the real-world details that the calculus elides. We evaluated our system by implementing front-ends for three streaming languages, and three important optimizations, and a back-end for the System S distributed streaming runtime. Overall, we significantly lower the barrier to entry for new stream-processing languages and thus grow the ecosystem of this crucial style of programming. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  journal = {Software: Practice and Experience},
  keywords = {optimizations,stream processing},
  language = {en}
}

@inproceedings{Sperber1996realistic,
  title = {Realistic {{Compilation}} by {{Partial Evaluation}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1996 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Sperber, Michael and Thiemann, Peter},
  year = {1996},
  pages = {206--214},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/231379.231419},
  abstract = {Two key steps in the compilation of strict functional languages are the conversion of higher-order functions to data structures (closures) and the transformation to tail-recursive style. We show how to perform both steps at once by applying first-order offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to low-level C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern Scheme-to-C compilers. In addition, we have integrated various optimizations such as constant propagation, higher-order removal, and arity raising simply by modifying the underlying interpreter. Purely first-order methods suffice to achieve the transformations. Our approach is an instance of semantics-directed compiler generation.},
  isbn = {0-89791-795-2},
  keywords = {compilation of higher-order functional languages,partial evaluation,semantics-directed compiler generation},
  series = {{{PLDI}} '96}
}

@inproceedings{Sperber1997two,
  title = {Two for the {{Price}} of {{One}}: {{Composing Partial Evaluation}} and {{Compilation}}},
  shorttitle = {Two for the {{Price}} of {{One}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1997 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Sperber, Michael and Thiemann, Peter},
  year = {1997},
  pages = {215--225},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/258915.258935},
  abstract = {One of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a source-to-source transformation for high-level languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several meta-computation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a run-time code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreter-based experiments with a source-to-source version of the partial evaluator before building a realistic compiler which generates object code automatically.},
  isbn = {0-89791-907-6},
  series = {{{PLDI}} '97}
}

@article{Spiwack2015Notes,
  title = {Notes on Axiomatising {{Hurkens}}'s {{Paradox}}},
  author = {Spiwack, Arnaud},
  year = {2015},
  month = jul,
  abstract = {An axiomatisation of Hurkens's paradox in dependent type theory is given without assuming any impredicative feature of said type theory.},
  archivePrefix = {arXiv},
  eprint = {1507.04577},
  eprinttype = {arxiv},
  journal = {arXiv:1507.04577 [cs]},
  keywords = {Computer Science - Logic in Computer Science},
  primaryClass = {cs}
}

@incollection{St-Amour2012typing,
  title = {Typing the {{Numeric Tower}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {{St-Amour}, Vincent and {Tobin-Hochstadt}, Sam and Flatt, Matthew and Felleisen, Matthias},
  editor = {Russo, Claudio and Zhou, Neng-Fa},
  year = {2012},
  month = jan,
  pages = {289--303},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-27694-1_21},
  abstract = {In the past, the creators of numerical programs had to choose between simple expression of mathematical formulas and static type checking. While the Lisp family and its dynamically typed relatives support the straightforward expression via a rich numeric tower, existing statically typed languages force programmers to pollute textbook formulas with explicit coercions or unwieldy notation. In this paper, we demonstrate how the type system of Typed Racket accommodates both a textbook programming style and expressive static checking. The type system provides a hierarchy of numeric types that can be freely mixed as well as precise specifications of sign, representation, and range information\textemdash{}all while supporting generic operations. In addition, the type system provides information to the compiler so that it can perform standard numeric optimizations.},
  copyright = {\textcopyright{}2012 Springer-Verlag GmbH Berlin Heidelberg},
  isbn = {978-3-642-27693-4 978-3-642-27694-1},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {7149},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Stanier2013intermediate,
  title = {Intermediate {{Representations}} in {{Imperative Compilers}}: {{A Survey}}},
  shorttitle = {Intermediate {{Representations}} in {{Imperative Compilers}}},
  author = {Stanier, James and Watson, Des},
  year = {2013},
  month = jul,
  volume = {45},
  pages = {26:1--26:27},
  issn = {0360-0300},
  doi = {10.1145/2480741.2480743},
  abstract = {Compilers commonly translate an input program into an intermediate representation (IR) before optimizing it and generating code. Over time there have been a number of different approaches to designing and implementing IRs. Different IRs have varying benefits and drawbacks. In this survey, we highlight key developments in the area of IR for imperative compilers, group them by a taxonomy and timeline, and comment on the divide between academic research and real-world compiler technology. We conclude that mainstream compilers, especially in the multicore era, could benefit from further IR innovations.},
  journal = {ACM Comput. Surv.},
  keywords = {Compilers,intermediate representations,optimization},
  number = {3}
}

@inproceedings{Stark2019Autosubst,
  title = {Autosubst 2: {{Reasoning}} with {{Multi}}-Sorted {{De Bruijn Terms}} and {{Vector Substitutions}}},
  shorttitle = {Autosubst 2},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN International Conference}} on {{Certified Programs}} and {{Proofs}}},
  author = {Stark, Kathrin and Sch{\"a}fer, Steven and Kaiser, Jonas},
  year = {2019},
  pages = {166--180},
  publisher = {{ACM}},
  address = {{Cascais, Portugal}},
  doi = {10.1145/3293880.3294101},
  abstract = {Formalising metatheory in the Coq proof assistant is tedious as reasoning with binders without native support requires a lot of uninteresting technicalities. To relieve users from so-produced boilerplate, the Autosubst framework automates working with de Bruijn terms: For each annotated inductive type, Autosubst generates a corresponding instantiation operation for parallel substitutions and a decision procedure for assumption-free substitution lemmas. However, Autosubst is implemented in Ltac, Coq's tactic language, and thus suffers from Ltac's limitations. In particular, Autosubst is restricted to Coq and unscoped, non-mutual inductive types with a single sort of variables. In this paper, we present a new version of Autosubst that overcomes these restrictions. Autosubst 2 is an external code generator, which translates second-order HOAS specifications into potentially mutual inductive term sorts. We extend the equational theory of Autosubst to the case of mutual inductive sorts by combining the application of multiple parallel substitutions into exactly one instantiation operation for each sort, i.e. we parallelise substitutions to vector substitutions. The resulting equational theory is both simpler and more expressive than that of the original Autosubst framework and allows us to present an even more elegant proof of part A of the POPLMark challenge.},
  isbn = {978-1-4503-6222-1},
  keywords = {de Bruijn repersentation,multi-sorted terms,parallel substiutions,sigma-calculus},
  series = {{{CPP}} 2019}
}

@article{Statman1982Completeness,
  title = {Completeness, {{Invariance}} and {$\lambda$}-{{Definability}}},
  author = {Statman, R.},
  year = {1982},
  volume = {47},
  pages = {17--26},
  issn = {0022-4812},
  doi = {10.2307/2273377},
  journal = {The Journal of Symbolic Logic},
  number = {1}
}

@article{Statman1985Logical,
  title = {Logical Relations and the Typed {$\lambda$}-Calculus},
  author = {Statman, R.},
  year = {1985},
  month = may,
  volume = {65},
  pages = {85--97},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(85)80001-2},
  journal = {Information and Control},
  number = {2}
}

@inproceedings{Steele2009Organizing,
  title = {Organizing {{Functional Code}} for {{Parallel Execution}} or, {{Foldl}} and {{Foldr Considered Slightly Harmful}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Steele, Jr., Guy L.},
  year = {2009},
  pages = {1--2},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596550.1596551},
  abstract = {Alan Perlis, inverting OscarWilde's famous quip about cynics, once suggested, decades ago, that a Lisp programmer is one who knows the value of everything and the cost of nothing. Now that the conference on Lisp and Functional Programming has become ICFP, some may think that OCaml and Haskell programmers have inherited this (now undeserved) epigram. I do believe that as multicore processors are becoming prominent, and soon ubiquitous, it behooves all programmers to rethink their programming style, strategies, and tactics, so that their code may have excellent performance. For the last six years I have been part of a team working on a programming language, Fortress, that has borrowed ideas not only from Fortran, not only from Java, not only from Algol and Alphard and CLU, not only from MADCAP and MODCAP and MIRFAC and the Klerer-May system-but also from Haskell, and I would like to repay the favor. In this talk I will discuss three ideas (none original with me) that I have found to be especially powerful in organizing Fortress programs so that they may be executed equally effectively either sequentially or in parallel: user-defined associative operators (and, more generally, user-defined monoids); conjugate transforms of data; and monoid-caching trees (as described, for example, by Hinze and Paterson). I will exhibit pleasant little code examples (some original with me) that make use of these ideas.},
  isbn = {978-1-60558-332-7},
  keywords = {associative operator,conjugate transform,monoid,reduction,tree},
  series = {{{ICFP}} '09}
}

@inproceedings{Steensgaard1996pointsto,
  title = {Points-to {{Analysis}} in {{Almost Linear Time}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Steensgaard, Bjarne},
  year = {1996},
  pages = {32--41},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/237721.237727},
  abstract = {We present an interprocedural flow-insensitive points-to analysis based on type inference methods with an almost linear time cost complexity To our knowledge, this is the asymptotically fastest non-trivial interprocedural points-to analysis algorithm yet described The algorithm is based on a non-standard type system. The type inferred for any variable represents a set of locations and includes a type which in turn represents a set of locations possibly pointed to by the variable. The type inferred for a function variable represents a set of functions It may point to and includes a type signature for these functions The results are equivalent to those of a flow-insensitive alias analysis (and control flow analysis) that assumes alias relations are reflexive and transitive.This work makes three contributions. The first is a type system for describing a universally valid storage shape graph for a program in linear space. The second is a constraint system which often leads to better results than the "obvious" constraint system for the given type system The third is an almost linear time algorithm for points-to analysis by solving a constraint system.},
  isbn = {0-89791-769-3},
  series = {{{POPL}} '96}
}

@article{Stefik2013empirical,
  title = {An {{Empirical Investigation}} into {{Programming Language Syntax}}},
  author = {Stefik, Andreas and Siebert, Susanna},
  year = {2013},
  month = nov,
  volume = {13},
  pages = {19:1--19:40},
  issn = {1946-6226},
  doi = {10.1145/2534973},
  abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.},
  journal = {Trans. Comput. Educ.},
  keywords = {Novice Programmers,programming languages,Syntax},
  number = {4}
}

@inproceedings{Stefik2014programming,
  title = {The {{Programming Language Wars}}: {{Questions}} and {{Responsibilities}} for the {{Programming Language Community}}},
  shorttitle = {The {{Programming Language Wars}}},
  booktitle = {Proceedings of the 2014 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \&\#38; {{Software}}},
  author = {Stefik, Andreas and Hanenberg, Stefan},
  year = {2014},
  pages = {283--299},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2661136.2661156},
  abstract = {The discipline of computer science has a long and complicated history with computer programming languages. Historically, inventors have created language products for a wide variety of reasons, from attempts at making domain specific tasks easier or technical achievements, to economic, social, or political reasons. As a consequence, the modern programming language industry now has a large variety of incompatible programming languages, each of which with unique syntax, semantics, toolsets, and often their own standard libraries, lifetimes, and costs. In this paper, we suggest that the programming language wars, a term which describes the broad divergence and impact of language designs, including often pseudo-scientific claims made that they are good or bad, may be negatively impacting the world. This broad problem, which is almost completely ignored in computer science, needs to be acted upon by the community.},
  isbn = {978-1-4503-3210-1},
  keywords = {evidence standards,stability of the academic literature,the programming language wars},
  series = {Onward! '14}
}

@inproceedings{Stein1987Delegation,
  title = {Delegation Is {{Inheritance}}},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}}},
  author = {Stein, Lynn Andrea},
  year = {1987},
  pages = {138--146},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/38765.38820},
  abstract = {Inheritance and delegation are alternate methods for incremental definition and sharing. It has commonly been believed that delegation provides a more powerful model. This paper demonstrates that there is a ``natural'' model of inheritance which captures all of the properties of delegation. Independently, certain constraints on the ability of delegation to capture inheritance are demonstrated. Finally, a new framework which fully captures both delegation and inheritance is outlined, and some of the ramifications of this hybrid model are explored.},
  isbn = {0-89791-247-0},
  keywords = {_tablet},
  series = {{{OOPSLA}} '87}
}

@incollection{Stepp2011equalitybased,
  title = {Equality-{{Based Translation Validator}} for {{LLVM}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Stepp, Michael and Tate, Ross and Lerner, Sorin},
  editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
  year = {2011},
  month = jan,
  pages = {737--742},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.},
  copyright = {\textcopyright{}2011 Springer-Verlag GmbH Berlin Heidelberg},
  isbn = {978-3-642-22109-5 978-3-642-22110-1},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {6806},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Steuwer2015generating,
  title = {Generating {{Performance Portable Code Using Rewrite Rules}}: {{From High}}-Level {{Functional Expressions}} to {{High}}-Performance {{OpenCL Code}}},
  shorttitle = {Generating {{Performance Portable Code Using Rewrite Rules}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe},
  year = {2015},
  pages = {205--217},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784754},
  abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.},
  isbn = {978-1-4503-3669-7},
  keywords = {Algorithmic patterns,code generation,GPU,OpenCL,performance portability,rewrite rules},
  series = {{{ICFP}} 2015}
}

@inproceedings{Stolarek2015Injective,
  title = {Injective {{Type Families}} for {{Haskell}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Stolarek, Jan and Peyton Jones, Simon and Eisenberg, Richard A.},
  year = {2015},
  pages = {118--128},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2804302.2804314},
  abstract = {Haskell, as implemented by the Glasgow Haskell Compiler (GHC), allows expressive type-level programming. The most popular type-level programming extension is TypeFamilies, which allows users to write functions on types. Yet, using type functions can cripple type inference in certain situations. In particular, lack of injectivity in type functions means that GHC can never infer an instantiation of a type variable appearing only under type functions. In this paper, we describe a small modification to GHC that allows type functions to be annotated as injective. GHC naturally must check validity of the injectivity annotations. The algorithm to do so is surprisingly subtle. We prove soundness for a simplification of our algorithm, and state and prove a completeness property, though the algorithm is not fully complete. As much of our reasoning surrounds functions defined by a simple pattern-matching structure, we believe our results extend beyond just Haskell. We have implemented our solution on a branch of GHC and plan to make it available to regular users with the next stable release of the compiler.},
  isbn = {978-1-4503-3808-0},
  keywords = {functional dependencies,Haskell,injectivity,type families,type-level programming},
  series = {Haskell '15}
}

@inproceedings{Stone2000Deciding,
  title = {Deciding {{Type Equivalence}} in a {{Language}} with {{Singleton Kinds}}},
  booktitle = {Proceedings of the 27th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Stone, Christopher A. and Harper, Robert},
  year = {2000},
  pages = {214--227},
  publisher = {{ACM}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/325694.325724},
  abstract = {Work on the TILT compiler for Standard ML led us to study a language with singleton kinds: S(A) is the kind of all types provably equivalent to the type A. Singletons are interesting because they provide a very general form of definitions for type variables, allow fine-grained control of type computations, and allow many equational constraints to be expressed within the type system.
Internally, TILT represents programs using a predicative variant of Girard's F\&ohgr; enriched with singleton kinds, dependent product and function kinds (\&Sgr; and \&Pgr;), and a sub-kinding relation. An important benefit of using a typed language as the representation of programs is that typechecking can detect many common compiler implementation errors. However, the decidability of typechecking for our particular representation is not obvious. In order to typecheck a term, we must be able to determine whether two type constructors are provably equivalent. But in the presence of singleton kinds, the equivalence of type constructors depends both on the typing context in which they are compared and on the kind at which they are compared.
In this paper we concentrate on the key issue for decidability of typechecking: determining the equivalence of well-formed type constructors. We define the \&lgr;\&Pgr;\&Sgr;S{$<$} calculus, a model of the constructors and kinds of TILT's intermediate language. Inspired by Coquand's result for type theory, we prove decidability of constructor equivalence for \&lgr;\&Pgr;\&Sgr;S{$\leq$} by exhibiting a novel \textemdash{} though slightly inefficient \textemdash{} type-directed comparison algorithm. The correctness of this algorithm is proved using an interesting variant of Kripke-style logical relations: unary relations are indexed by a single possible world (in our case, a typing context), but binary relations are indexed by two worlds. Using this result we can then show the correctness of a natural, practical algorithm used by the TILT compiler.},
  isbn = {978-1-58113-125-3},
  series = {{{POPL}} '00}
}

@article{Stone2006extensional,
  title = {Extensional {{Equivalence}} and {{Singleton Types}}},
  author = {Stone, Christopher A. and Harper, Robert},
  year = {2006},
  month = oct,
  volume = {7},
  pages = {676--722},
  issn = {1529-3785},
  doi = {10.1145/1183278.1183281},
  abstract = {We study the \&lamda;{$\Pi\Sigma$}S{$\leq$} calculus, which contains singleton types S(M) classifying terms of base type provably equivalent to the term M. The system includes dependent types for pairs and functions ({$\Sigma$} and {$\Pi$}) and a subtyping relation induced by regarding singletons as subtypes of the base type. The decidability of type checking for this language is non-obvious, since to type check we must be able to determine equivalence of well-formed terms. But in the presence of singleton types, the provability of an equivalence judgment {$\Gamma$} {$\vdash$} M1 \&eqiv;M2 : A can depend both on the typing context {$\Gamma$} and on the particular type A at which M1 and M2 are compared.We show how to prove decidability of term equivalence, hence of type checking, in \&lamda;{$\Pi\Sigma$}S{$\leq$} by exhibiting a type-directed algorithm for directly computing normal forms. The correctness of normalization is shown using an unusual variant of Kripke logical relations organized around sets; rather than defining a logical equivalence relation, we work directly with (subsets of) the corresponding equivalence classes.We then provide a more efficient algorithm for checking type equivalence without constructing normal forms. We also show that type checking, subtyping, and all other judgments of the system are decidable.The \&lamda;{$\Pi\Sigma$}S{$\leq$} calculus models type constructors and kinds in the intermediate language used by the TILT compiler for Standard ML to implement the SML module system. The decidability of \&lamda;{$\Pi\Sigma$}S{$\leq$} term equivalence allows us to show decidability of type checking for TILT's intermediate language. We also obtain a consistency result that allows us to prove type safety for the intermediate language. The algorithms derived here form the core of the type checker used for internal type checking in TILT.},
  journal = {ACM Trans. Comput. Logic},
  keywords = {_tablet,equivalence algorithms,logical relations,singleton types},
  number = {4}
}

@article{Stovring2012First,
  title = {First Steps in Synthetic Guarded Domain Theory: Step-Indexing in the Topos of Trees},
  shorttitle = {First Steps in Synthetic Guarded Domain Theory},
  author = {St{\o}vring, Kristian and Schwinghammer, Jan and M{\o}gelberg, Rasmus Ejlers and Birkedal, Lars},
  year = {2012},
  month = oct,
  volume = {Volume 8, Issue 4},
  abstract = {We present the topos S of trees as a model of guarded recursion. We study the internal dependently-typed higher-order logic of S and show that S models two modal operators, on predicates and types, which serve as guards in recursive definitions of terms, predicates, and types. In particular, we show how to solve recursive type equations involving dependent types. We propose that the internal logic of S provides the right setting for the synthetic construction of abstract versions of step-indexed models of programming languages and program logics. As an example, we show how to construct a model of a programming language with higher-order store and recursive types entirely inside the internal logic of S. Moreover, we give an axiomatic categorical treatment of models of synthetic guarded domain theory and prove that, for any complete Heyting algebra A with a well-founded basis, the topos of sheaves over A forms a model of synthetic guarded domain theory, generalizing the results for S.},
  journal = {Logical Methods in Computer Science},
  language = {en}
}

@incollection{Stoyan1991Artificial,
  title = {Artificial {{Intelligence}} and {{Mathematical Theory}} of {{Computation}}},
  author = {Stoyan, Herbert},
  editor = {Lifschitz, Vladimir},
  year = {1991},
  pages = {409--426},
  publisher = {{Academic Press Professional, Inc.}},
  address = {{San Diego, CA, USA}},
  isbn = {0-12-450010-2}
}

@inproceedings{Stucki2015rrb,
  title = {{{RRB Vector}}: {{A Practical General Purpose Immutable Sequence}}},
  shorttitle = {{{RRB Vector}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Stucki, Nicolas and Rompf, Tiark and Ureche, Vlad and Bagwell, Phil},
  year = {2015},
  pages = {342--354},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784739},
  abstract = {State-of-the-art immutable collections have wildly differing performance characteristics across their operations, often forcing programmers to choose different collection implementations for each task. Thus, changes to the program can invalidate the choice of collections, making code evolution costly. It would be desirable to have a collection that performs well for a broad range of operations. To this end, we present the RRB-Vector, an immutable sequence collection that offers good performance across a large number of sequential and parallel operations. The underlying innovations are: (1) the Relaxed-Radix-Balanced (RRB) tree structure, which allows efficient structural reorganization, and (2) an optimization that exploits spatio-temporal locality on the RRB data structure in order to offset the cost of traversing the tree. In our benchmarks, the RRB-Vector speedup for parallel operations is lower bounded by 7x when executing on 4 CPUs of 8 cores each. The performance for discrete operations, such as appending on either end, or updating and removing elements, is consistently good and compares favorably to the most important immutable sequence collections in the literature and in use today. The memory footprint of RRB-Vector is on par with arrays and an order of magnitude less than competing collections.},
  isbn = {978-1-4503-3669-7},
  keywords = {arrays,Data Structures,Immutable,Radix-Balanced,Relaxed-Radix-Balanced,Sequences,Trees,Vectors},
  series = {{{ICFP}} 2015}
}

@article{Stucki2017Higherorder,
  title = {Higher-Order Subtyping with Type Intervals},
  author = {Stucki, Sandro},
  year = {2017},
  doi = {10.5075/epfl-thesis-8014},
  abstract = {Modern, statically typed programming languages provide various abstraction facilities at both the term- and type-level. Common abstraction mechanisms for types include parametric polymorphism -- a hallmark of functional languages -- and subtyping -- which is pervasive in object-oriented languages. Additionally, both kinds of languages may allow parametrized (or generic) datatype definitions in modules or classes. When several of these features are present in the same language, new and more expressive combinations arise, such as (1) bounded quantification, (2) bounded operator abstractions and (3) translucent type definitions. An example of such a language is Scala, which features all three of the aforementioned type-level constructs. This increases the expressivity of the language, but also the complexity of its type system. From a theoretical point of view, the various abstraction mechanisms have been studied through different extensions of Girard's higher-order polymorphic lambda-calculus F-omega. Higher-order subtyping and bounded polymorphism (1 and 2) have been formalized in F-omega-sub and its many variants; type definitions of various degrees of opacity (3) have been formalized through extensions of F-omega with singleton types. In this dissertation, I propose type intervals as a unifying concept for expressing (1--3) and other related constructs. In particular, I develop an extension of F-omega with interval kinds as a formal theory of higher-order subtyping with type intervals, and show how the familiar concepts of higher-order bounded quantification, bounded operator abstraction and singleton kinds can all be encoded in a semantics-preserving way using interval kinds. Going beyond the status quo, the theory is expressive enough to also cover less familiar constructs, such as lower-bounded operator abstractions and first-class, higher-order inequality constraints. I establish basic metatheoretic properties of the theory: I prove that subject reduction holds for well-kinded types w.r.t. full beta-reduction, that types and kinds are weakly normalizing, and that the theory is type safe w.r.t. its call-by-value operational reduction semantics. Key to this metatheoretic development is the use of hereditary substitution and the definition of an equivalent, canonical presentation of subtyping, which involves only normal types and kinds. The resulting metatheory is entirely syntactic, i.e. does not involve any model constructions, and has been fully mechanized in Agda. The extension of F-omega with interval kinds constitutes a stepping stone to the development of a higher-order version of the calculus of Dependent Object Types (DOT) -- the theoretical foundation of Scala's type system. In the last part of this dissertation, I briefly sketch a possible extension of the theory toward this goal and discuss some of the challenges involved in adapting the existing metatheory to that extension. Stucki, Sandro},
  language = {en}
}

@inproceedings{Stucki2018Truly,
  title = {Truly Abstract Interfaces for Algebraic Data Types: {{The}} Extractor Typing Problem},
  shorttitle = {Truly Abstract Interfaces for Algebraic Data Types},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  author = {Stucki, Nicolas and Giarrusso, Paolo G. and Odersky, Martin},
  year = {2018},
  pages = {56--60},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3241653.3241658},
  abstract = {Pattern matching enables inspecting algebraic data types, but typically prevents hiding the implementation of the matched algebraic data type. In Scala, instead, extractors also allow pattern matching on non-algebraic data types and invoking methods on the obtained objects, while partially decoupling API consumers from the API implementation.  But as we show in this paper, pattern matching using extractors is restricted compared to matching against case classes. We argue this violates the appropriate variant of the uniform access principle. To address this problem, we propose a small language extension, which enables defining truly abstract interfaces and freely evolve their implementation.},
  isbn = {978-1-4503-5836-1},
  keywords = {abstract types,extractors,interfaces,pattern matching,Scala},
  series = {Scala 2018}
}

@book{Stump1999Coquand,
  title = {On {{Coquand}}'s "{{An Analysis}} of {{Girard}}'s {{Paradox}}"},
  author = {Stump, Aaron},
  year = {1999},
  abstract = {In his paper "An Analysis of Girard's Paradox" [3], Coquand presents a result  of Girard that minimal higher-order logic extended with quantification over types is  inconsistent. Using Girard's idea, he shows that some other extensions of minimal  higher-order logic, several extensions of the calculus of constructions, and an early  calculus of Martin-Lof with type:type are also inconsistent. He also presents several  consistent extensions of minimal higher-order logic and the calculus of constructions.  In this paper, I survey relevant background material and present two of Coquand's  proofs of inconsistency.  1 Background  In this section, I present some material that is relevant for understanding Coquand's results. Readers familiar with this material may wish to skip to section 2 and refer to this section later as necessary.  1.1 Styles of Axiomatization  There are two styles commonly used for axiomatizing various logics; the style of natural deduction and the style of Hilbert. Syst...}
}

@article{Stump2017calculus,
  title = {The Calculus of Dependent Lambda Eliminations},
  author = {Stump, Aaron},
  year = {2017/ed},
  volume = {27},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796817000053},
  abstract = {Abstract
Modern constructive type theory is based on pure dependently typed lambda calculus, augmented with user-defined datatypes. This paper presents an alternative called the Calculus of Dependent Lambda Eliminations, based on pure lambda encodings with no auxiliary datatype system. New typing constructs are defined that enable induction, as well as large eliminations with lambda encodings. These constructs are constructor-constrained recursive types, and a lifting operation to lift simply typed terms to the type level. Using a lattice-theoretic denotational semantics for types, the language is proved logically consistent. The power of CDLE is demonstrated through several examples, which have been checked with a prototype implementation called Cedille.},
  journal = {Journal of Functional Programming},
  language = {en}
}

@article{Stump2018realizability,
  title = {From Realizability to Induction via Dependent Intersection},
  author = {Stump, Aaron},
  year = {2018},
  month = jul,
  volume = {169},
  pages = {637--655},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2018.03.002},
  abstract = {In this paper, it is shown that induction is derivable in a type-assignment formulation of the second-order dependent type theory {$\lambda$}P2, extended with the implicit product type of Miquel, dependent intersection type of Kopylov, and a built-in equality type. The crucial idea is to use dependent intersections to internalize a result of Leivant's showing that Church-encoded data may be seen as realizing their own type correctness statements, under the Curry\textendash{}Howard isomorphism.},
  journal = {Annals of Pure and Applied Logic},
  keywords = {Derivable induction,Extrinsic typing,Internalized realizability,Lambda encodings},
  number = {7}
}

@misc{SubtypingRelation,
  title = {On {{Subtyping}}-{{Relation Completeness}}, with an {{Application}} to {{Iso}}-{{Recursive Types}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/314306031\_On\_Subtyping-Relation\_Completeness\_with\_an\_Application\_to\_Iso-Recursive\_Types},
  journal = {ResearchGate}
}

@article{Sujeeth2014delite,
  title = {Delite: {{A Compiler Architecture}} for {{Performance}}-{{Oriented Embedded Domain}}-{{Specific Languages}}},
  shorttitle = {Delite},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
  year = {2014},
  month = apr,
  volume = {13},
  pages = {134:1--134:25},
  issn = {1539-9087},
  doi = {10.1145/2584665},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  journal = {ACM Trans. Embed. Comput. Syst.},
  keywords = {code generation,domain-specific languages,language virtualization,multistage programming},
  number = {4s}
}

@article{Sulzmann2008Type,
  title = {Type Inference for {{GADTs}} via {{Herbrand}} Constraint Abduction},
  author = {Sulzmann, Martin and Schrijvers, Tom and Stuckey, Peter J.},
  year = {2008},
  month = jan,
  language = {en}
}

@inproceedings{Sumii2005Bisimulation,
  title = {A {{Bisimulation}} for {{Type Abstraction}} and {{Recursion}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Sumii, Eijiro and Pierce, Benjamin C.},
  year = {2005},
  pages = {63--74},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1040305.1040311},
  abstract = {We present a sound, complete, and elementary proof method, based on bisimulation, for contextual equivalence in a {$\lambda$}-calculus with full universal, existential, and recursive types. Unlike logical relations (either semantic or syntactic), our development is elementary, using only sets and relations and avoiding advanced machinery such as domain theory, admissibility, and {$TT$}-closure. Unlike other bisimulations, ours is complete even for existential types. The key idea is to consider sets of relations---instead of just relations---as bisimulations.},
  isbn = {1-58113-830-X},
  keywords = {bisimulations,contextual equivalence,existential types,Lambda-calculus,logical relations,recursive types},
  series = {{{POPL}} '05}
}

@inproceedings{Summers2011Freedom,
  title = {Freedom before Commitment: {{A}} Lightweight Type System for Object Initialisation},
  shorttitle = {Freedom before Commitment},
  booktitle = {Proceedings of the 2011 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Summers, Alexander J. and Mueller, Peter},
  year = {2011},
  pages = {1013--1032},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2048066.2048142},
  abstract = {One of the main purposes of object initialisation is to establish invariants such as a field being non-null or an immutable data structure containing specific values. These invariants are then implicitly assumed by the rest of the implementation, for instance, to ensure that a field may be safely dereferenced or that immutable data may be accessed concurrently. Consequently, letting an object escape from its constructor is dangerous; the escaping object might not yet satisfy its invariants, leading to errors in code that relies on them. Nevertheless, preventing objects entirely from escaping from their constructors is too restrictive; it is often useful to call auxiliary methods on the object under initialisation or to pass it to another constructor to set up mutually-recursive structures. We present a type system that tracks which objects are fully initialised and which are still under initialisation. The system can be used to prevent objects from escaping, but also to allow safe escaping by making explicit which objects might not yet satisfy their invariants. We designed, formalised and implemented our system as an extension to a non-null type system, but it is not limited to this application. Our system is conceptually simple and requires little annotation overhead; it is sound and sufficiently expressive for many common programming idioms. Therefore, we believe it to be the first such system suitable for mainstream use.},
  isbn = {978-1-4503-0940-0},
  keywords = {expressive,initialisation,modular,non-null,simple,sound,type-system},
  series = {{{OOPSLA}} '11}
}

@inproceedings{Suzuki2016finally,
  title = {Finally, {{Safely}}-Extensible and {{Efficient Language}}-Integrated {{Query}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  author = {Suzuki, Kenichi and Kiselyov, Oleg and Kameyama, Yukiyoshi},
  year = {2016},
  pages = {37--48},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2847538.2847542},
  abstract = {Language-integrated query is an embedding of database queries into a host language to code queries at a higher level than the all-to-common concatenation of strings of SQL fragments. The eventually produced SQL is ensured to be well-formed and well-typed, and hence free from the embarrassing (security) problems. Language-integrated query takes advantage of the host language's functional and modular abstractions to compose and reuse queries and build query libraries. Furthermore, language-integrated query systems like T-LINQ generate efficient SQL, by applying a number of program transformations to the embedded query. Alas, the set of transformation rules is not designed to be extensible. We demonstrate a new technique of integrating database queries into a typed functional programming language, so to write well-typed, composable queries and execute them efficiently on any SQL back-end as well as on an in-memory noSQL store. A distinct feature of our framework is that both the query language as well as the transformation rules needed to generate efficient SQL are safely user-extensible, to account for many variations in the SQL back-ends, as well for domain-specific knowledge. The transformation rules are guaranteed to be type-preserving and hygienic by their very construction. They can be built from separately developed and reusable parts and arbitrarily composed into optimization pipelines. With this technique we have embedded into OCaml a relational query language that supports a very large subset of SQL including grouping and aggregation. Its types cover the complete set of intricate SQL behaviors.},
  isbn = {978-1-4503-4097-7},
  keywords = {EDSL,language-integrated query,LINQ,sql,tagless-final},
  series = {{{PEPM}} 2016}
}

@inproceedings{Svendsen2014Impredicative,
  title = {Impredicative {{Concurrent Abstract Predicates}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Svendsen, Kasper and Birkedal, Lars},
  year = {2014},
  month = apr,
  pages = {149--168},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-54833-8_9},
  abstract = {We present impredicative concurrent abstract predicates \textendash{} iCAP \textendash{} a program logic for modular reasoning about concurrent, higher-order, reentrant, imperative code. Building on earlier work, iCAP uses protocols to reason about shared mutable state. A key novel feature of iCAP is the ability to define impredicative protocols; protocols that are parameterized on arbitrary predicates, including predicates that themselves refer to protocols. We demonstrate the utility of impredicative protocols through a series of examples, including the specification and verification, in the logic, of a spin-lock, a reentrant event loop, and a concurrent bag implemented using cooperation, against modular specifications.},
  isbn = {978-3-642-54832-1 978-3-642-54833-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Svendsen2016Transfinite,
  title = {Transfinite {{Step}}-{{Indexing}}: {{Decoupling Concrete}} and {{Logical Steps}}},
  shorttitle = {Transfinite {{Step}}-{{Indexing}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Svendsen, Kasper and Sieczkowski, Filip and Birkedal, Lars},
  year = {2016},
  month = apr,
  pages = {727--751},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-49498-1_28},
  abstract = {Step-indexing has proven to be a powerful technique for defining logical relations for languages with advanced type systems and models of expressive program logics. In both cases, the model is stratified using natural numbers to solve a recursive equation that has no naive solutions. As a result of this stratification, current models require that each unfolding of the recursive equation \textendash{} each logical step \textendash{} must coincide with a concrete reduction step. This tight coupling is problematic for applications where the number of logical steps cannot be statically bounded.In this paper we demonstrate that this tight coupling between logical and concrete steps is artificial and show how to loosen it using transfinite step-indexing. We present a logical relation that supports an arbitrary but finite number of logical steps for each concrete step.},
  isbn = {978-3-662-49497-4 978-3-662-49498-1},
  keywords = {_tablet},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Svenningsson2013combining,
  title = {Combining {{Deep}} and {{Shallow Embedding}} for {{EDSL}}},
  booktitle = {Trends in {{Functional Programming}}},
  author = {Svenningsson, Josef and Axelsson, Emil},
  editor = {Loidl, Hans-Wolfgang and Pe{\~n}a, Ricardo},
  year = {2013},
  month = jan,
  pages = {21--36},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {When compiling embedded languages it is natural to use an abstract syntax tree to represent programs. This is known as a deep embedding and it is a rather cumbersome technique compared to other forms of embedding, typically leading to more code and being harder to extend. In shallow embeddings, language constructs are mapped directly to their semantics which yields more flexible and succinct implementations. But shallow embeddings are not well-suited for compiling embedded languages. We present a technique to combine deep and shallow embedding in the context of compiling embedded languages in order to provide the benefits of both techniques. In particular it helps keeping the deep embedding small and it makes extending the embedded language much easier. Our technique also has some unexpected but welcome knock-on effects. It provides fusion of functions to remove intermediate results for free without any additional effort. It also helps to give the embedded language a more natural programming interface.},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-40446-7 978-3-642-40447-4},
  keywords = {Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques},
  language = {en},
  number = {7829},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Swasey2017Robust,
  title = {Robust and Compositional Verification of Object Capability Patterns},
  author = {Swasey, David and Garg, Deepak and Dreyer, Derek},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {89:1--89:26},
  issn = {2475-1421},
  doi = {10.1145/3133913},
  abstract = {In scenarios such as web programming, where code is linked together from multiple sources, object capability patterns (OCPs) provide an essential safeguard, enabling programmers to protect the private state of their objects from corruption by unknown and untrusted code. However, the benefits of OCPs in terms of program verification have never been properly formalized. In this paper, building on the recently developed Iris framework for concurrent separation logic, we develop OCPL, the first program logic for compositionally specifying and verifying OCPs in a language with closures, mutable state, and concurrency. The key idea of OCPL is to account for the interface between verified and untrusted code by adopting a well-known idea from the literature on security protocol verification, namely robust safety. Programs that export only properly wrapped values to their environment can be proven robustly safe, meaning that their untrusted environment cannot violate their internal invariants. We use OCPL to give the first general, compositional, and machine-checked specs for several commonly-used OCPs{\^a}including the dynamic sealing, membrane, and caretaker patterns{\^a}which we then use to verify robust safety for representative client code. All our results are fully mechanized in the Coq proof assistant.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {compositional verification,logical relations,object capabilities,robust safety,separation logic},
  number = {OOPSLA}
}

@article{Swierstra2008data,
  title = {Data Types {\`a} La Carte},
  author = {Swierstra, Wouter},
  year = {2008},
  month = jul,
  volume = {18},
  pages = {423--436},
  issn = {1469-7653},
  doi = {10.1017/S0956796808006758},
  abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell's monolithic IO monad.},
  journal = {Journal of Functional Programming},
  number = {04}
}

@inproceedings{Syme2007Extensible,
  title = {Extensible Pattern Matching via a Lightweight Language Extension},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Syme, Don and Neverov, Gregory and Margetson, James},
  year = {2007},
  pages = {29--40},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1291151.1291159},
  abstract = {Pattern matching of algebraic data types (ADTs) is a standard feature in typed functional programming languages, but it is well known that it interacts poorly with abstraction. While several partial solutions to this problem have been proposed, few have been implemented or used. This paper describes an extension to the .NET language F\# called active patterns, which supports pattern matching over abstract representations of generic heterogeneous data such as XML and term structures, including where these are represented via object models in other .NET languages. Our design is the first to incorporate both ad hoc pattern matching functions for partial decompositions and "views" for total decompositions, and yet remains a simple and lightweight extension. We give a description of the language extension along with numerous motivating examples. Finally we describe how this feature would interact with other reasonable and related language extensions: existential types quantified at data discrimination tags, GADTs, and monadic generalizations of pattern matching.},
  isbn = {978-1-59593-815-2},
  keywords = {F\#,functional programming,ML,pattern matching},
  series = {{{ICFP}} '07}
}

@inproceedings{Taha1997multistage,
  title = {Multi-Stage {{Programming}} with {{Explicit Annotations}}},
  booktitle = {Proceedings of the 1997 {{ACM SIGPLAN Symposium}} on {{Partial Evaluation}} and {{Semantics}}-Based {{Program Manipulation}}},
  author = {Taha, Walid and Sheard, Tim},
  year = {1997},
  pages = {203--217},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/258993.259019},
  abstract = {We introduce MetaML, a statically-typed multi-stage programming language extending Nielson and Nielson's two stage notation to an arbitrary number of stages. MetaML extends previous work by introducing four distinct staging annotations which generalize those published previously [25, 12, 7, 6]We give a static semantics in which type checking is done once and for all before the first stage, and a dynamic semantics which introduces a new concept of cross-stage persistence, which requires that variables available in any stage are also available in all future stages.We illustrate that staging is a manual form of binding time analysis. We explain why, even in the presence of automatic binding time analysis, explicit annotations are useful, especially for programs with more than two stages.A thesis of this paper is that multi-stage languages are useful as programming languages in their own right, and should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three stage example, and an extended two-stage example elaborating a number of practical issues.},
  isbn = {0-89791-917-3},
  series = {{{PEPM}} '97}
}

@article{Taha2000metaml,
  title = {{{MetaML}} and Multi-Stage Programming with Explicit Annotations},
  author = {Taha, Walid and Sheard, Tim},
  year = {2000},
  month = oct,
  volume = {248},
  pages = {211--242},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(00)00053-0},
  abstract = {We introduce MetaML, a practically motivated, statically typed multi-stage programming language. MetaML is a ``real'' language. We have built an implementation and used it to solve multi-stage problems. MetaML allows the programmer to construct, combine, and execute code fragments in a type-safe manner. Code fragments can contain free variables, but they obey the static-scoping principle. MetaML performs type-checking for all stages once and for all before the execution of the first stage. Certain anomalies with our first MetaML implementation led us to formalize an illustrative subset of the MetaML implementation. We present both a big-step semantics and type system for this subset, and prove the type system's soundness with respect to a big-step semantics. From a software engineering point of view, this means that generators written in the MetaML subset never generate unsafe programs. A type system and semantics for full MetaML is still ongoing work. We argue that multi-stage languages are useful as programming languages in their own right, that they supply a sound basis for high-level program generation technology, and that they should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three-stage example elaborating a number of practical issues. The design of MetaML was based on two main principles that we identified as fundamental for high-level program generation, namely, cross-stage persistence and cross-stage safety. We present these principles, explain the technical problems they give rise to, and how we address with these problems in our implementation.},
  journal = {Theoretical Computer Science},
  keywords = {functional programming,High-level program generation,Multi-level languages,Multi-stage languages,Programming language semantics,Type-safety,Type-systems,λ-Calculus},
  number = {1\textendash{}2}
}

@inproceedings{Tahboub2018How,
  title = {How to {{Architect}} a {{Query Compiler}}, {{Revisited}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Tahboub, Ruby Y. and Essertel, Gr{\'e}gory M. and Rompf, Tiark},
  year = {2018},
  pages = {307--322},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3183713.3196893},
  abstract = {To leverage modern hardware platforms to their fullest, more and more database systems embrace compilation of query plans to native code. In the research community, there is an ongoing debate about the best way to architect such query compilers. This is perceived to be a difficult task, requiring techniques fundamentally different from traditional interpreted query execution.   We aim to contribute to this discussion by drawing attention to an old but underappreciated idea known as Futamura projections, which fundamentally link interpreters and compilers. Guided by this idea, we demonstrate that efficient query compilation can actually be very simple, using techniques that are no more difficult than writing a query interpreter in a high-level language. Moreover, we demonstrate how intricate compilation patterns that were previously used to justify multiple compiler passes can be realized in one single, straightforward, generation pass. Key examples are injection of specialized index structures, data representation changes such as string dictionaries, and various kinds of code motion to reduce the amount of work on the critical path.  We present LB2: a high-level query compiler developed in this style that performs on par with, and sometimes beats, the best compiled query engines on the standard TPC-H benchmark.},
  isbn = {978-1-4503-4703-7},
  keywords = {futamura projections,query compilation},
  series = {{{SIGMOD}} '18}
}

@article{Tait1967Intensional,
  title = {Intensional Interpretations of Functionals of Finite Type {{I}}},
  author = {Tait, W. W.},
  year = {1967},
  month = aug,
  volume = {32},
  pages = {198--212},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2271658},
  abstract = {T0 will denote G{\"o}del's theory T[3] of functionals of finite type (f.t.) with intuitionistic quantification over each f.t. added. T1 will denote T0 together with definition by bar recursion of type o, the axiom schema of bar induction, and the schemaof choice. Precise descriptions of these systems are given below in \textsection{}4. The main results of this paper are interpretations of T0 in intuitionistic arithmetic U0 and of T1 in intuitionistic analysis is U1. U1 is U0 with quantification over functionals of type (0,0) and the axiom schemata AC00 and of bar induction.},
  journal = {The Journal of Symbolic Logic},
  number = {2}
}

@inproceedings{Tait1975realizability,
  title = {A Realizability Interpretation of the Theory of Species},
  booktitle = {Logic {{Colloquium}}},
  author = {Tait, William W.},
  editor = {Parikh, Rohit},
  year = {1975},
  pages = {240--251},
  publisher = {{Springer Berlin Heidelberg}},
  isbn = {978-3-540-37483-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Mathematics}}}
}

@article{Tarditi2004til,
  title = {{{TIL}}: {{A Type}}-Directed, {{Optimizing Compiler}} for {{ML}}},
  shorttitle = {{{TIL}}},
  author = {Tarditi, David and Morrisett, Greg and Cheng, Perry and Stone, Chris and Harper, Robert and Lee, Peter},
  year = {2004},
  month = apr,
  volume = {39},
  pages = {554--567},
  issn = {0362-1340},
  doi = {10.1145/989393.989449},
  abstract = {The goal of the TIL project was to explore the use of Typed Intermediate Languages to produce high-performance native code from Standard ML (SML). We believed that existing SML compilers were doing a good job of conventional functional language optimizations, as one might find in a LISP compiler, but that inadequate use was made of the rich type information present in the source language. Our goal was to show that we could get much greater performance by propagating type information through to the back end of the compiler, without sacrificing the advantages afforded by loop-oriented and other optimizations.We also confirmed that using typed intermediate languages dramatically improved the reliability and maintainability of the compiler itself. In particular, we were able to use the type system to express critical invariants, and enforce those invariants through type checking. In this respect, TIL introduced and popularized the notion of a certifying compiler, which attaches a checkable certificate of safety to its generated code. In turn, this led directly to the idea of certified object code, inspiring the development of Proof-Carrying Code and Typed Assembly Language as certified object code formats.},
  journal = {SIGPLAN Not.},
  number = {4}
}

@inproceedings{Tate2009equality,
  title = {Equality {{Saturation}}: {{A New Approach}} to {{Optimization}}},
  shorttitle = {Equality {{Saturation}}},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  year = {2009},
  pages = {264--276},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1480881.1480915},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  isbn = {978-1-60558-379-2},
  keywords = {compiler optimization,equality reasoning,intermediate representation},
  series = {{{POPL}} '09}
}

@inproceedings{Tate2010generating,
  title = {Generating {{Compiler Optimizations}} from {{Proofs}}},
  booktitle = {Proceedings of the 37th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Tate, Ross and Stepp, Michael and Lerner, Sorin},
  year = {2010},
  pages = {389--402},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1706299.1706345},
  abstract = {We present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proof-carrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules. We present a category-theoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in debugging type errors. Finally, we show experimentally that our technique enables programmers to train a compiler with application-specific optimizations by providing concrete examples of original programs and the desired transformed programs. We also show how it enables a compiler to learn efficient-to-run optimizations from expensive-to-run super-optimizers.},
  isbn = {978-1-60558-479-9},
  keywords = {compiler optimization,explanation-based learning,proof generalization},
  series = {{{POPL}} '10}
}

@article{Tate2011equality,
  title = {Equality {{Saturation}}: {{A New Approach}} to {{Optimization}}},
  shorttitle = {Equality {{Saturation}}},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  year = {2011},
  month = mar,
  volume = {7},
  issn = {18605974},
  doi = {10.2168/LMCS-7(1:10)2011},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  archivePrefix = {arXiv},
  eprint = {1012.1802},
  eprinttype = {arxiv},
  journal = {Logical Methods in Computer Science},
  keywords = {Computer Science - Programming Languages,D.3.4},
  number = {1}
}

@inproceedings{Tate2011Taming,
  title = {Taming {{Wildcards}} in {{Java}}'s {{Type System}}},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Tate, Ross and Leung, Alan and Lerner, Sorin},
  year = {2011},
  pages = {614--627},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1993498.1993570},
  abstract = {Wildcards have become an important part of Java's type system since their introduction 7 years ago. Yet there are still many open problems with Java's wildcards. For example, there are no known sound and complete algorithms for subtyping (and consequently type checking) Java wildcards, and in fact subtyping is suspected to be undecidable because wildcards are a form of bounded existential types. Furthermore, some Java types with wildcards have no joins, making inference of type arguments for generic methods particularly difficult. Although there has been progress on these fronts, we have identified significant shortcomings of the current state of the art, along with new problems that have not been addressed. In this paper, we illustrate how these shortcomings reflect the subtle complexity of the problem domain, and then present major improvements to the current algorithms for wildcards by making slight restrictions on the usage of wildcards. Our survey of existing Java programs suggests that realistic code should already satisfy our restrictions without any modifications. We present a simple algorithm for subtyping which is both sound and complete with our restrictions, an algorithm for lazily joining types with wildcards which addresses some of the shortcomings of prior work, and techniques for improving the Java type system as a whole. Lastly, we describe various extensions to wildcards that would be compatible with our algorithms.},
  isbn = {978-1-4503-0663-8},
  keywords = {existential types,joins,parametric types,single-instantiation inheritance,subtyping,type inference,wildcards},
  series = {{{PLDI}} '11}
}

@article{Thurston1994proof,
  title = {On Proof and Progress in Mathematics},
  author = {Thurston, William P.},
  year = {1994},
  volume = {30},
  pages = {161--177},
  issn = {0273-0979, 1088-9485},
  doi = {10.1090/S0273-0979-1994-00502-6},
  journal = {Bulletin of the American Mathematical Society},
  number = {2}
}

@inproceedings{Tobin-Hochstadt2017Migratory,
  title = {Migratory {{Typing}}: {{Ten Years Later}}},
  shorttitle = {Migratory {{Typing}}},
  booktitle = {2nd {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2017)},
  author = {{Tobin-Hochstadt}, Sam and Felleisen, Matthias and Findler, Robert and Flatt, Matthew and Greenman, Ben and Kent, Andrew M. and {St-Amour}, Vincent and Strickland, T. Stephen and Takikawa, Asumu},
  editor = {Lerner, Benjamin S. and Bod{\'i}k, Rastislav and Krishnamurthi, Shriram},
  year = {2017},
  volume = {71},
  pages = {17:1--17:17},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.SNAPL.2017.17},
  isbn = {978-3-95977-032-3},
  keywords = {design principles,gradual typing,type systems},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@article{Torgersen2004Adding,
  title = {Adding {{Wildcards}} to the {{Java Programming Language}}.},
  author = {Torgersen, Mads and Ernst, Erik and Hansen, Christian Plesner and {von der Ahe}, Peter and Bracha, Gilad and Gafter, Neal},
  year = {2004},
  volume = {3},
  pages = {97},
  issn = {1660-1769},
  doi = {10.5381/jot.2004.3.11.a5},
  journal = {The Journal of Object Technology},
  language = {en},
  number = {11}
}

@inproceedings{Torlak2014lightweight,
  title = {A {{Lightweight Symbolic Virtual Machine}} for {{Solver}}-Aided {{Host Languages}}},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Torlak, Emina and Bodik, Rastislav},
  year = {2014},
  pages = {530--541},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594340},
  abstract = {Solver-aided domain-specific languages (SDSLs) are an emerging class of computer-aided programming systems. They ease the construction of programs by using satisfiability solvers to automate tasks such as verification, debugging, synthesis, and non-deterministic execution. But reducing programming tasks to satisfiability problems involves translating programs to logical constraints, which is an engineering challenge even for domain-specific languages. We have previously shown that translation to constraints can be avoided if SDSLs are implemented by (traditional) embedding into a host language that is itself solver-aided. This paper describes how to implement a symbolic virtual machine (SVM) for such a host language. Our symbolic virtual machine is lightweight because it compiles to constraints only a small subset of the host's constructs, while allowing SDSL designers to use the entire language, including constructs for DSL embedding. This lightweight compilation employs a novel symbolic execution technique with two key properties: it produces compact encodings, and it enables concrete evaluation to strip away host constructs that are outside the subset compilable to constraints. Our symbolic virtual machine architecture is at the heart of Rosette, a solver-aided language that is host to several new SDSLs.},
  isbn = {978-1-4503-2784-8},
  keywords = {solver-aided languages,symbolic virtual machine},
  series = {{{PLDI}} '14}
}

@inproceedings{Toro2015Customizable,
  title = {Customizable Gradual Polymorphic Effects for {{Scala}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  author = {Toro, Mat{\'i}as and Tanter, {\'E}ric},
  year = {2015},
  pages = {935--953},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814270.2814315},
  abstract = {Despite their obvious advantages in terms of static reasoning, the adoption of effect systems is still rather limited in practice. Recent advances such as generic effect systems, lightweight effect polymorphism, and gradual effect checking, all represent promising steps towards making effect systems suitable for widespread use. However, no existing system combines these approaches: the theory of gradual polymorphic effects has not been developed, and there are no implementations of gradual effect checking. In addition, a limiting factor in the adoption of effect systems is their unsuitability for localized and customized effect disciplines. This paper addresses these issues by presenting the first implementation of gradual effect checking, for Scala, which supports both effect polymorphism and a domain-specific language called Effscript to declaratively define and customize effect disciplines. We report on the theory, implementation, and practical application of the system.},
  isbn = {978-1-4503-3689-5},
  keywords = {effect polymorphism,effscript,gradual typing,Scala,Type-and-effect systems},
  series = {{{OOPSLA}} 2015}
}

@article{Turner1979new,
  title = {A New Implementation Technique for Applicative Languages},
  author = {Turner, D. A.},
  year = {1979},
  month = jan,
  volume = {9},
  pages = {31--49},
  issn = {1097-024X},
  doi = {10.1002/spe.4380090105},
  abstract = {It is shown how by using results from combinatory logic an applicative language, such as LISP, can be translated into a form from which all bound variables have been removed. A machine is described which can efficiently execute the resulting code. This implementation is compared with a conventional interpreter and found to have a number of advantages. Of these the most important is that programs which exploit higher order functions to achieve great compactness of expression are executed much more efficiently.},
  copyright = {Copyright \textcopyright{} 1979 John Wiley \& Sons, Ltd.},
  journal = {Software: Practice and Experience},
  keywords = {Applicative languages,Bracket abstraction,Combinators,Lazy Evaluation,Normal graph reduction,Substitution machine},
  language = {en},
  number = {1}
}

@article{Turner2007understanding,
  title = {Understanding {{Programming Languages}}},
  author = {Turner, Raymond},
  year = {2007},
  month = jul,
  volume = {17},
  pages = {203--216},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-007-9062-6},
  abstract = {We document the influence on programming language semantics of the Platonism/formalism divide in the philosophy of mathematics.},
  journal = {Minds and Machines},
  keywords = {Artificial Intelligence (incl. Robotics),Denotational,Interdisciplinary Studies,Operational,Philosophy of Mind,Programming language semantics,Systems Theory; Control},
  language = {en},
  number = {2}
}

@misc{Typeandscope,
  title = {Type-and-Scope Safe Programs and Their Proofs},
  howpublished = {http://dl.acm.org/citation.cfm?id=3018613\&CFID=611533572\&CFTOKEN=37240912}
}

@inproceedings{Ulrich2015flatter,
  title = {The {{Flatter}}, the {{Better}}: {{Query Compilation Based}} on the {{Flattening Transformation}}},
  shorttitle = {The {{Flatter}}, the {{Better}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Ulrich, Alexander and Grust, Torsten},
  year = {2015},
  pages = {1421--1426},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2723372.2735359},
  abstract = {We demonstrate the insides and outs of a query compiler based on the flattening transformation, a translation technique designed by the programming language community to derive efficient data-parallel implementations from iterative programs. Flattening admits the straightforward formulation of intricate query logic including deeply nested loops over (possibly ordered) data or the construction of rich data structures. To demonstrate the level of expressiveness that can be achieved, we will bring a compiler frontend that accepts queries embedded into the Haskell programming language. Compilation via flattening takes places in a series of simple steps all of which will be made tangible by the demonstration. The final output is a program of lifted primitive operations which existing query engines can efficiently implement. We provide backends based on PostgreSQL and VectorWise to make this point however, most set-oriented or data-parallel engines could benefit from a flattening-based query compiler.},
  isbn = {978-1-4503-2758-9},
  keywords = {flattening,list comprehensions,nested data parallelism},
  series = {{{SIGMOD}} '15}
}

@inproceedings{Ungar1987self,
  title = {Self: {{The Power}} of {{Simplicity}}},
  shorttitle = {Self},
  booktitle = {Conference {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}}},
  author = {Ungar, David and Smith, Randall B.},
  year = {1987},
  pages = {227--242},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/38765.38828},
  abstract = {Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.},
  isbn = {0-89791-247-0},
  series = {{{OOPSLA}} '87}
}

@article{Ungar1991organizing,
  title = {Organizing Programs without Classes},
  author = {Ungar, David and Chambers, Craig and Chang, Bay-Wei and H{\"o}lzle, Urs},
  year = {1991},
  month = jul,
  volume = {4},
  pages = {223--242},
  issn = {0892-4635, 1573-0557},
  doi = {10.1007/BF01806107},
  abstract = {All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model\textemdash{}dividing types into prototypes and traits\textemdash{}supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless languages can even express ``class-based'' encapsulation. These stylized uses of object inheritance become instantly recognizable idioms, and extend the repertory of organizing principles to cover a wider range of programs.},
  journal = {LISP and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),Numeric Computing,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {3}
}

@book{Ursini1996logic,
  title = {Logic and {{Algebra}}},
  author = {Ursini, A. and Agliano, P.},
  year = {1996},
  publisher = {{Taylor \& Francis}},
  isbn = {978-0-8247-9606-8},
  lccn = {96013087},
  series = {Lecture {{Notes}} in {{Pure}} and {{Applied Mathematics}}}
}

@incollection{Urzyczyn1995positive,
  title = {Positive Recursive Type Assignment},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 1995},
  author = {Urzyczyn, Pawel},
  editor = {Wiedermann, Ji{\v r}{\'i} and H{\'a}jek, Petr},
  year = {1995},
  pages = {382--391},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We consider several different definitions of type assignment with positive recursive types, from the point of view of their typing ability. We discuss the relationships between these systems. In particular, we show that the class of typable pure lambda terms remains the same for different type disciplines involving positive type fixpoints, and that type reconstruction is decidable.},
  copyright = {\textcopyright{}1995 Springer-Verlag},
  isbn = {978-3-540-60246-0 978-3-540-44768-9},
  keywords = {Algorithm Analysis and Problem Complexity,Computation by Abstract Devices,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {969},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{v.Gleissenthall2019Pretend,
  title = {Pretend {{Synchrony}}: {{Synchronous Verification}} of {{Asynchronous Distributed Programs}}},
  shorttitle = {Pretend {{Synchrony}}},
  author = {{v. Gleissenthall}, Klaus and K\i{}c\i, Rami G{\"o}khan and Bakst, Alexander and Stefan, Deian and Jhala, Ranjit},
  year = {2019},
  month = jan,
  volume = {3},
  pages = {59:1--59:30},
  issn = {2475-1421},
  doi = {10.1145/3290372},
  abstract = {We present pretend synchrony, a new approach to verifying distributed systems, based on the observation that while distributed programs must execute asynchronously, we can often soundly treat them as if they were synchronous when verifying their correctness. To do so, we compute a synchronization, a semantically equivalent program where all sends, receives, and message buffers, have been replaced by simple assignments, yielding a program that can be verified using Floyd-Hoare style Verification Conditions and SMT. We implement our approach as a framework for writing verified distributed programs in Go and evaluate it with four challenging case studies\textemdash{} the classic two-phase commit protocol, the Raft leader election protocol, single-decree Paxos protocol, and a Multi-Paxos based distributed key-value store. We find that pretend synchrony allows us to develop performant systems while making verification of functional correctness simpler by reducing manually specified invariants by a factor of 6, and faster, by reducing checking time by three orders of magnitude.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {concurrency,consensus,distributed programs,Go,message passing,parameterized systems,pretend synchrony,reduction},
  number = {POPL}
}

@article{vanAntwerpen2018Scopes,
  title = {Scopes {{As Types}}},
  author = {{van Antwerpen}, Hendrik and Bach Poulsen, Casper and Rouvoet, Arjen and Visser, Eelco},
  year = {2018},
  month = oct,
  volume = {2},
  pages = {114:1--114:30},
  issn = {2475-1421},
  doi = {10.1145/3276484},
  abstract = {Scope graphs are a promising generic framework to model the binding structures of programming languages, bridging formalization and implementation, supporting the definition of type checkers and the automation of type safety proofs. However, previous work on scope graphs has been limited to simple, nominal type systems. In this paper, we show that viewing scopes as types enables us to model the internal structure of types in a range of non-simple type systems (including structural records and generic classes) using the generic representation of scopes. Further, we show that relations between such types can be expressed in terms of generalized scope graph queries. We extend scope graphs with scoped relations and queries. We introduce Statix, a new domain-specific meta-language for the specification of static semantics, based on scope graphs and constraints. We evaluate the scopes as types approach and the Statix design in case studies of the simply-typed lambda calculus with records, System F, and Featherweight Generic Java.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {domain-specific language,name resolution,scope graphs,static semantics,type checker,type system},
  number = {OOPSLA}
}

@incollection{vanBenthemJutting1994checking,
  title = {Checking Algorithms for {{Pure Type Systems}}},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {{van Benthem Jutting}, L. S. and McKinna, J. and Pollack, R.},
  editor = {Barendregt, Henk and Nipkow, Tobias},
  year = {1994},
  month = jan,
  pages = {19--61},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We have presented efficient syntax directed presentations of two subclasses of PTS: the semi-full systems, via the {$\vdash$} sdsf relation the functional systems, via the {$\vdash$}f relation The only remaining defect in these presentations lies in the possible failure of tests for conversion in the application rule. Thus for normalizing functional and semi-full systems, everything has been said. For non-functional systems the situation is less clear. We know of no a priori bound on the amount of reduction necessary to correctly type {$\lambda$}-abstractions, so we must be content with the collective completeness of the family of syntax directed systems {$\vdash$}sd-n. We have made little impact on the Expansion Postponement problem, which we leave as future work. We can however bask in the relative peace of mind gained from the machine-checked presentation of most (i.e. those not concerning schematic judgments) of the above results.},
  copyright = {\textcopyright{}1994 Springer-Verlag},
  isbn = {978-3-540-58085-0 978-3-540-48440-0},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages},
  language = {en},
  number = {806},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Vassena2016Generic,
  title = {Generic {{Diff3}} for {{Algebraic Datatypes}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Type}}-{{Driven Development}}},
  author = {Vassena, Marco},
  year = {2016},
  pages = {62--71},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2976022.2976026},
  abstract = {Many version control systems, including Git and Mercurial, rely on diff3 to merge different revisions of the same file. More precisely diff3 automatically merges two text files, given a common base version, comparing them line by line and raising conflicts when the changes made are irreconcilable. The program ignores the actual structure of the data stored in the files, hence it might generate spurious conflicts, which must be manually resolved by the user. In this paper, we present a state-based, three-way, persistent, data-type generic diff3 algorithm whose increased precision in detecting changes reduces the number of false conflicts raised and improves its merging capabilities. We have implemented the algorithm in Agda, a proof assistant with dependent types, and developed a model to reason about ``diffing'' and merging. We have formalized sanity properties and specifications of diff3 and proved that our algorithm meets them. Furthermore, we have identified the minimal conditions under which the merging algorithm raises a conflict and established a structural invariant preserved.},
  isbn = {978-1-4503-4435-7},
  keywords = {Datatype-generic programming,dependent types,Diff3},
  series = {{{TyDe}} 2016}
}

@incollection{Vaziri2014stream,
  title = {Stream {{Processing}} with a {{Spreadsheet}}},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Vaziri, Mandana and Tardieu, Olivier and Rabbah, Rodric and Suter, Philippe and Hirzel, Martin},
  editor = {Jones, Richard},
  year = {2014},
  month = jan,
  pages = {360--384},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Continuous data streams are ubiquitous and represent such a high volume of data that they cannot be stored to disk, yet it is often crucial for them to be analyzed in real-time. Stream processing is a programming paradigm that processes these immediately, and enables continuous analytics. Our objective is to make it easier for analysts, with little programming experience, to develop continuous analytics applications directly. We propose enhancing a spreadsheet, a pervasive tool, to obtain a programming platform for stream processing. We present the design and implementation of an enhanced spreadsheet that enables visualizing live streams, live programming to compute new streams, and exporting computations to be run on a server where they can be shared with other users, and persisted beyond the life of the spreadsheet. We formalize our core language, and present case studies that cover a range of stream processing applications.},
  copyright = {\textcopyright{}2014 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  keywords = {Computer Communication Networks,Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {8586},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Vazou2015bounded,
  title = {Bounded {{Refinement Types}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Vazou, Niki and Bakst, Alexander and Jhala, Ranjit},
  year = {2015},
  pages = {48--61},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784745},
  abstract = {We present a notion of bounded quantification for refinement types and show how it expands the expressiveness of refinement typing by using it to develop typed combinators for: (1) relational algebra and safe database access, (2) Floyd-Hoare logic within a state transformer monad equipped with combinators for branching and looping, and (3) using the above to implement a refined IO monad that tracks capabilities and resource usage. This leap in expressiveness comes via a translation to ``ghost" functions, which lets us retain the automated and decidable SMT based checking and inference that makes refinement typing effective in practice.},
  isbn = {978-1-4503-3669-7},
  keywords = {abstract interpretation,Haskell,refinement types},
  series = {{{ICFP}} 2015}
}

@article{Vazou2017Refinement,
  title = {Refinement {{Reflection}}: {{Complete Verification}} with {{SMT}}},
  shorttitle = {Refinement {{Reflection}}},
  author = {Vazou, Niki and Tondwalkar, Anish and Choudhury, Vikraman and Scott, Ryan G. and Newton, Ryan R. and Wadler, Philip and Jhala, Ranjit},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {53:1--53:31},
  issn = {2475-1421},
  doi = {10.1145/3158141},
  abstract = {We introduce Refinement Reflection, a new framework for building SMT-based deductive verifiers. The key idea is to reflect the code implementing a user-defined function into the function's (output) refinement type. As a consequence, at uses of the function, the function definition is instantiated in the SMT logic in a precise fashion that permits decidable verification. Reflection allows the user to write equational proofs of programs just by writing other programs using pattern-matching and recursion to perform case-splitting and induction. Thus, via the propositions-as-types principle, we show that reflection permits the specification of arbitrary functional correctness properties. Finally, we introduce a proof-search algorithm called Proof by Logical Evaluation that uses techniques from model checking and abstract interpretation, to completely automate equational reasoning. We have implemented reflection in Liquid Haskell and used it to verify that the widely used instances of the Monoid, Applicative, Functor, and Monad typeclasses actually satisfy key algebraic laws required to make the clients safe, and have used reflection to build the first library that actually verifies assumptions about associativity and ordering that are crucial for safe deterministic parallelism.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {Haskell,refinement types,theorem proving,verification},
  number = {POPL}
}

@inproceedings{Verwaest2010pinocchio,
  title = {Pinocchio: {{Bringing Reflection}} to {{Life}} with {{First}}-Class {{Interpreters}}},
  shorttitle = {Pinocchio},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  author = {Verwaest, Toon and Bruni, Camillo and Gurtner, David and Lienhard, Adrian and Niestrasz, Oscar},
  year = {2010},
  pages = {774--789},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1869459.1869522},
  abstract = {To support development tools like debuggers, runtime systems need to provide a meta-programming interface to alter their semantics and access internal data. Reflective capabilities are typically fixed by the Virtual Machine (VM). Unanticipated reflective features must either be simulated by complex program transformations, or they require the development of a specially tailored VM. We propose a novel approach to behavioral reflection that eliminates the barrier between applications and the VM by manipulating an explicit tower of first-class interpreters. Pinocchio is a proof-of-concept implementation of our approach which enables radical changes to the interpretation of programs by explicitly instantiating subclasses of the base interpreter. We illustrate the design of Pinocchio through non-trivial examples that extend runtime semantics to support debugging, parallel debugging, and back-in-time object-flow debugging. Although performance is not yet addressed, we also discuss numerous opportunities for optimization, which we believe will lead to a practical approach to behavioral reflection.},
  isbn = {978-1-4503-0203-6},
  keywords = {behavioral reflection,debugging,metacircularity,object-flow analysis,reflection,Smalltalk,virtual machines},
  series = {{{OOPSLA}} '10}
}

@inproceedings{Visser1998building,
  title = {Building {{Program Optimizers}} with {{Rewriting Strategies}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Visser, Eelco and Benaissa, Zine-el-Abidine and Tolmach, Andrew},
  year = {1998},
  pages = {13--26},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289425},
  abstract = {We describe a language for defining term rewriting strategies, and its application to the production of program optimizers. Valid transformations on program terms can be described by a set of rewrite rules; rewriting strategies are used to describe when and how the various rules should be applied in order to obtain the desired optimization effects. Separating rules from strategies in this fashion makes it easier to reason about the behavior of the optimizer as a whole, compared to traditional monolithic optimizer implementations. We illustrate the expressiveness of our language by using it to describe a simple optimizer for an ML-like intermediate representation.The basic strategy language uses operators such as sequential composition, choice, and recursion to build transformers from a set of labeled unconditional rewrite rules. We also define an extended language in which the side-conditions and contextual rules that arise in realistic optimizer specifications can themselves be expressed as strategy-driven rewrites. We show that the features of the basic and extended languages can be expressed by breaking down the rewrite rules into their primitive building blocks, namely matching and building terms in variable binding environments. This gives us a low-level core language which has a clear semantics, can be implemented straightforwardly and can itself be optimized. The current implementation generates C code from a strategy specification.},
  isbn = {1-58113-024-4},
  series = {{{ICFP}} '98}
}

@incollection{Visser2001stratego,
  title = {Stratego: {{A Language}} for {{Program Transformation Based}} on {{Rewriting Strategies System Description}} of {{Stratego}} 0.5},
  shorttitle = {Stratego},
  booktitle = {Rewriting {{Techniques}} and {{Applications}}},
  author = {Visser, Eelco},
  editor = {Middeldorp, Aart},
  year = {2001},
  month = jan,
  pages = {357--361},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Program transformation is used in many areas of software engineering. Examples include compilation, optimization, synthesis, refactoring, migration, normalization and improvement [15]. Rewrite rules are a natural formalism for expressing single program transformations. However, using a standard strategy for normalizing a program with a set of rewrite rules is not adequate for implementing program transformation systems. It may be necessary to apply a rule only in some phase of a transformation, to apply rules in some order, or to apply a rule only to part of a program. These restrictions may be necessary to avoid non-termination or to choose a specific path in a non-con uent rewrite system. Stratego is a language for the specification of program transformation systems based on the paradigm of rewriting strategies. It supports the separation of strategies from transformation rules, thus allowing careful control over the application of these rules. As a result of this separation, transformation rules are reusable in multiple difierent transformations and generic strategies capturing patterns of control can be described independently of the transformation rules they apply. Such strategies can even be formulated independently of the object language by means of the generic term traversal capabilities of Stratego. In this short paper I give a description of version 0.5 of the Stratego system, discussing the features of the language (Section 2), the library (Section 3), the compiler (Section 4) and some of the applications that have been built (Section 5). Stratego is available as free software under the GNU General Public License from http://www.stratego-language.org.},
  copyright = {\textcopyright{}2001 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-42117-7 978-3-540-45127-3},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Symbolic and Algebraic Manipulation},
  language = {en},
  number = {2051},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Visser2014separation,
  title = {Separation of {{Concerns}} in {{Language Definition}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Modularity}}},
  author = {Visser, Eelco},
  year = {2014},
  pages = {1--2},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2584469.2584662},
  abstract = {Effectively applying linguistic abstraction to emerging domains of computation requires the ability to rapidly develop software languages. However, a software language is a complex software system in its own right and can take significant effort to design and implement. We are currently investigating a radical separation of concerns in language definition by designing high-level declarative meta-languages specialized to the various concerns of language definition that can be used as the single source of production quality (incremental) semantic operations and as a model for reasoning about language properties.},
  isbn = {978-1-4503-2772-5},
  keywords = {language design,language work- bench,meta-languages,nabl,sdf3,spoofax,stratego},
  series = {{{MODULARITY}} '14}
}

@article{Voelterprojecting,
  title = {Projecting a {{Modular Future}}},
  author = {Voelter, Markus and Warmer, Jos and Kolb, Bernd}
}

@inproceedings{Voigtlander2009free,
  title = {Free {{Theorems Involving Type Constructor Classes}}: {{Functional Pearl}}},
  shorttitle = {Free {{Theorems Involving Type Constructor Classes}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Voigtl{\"a}nder, Janis},
  year = {2009},
  pages = {173--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1596550.1596577},
  abstract = {Free theorems are a charm, allowing the derivation of useful statements about programs from their (polymorphic) types alone. We show how to reap such theorems not only from polymorphism over ordinary types, but also from polymorphism over type constructors restricted by class constraints. Our prime application area is that of monads, which form the probably most popular type constructor class of Haskell. To demonstrate the broader scope, we also deal with a transparent way of introducing difference lists into a program, endowed with a neat and general correctness proof.},
  isbn = {978-1-60558-332-7},
  keywords = {parametricity,relational},
  series = {{{ICFP}} '09}
}

@article{Volanschi2012Pattern,
  title = {Pattern Matching for the Masses Using Custom Notations},
  author = {Volanschi, Nic},
  year = {2012},
  month = may,
  volume = {77},
  pages = {609--635},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2011.12.002},
  abstract = {For many programmers, the notion of ``pattern matching'' evokes nothing more than regular expressions for matching unstructured text, or technologies such as XPath to match semi-structured data in XML. This common perception of pattern matching is partly due to the success of regular expressions and XPath, which are supported in many popular programming languages today, either as standard libraries or as part of the language. But it is also due to the fact that many programmers never used another elegant form of pattern matching\textemdash{}on structured data, i.e., the native data structures of a programming language. This form of matching is common in functional or logic languages used in the research, but unfortunately much less used in the software industry. It is indeed very surprising that none of the popular languages in use today support, in their standard form, a nearly general form of structured data matching, decades after this technology has been discovered and continuously improved. This paper shows that programmers do not have to wait for next generation languages to integrate pattern matching, neither need they use non-standard pre-processors, thereby losing some advantages that are most important in an industrial setting: official support, compatibility, standardization, etc. Instead, pattern matching of native data in custom notations can be implemented as a minimalist library in popular object languages. Thus, some of the comfortable existing notations from logic languages can be reused, existing standard notations for structured data such as JSON (JavaScript Object Notation) can be smoothly extended to support pattern matching, and new notations can be designed. As in most library implementations of regular expressions, custom notation patterns are simply represented as strings. They can be used in two different modes: interpreted and compiled. This paper presents two open-source implementations of custom matching notations, for Java and JavaScript, exhibiting a reasonable overhead compared to other forms of pattern matching.},
  journal = {Science of Computer Programming},
  keywords = {Customization,Data notations,Pattern matching},
  number = {5}
}

@inproceedings{Vouillon2004Semantic,
  title = {Semantic {{Types}}: {{A Fresh Look}} at the {{Ideal Model}} for {{Types}}},
  shorttitle = {Semantic {{Types}}},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Vouillon, Jerome and Melli{\`e}s, Paul-Andr{\'e}},
  year = {2004},
  pages = {52--63},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/964001.964006},
  abstract = {We present a generalization of the ideal model for recursive polymorphic types. Types are defined as sets of terms instead of sets of elements of a semantic domain. Our proof of the existence of types (computed by fixpoint of a typing operator) does not rely on metric properties, but on the fact that the identity is the limit of a sequence of projection terms. This establishes a connection with the work of Pitts on relational properties of domains. This also suggests that ideals are better understood as closed sets of terms defined by orthogonality with respect to a set of contexts.},
  isbn = {978-1-58113-729-3},
  keywords = {ideal model,inductive/coinductive principle,polymorphism,realizability,recursive types,subtyping},
  series = {{{POPL}} '04}
}

@article{Vytiniotis2009Relating,
  title = {Relating Step-Indexed Logical Relations and Bisimulations},
  author = {Vytiniotis, Dimitrios and Koutavas, Vasileios},
  year = {2009},
  month = mar,
  abstract = {Operational logical relations and bisimulations are two particularly successful syntactic techniques for reasoning about program equivalence. Although both techniques seem to have common intuitions, their basis is on different mathematical principles: induction for the former, and co-induction for the latter. The intuitive understanding of the two techniques seems more common, but their mathematical connection more \ldots{}},
  journal = {Microsoft Research},
  language = {en-US}
}

@inproceedings{Vytiniotis2010Let,
  title = {Let {{Should Not Be Generalized}}},
  booktitle = {Proceedings of the 5th {{ACM SIGPLAN Workshop}} on {{Types}} in {{Language Design}} and {{Implementation}}},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Schrijvers, Tom},
  year = {2010},
  pages = {39--50},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1708016.1708023},
  abstract = {From the dawn of time, all derivatives of the classic Hindley-Milner type system have supported implicit generalisation of local let-bindings. Yet, as we will show, for more sophisticated type systems implicit let-generalisation imposes a disproportionate complexity burden. Moreover, it turns out that the feature is very seldom used, so we propose to eliminate it. The payoff is a substantial simplification, both of the specification of the type system, and of its implementation.},
  isbn = {978-1-60558-891-9},
  keywords = {Generalized Algebraic Data Types,Haskell,type classes,type families,type inference},
  series = {{{TLDI}} '10}
}

@article{Vytiniotis2010parametricity,
  title = {Parametricity, Type Equality, and Higher-Order Polymorphism},
  author = {Vytiniotis, Dimitrios and Weirich, Stephanie},
  year = {2010},
  month = mar,
  volume = {20},
  pages = {175--210},
  issn = {1469-7653},
  doi = {10.1017/S0956796810000079},
  abstract = {Propositions that express type equality are a frequent ingredient of modern functional programming \textendash{} they can encode generic functions, dynamic types, and GADTs. Via the Curry\textendash{}Howard correspondence, these propositions are ordinary types inhabited by proof terms, computed using runtime type representations. In this paper we show that two examples of type equality propositions actually do reflect type equality; they are only inhabited when their arguments are equal and their proofs are unique (up to equivalence.) We show this result in the context of a strongly normalizing language with higher-order polymorphism and primitive recursion over runtime-type representations by proving Reynolds's abstraction theorem. We then use this theorem to derive ``free'' theorems about equality types.},
  journal = {Journal of Functional Programming},
  number = {02}
}

@article{Vytiniotis2011OutsideIn,
  title = {{{OutsideIn}}({{X}}): {{Modular}} Type Inference with Local Assumptions},
  shorttitle = {{{OutsideIn}}({{X}})},
  author = {Vytiniotis, Dimitrios and Jones, Simon Peyton and Schrijvers, Tom and Sulzmann, Martin},
  year = {2011},
  month = sep,
  volume = {21},
  abstract = {Advanced type system features, such as GADTs, type classes and type families, have proven to be invaluable language extensions for ensuring data invariants and program correctness. Unfortunately, they pose a tough problem for type inference when they are used as local type assumptions. Local type assumptions often result in the lack of principal types and \ldots{}},
  journal = {Journal of Functional Programming},
  keywords = {_tablet}
}

@inproceedings{Vytiniotis2012equality,
  title = {Equality {{Proofs}} and {{Deferred Type Errors}}: {{A Compiler Pearl}}},
  shorttitle = {Equality {{Proofs}} and {{Deferred Type Errors}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Magalh{\~a}es, Jos{\'e} Pedro},
  year = {2012},
  pages = {341--352},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2364527.2364554},
  abstract = {The Glasgow Haskell Compiler is an optimizing compiler that expresses and manipulates first-class equality proofs in its intermediate language. We describe a simple, elegant technique that exploits these equality proofs to support deferred type errors. The technique requires us to treat equality proofs as possibly-divergent terms; we show how to do so without losing either soundness or the zero-overhead cost model that the programmer expects.},
  isbn = {978-1-4503-1054-3},
  keywords = {deferred type errors,system fc,type equalities},
  series = {{{ICFP}} '12}
}

@inproceedings{Vytiniotis2013halo,
  title = {{{HALO}}: {{Haskell}} to {{Logic Through Denotational Semantics}}},
  shorttitle = {{{HALO}}},
  booktitle = {Proceedings of the 40th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Claessen, Koen and Ros{\'e}n, Dan},
  year = {2013},
  pages = {431--442},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2429069.2429121},
  abstract = {Even well-typed programs can go wrong in modern functional languages, by encountering a pattern-match failure, or simply returning the wrong answer. An increasingly-popular response is to allow programmers to write contracts that express semantic properties, such as crash-freedom or some useful post-condition. We study the static verification of such contracts. Our main contribution is a novel translation to first-order logic of both Haskell programs, and contracts written in Haskell, all justified by denotational semantics. This translation enables us to prove that functions satisfy their contracts using an off-the-shelf first-order logic theorem prover.},
  isbn = {978-1-4503-1832-7},
  keywords = {first-order logic,static contract checking},
  series = {{{POPL}} '13}
}

@inproceedings{Wadler1987Views,
  title = {Views: {{A}} Way for Pattern Matching to Cohabit with Data Abstraction},
  shorttitle = {Views},
  booktitle = {Proceedings of the 14th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Wadler, P.},
  year = {1987},
  pages = {307--313},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/41625.41653},
  abstract = {Pattern matching and data abstraction are important concepts in designing programs, but they do not fit well together. Pattern matching depends on making public a free data type representation, while data abstraction depends on hiding the representation. This paper proposes the views mechanism as a means of reconciling this conflict. A view allows any type to be viewed as a free data type, thus combining the clarity of pattern matching with the efficiency of data abstraction.},
  isbn = {978-0-89791-215-0},
  series = {{{POPL}} '87}
}

@inproceedings{Wadler1989how,
  title = {How to {{Make Ad}}-Hoc {{Polymorphism Less Ad Hoc}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Wadler, P. and Blott, S.},
  year = {1989},
  pages = {60--76},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/75277.75283},
  abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the ``eqtype variables'' of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
  isbn = {0-89791-294-2},
  series = {{{POPL}} '89}
}

@inproceedings{Wadler1989Theorems,
  title = {Theorems for {{Free}}!},
  booktitle = {Proceedings of the {{Fourth International Conference}} on {{Functional Programming Languages}} and {{Computer Architecture}}},
  author = {Wadler, Philip},
  year = {1989},
  pages = {347--359},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/99370.99404},
  isbn = {978-0-89791-328-7},
  series = {{{FPCA}} '89}
}

@article{Wadler1992comprehending,
  title = {Comprehending Monads},
  author = {Wadler, Philip},
  year = {1992},
  month = dec,
  volume = {2},
  pages = {461--493},
  issn = {1469-8072},
  doi = {10.1017/S0960129500001560},
  abstract = {Category theorists invented monads in the 1960's to express concisely certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to express concisely certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can express concisely in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.},
  journal = {Mathematical Structures in Computer Science},
  number = {04}
}

@incollection{Wadler1995monads,
  title = {Monads for Functional Programming},
  booktitle = {Advanced {{Functional Programming}}},
  author = {Wadler, Philip},
  editor = {Jeuring, Johan and Meijer, Erik},
  year = {1995},
  month = may,
  pages = {24--52},
  publisher = {{Springer Berlin Heidelberg}},
  copyright = {\textcopyright{}1995 Springer-Verlag},
  isbn = {978-3-540-59451-2 978-3-540-49270-2},
  keywords = {Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {925},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Wadler1998marriage,
  title = {The Marriage of Effects and Monads},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Wadler, Philip},
  year = {1998},
  pages = {63--74},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289429},
  abstract = {Gifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect systems into a corresponding monad system.},
  isbn = {978-1-58113-024-9},
  series = {{{ICFP}} '98}
}

@inproceedings{Wadler2003callbyvalue,
  title = {Call-by-Value Is {{Dual}} to {{Call}}-by-Name},
  booktitle = {Proceedings of the {{Eighth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Wadler, Philip},
  year = {2003},
  pages = {189--201},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/944705.944723},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \& are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).},
  isbn = {1-58113-756-7},
  keywords = {Curry-Howard correspondence,De Morgan dual,lambda calculus,lambda mu calculus,Logic,natural deduction,sequent calculus},
  series = {{{ICFP}} '03}
}

@article{Wadler2003Girard,
  title = {The {{Girard}}\textendash{{Reynolds}} Isomorphism},
  author = {Wadler, Philip},
  year = {2003},
  month = nov,
  volume = {186},
  pages = {260--284},
  issn = {0890-5401},
  doi = {10.1016/S0890-5401(03)00141-X},
  abstract = {The second-order polymorphic lambda calculus, F2, was independently discovered by Girard and Reynolds. Girard additionally proved a Representation Theorem: every function on natural numbers that can be proved total in second-order intuitionistic predicate logic, P2, can be represented in F2. Reynolds additionally proved an Abstraction Theorem: for a suitable notion of logical relation, every term in F2 takes related arguments into related results. We observe that the essence of Girard's result is a projection from P2 into F2, and that the essence of Reynolds's result is an embedding of F2 into P2, and that the Reynolds embedding followed by the Girard projection is the identity. The Girard projection discards all first-order quantifiers, so it seems unreasonable to expect that the Girard projection followed by the Reynolds embedding should also be the identity. However, we show that in the presence of Reynolds's parametricity property that this is indeed the case, for propositions corresponding to inductive definitions of naturals or other algebraic types.},
  journal = {Information and Computation},
  number = {2},
  series = {Theoretical {{Aspects}} of {{Computer Software}} ({{TACS}} 2001)}
}

@article{Wadler2003marriage,
  title = {The Marriage of Effects and Monads},
  author = {Wadler, Philip and Thiemann, Peter},
  year = {2003},
  month = jan,
  volume = {4},
  pages = {1--32},
  issn = {1529-3785},
  doi = {10.1145/601775.601776},
  abstract = {Gifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect system into a corresponding monad system.},
  journal = {ACM Trans. Comput. Logic},
  keywords = {effect,Monad,region,type,type reconstruction},
  number = {1}
}

@article{Wadler2007girardreynolds,
  title = {The {{Girard}}\textendash{{Reynolds}} Isomorphism (Second Edition)},
  author = {Wadler, Philip},
  year = {2007},
  month = may,
  volume = {375},
  pages = {201--226},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.12.042},
  abstract = {Jean-Yves Girard and John Reynolds independently discovered the second-order polymorphic lambda calculus, F2. Girard additionally proved a Representation Theorem: every function on natural numbers that can be proved total in second-order intuitionistic predicate logic, P2, can be represented in F2. Reynolds additionally proved an Abstraction Theorem: every term in F2 satisfies a suitable notion of logical relation; and formulated a notion of parametricity satisfied by well-behaved models.

We observe that the essence of Girard's result is a projection from P2 into F2, and that the essence of Reynolds's result is an embedding of F2 into P2, and that the Reynolds embedding followed by the Girard projection is the identity. We show that the inductive naturals are exactly those values of type natural that satisfy Reynolds's notion of parametricity, and as a consequence characterize situations in which the Girard projection followed by the Reynolds embedding is also the identity.

An earlier version of this paper used a logic over untyped terms. This version uses a logic over typed term, similar to ones considered by Abadi and Plotkin and by Takeuti, which better clarifies the relationship between F2 and P2.

This paper uses colour to enhance its presentation. If the link below is not blue, follow it for the colour version.

http://homepages.inf.ed.ac.uk/wadler},
  journal = {Theoretical Computer Science},
  keywords = {Abstraction Theorem,Curry–Howard,Girard–Reynolds type system,Polymorphic lambda calculus,Representation Theorem,system f},
  number = {1\textendash{}3},
  series = {Festschrift for {{John C}}. {{Reynolds}}'s 70th Birthday}
}

@article{Wand1988mystery,
  title = {The Mystery of the Tower Revealed: {{A}} Nonreflective Description of the Reflective Tower},
  shorttitle = {The Mystery of the Tower Revealed},
  author = {Wand, Mitchell and Friedman, Daniel P.},
  year = {1988},
  month = jun,
  volume = {1},
  pages = {11--38},
  issn = {0892-4635, 1573-0557},
  doi = {10.1007/BF01806174},
  abstract = {In an important series of papers [8, 9], Brian Smith has discussed the nature of programs that know about their text and the context in which they are executed. He called this kind of knowledgereflection. Smith proposed a programming language, called 3-LISP, which embodied such self-knowledge in the domain of metacircular interpreters. Every 3-LISP program is interpreted by a metacircular interpreter, also written in 3-LISP. This gives rise to a picture of an infinite tower of metacircular interpreters, each being interpreted by the one above it. Such a metaphor poses a serious challenge for conventional modes of understanding of programming languages. In our earlier work on reflection [4], we showed how a useful species of reflection could be modeled without the use of towers. In this paper, we give a semantic account of the reflective tower. This account is self-contained in the sense that it does not employ reflection to explain reflection.},
  journal = {LISP and Symbolic Computation},
  keywords = {Artificial Intelligence (incl. Robotics),Numeric Computing,Programming Languages; Compilers; Interpreters,Software Engineering/Programming and Operating Systems},
  language = {en},
  number = {1}
}

@inproceedings{Wang2015draining,
  title = {Draining the {{Swamp}}: {{Micro Virtual Machines}} as {{Solid Foundation}} for {{Language Development}}},
  booktitle = {1st {{Summit}} on {{Advances}} in {{Programming Languages}} ({{SNAPL}} 2015)},
  author = {Wang, Kunshan and Lin, Yi and Blackburn, Stephen M. and Norrish, Michael and Hosking, Antony L.},
  editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
  year = {2015},
  volume = {32},
  pages = {321--336},
  publisher = {{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.321},
  isbn = {978-3-939897-80-4},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  urn = {urn:nbn:de:0030-drops-50341}
}

@inproceedings{Wang2017Strong,
  title = {Towards Strong Normalization for Dependent Object Types ({{DOT}})},
  booktitle = {31st {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2017)},
  author = {Wang, Fei and Rompf, Tiark},
  editor = {M{\"u}ller, Peter},
  year = {2017},
  volume = {74},
  pages = {27:1--27:25},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  doi = {10.4230/LIPIcs.ECOOP.2017.27},
  isbn = {978-3-95977-035-4},
  keywords = {_tablet,DOT,logical relations,recursive types,Scala,Strong Normalization},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})}
}

@article{Wang2018Demystifying,
  title = {Demystifying Differentiable Programming: {{Shift}}/Reset the Penultimate Backpropagator},
  shorttitle = {Demystifying Differentiable Programming},
  author = {Wang, Fei and Wu, Xilun and Essertel, Gregory and Decker, James and Rompf, Tiark},
  year = {2018},
  month = mar,
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@inproceedings{Warth2007OMeta,
  title = {{{OMeta}}: {{An}} Object-Oriented Language for Pattern Matching},
  shorttitle = {{{OMeta}}},
  booktitle = {Proceedings of the 2007 {{Symposium}} on {{Dynamic Languages}}},
  author = {Warth, Alessandro and Piumarta, Ian},
  year = {2007},
  pages = {11--19},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1297081.1297086},
  abstract = {This paper introduces OMeta, a new object-oriented language for pattern matching. OMeta is based on a variant of Parsing Expression Grammars (PEGs) [5]---a recognition-based foundation for describing syntax---which we have extended to handle arbitrary kinds of data. We show that OMeta's general-purpose pattern matching provides a natural and convenient way for programmers to implement tokenizers, parsers, visitors, and tree transformers, all of which can be extended in interesting ways using familiar object-oriented mechanisms. This makes OMeta particularly well-suited as a medium for experimenting with new designs for programming languages and extensions to existing languages.},
  isbn = {978-1-59593-868-8},
  keywords = {metacircular implementation,parsing,pattern matching},
  series = {{{DLS}} '07}
}

@inproceedings{Warth2008packrat,
  title = {Packrat {{Parsers Can Support Left Recursion}}},
  booktitle = {Proceedings of the 2008 {{ACM SIGPLAN Symposium}} on {{Partial Evaluation}} and {{Semantics}}-Based {{Program Manipulation}}},
  author = {Warth, Alessandro and Douglass, James R. and Millstein, Todd},
  year = {2008},
  pages = {103--110},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1328408.1328424},
  abstract = {Packrat parsing offers several advantages over other parsing techniques, such as the guarantee of linear parse times while supporting backtracking and unlimited look-ahead. Unfortunately, the limited support for left recursion in packrat parser implementations makes them difficult to use for a large class of grammars (Java's, for example). This paper presents a modification to the memoization mechanism used by packrat parser implementations that makes it possible for them to support (even indirectly or mutually) left-recursive rules. While it is possible for a packrat parser with our modification to yield super-linear parse times for some left-recursive grammars, our experimentsshow that this is not the case for typical uses of left recursion.},
  isbn = {978-1-59593-977-7},
  keywords = {left recursion,packrat parsing},
  series = {{{PEPM}} '08}
}

@incollection{Watkins2003concurrent,
  title = {A {{Concurrent Logical Framework}}: {{The Propositional Fragment}}},
  shorttitle = {A {{Concurrent Logical Framework}}},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  author = {Watkins, Kevin and Cervesato, Iliano and Pfenning, Frank and Walker, David},
  editor = {Berardi, Stefano and Coppo, Mario and Damiani, Ferruccio},
  year = {2003},
  month = apr,
  pages = {355--377},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-540-24849-1_23},
  abstract = {We present the propositional fragment CLF0 of the Concurrent Logical Framework (CLF). CLF extends the Linear Logical Framework to allow the natural representation of concurrent computations in an object language. The underlying type theory uses monadic types to segregate values from computations. This separation leads to a tractable notion of definitional equality that identifies computations differing only in the order of execution of independent steps. From a logical point of view our type theory can be seen as a novel combination of lax logic and dual intuitionistic linear logic. An encoding of a small Petri net exemplifies the representation methodology, which can be summarized as ``concurrent computations as monadic expressions''.},
  copyright = {\textcopyright{}2004 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-22164-7 978-3-540-24849-1},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters},
  language = {en},
  number = {3085},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{Weeks2003mltondevel,
  title = {[{{MLton}}-Devel] {{CPS}} vs {{SSA}}},
  author = {Weeks, Stephen},
  year = {Fri, 10 Jan 2003 17:35:38 -0800}
}

@incollection{Wegner1988Inheritance,
  title = {Inheritance as an {{Incremental Modification Mechanism}} or {{What Like Is}} and {{Isn}}'t {{Like}}},
  booktitle = {{{ECOOP}} '88 {{European Conference}} on {{Object}}-{{Oriented Programming}}},
  author = {Wegner, Peter and Zdonik, Stanley B.},
  editor = {Gjessing, Stein and Nygaard, Kristen},
  year = {1988},
  month = aug,
  pages = {55--77},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/3-540-45910-3_4},
  abstract = {Incremental modification is a fundamental mechanism not only in software systems, but also in physical and mathematical systems. Inheritance owes its importance in large measure to its flexibility as a discrete incremental modification mechanism. Four increasingly permissive properties of incremental modification realizable by inheritance are examined: behavior compatibility, signature compatibility. name compatibility, and cancellation. Inheritance for entities with finite sets of attributes is defined and characterized as incremental modification with deferred binding of self-reference. Types defined as predicates for type checking are contrasted with classes defined as templates for object generation. Mathematical, operational, and conceptual models of inheritance are then examined in detail, leading to a discussion of algebraic models of behavioral compatibility. horizontal and vertical signature modification, algorithmically defined name modification, additive and subtractive exceptions, abstract inheritance networks, and parametric polymorphism. Liketypes are defined as a symmetrical general form of incremental modification that provide a framework for modeling similarity. The combination of safe behaviorally compatible changes and less safe radical incremental changes in a single programming language is considered.},
  copyright = {\textcopyright{}1988 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-50053-7 978-3-540-45910-1},
  keywords = {Artificial Intelligence (incl. Robotics),Programming Techniques},
  language = {en},
  number = {322},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Wehr2008subtyping,
  title = {Subtyping Existential Types},
  author = {Wehr, Stefan and Thiemann, Peter},
  year = {2008},
  journal = {10th FTfJP, informal proceedings}
}

@article{Wei2018Refunctionalization,
  title = {Refunctionalization of {{Abstract Abstract Machines}}: {{Bridging}} the {{Gap Between Abstract Abstract Machines}} and {{Abstract Definitional Interpreters}} ({{Functional Pearl}})},
  shorttitle = {Refunctionalization of {{Abstract Abstract Machines}}},
  author = {Wei, Guannan and Decker, James and Rompf, Tiark},
  year = {2018},
  month = jul,
  volume = {2},
  pages = {105:1--105:28},
  issn = {2475-1421},
  doi = {10.1145/3236800},
  abstract = {Abstracting abstract machines is a systematic methodology for constructing sound static analyses for higher-order languages, by deriving small-step abstract abstract machines (AAMs) that perform abstract interpretation from abstract machines that perform concrete evaluation. Darais et al. apply the same underlying idea to monadic definitional interpreters, and obtain monadic abstract definitional interpreters (ADIs) that perform abstract interpretation in big-step style using monads. Yet, the relation between small-step abstract abstract machines and big-step abstract definitional interpreters is not well studied.   In this paper, we explain their functional correspondence and demonstrate how to systematically transform small-step abstract abstract machines into big-step abstract definitional interpreters. Building on known semantic interderivation techniques from the concrete evaluation setting, the transformations include linearization, lightweight fusion, disentanglement, refunctionalization, and the left inverse of the CPS transform. Linearization expresses nondeterministic choice through first-order data types, after which refunctionalization transforms the first-order data types that represent continuations into higher-order functions. The refunctionalized AAM is an abstract interpreter written in continuation-passing style (CPS) with two layers of continuations, which can be converted back to direct style with delimited control operators. Based on the known correspondence between delimited control and monads, we demonstrate that the explicit use of monads in abstract definitional interpreters is optional.   All transformations properly handle the collecting semantics and nondeterminism of abstract interpretation. Remarkably, we reveal how precise call/return matching in control-flow analysis can be obtained by refunctionalizing a small-step abstract abstract machine with proper caching.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {abstract machines,control-flow analysis,refunctionalization,Scala},
  number = {ICFP}
}

@inproceedings{Weirich2013System,
  title = {System {{FC}} with {{Explicit Kind Equality}}},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Weirich, Stephanie and Hsu, Justin and Eisenberg, Richard A.},
  year = {2013},
  pages = {275--286},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500599},
  abstract = {System FC, the core language of the Glasgow Haskell Compiler, is an explicitly-typed variant of System F with first-class type equality proofs called coercions. This extensible proof system forms the foundation for type system extensions such as type families (type-level functions) and Generalized Algebraic Datatypes (GADTs). Such features, in conjunction with kind polymorphism and datatype promotion, support expressive compile-time reasoning. However, the core language lacks explicit kind equality proofs. As a result, type-level computation does not have access to kind-level functions or promoted GADTs, the type-level analogues to expression-level features that have been so useful. In this paper, we eliminate such discrepancies by introducing kind equalities to System FC. Our approach is based on dependent type systems with heterogeneous equality and the "Type-in-Type" axiom, yet it preserves the metatheoretic properties of FC. In particular, type checking is simple, decidable and syntax directed. We prove the preservation and progress theorems for the extended language.},
  isbn = {978-1-4503-2326-0},
  keywords = {dependent types,equality,haskell},
  series = {{{ICFP}} '13}
}

@article{Weirich2017Specification,
  title = {A {{Specification}} for {{Dependent Types}} in {{Haskell}}},
  author = {Weirich, Stephanie and Voizard, Antoine and {de Amorim}, Pedro Henrique Avezedo and Eisenberg, Richard A.},
  year = {2017},
  month = aug,
  volume = {1},
  pages = {31:1--31:29},
  issn = {2475-1421},
  doi = {10.1145/3110275},
  abstract = {We propose a core semantics for Dependent Haskell, an extension of Haskell with full-spectrum dependent types. Our semantics consists of two related languages. The first is a Curry-style dependently-typed language with nontermination, irrelevant arguments, and equality abstraction. The second, inspired by the Glasgow Haskell Compiler's core language FC, is its explicitly-typed analogue, suitable for implementation in GHC. All of our results---chiefly, type safety, along with theorems that relate these two languages---have been formalized using the Coq proof assistant. Because our work is backwards compatible with Haskell, our type safety proof holds in the presence of nonterminating computation. However, unlike other full-spectrum dependently-typed languages, such as Coq, Agda or Idris, because of this nontermination, Haskell's term language does not correspond to a consistent logic.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {dependent types,Haskell},
  number = {ICFP}
}

@inproceedings{Weise1993programmable,
  title = {Programmable {{Syntax Macros}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1993 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Weise, Daniel and Crew, Roger},
  year = {1993},
  pages = {156--165},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/155090.155105},
  abstract = {Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP (the C preprocessor), syntax macros operate on Abstract Syntax Trees (ASTs). Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, difficult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems by having the macro language be a  minimal extension of the programming language, by introducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro definition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros.},
  isbn = {0-89791-598-4},
  series = {{{PLDI}} '93}
}

@incollection{Wells2002essence,
  title = {The {{Essence}} of {{Principal Typings}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Wells, J. B.},
  editor = {Widmayer, Peter and Eidenbenz, Stephan and Triguero, Francisco and Morales, Rafael and Conejo, Ricardo and Hennessy, Matthew},
  year = {2002},
  month = jan,
  pages = {913--925},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Let S be some type system. A typing in S for a typable term M is the collection of all of the information other than M which appears in the final judgement of a proof derivation showing that M is typable. For example, suppose there is a derivation in S ending with the judgement A {$\vdash$} M: {$\tau$} meaning that M has result type {$\tau$} when assuming the types of free variables are given by A. Then (A, {$\tau$}) is a typing for M. A principal typing in S for a term M is a typing for M which somehow represents all other possible typings in S for M. It is important not to confuse this with a weaker notion in connection with the Hindley/Milner type system often called ``principal types''. Previous definitions of principal typings for specific type systems have involved various syntactic operations on typings such as substitution of types for type variables, expansion, lifting, etc. This paper presents a new general definition of principal typings which does not depend on the details of any particular type system. This paper shows that the new general definition correctly generalizes previous system-dependent definitions. This paper explains why the new definition is the right one. Furthermore, the new definition is used to prove that certain polymorphic type systems using {$\forall$}-quantifiers, namely System F and the Hindley/Milner system, do not have principal typings. All proofs can be found in a longer version available at the author's home page.},
  copyright = {\textcopyright{}2002 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-43864-9 978-3-540-45465-6},
  keywords = {_tablet,Computer Graphics,Data Structures; Cryptology and Information Theory,Mathematics of Computing,Software Engineering/Programming and Operating Systems,Theory of Computation},
  language = {en},
  number = {2380},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{Wellsnotes,
  title = {Notes on the {$\lambda$}-{{Calculus}}},
  author = {Wells, Charles}
}

@inproceedings{Wildmoser2004Certifying,
  title = {Certifying {{Machine Code Safety}}: {{Shallow Versus Deep Embedding}}},
  shorttitle = {Certifying {{Machine Code Safety}}},
  booktitle = {Theorem {{Proving}} in {{Higher Order Logics}}},
  author = {Wildmoser, Martin and Nipkow, Tobias},
  editor = {Slind, Konrad and Bunker, Annette and Gopalakrishnan, Ganesh},
  year = {2004},
  pages = {305--320},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {We formalise a simple assembly language with procedures and a safety policy for arithmetic overflow in Isabelle/HOL. To verify individual programs we use a safety logic. Such a logic can be realised in Isabelle/HOL either as shallow or deep embedding. In a shallow embedding logical formulas are written as HOL predicates, whereas a deep embedding models formulas as a datatype. This paper presents and discusses both variants pointing out their specific strengths and weaknesses.},
  isbn = {978-3-540-30142-4},
  keywords = {Elimination Rule,Logical Formula,Program Counter,Safety Policy,Weak Precondition},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Wright1994Syntactic,
  title = {A {{Syntactic Approach}} to {{Type Soundness}}},
  author = {Wright, A. K. and Felleisen, M.},
  year = {1994},
  month = nov,
  volume = {115},
  pages = {38--94},
  issn = {0890-5401},
  doi = {10.1006/inco.1994.1093},
  abstract = {We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.},
  journal = {Information and Computation},
  number = {1}
}

@incollection{Wu2015fusion,
  title = {Fusion for {{Free}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Wu, Nicolas and Schrijvers, Tom},
  editor = {Hinze, Ralf and Voigtl{\"a}nder, Janis},
  year = {2015},
  month = jun,
  pages = {302--322},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-19797-5_15},
  abstract = {Algebraic effect handlers are a recently popular approach for modelling side-effects that separates the syntax and semantics of effectful operations. The shape of syntax is captured by functors, and free monads over these functors denote syntax trees. The semantics is captured by algebras, and effect handlers pass these over the syntax trees to interpret them into a semantic domain. This approach is inherently modular: different functors can be composed to make trees with richer structure. Such trees are interpreted by applying several handlers in sequence, each removing the syntactic constructs it recognizes. Unfortunately, the construction and traversal of intermediate trees is painfully inefficient and has hindered the adoption of the handler approach. This paper explains how a sequence of handlers can be fused into one, so that multiple tree traversals can be reduced to a single one and no intermediate trees need to be allocated. At the heart of this optimization is keeping the notion of a free monad abstract, thus enabling a change of representation that opens up the possibility of fusion. We demonstrate how the ensuing code can be inlined at compile time to produce efficient handlers.},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  isbn = {978-3-319-19796-8 978-3-319-19797-5},
  keywords = {Discrete Mathematics in Computer Science,Logics and Meanings of Programs,Math Applications in Computer Science,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Software Engineering},
  language = {en},
  number = {9129},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Wurthinger2013one,
  title = {One {{VM}} to {{Rule Them All}}},
  booktitle = {Proceedings of the 2013 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}}},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  year = {2013},
  pages = {187--204},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2509578.2509581},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
  isbn = {978-1-4503-2472-4},
  keywords = {dynamic languages,java,javascript,language implementation,optimization,virtual machine},
  series = {Onward! '13}
}

@inproceedings{Xi2003guarded,
  title = {Guarded {{Recursive Datatype Constructors}}},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Xi, Hongwei and Chen, Chiyan and Chen, Gang},
  year = {2003},
  pages = {224--235},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/604131.604150},
  abstract = {We introduce a notion of guarded recursive (g.r.) datatype constructors, generalizing the notion of recursive datatypes in functional programming languages such as ML and Haskell. We address both theoretical and practical issues resulted from this generalization. On one hand, we design a type system to formalize the notion of g.r. datatype constructors and then prove the soundness of the type system. On the other hand, we present some significant applications (e.g., implementing objects, implementing staged computation, etc.) of g.r. datatype constructors, arguing that g.r. datatype constructors can have far-reaching consequences in programming. The main contribution of the paper lies in the recognition and then the formalization of a programming notion that is of both theoretical interest and practical use.},
  isbn = {1-58113-628-5},
  keywords = {constructors,datatype,guarded,recursive},
  series = {{{POPL}} '03}
}

@inproceedings{Xie2018Let,
  title = {Let {{Arguments Go First}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Xie, Ningning and Oliveira, Bruno C. d S.},
  year = {2018},
  month = apr,
  pages = {272--299},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-89884-1_10},
  abstract = {Bi-directional type checking has proved to be an extremely useful and versatile tool for type checking and type inference. The conventional presentation of bi-directional type checking consists of two modes: inference mode and checked mode. In traditional bi-directional type-checking, type annotations are used to guide (via the checked mode) the type inference/checking procedure to determine the type of an expression, and type information flows from functions to arguments.This paper presents a variant of bi-directional type checking where the type information flows from arguments to functions. This variant retains the inference mode, but adds a so-called application mode. Such design can remove annotations that basic bi-directional type checking cannot, and is useful when type information from arguments is required to type-check the functions being applied. We present two applications and develop the meta-theory (mostly verified in Coq) of the application mode.},
  isbn = {978-3-319-89883-4 978-3-319-89884-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Xiong2001spl,
  title = {{{SPL}}: {{A Language}} and {{Compiler}} for {{DSP Algorithms}}},
  shorttitle = {{{SPL}}},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2001 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
  year = {2001},
  pages = {298--308},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/378795.378860},
  abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as ``hard-wired'' systems like FFTW.},
  isbn = {1-58113-414-2},
  series = {{{PLDI}} '01}
}

@inproceedings{Yachi2016Sound,
  title = {A {{Sound}} and {{Complete Bisimulation}} for {{Contextual Equivalence}} in \$\$\textbackslash{}lambda \$\$-{{Calculus}} with {{Call}}/Cc},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Yachi, Taichi and Sumii, Eijiro},
  year = {2016},
  month = nov,
  pages = {171--186},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-47958-3_10},
  abstract = {We develop a sound and complete proof method of contextual equivalence in {$\lambda\lambda\backslash$}lambda -calculus with the abortive control operator call/cc (as opposed to delimited control operators like {$\mathtt{s}\mathtt{h}\mathtt{i}\mathtt{f}\mathtt{t}$}shift\textbackslash{}mathtt \{shift\} and {$\mathtt{r}\mathtt{e}\mathtt{s}\mathtt{e}\mathtt{t}$}reset\textbackslash{}mathtt \{reset\}), and prove the non-trivial equivalence between {$\lambda$}f.f(){$\lambda$}f.f()\textbackslash{}lambda f.\textbackslash,f() and {$\lambda$}f.f();f(){$\lambda$}f.f();f()\textbackslash{}lambda f.\textbackslash,f();f() for example, both for the first time to our knowledge. Although our method is based on environmental bisimulations (Sumii et al. 2004-), it makes an essential and general change to their metatheory, which is not only necessary for handling call/cc but is also applicable in other languages with no control operator.},
  isbn = {978-3-319-47957-6 978-3-319-47958-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Yallop2014lightweight,
  title = {Lightweight {{Higher}}-{{Kinded Polymorphism}}},
  booktitle = {Functional and {{Logic Programming}}},
  author = {Yallop, Jeremy and White, Leo},
  editor = {Codish, Michael and Sumii, Eijiro},
  year = {2014},
  month = jun,
  pages = {119--135},
  publisher = {{Springer International Publishing}},
  abstract = {Higher-kinded polymorphism \textemdash{}i.e. abstraction over type constructors\textemdash{} is an essential component of many functional programming techniques such as monads, folds, and embedded DSLs. ML-family languages typically support a form of abstraction over type constructors using functors, but the separation between the core language and the module language leads to awkwardness as functors proliferate. We show how to express higher-kinded polymorphism in OCaml without functors, using an abstract type app to represent type application, and opaque brands to denote abstractable type constructors. We demonstrate the flexibility of our approach by using it to translate a variety of standard higher-kinded programs into functor-free OCaml code.},
  copyright = {\textcopyright{}2014 Springer International Publishing Switzerland},
  isbn = {978-3-319-07150-3 978-3-319-07151-0},
  keywords = {Artificial Intelligence (incl. Robotics),Logics and Meanings of Programs,Mathematical Logic and Formal Languages,Programming Languages; Compilers; Interpreters,Programming Techniques,Software Engineering},
  language = {en},
  number = {8475},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Yang1998encoding,
  title = {Encoding {{Types}} in {{ML}}-like {{Languages}}},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Yang, Zhe},
  year = {1998},
  pages = {289--300},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/289423.289458},
  abstract = {A Hindley-Milner type system such as ML's seems to prohibit type-indexed values, i.e., functions that map a family of types to a family of values. Such functions generally perform case analysis on the input types and return values of possibly different types. The goal of our work is to demonstrate how to program with type-indexed values within a Hindley-Milner type system.Our first approach is to interpret an input type as its corresponding value, recursively. This solution is type-safe, in the sense that the ML type system statically prevents any mismatch between the input type and function arguments that depend on this type.Such specific type interpretations, however, prevent us from combining different type-indexed values that share the same type. To meet this objection, we focus on finding a value-independent type encoding that can be shared by different functions. We propose and compare two solutions. One requires first-class and higher-order polymorphism, and, thus, is not implementable in the core language of ML, but it can be programmed using higher-order functors in Standard ML of New Jersey. Its usage, however, is clumsy. The other approach uses embedding/projection functions. It appears to be more practical.We demonstrate the usefulness of type-indexed values through examples including type-directed partial evaluation, C printf-like formatting, and subtype coercions. Finally, we discuss the tradeoffs between our approach and some other solutions based on more expressive typing disciplines.},
  isbn = {1-58113-024-4},
  series = {{{ICFP}} '98}
}

@article{Yang2004encoding,
  title = {Encoding Types in {{ML}}-like Languages},
  author = {Yang, Zhe},
  year = {2004},
  month = may,
  volume = {315},
  pages = {151--190},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2003.11.017},
  abstract = {This article presents several general approaches to programming with type-indexed families of values within a Hindley\textendash{}Milner type system. A type-indexed family of values is a function that maps a family of types to a family of values. The function performs a case analysis on the input types and returns values of possibly different types. Such a case analysis on types seems to be prohibited by the Hindley\textendash{}Milner type system. Our approaches solve the problem by using type encodings. The compile-time types of the type encodings reflect the types themselves, thereby making the approaches type-safe, in the sense that the underlying type system statically prevents any mismatch between the input type and the function arguments that depend on this type.

A type encoding could be either value-dependent, meaning that the type encoding is tied to a specific type-indexed family, or value-independent, meaning that the type encoding can be shared by various type-indexed families. Our first approach is value-dependent: we simply interpret a type as its corresponding value. Our second approach provides value-independent type encodings through embedding and projection functions; they are universal type interpretations, in that they can be used to compute other type interpretations. We also present an alternative approach to value-independent type encodings, using higher-order functors.

We demonstrate our techniques through applications such as C printf-like formatting, type-directed partial evaluation, and subtype coercions.},
  journal = {Theoretical Computer Science},
  number = {1},
  series = {Mathematical {{Foundations}} of {{Programming Semantics}}}
}

@article{Yang2017Unifying,
  title = {Unifying Typing and Subtyping},
  author = {Yang, Yanpeng and Oliveira, Bruno C. d. S.},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {47:1--47:26},
  issn = {2475-1421},
  doi = {10.1145/3133871},
  abstract = {In recent years dependent types have become a hot topic in programming language research. A key reason why dependent types are interesting is that they allow unifying types and terms, which enables both additional expressiveness and economy of concepts. Unfortunately there has been much less work on dependently typed calculi for object-oriented programming. This is partly because it is widely acknowledged that the combination between dependent types and subtyping is particularly challenging. This paper presents {$\lambda$} I{$\leq$}, which is a dependently typed generalization of System F{$\leq$}. The resulting calculus follows the style of Pure Type Systems, and contains a single unified syntactic sort that accounts for expressions, types and kinds. To address the challenges posed by the combination of dependent types and subtyping, {$\lambda$} I{$\leq$} employs a novel technique that unifies typing and subtyping. In {$\lambda$} I{$\leq$} there is only a judgement that is akin to a typed version of subtyping. Both the typing relation, as well as type well-formedness are just special cases of the subtyping relation. The resulting calculus has a rich metatheory and enjoys of several standard and desirable properties, such as subject reduction, transitivity of subtyping, narrowing as well as standard substitution lemmas. All the metatheory of {$\lambda$} I{$\leq$} is mechanically proved in the Coq theorem prover. Furthermore, (and as far as we are aware) {$\lambda$} I{$\leq$} is the first dependently typed calculus that completely subsumes System F{$\leq$}, while preserving various desirable properties.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {dependent types,object-oriented programming,subtyping},
  number = {OOPSLA}
}

@inproceedings{Zeilberger2008focusing,
  title = {Focusing and {{Higher}}-Order {{Abstract Syntax}}},
  booktitle = {Proceedings of the 35th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Zeilberger, Noam},
  year = {2008},
  pages = {359--369},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1328438.1328482},
  abstract = {Focusing is a proof-search strategy, originating in linear logic, that elegantly eliminates inessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs with explicit evaluation order. Higher-order abstract syntax (HOAS) is a technique for representing higher-order programming language constructs (e.g., {$\lambda$}'s) by higher-order terms at the"meta-level", thereby avoiding some of the bureaucratic headaches of first-order representations (e.g., capture-avoiding substitution). This paper begins with a fresh, judgmental analysis of focusing for intuitionistic logic (with a full suite of propositional connectives), recasting the "derived rules" of focusing as iterated inductive definitions. This leads to a uniform presentation, allowing concise, modular proofs of the identity and cut principles. Then we show how this formulation of focusing induces, through the Curry-Howard isomorphism, a new kind of higher-order encoding of abstract syntax: functions are encoded by maps from patterns to expressions. Dually, values are encoded as patterns together with explicit substitutions. This gives us pattern-matching "for free", and lets us reason about a rich type system with minimal syntactic overhead. We describe how to translate the language and proof of type safety almost directly into Coq using HOAS, and finally, show how the system's modular design pays off in enabling a very simple extension with recursion and recursive types.},
  isbn = {978-1-59593-689-9},
  keywords = {focusing,higher-order abstract syntax,pattern-matching},
  series = {{{POPL}} '08}
}

@inproceedings{Zeller2011failure,
  title = {Failure Is a {{Four}}-Letter {{Word}}: {{A Parody}} in {{Empirical Research}}},
  shorttitle = {Failure Is a {{Four}}-Letter {{Word}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Predictive Models}} in {{Software Engineering}}},
  author = {Zeller, Andreas and Zimmermann, Thomas and Bird, Christian},
  year = {2011},
  pages = {5:1--5:7},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2020390.2020395},
  abstract = {Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74\% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce "actionable" results.},
  isbn = {978-1-4503-0709-3},
  keywords = {empirical research,parody},
  series = {Promise '11}
}

@inproceedings{Zenger2001extensible,
  title = {Extensible Algebraic Datatypes with Defaults},
  booktitle = {Proceedings of the {{Sixth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Zenger, Matthias and Odersky, Martin},
  year = {2001},
  pages = {241--252},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/507635.507665},
  abstract = {A major problem for writing extensible software arises when recursively defined datatypes and operations on these types have to be extended simultaneously without modifying existing code. This paper introduces Extensible Algebraic Datatypes with defaults, which promote a simple programming pattern to solve this well-known problem. We show that it is possible to encode extensible algebraic datatypes in an object-oriented language, using a new design pattern for extensible visitors. Extensible algebraic datatypes have been successfully applied in the implementation of an extensible Java compiler. Our technique allows for the reuse of existing components in compiler extensions without the need for any adaptations.},
  isbn = {1-58113-415-0},
  series = {{{ICFP}} '01}
}

@article{Zhang2017Familia,
  title = {Familia: {{Unifying}} Interfaces, Type Classes, and Family Polymorphism},
  shorttitle = {Familia},
  author = {Zhang, Yizhou and Myers, Andrew C.},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {70:1--70:31},
  issn = {2475-1421},
  doi = {10.1145/3133894},
  abstract = {Parametric polymorphism and inheritance are both important, extensively explored language mechanisms for providing code reuse and extensibility. But harmoniously integrating these apparently distinct mechanisms\textemdash{}and powerful recent forms of them, including type classes and family polymorphism\textemdash{}in a single language remains an elusive goal. In this paper, we show that a deep unification can be achieved by generalizing the semantics of interfaces and classes. The payoff is a significant increase in expressive power with little increase in programmer-visible complexity. Salient features of the new programming language include retroactive constraint modeling, underpinning both object-oriented programming and generic programming, and module-level inheritance with further-binding, allowing family polymorphism to be deployed at large scale. The resulting mechanism is syntactically light, and the more advanced features are transparent to the novice programmer. We describe the design of a programming language that incorporates this mechanism; using a core calculus, we show that the type system is sound. We demonstrate that this language is highly expressive by illustrating how to use it to implement highly extensible software and by showing that it can not only concisely model state-of-the-art features for code reuse, but also go beyond them.},
  journal = {Proc. ACM Program. Lang.},
  keywords = {_tablet,extensibility,Familia,family polymorphism,genericity,language design,type classes,type-safety},
  number = {OOPSLA}
}

@inproceedings{Zhang2018Pattern,
  title = {Pattern {{Matching}} in an {{Open World}}},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  author = {Zhang, Weixin and Oliveira, Bruno C. d. S.},
  year = {2018},
  pages = {134--146},
  publisher = {{ACM}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/3278122.3278124},
  abstract = {Pattern matching is a pervasive and useful feature in functional programming. There have been many attempts to bring similar notions to Object-Oriented Programming (OOP) in the past. However, a key challenge in OOP is how pattern matching can coexist with the open nature of OOP data structures, while at the same time guaranteeing other desirable properties for pattern matching.  This paper discusses several desirable properties for pattern matching in an OOP context and shows how existing approaches are lacking some of these properties. We argue that the traditional semantics of pattern matching, which is based on the order of patterns and adopted by many approaches, is in conflict with the openness of data structures. Therefore we suggest that a more restricted, top-level pattern matching model, where the order of patterns is irrelevant, is worthwhile considering in an OOP context. To compensate for the absence of ordered patterns we propose a complementary mechanism for case analysis with defaults, which can be used when nested and/or multiple case analysis is needed. To illustrate our points we develop Castor: a meta-programming library inScala that adopts both ideas. Castor generates code that uses type-safe extensible visitors, and largely removes boilerplate code typically associated with visitors. We illustrate the applicability of our approach with a case study modularizing the interpreters in the famous book ''Types and Programming Languages''.},
  isbn = {978-1-4503-6045-6},
  keywords = {Meta-programming,OOP,Pattern matching},
  series = {{{GPCE}} 2018}
}

@inproceedings{Zhu2015learning,
  title = {Learning {{Refinement Types}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Zhu, He and Nori, Aditya V. and Jagannathan, Suresh},
  year = {2015},
  pages = {400--411},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784766},
  abstract = {We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.},
  isbn = {978-1-4503-3669-7},
  keywords = {Higher-Order Verification,Learning,refinement types,Testing},
  series = {{{ICFP}} 2015}
}

@inproceedings{Zibin2012Object,
  title = {Object {{Initialization}} in {{X10}}},
  booktitle = {{{ECOOP}} 2012 \textendash{} {{Object}}-{{Oriented Programming}}},
  author = {Zibin, Yoav and Cunningham, David and Peshansky, Igor and Saraswat, Vijay},
  editor = {Noble, James},
  year = {2012},
  pages = {207--231},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {X10 is an object oriented programming language with a sophisticated type system (constraints, class invariants, non-erased generics, closures) and concurrency constructs (asynchronous activities, multiple places). Object initialization is a cross-cutting concern that interacts with all of these features in delicate ways that may cause type, runtime, and security errors. This paper discusses possible designs for object initialization, and the ``hardhat'' design chosen and implemented in X10 version 2.2. Our implementation includes a fixed-point inter-procedural (intra-class) data-flow analysis that infers, for each method called during initialization, the set of fields that are read, and those that are asynchronously and synchronously assigned. Our codebase of more than 200K lines of code only had 104 annotations. Finally, we formalize the essence of initialization checking with an effect system intended to complement a standard FJ style formalization of the type system for X10. This system is substantially simpler than the masked types of [10], and it is more practical (for X10) than the free-committed types of [12]. This is the first formalization of a type and (flow-sensitive) effect system for safe initialization in the presence of concurrency constructs.},
  isbn = {978-3-642-31057-7},
  keywords = {Class Invariant,Initialization Rule,Object Initialization,Object Oriented Programming Language,Type System},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{Ziliani2013mtac,
  title = {Mtac: {{A Monad}} for {{Typed Tactic Programming}} in {{Coq}}},
  shorttitle = {Mtac},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Ziliani, Beta and Dreyer, Derek and Krishnaswami, Neelakantan R. and Nanevski, Aleksandar and Vafeiadis, Viktor},
  year = {2013},
  pages = {87--100},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500365.2500579},
  abstract = {Effective support for custom proof automation is essential for large scale interactive proof development. However, existing languages for automation via *tactics* either (a) provide no way to specify the behavior of tactics within the base logic of the accompanying theorem prover, or (b) rely on advanced type-theoretic machinery that is not easily integrated into established theorem provers. We present Mtac, a lightweight but powerful extension to Coq that supports dependently-typed tactic programming. Mtac tactics have access to all the features of ordinary Coq programming, as well as a new set of typed tactical primitives. We avoid the need to touch the trusted kernel typechecker of Coq by encapsulating uses of these new tactical primitives in a *monad*, and instrumenting Coq so that it executes monadic tactics during type inference.},
  isbn = {978-1-4503-2326-0},
  keywords = {coq,custom proof automation,interactive theorem proving,Monads,tactics,typed meta-programming},
  series = {{{ICFP}} '13}
}

@inproceedings{Ziliani2015unification,
  title = {A {{Unification Algorithm}} for {{Coq Featuring Universe Polymorphism}} and {{Overloading}}},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Ziliani, Beta and Sozeau, Matthieu},
  year = {2015},
  pages = {179--191},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2784731.2784751},
  abstract = {Unification is a core component of every proof assistant or programming language featuring dependent types. In many cases, it must deal with higher-order problems up to conversion. Since unification in such conditions is undecidable, unification algorithms may include several heuristics to solve common problems. However, when the stack of heuristics grows large, the result and complexity of the algorithm can become unpredictable. Our contributions are twofold: (1) We present a full description of a new unification algorithm for the Calculus of Inductive Constructions (the base logic of Coq), including universe polymorphism, canonical structures (the overloading mechanism baked into Coq's unification), and a small set of useful heuristics. (2) We implemented our algorithm, and tested it on several libraries, providing evidence that the selected set of heuristics suffices for large developments.},
  isbn = {978-1-4503-3669-7},
  keywords = {coq,interactive theorem proving,overloading,unification,universe polymorphism},
  series = {{{ICFP}} 2015}
}


