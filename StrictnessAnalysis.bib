%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Paolo Giosu√© Giarrusso at 2014-04-01 04:24:27 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Sergey14MHC,
	Acmid = {2535861},
	Address = {New York, NY, USA},
	Author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon},
	Booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	Date-Added = {2014-04-01 02:24:19 +0000},
	Date-Modified = {2014-04-01 02:24:27 +0000},
	Doi = {10.1145/2535838.2535861},
	Isbn = {978-1-4503-2544-8},
	Keywords = {cardinality analysis, compilers, functional programming languages, haskell, lazy evaluation, operational semantics, program optimisation, static analysis, thunks, types and effects},
	Location = {San Diego, California, USA},
	Numpages = {13},
	Pages = {335--347},
	Publisher = {ACM},
	Series = {POPL '14},
	Title = {Modular, Higher-order Cardinality Analysis in Theory and Practice},
	Url = {http://doi.acm.org/10.1145/2535838.2535861},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2535838.2535861},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2535838.2535861}}

@article{Burn86,
	Author = {Geoffrey L. Burn and Chris Hankin and Samson Abramsky},
	Date-Added = {2014-02-12 00:24:38 +0000},
	Date-Modified = {2014-02-12 00:24:41 +0000},
	Doi = {http://dx.doi.org/10.1016/0167-6423(86)90010-9},
	Issn = {0167-6423},
	Journal = {Science of Computer Programming},
	Number = {0},
	Pages = {249 - 278},
	Title = {Strictness analysis for higher-order functions},
	Url = {http://www.sciencedirect.com/science/article/pii/0167642386900109},
	Volume = {7},
	Year = {1986},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0167642386900109},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/0167-6423(86)90010-9}}

@inproceedings{Wadler87PSA,
	Abstract = {Contexts have been proposed as a means of performing strictness analysis on non-flat domains. Roughly speaking, a context describes how much a sub-expression will be evaluated by the surrounding program. This paper shows how contexts can be represented using the notion of projection from domain theory. This is clearer than the previous explanation of contexts in terms of continuations. In addition, this paper describes finite domains of contexts over the non-flat list domain. This means that recursive context equations can be solved using standard fixpoint techniques, instead of the algebraic manipulation previously used.},
	Acmid = {652502},
	Address = {London, UK, UK},
	Author = {Wadler, Philip and Hughes, R. J. M.},
	Booktitle = fpca,
	Date-Added = {2014-02-09 23:56:49 +0000},
	Date-Modified = {2014-02-09 23:56:49 +0000},
	Isbn = {3-540-18317-5},
	Numpages = {23},
	Pages = {385--407},
	Publisher = {Springer-Verlag},
	Title = {Projections for Strictness Analysis},
	Url = {http://dl.acm.org/citation.cfm?id=645419.652502},
	Year = {1987},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=645419.652502}}

@inproceedings{Hughes89PPS,
	Abstract = {We apply the categorical properties of polymorphic functions to compile-time analysis, specifically projection-based strictness analysis. First we interpret parameterised types as functors in a suitable category, and show that they preserve monics and epics. Then we define ``strong'' and ``weak'' polymorphism --- the latter admitting certain projections that are not polymorphic in the usual sense. We prove that, under the right conditions, a weakly polymorphic function is characterised by a single instance. It follows that the strictness analysis of one simple instance of a polymorphic function yields results that apply to all. We show how this theory may be applied.
In comparison to earlier polymorphic strictness analysis methods, ours can apply polymorphic information to a particular instance very simply. The categorical approach simplifies our proofs, enabling them to be carried out at a higher level, and making them independent of the precise form of the programming language to be analysed.},
	Acmid = {755572},
	Address = {London, UK, UK},
	Author = {Hughes, John},
	Booktitle = {Category Theory and Computer Science},
	Date-Added = {2014-02-09 23:44:39 +0000},
	Date-Modified = {2014-02-09 23:45:11 +0000},
	Isbn = {3-540-51662-X},
	Numpages = {19},
	Pages = {82--100},
	Publisher = {Springer-Verlag},
	Title = {Projections for Polymorphic Strictness Analysis},
	Url = {http://dl.acm.org/citation.cfm?id=648332.755572},
	Year = {1989},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=648332.755572}}

@inproceedings{Holst91,
	Abstract = {Partial evaluation of a program p with respect to partial input data s produces a residual program r. If the program r is subsequently run a number of times on values d_i, there is a potential overall gain in efficiency since the static computations are performed only once, namely when the residual program r is generated. In a fully lazy language, a partial function application which is subsequently applied several times yields much of the same sharing. In a lazy language the result of a partial application may be thought of as a "residual graph". This residual graph is not as efficient as a residual program produced by traditional partial evaluators, but simple program transformations can improve the sharing properties of a lazy functional program p to match those obtained by partial evaluation. Since we work in a lazy language and do not generate a residual program we avoid the termination problems suffered by all partial evaluators so far. These results also pave the way for a new class of optimizations of lazy functional programs: namely the binding time transformations employed by users and writers of partial evaluators so far.},
	Acmid = {115890},
	Address = {New York, NY, USA},
	Annote = {The abstract mentions without reference "full laziness".
But see for instance Peyton-Jones98 (Sec. 7.2), who traces it back to yet other papers; in short, it's one way to reduce inside bindings, by simply floating bindings outside lambdas when possible/appropriate.},
	Author = {Holst, Carsten Kehler and Gomard, Darsten Krogh},
	Booktitle = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-based Program Manipulation},
	Date-Added = {2014-02-09 21:38:20 +0000},
	Date-Modified = {2014-02-09 23:00:44 +0000},
	Doi = {10.1145/115865.115890},
	Isbn = {0-89791-433-3},
	Location = {New Haven, Connecticut, USA},
	Numpages = {11},
	Pages = {223--233},
	Publisher = {ACM},
	Series = {PEPM '91},
	Title = {Partial Evaluation is Fuller Laziness},
	Url = {http://doi.acm.org/10.1145/115865.115890},
	Year = {1991},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/115865.115890},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/115865.115890}}

@inproceedings{Chang14,
	Acmid = {2535887},
	Address = {New York, NY, USA},
	Author = {Chang, Stephen and Felleisen, Matthias},
	Booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	Date-Added = {2014-02-09 20:30:14 +0000},
	Date-Modified = {2014-02-09 23:36:11 +0000},
	Doi = {10.1145/2535838.2535887},
	Isbn = {978-1-4503-2544-8},
	Keywords = {code refactoring, laziness, profiling},
	Location = {San Diego, California, USA},
	Numpages = {12},
	Pages = {349--360},
	Publisher = {ACM},
	Rating = {4},
	Series = {POPL '14},
	Title = {Profiling for Laziness},
	Url = {http://doi.acm.org/10.1145/2535838.2535887},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2535838.2535887},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2535838.2535887}}

@incollection{Chang13,
	Abstract = {Lazy functional programming has many benefits that strict functional languages can simulate via lazy data constructors. In recognition, ML, Scheme, and other strict functional languages have supported lazy stream programming with delay and force for several decades. Unfortunately, the manual insertion of delay and force can be tedious and error-prone.
We present a semantics-based refactoring that helps strict programmers manage manual lazy programming. The refactoring uses a static analysis to identify where additional delays and forces might be needed to achieve the desired simplification and performance benefits, once the programmer has added the initial lazy data constructors. The paper presents a correctness argument for the underlying transformations and some preliminary experiences with a prototype tool implementation.},
	Author = {Chang, Stephen},
	Booktitle = {Programming Languages and Systems},
	Date-Added = {2014-02-09 20:15:16 +0000},
	Date-Modified = {2014-02-09 23:36:07 +0000},
	Doi = {10.1007/978-3-642-37036-6_5},
	Editor = {Felleisen, Matthias and Gardner, Philippa},
	Isbn = {978-3-642-37035-9},
	Pages = {81-100},
	Publisher = {Springer Berlin Heidelberg},
	Rating = {4},
	Series = {Lecture Notes in Computer Science},
	Title = {Laziness by Need},
	Url = {http://dx.doi.org/10.1007/978-3-642-37036-6_5},
	Volume = {7792},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-642-37036-6_5}}

@incollection{Leung91,
	Abstract = {In this work we describe an innovative strictness analysis method for reasoning about simple and exhaustive demand in higher-order lazy languages. By reasoning about simple demand, we mean determining whether a function requires its input in head normal form whenever its output is so required, i.e., is the function strict? Similarly, by exhaustive demand we mean determining whether a function requires its input in normal form whenever its output is so required, i.e., is the function fully strict?.
Our method, which is based on type inference, supports reasoning about strictness and full-strictness within a unified framework by making essential use of subtypes. In contrast to previous proposals, our method does not require fixpoint iteration and hence should be more amenable to practical use. Furthermore, our method can handle such features as pattern-matching and user-defined datatypes which are important in practical programming. The insight behind our results is that program properties such as strictness can be treated as types possessed by programs. As a consequence, we are able to design an inference algorithm, which given program P constructs a representation for all possible ``strictness'' types deducible for P. This representation takes the form of a set of constraints. We show that strictness and full-strictness properties of P can be derived, in a precise formal sense, from this set of constraints. We include a systematic comparison between our techniques and results and those of higher-order abstract interpretation.},
	Author = {Leung, Allen and Mishra, Prateek},
	Booktitle = {Functional Programming Languages and Computer Architecture},
	Date-Added = {2014-02-09 19:40:19 +0000},
	Date-Modified = {2014-02-09 19:40:33 +0000},
	Doi = {10.1007/3540543961_16},
	Editor = {Hughes, John},
	Isbn = {978-3-540-54396-1},
	Pages = {328-351},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Reasoning about simple and exhaustive demand in higher-order lazy languages},
	Url = {http://dx.doi.org/10.1007/3540543961_16},
	Volume = {523},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3540543961_16}}

@article{Launchbury96,
	Abstract = { ABSTRACT The projection-based strictness analysis of Wadler and Hughes is elegant and theoretically satisfying except in one respect: the need for lifting. The domains and functions over which the analysis is performed need to be transformed, leading to a less direct correspondence between analysis and program than might be hoped for. In this paper we shall see that the projection analysis may be reformulated in terms of partial projections, so removing this infelicity. There are additional benefits of the formulation: the two forms of information captured by the projection are distinguished, and the operational significance of the range of the projection fits exactly with the theory of unboxed types. },
	Annote = {Like me, they hate the double-lifting needed by Wadler's projection-based framework, and propose a different presentation. I like this, since I think Wadler's paper is not just ugly but has some technical problems in the formulation of his concepts.},
	Author = {Launchbury,John and Baraki,Gebreselassie},
	Date-Added = {2014-02-09 19:38:22 +0000},
	Date-Modified = {2014-02-09 19:38:36 +0000},
	Doi = {10.1017/S0956796800001878},
	Issn = {1469-7653},
	Issue = {04},
	Journal = {Journal of Functional Programming},
	Keywords = {projection-based, theory},
	Month = {7},
	Numpages = {23},
	Pages = {563--585},
	Title = {Representing demand by partial projections},
	Url = {http://journals.cambridge.org/article_S0956796800001878},
	Volume = {6},
	Year = {1996},
	Bdsk-Url-1 = {http://journals.cambridge.org/article_S0956796800001878},
	Bdsk-Url-2 = {http://dx.doi.org/10.1017/S0956796800001878}}

@article{Park95,
	Author = {Young G Park and Benjamin Goldberg},
	Date-Added = {2014-02-09 19:32:24 +0000},
	Date-Modified = {2014-02-09 19:32:48 +0000},
	Doi = {http://dx.doi.org/10.1016/0020-0190(95)00120-2},
	Issn = {0020-0190},
	Journal = {Information Processing Letters},
	Keywords = {higher-order, polymorphic},
	Number = {6},
	Pages = {343 - 348},
	Title = {Order-of-demand analysis for lazy languages},
	Url = {http://www.sciencedirect.com/science/article/pii/0020019095001202},
	Volume = {55},
	Year = {1995},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0020019095001202},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/0020-0190(95)00120-2}}

@incollection{Paterson96,
	Abstract = {Projection-based strictness analysis is a powerful technique, able to cope with non-flat domains and latent demand. By analysing the projections as embedding-projection pairs, we develop an algorithm to translate lazy functions into a strict functional language with explicit closures. The translated functions typically operate on simpler types than the originals, in particular types containing fewer liftings, which correspond to the operational notion of closures. Like the analysis on which it is based, our algorithm is restricted to first-order functions.},
	Author = {Paterson, Ross},
	Booktitle = {Static Analysis},
	Date-Added = {2014-02-09 19:29:08 +0000},
	Date-Modified = {2014-02-09 19:29:33 +0000},
	Doi = {10.1007/3-540-61739-6_46},
	Editor = {Cousot, Radhia and Schmidt, David A.},
	Isbn = {978-3-540-61739-6},
	Keywords = {first-order, projection-based},
	Pages = {255-269},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Compiling laziness using projections},
	Url = {http://dx.doi.org/10.1007/3-540-61739-6_46},
	Volume = {1145},
	Year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-61739-6_46}}

@incollection{Peyton-Jones94,
	Abstract = {We describe a simple strictness analyser for purely-functional programs, show how its results are used to improve programs, and provide measurements of the effects of these improvements. These measurements are given both in terms of overall run-time, and in terms of internal operations such as allocations, updates, etc.
Despite its simplicity, the analyser handles higher-order functions, and non-flat domains provided they are non-recursive.},
	Author = {Peyton Jones, Simon L. and Partain, Will},
	Booktitle = {Functional Programming, Glasgow 1993},
	Date-Added = {2014-02-09 19:28:03 +0000},
	Date-Modified = {2014-02-09 23:35:46 +0000},
	Doi = {10.1007/978-1-4471-3236-3_17},
	Editor = {O'Donnell, JohnT. and Hammond, Kevin},
	Isbn = {978-3-540-19879-6},
	Language = {English},
	Pages = {201-221},
	Publisher = {Springer London},
	Rating = {4},
	Series = {Workshops in Computing},
	Title = {Measuring the effectiveness of a simple strictness analyser},
	Url = {http://dx.doi.org/10.1007/978-1-4471-3236-3_17},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-1-4471-3236-3_17}}

@incollection{Baraki91,
	Abstract = {Strictness analysis of monomorphic functional programs by abstract interpretation has been extensively studied. Abramsky's work describes the extension to polymorphic functions. However, if different instances of a polymorphic function are used in a program, we have to re-analyse to compute abstract functions at each instance used. In the first-order case, Hughes showed how to compute the needed abstract functions without re-analysis. In this paper we develop a method of computing safe approximations to abstract functions of higher-order functions. The approximations are computed from abstract functions of simple instances of polymorphic functions. In the first-order case, the method gives exact values.},
	Annote = {This paper is so summarized by Jensen94:

Functions. Other works have extended the precision of the analysis. Among these we mention Burn, Hankin and Abramsky, who first described how to analyse higher order functions [BHA86], Abramsky, who showed that the analysis presented in [BHA86] is polymorphicly invariant [Abr85], and Baraki [Bar91] who constructed a method to approximate strictness functions for polymorphic functions from their simplest instantiation. Several other works have presented extensions for higher order functions.},
	Author = {Baraki, Gebreselassie},
	Booktitle = {Functional Programming Languages and Computer Architecture},
	Date-Added = {2014-02-09 19:21:37 +0000},
	Date-Modified = {2014-02-09 19:25:53 +0000},
	Doi = {10.1007/3540543961_18},
	Editor = {Hughes, John},
	Isbn = {978-3-540-54396-1},
	Pages = {367-378},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {A note on abstract interpretation of polymorphic functions},
	Url = {http://dx.doi.org/10.1007/3540543961_18},
	Volume = {523},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3540543961_18}}

@incollection{Jensen94,
	Abstract = {Strictness analysis has been a living field of investigation since Mycroft's original work in 1980, and is getting increasingly significant with the still wider use of lazy functional programming languages. This paper focuses on an actual implementation of a strictness analyser for Haskell. The analyser uses abstract interpretation with chaotic fix-point iteration. The demand-driven nature of this iteration technique allows us to use large domains including function domains in the style of Burn et al. [BHA86] and Wadler [Wad87] and retain reasonable efficiency.
The implementation, furthermore, allows us to introduce a new way of handling polymorphism by parameterizing the strictness functions with domain-variables.
Finally we present some results of efficiency and precision and compare them to other works.},
	Annote = {Older systems "have one major drawback: they assign the 2-point domain to recursive functional arguments, which means that any analysis on a higher order function will yield very imprecise results."

Instead, "this paper describes an implementation of an experimental strictness analyser
that is able to do full higher order analysis of Haskell."

},
	Author = {Jensen, KristianDamm and Hj{\ae}resen, Peter and Rosendahl, Mads},
	Booktitle = {Static Analysis},
	Date-Added = {2014-02-09 19:09:20 +0000},
	Date-Modified = {2014-02-09 23:35:28 +0000},
	Doi = {10.1007/3-540-58485-4_51},
	Editor = {Charlier, Baudouin},
	Isbn = {978-3-540-58485-8},
	Keywords = {abstract interpretation, polymorphic, higher-order},
	Pages = {346-362},
	Publisher = {Springer Berlin Heidelberg},
	Rating = {4},
	Series = {Lecture Notes in Computer Science},
	Title = {Efficient strictness analysis of Haskell},
	Url = {http://dx.doi.org/10.1007/3-540-58485-4_51},
	Volume = {864},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-58485-4_51}}

@incollection{Amtoft94,
	Abstract = {We convert, via a version that uses constraints, a type inference system for strictness analysis into an algorithm which given an expression finds the set of possible typings. Although this set in general does not possess a minimal element, it can be represented compactly by means of symbolic expressions in normal form --- such expressions have the property that once values for the constraint variables with negative polarity have been supplied it is straight-forward to compute the minimal values for the constraint variables with positive polarity. The normalization process works on the fly, i.e. by a leaf-to-root traversal of the inference tree.},
	Annote = {The actual result seems to be about type inference --- it only uses ideas from strictness analysis.},
	Author = {Amtoft, Torben},
	Booktitle = {Programming Languages and Systems --- ESOP '94},
	Date-Added = {2014-02-09 19:07:55 +0000},
	Date-Modified = {2014-02-09 19:08:11 +0000},
	Doi = {10.1007/3-540-57880-3_3},
	Editor = {Sannella, Donald},
	Isbn = {978-3-540-57880-2},
	Pages = {43-57},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Local type reconstruction by means of symbolic fixed point iteration},
	Url = {http://dx.doi.org/10.1007/3-540-57880-3_3},
	Volume = {788},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-57880-3_3}}

@incollection{Gandhe95,
	Abstract = {The problem of analyzing functional programs requires us to identify subterms of a given term with certain properties. In this paper, we introduce a variant of Œª-calculus called labeled Œª-calculus to help us build a theory of positional analysis. Labeled Œª-calculus uses sets of labels as a means of naming subterms and keeping track of them. We then define a stronger form of strictness called need and show how labeled Œª-calculus can be used to compute need. We extend the notion of the need to subterms and use labeled Œª-calculus to identify needed subterms. We also use this notion to qualify a function's need for its argument --- to identify how much of the argument to a function is needed to evaluate an application to head normal form.},
	Annote = {They discuss need, so maybe their analysis is not just about strictness.

But the venue, their novelty claims, and so on makes me suspicious --- their idea of "need" resembles head/tail-strictness and generalizations, but they don't cite enough papers on strictness analysis (only 2).
They refer to a 1987 paper as "a good survey".

After reading the intro, the idea itself seems possibly interesting to me. But I doubt the work is actually high quality.
Finally, an editor is Jean-Jacques L{\'e}vy, but he's distinct from the Paul Blain Levy from call-by-push-value; still, he seems well-published in his own right (a 300- and 500- citations papers).
Still, the conference is a track within the "1995 Asian Computing Science Conference, ACSC '95 Pathumthani, Thailand, December 11--13, 1995 Proceedings".},
	Author = {Gandhe, Milind and Venkatesh, G. and Sanyal, Amitabha},
	Booktitle = {Algorithms, Concurrency and Knowledge},
	Date-Added = {2014-02-09 18:58:01 +0000},
	Date-Modified = {2014-02-09 19:06:57 +0000},
	Doi = {10.1007/3-540-60688-2_38},
	Editor = {Kanchanasut, Kanchana and L{\'e}vy, Jean-Jacques},
	Isbn = {978-3-540-60688-8},
	Pages = {103-110},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Labeled Œª-calculus and a generalised notion of strictness},
	Url = {http://dx.doi.org/10.1007/3-540-60688-2_38},
	Volume = {1023},
	Year = {1995},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-60688-2_38}}

@inproceedings{Abramsky91,
	Abstract = {This paper defines the categorical notions of relators and transformations and shows that these concepts enable us to give a semantics for polymorphic, higher order functional programs. We demonstrate the pertinence of this semantics to the analysis of polymorphic programs by proving that strictness analysis is a polymorphic invariant.
},
	Acmid = {99593},
	Address = {New York, NY, USA},
	Author = {Abramsky, Samson and Jensen, Thomas P.},
	Booktitle = {Proceedings of the 18th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	Date-Added = {2014-02-09 18:56:13 +0000},
	Date-Modified = {2014-02-09 18:57:14 +0000},
	Doi = {10.1145/99583.99593},
	Isbn = {0-89791-419-8},
	Keywords = {higher-order; polymorphic},
	Location = {Orlando, Florida, USA},
	Numpages = {6},
	Pages = {49--54},
	Publisher = {ACM},
	Series = {POPL '91},
	Title = {A Relational Approach to Strictness Analysis for Higher-order Polymorphic Functions},
	Url = {http://doi.acm.org/10.1145/99583.99593},
	Year = {1991},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/99583.99593},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/99583.99593}}

@inproceedings{Cousot94,
	Abstract = {The original formulation of abstract interpretation represents program properties by sets. A property is understood as the set of semantic values satisfying it. Strongest program properties are defined by the collecting semantics which extends the standard semantics to powersets of semantic values. The approximation relation corresponding to the logical implication of program properties is subset inclusion. This was expressed using set and lattice theory in the context of transition systems. Some applications of abstract interpretation, such as strictness analysis for lazy functional languages, require infinite behaviours of higher-order functions to be taken into account. We solve the problem by returning to the sources of abstract interpretation, which consists in considering collecting semantics. By using Galois connections, properties of the standard semantics naturally transfer to the collecting and then to the abstract semantics. This set-theoretic abstract interpretation framework is formulated in a way which is independent of both the programming language and the method used to specify its semantics. It is illustrated for a higher-order monomorphically typed lazy functional language starting from its standard denotational semantics},
	Author = {Cousot, P. and Cousot, R.},
	Booktitle = {Computer Languages, 1994., Proceedings of the 1994 International Conference on},
	Date-Added = {2014-02-09 18:53:57 +0000},
	Date-Modified = {2014-02-09 18:55:14 +0000},
	Doi = {10.1109/ICCL.1994.288389},
	Keywords = {abstract interpretation;higher-order;monomorphic},
	Month = {May},
	Pages = {95-112},
	Title = {Higher-order abstract interpretation (and application to comportment analysis generalizing strictness, termination, projection and PER analysis of functional languages)},
	Year = {1994},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCL.1994.288389}}

@incollection{Benton93,
	Abstract = {A new construction of a finite set of strictness properties for any lazy algebraic datatype is presented. The construction is based on the categorical view of the solutions to the recursive domain equations associated with such types as initial algebras. We then show how the initial algebra induction principle can be used to reason about the entailment relation on the chosen collection of properties. We examine the lattice of properties given by our construction for the type nlist of lazy lists of natural numbers and give proof rules which extend the conjunctive strictness logic of [2] to a language including the type nlist.},
	Annote = {See its reference to [2], Strictness logic and polymorphic invariance.},
	Author = {Benton, P.N.},
	Booktitle = {Static Analysis},
	Date-Added = {2014-02-09 18:52:15 +0000},
	Date-Modified = {2014-02-09 23:40:31 +0000},
	Doi = {10.1007/3-540-57264-3_42},
	Editor = {Cousot, Patrick and Falaschi, Moreno and Fil{\'e}, Gilberto and Rauzy, Antoine},
	Isbn = {978-3-540-57264-0},
	Pages = {206-217},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Strictness properties of lazy algebraic datatypes},
	Url = {http://dx.doi.org/10.1007/3-540-57264-3_42},
	Volume = {724},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3-540-57264-3_42}}

@inproceedings{Kuo89,
	Abstract = {In this work we develop a new framework for strictness analysis. We provide a new definition of strictness based on operational semantics which directly addresses the primary aspect of the strictness analysis problem: non-termination of evaluation. Our definition excludes other irrelevant details such as the type system of the language, and imposes minimal constraints on semantic equality of expressions.
We provide a new syntactic representation for strictness properties of programs and a new computational technique for strictness analysis. The syntactic representation takes the form of a simple type language for expressing strictness properties and is much simpler than representations used in previous work. The computational technique is based on well-understood type inference techniques and improves on previous work as it does not require fixpoint iteration.
Our framework is parameterized with respect to the language of strictness properties and type inference techniques. As improvements in computational methods become available, both the language and type inference technique can be naturally extended to more complex and accurate strictness analysis.},
	Acmid = {99390},
	Address = {New York, NY, USA},
	Annote = {Main result here: this approach is simpler (and faster?) than abstract interpretation.
Main result in Jensen91: the two approaches have the same power.},
	Author = {Kuo, Tsung-Min and Mishra, Prateek},
	Booktitle = {Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture},
	Date-Added = {2014-02-09 18:39:08 +0000},
	Date-Modified = {2014-02-09 18:45:49 +0000},
	Doi = {10.1145/99370.99390},
	Isbn = {0-89791-328-0},
	Location = {Imperial College, London, United Kingdom},
	Numpages = {13},
	Pages = {260--272},
	Publisher = {ACM},
	Series = {FPCA '89},
	Title = {Strictness Analysis: A New Perspective Based on Type Inference},
	Url = {http://doi.acm.org/10.1145/99370.99390},
	Year = {1989},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/99370.99390},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/99370.99390}}

@article{Kamin92,
	Abstract = {A property P of a language is said to be definable by abstract interpretation if there is a monotonic map abs from the domain of standard semantics to an abstract domain A of finite height, and a partition of the abstract domain into parts A P and A non-P such that any value has property P if and only if abs maps it to an element of A P. Head-strictness is a property of functions over lists which asserts, roughly speaking, that whenever the function looks at some prefix of a list, it looks at every element in that prefix. We prove that head-strictness is not definable by abstract interpretation.},
	Author = {Samuel Kamin},
	Date-Added = {2014-02-09 18:38:12 +0000},
	Date-Modified = {2014-02-09 18:49:01 +0000},
	Doi = {http://dx.doi.org/10.1016/0020-0190(92)90179-Y},
	Issn = {0020-0190},
	Journal = {Information Processing Letters},
	Keywords = {abstract interpretation},
	Number = {4},
	Pages = {195 - 198},
	Title = {Head-strictness is not a monotonic abstract property},
	Url = {http://www.sciencedirect.com/science/article/pii/002001909290179Y},
	Volume = {41},
	Year = {1992},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/002001909290179Y},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/0020-0190(92)90179-Y}}

@incollection{Wright91,
	Abstract = {Results from Unification Theory and type inference with coercions are combined to produce a new method for performing strictness analysis of functional programs. A formal deduction system is developed in which extended types are derivable for terms of the Œª-calculus. These extended types contain Boolean rings describing the reduction behaviour of terms. Algorithms implementing the method are described, as well as proofs of their correctness. The method is extended to deal with recursion, polymorphism, and constants.},
	Annote = {Not clear what's better here.},
	Author = {Wright, DavidA.},
	Booktitle = {TAPSOFT '91},
	Date-Added = {2014-02-09 18:30:54 +0000},
	Date-Modified = {2014-02-09 18:47:59 +0000},
	Doi = {10.1007/3540539816_70},
	Editor = {Abramsky, S. and Maibaum, T.S.E.},
	Isbn = {978-3-540-53981-0},
	Keywords = {type inference, polymorphic, higher-order},
	Pages = {235-258},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {A new technique for strictness analysis},
	Url = {http://dx.doi.org/10.1007/3540539816_70},
	Volume = {494},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3540539816_70}}

@incollection{Jensen91,
	Abstract = {This paper presents a framework for comparing two strictness analysis techniques: Abstract interpretation and non-standard type inference. The comparison is based on the representation of a lattice by its ideals. A formal system for deducing inclusions between ideals of a lattice is presented and proved sound and complete. Viewing the ideals as strictness properties we use the formal system to define a program logic for deducing strictness properties of expressions in a typed lambda calculus. This strictness logic is shown to be sound and complete with respect to the abstract interpretation, which establishes the main result that strictness analysis by type-inference and by abstract interpretation are equally powerful techniques.},
	Annote = {Key result:
"strictness analysis by type-inference and by abstract interpretation are equally powerful techniques".

Practical implication: one shouldn't learn strictness analysis by type-inference to solve problems which abstract interpretation can't solve --- as long as one applies techniques existing by the time of this paper.

But this does not cite Wright91, but Kuo89 (Kuo and Mishra 1989, FPCA); Wright91 cites Kuo89, so it must be a (slight) extension.},
	Author = {Jensen, ThomasP.},
	Booktitle = {Functional Programming Languages and Computer Architecture},
	Date-Added = {2014-02-09 18:27:10 +0000},
	Date-Modified = {2014-02-09 18:45:31 +0000},
	Doi = {10.1007/3540543961_17},
	Editor = {Hughes, John},
	Isbn = {978-3-540-54396-1},
	Pages = {352-366},
	Publisher = {Springer Berlin Heidelberg},
	Series = {Lecture Notes in Computer Science},
	Title = {Strictness analysis in logical form},
	Url = {http://dx.doi.org/10.1007/3540543961_17},
	Volume = {523},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/3540543961_17}}
